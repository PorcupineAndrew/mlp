======================================================
learning_rate: 0.001
weight_decay: 0.01
momentum: 0.01
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Sigm
loss: Euclidean
result dir: ./result/exp_2
======================================================
10:21:50.736 Training @ 0 epoch...
10:21:51.265   Training iter 50, batch loss 1.3117, batch acc 0.1028
10:21:51.780   Training iter 100, batch loss 1.3030, batch acc 0.1022
10:21:52.299   Training iter 150, batch loss 1.2973, batch acc 0.0952
10:21:52.832   Training iter 200, batch loss 1.2902, batch acc 0.0986
10:21:53.365   Training iter 250, batch loss 1.2828, batch acc 0.0998
10:21:53.866   Training iter 300, batch loss 1.2761, batch acc 0.1028
10:21:54.372   Training iter 350, batch loss 1.2689, batch acc 0.1100
10:21:54.887   Training iter 400, batch loss 1.2627, batch acc 0.1012
10:21:55.410   Training iter 450, batch loss 1.2560, batch acc 0.1022
10:21:55.905   Training iter 500, batch loss 1.2485, batch acc 0.1052
10:21:56.406   Training iter 550, batch loss 1.2423, batch acc 0.1122
10:21:56.915   Training iter 600, batch loss 1.2363, batch acc 0.1034
10:21:56.917 Testing @ 0 epoch...
10:21:56.955     Testing, total mean loss 1.23313, total acc 0.09820
10:21:56.955 Plot @ 0 epoch...
10:21:56.955 Training @ 1 epoch...
10:21:57.481   Training iter 50, batch loss 1.2294, batch acc 0.1070
10:21:58.035   Training iter 100, batch loss 1.2229, batch acc 0.1064
10:21:58.560   Training iter 150, batch loss 1.2162, batch acc 0.1080
10:21:59.068   Training iter 200, batch loss 1.2104, batch acc 0.0984
10:21:59.563   Training iter 250, batch loss 1.2054, batch acc 0.1016
10:22:00.081   Training iter 300, batch loss 1.1983, batch acc 0.1108
10:22:00.570   Training iter 350, batch loss 1.1917, batch acc 0.1062
10:22:01.088   Training iter 400, batch loss 1.1868, batch acc 0.1058
10:22:01.638   Training iter 450, batch loss 1.1807, batch acc 0.1018
10:22:02.160   Training iter 500, batch loss 1.1747, batch acc 0.1090
10:22:02.690   Training iter 550, batch loss 1.1687, batch acc 0.1046
10:22:03.224   Training iter 600, batch loss 1.1630, batch acc 0.1048
10:22:03.226 Training @ 2 epoch...
10:22:03.772   Training iter 50, batch loss 1.1565, batch acc 0.1088
10:22:04.314   Training iter 100, batch loss 1.1507, batch acc 0.1060
10:22:04.848   Training iter 150, batch loss 1.1454, batch acc 0.1098
10:22:05.380   Training iter 200, batch loss 1.1398, batch acc 0.1032
10:22:05.915   Training iter 250, batch loss 1.1350, batch acc 0.1042
10:22:06.447   Training iter 300, batch loss 1.1287, batch acc 0.1114
10:22:06.992   Training iter 350, batch loss 1.1223, batch acc 0.1218
10:22:07.566   Training iter 400, batch loss 1.1173, batch acc 0.1098
10:22:08.124   Training iter 450, batch loss 1.1128, batch acc 0.1008
10:22:08.667   Training iter 500, batch loss 1.1072, batch acc 0.1076
10:22:09.173   Training iter 550, batch loss 1.1031, batch acc 0.1036
10:22:09.669   Training iter 600, batch loss 1.0963, batch acc 0.1070
10:22:09.671 Training @ 3 epoch...
10:22:10.188   Training iter 50, batch loss 1.0917, batch acc 0.1054
10:22:10.701   Training iter 100, batch loss 1.0863, batch acc 0.1134
10:22:11.217   Training iter 150, batch loss 1.0806, batch acc 0.1118
10:22:11.751   Training iter 200, batch loss 1.0762, batch acc 0.1080
10:22:12.252   Training iter 250, batch loss 1.0709, batch acc 0.1124
10:22:12.785   Training iter 300, batch loss 1.0667, batch acc 0.1098
10:22:13.290   Training iter 350, batch loss 1.0613, batch acc 0.1132
10:22:13.789   Training iter 400, batch loss 1.0571, batch acc 0.1048
10:22:14.319   Training iter 450, batch loss 1.0524, batch acc 0.1050
10:22:14.815   Training iter 500, batch loss 1.0464, batch acc 0.1204
10:22:15.320   Training iter 550, batch loss 1.0421, batch acc 0.1096
10:22:15.816   Training iter 600, batch loss 1.0381, batch acc 0.1076
10:22:15.818 Training @ 4 epoch...
10:22:16.329   Training iter 50, batch loss 1.0334, batch acc 0.1114
10:22:16.790   Training iter 100, batch loss 1.0277, batch acc 0.1118
10:22:17.252   Training iter 150, batch loss 1.0239, batch acc 0.1124
10:22:17.717   Training iter 200, batch loss 1.0192, batch acc 0.1152
10:22:18.185   Training iter 250, batch loss 1.0151, batch acc 0.1156
10:22:18.649   Training iter 300, batch loss 1.0119, batch acc 0.1084
10:22:19.122   Training iter 350, batch loss 1.0066, batch acc 0.1098
10:22:19.591   Training iter 400, batch loss 1.0021, batch acc 0.1144
10:22:20.076   Training iter 450, batch loss 0.9981, batch acc 0.1090
10:22:20.556   Training iter 500, batch loss 0.9941, batch acc 0.1142
10:22:21.040   Training iter 550, batch loss 0.9891, batch acc 0.1084
10:22:21.502   Training iter 600, batch loss 0.9840, batch acc 0.1178
10:22:21.504 Training @ 5 epoch...
10:22:21.976   Training iter 50, batch loss 0.9811, batch acc 0.1124
10:22:22.431   Training iter 100, batch loss 0.9775, batch acc 0.1110
10:22:22.893   Training iter 150, batch loss 0.9732, batch acc 0.1118
10:22:23.375   Training iter 200, batch loss 0.9688, batch acc 0.1150
10:22:23.858   Training iter 250, batch loss 0.9643, batch acc 0.1244
10:22:24.351   Training iter 300, batch loss 0.9617, batch acc 0.1102
10:22:24.857   Training iter 350, batch loss 0.9570, batch acc 0.1098
10:22:25.368   Training iter 400, batch loss 0.9534, batch acc 0.1174
10:22:25.858   Training iter 450, batch loss 0.9498, batch acc 0.1100
10:22:26.381   Training iter 500, batch loss 0.9461, batch acc 0.1190
10:22:26.927   Training iter 550, batch loss 0.9424, batch acc 0.1178
10:22:27.470   Training iter 600, batch loss 0.9386, batch acc 0.1254
10:22:27.472 Testing @ 5 epoch...
10:22:27.509     Testing, total mean loss 0.93691, total acc 0.11110
10:22:27.509 Training @ 6 epoch...
10:22:28.062   Training iter 50, batch loss 0.9349, batch acc 0.1136
10:22:28.606   Training iter 100, batch loss 0.9320, batch acc 0.1150
10:22:29.164   Training iter 150, batch loss 0.9278, batch acc 0.1190
10:22:29.700   Training iter 200, batch loss 0.9241, batch acc 0.1222
10:22:30.248   Training iter 250, batch loss 0.9203, batch acc 0.1194
10:22:30.795   Training iter 300, batch loss 0.9173, batch acc 0.1154
10:22:31.354   Training iter 350, batch loss 0.9135, batch acc 0.1202
10:22:31.912   Training iter 400, batch loss 0.9107, batch acc 0.1158
10:22:32.456   Training iter 450, batch loss 0.9066, batch acc 0.1190
10:22:32.984   Training iter 500, batch loss 0.9038, batch acc 0.1240
10:22:33.495   Training iter 550, batch loss 0.9006, batch acc 0.1210
10:22:34.004   Training iter 600, batch loss 0.8961, batch acc 0.1206
10:22:34.006 Training @ 7 epoch...
10:22:34.530   Training iter 50, batch loss 0.8934, batch acc 0.1290
10:22:35.058   Training iter 100, batch loss 0.8901, batch acc 0.1244
10:22:35.586   Training iter 150, batch loss 0.8884, batch acc 0.1178
10:22:36.113   Training iter 200, batch loss 0.8839, batch acc 0.1246
10:22:36.638   Training iter 250, batch loss 0.8814, batch acc 0.1214
10:22:37.156   Training iter 300, batch loss 0.8774, batch acc 0.1220
10:22:37.683   Training iter 350, batch loss 0.8755, batch acc 0.1186
10:22:38.200   Training iter 400, batch loss 0.8720, batch acc 0.1180
10:22:38.724   Training iter 450, batch loss 0.8694, batch acc 0.1248
10:22:39.251   Training iter 500, batch loss 0.8656, batch acc 0.1224
10:22:39.775   Training iter 550, batch loss 0.8630, batch acc 0.1220
10:22:40.309   Training iter 600, batch loss 0.8592, batch acc 0.1282
10:22:40.311 Training @ 8 epoch...
10:22:40.833   Training iter 50, batch loss 0.8569, batch acc 0.1310
10:22:41.357   Training iter 100, batch loss 0.8543, batch acc 0.1304
10:22:41.873   Training iter 150, batch loss 0.8512, batch acc 0.1192
10:22:42.414   Training iter 200, batch loss 0.8490, batch acc 0.1198
10:22:42.958   Training iter 250, batch loss 0.8455, batch acc 0.1272
10:22:43.497   Training iter 300, batch loss 0.8433, batch acc 0.1280
10:22:44.034   Training iter 350, batch loss 0.8405, batch acc 0.1338
10:22:44.577   Training iter 400, batch loss 0.8382, batch acc 0.1244
10:22:45.120   Training iter 450, batch loss 0.8352, batch acc 0.1240
10:22:45.649   Training iter 500, batch loss 0.8332, batch acc 0.1272
10:22:46.181   Training iter 550, batch loss 0.8288, batch acc 0.1352
10:22:46.686   Training iter 600, batch loss 0.8267, batch acc 0.1288
10:22:46.688 Training @ 9 epoch...
10:22:47.173   Training iter 50, batch loss 0.8248, batch acc 0.1310
10:22:47.649   Training iter 100, batch loss 0.8222, batch acc 0.1272
10:22:48.137   Training iter 150, batch loss 0.8186, batch acc 0.1402
10:22:48.608   Training iter 200, batch loss 0.8176, batch acc 0.1280
10:22:49.069   Training iter 250, batch loss 0.8149, batch acc 0.1294
10:22:49.553   Training iter 300, batch loss 0.8114, batch acc 0.1444
10:22:50.054   Training iter 350, batch loss 0.8091, batch acc 0.1302
10:22:50.569   Training iter 400, batch loss 0.8073, batch acc 0.1294
10:22:51.101   Training iter 450, batch loss 0.8044, batch acc 0.1438
10:22:51.615   Training iter 500, batch loss 0.8026, batch acc 0.1358
10:22:52.124   Training iter 550, batch loss 0.8007, batch acc 0.1296
10:22:52.611   Training iter 600, batch loss 0.7979, batch acc 0.1306
10:22:52.613 Training @ 10 epoch...
10:22:53.128   Training iter 50, batch loss 0.7954, batch acc 0.1320
10:22:53.640   Training iter 100, batch loss 0.7932, batch acc 0.1396
10:22:54.154   Training iter 150, batch loss 0.7909, batch acc 0.1360
10:22:54.647   Training iter 200, batch loss 0.7888, batch acc 0.1372
10:22:55.134   Training iter 250, batch loss 0.7862, batch acc 0.1484
10:22:55.604   Training iter 300, batch loss 0.7847, batch acc 0.1340
10:22:56.075   Training iter 350, batch loss 0.7826, batch acc 0.1336
10:22:56.545   Training iter 400, batch loss 0.7790, batch acc 0.1436
10:22:57.030   Training iter 450, batch loss 0.7783, batch acc 0.1424
10:22:57.525   Training iter 500, batch loss 0.7763, batch acc 0.1402
10:22:57.987   Training iter 550, batch loss 0.7736, batch acc 0.1462
10:22:58.483   Training iter 600, batch loss 0.7708, batch acc 0.1600
10:22:58.484 Testing @ 10 epoch...
10:22:58.521     Testing, total mean loss 0.77072, total acc 0.14090
10:22:58.521 Training @ 11 epoch...
10:22:59.027   Training iter 50, batch loss 0.7701, batch acc 0.1380
10:22:59.533   Training iter 100, batch loss 0.7676, batch acc 0.1466
10:23:00.045   Training iter 150, batch loss 0.7651, batch acc 0.1512
10:23:00.565   Training iter 200, batch loss 0.7631, batch acc 0.1520
10:23:01.093   Training iter 250, batch loss 0.7614, batch acc 0.1494
10:23:01.648   Training iter 300, batch loss 0.7599, batch acc 0.1396
10:23:02.206   Training iter 350, batch loss 0.7578, batch acc 0.1470
10:23:02.786   Training iter 400, batch loss 0.7556, batch acc 0.1418
10:23:03.346   Training iter 450, batch loss 0.7532, batch acc 0.1496
10:23:03.862   Training iter 500, batch loss 0.7520, batch acc 0.1546
10:23:04.373   Training iter 550, batch loss 0.7498, batch acc 0.1628
10:23:04.886   Training iter 600, batch loss 0.7480, batch acc 0.1566
10:23:04.888 Training @ 12 epoch...
10:23:05.418   Training iter 50, batch loss 0.7471, batch acc 0.1482
10:23:05.946   Training iter 100, batch loss 0.7455, batch acc 0.1488
10:23:06.463   Training iter 150, batch loss 0.7432, batch acc 0.1610
10:23:06.977   Training iter 200, batch loss 0.7407, batch acc 0.1524
10:23:07.499   Training iter 250, batch loss 0.7386, batch acc 0.1578
10:23:08.032   Training iter 300, batch loss 0.7374, batch acc 0.1592
10:23:08.561   Training iter 350, batch loss 0.7351, batch acc 0.1570
10:23:09.081   Training iter 400, batch loss 0.7335, batch acc 0.1598
10:23:09.608   Training iter 450, batch loss 0.7315, batch acc 0.1656
10:23:10.142   Training iter 500, batch loss 0.7297, batch acc 0.1610
10:23:10.670   Training iter 550, batch loss 0.7286, batch acc 0.1606
10:23:11.209   Training iter 600, batch loss 0.7269, batch acc 0.1582
10:23:11.211 Training @ 13 epoch...
10:23:11.728   Training iter 50, batch loss 0.7258, batch acc 0.1594
10:23:12.246   Training iter 100, batch loss 0.7239, batch acc 0.1634
10:23:12.772   Training iter 150, batch loss 0.7218, batch acc 0.1654
10:23:13.293   Training iter 200, batch loss 0.7200, batch acc 0.1674
10:23:13.813   Training iter 250, batch loss 0.7186, batch acc 0.1710
10:23:14.329   Training iter 300, batch loss 0.7174, batch acc 0.1658
10:23:14.868   Training iter 350, batch loss 0.7166, batch acc 0.1662
10:23:15.419   Training iter 400, batch loss 0.7150, batch acc 0.1636
10:23:15.965   Training iter 450, batch loss 0.7125, batch acc 0.1676
10:23:16.516   Training iter 500, batch loss 0.7112, batch acc 0.1650
10:23:17.058   Training iter 550, batch loss 0.7090, batch acc 0.1774
10:23:17.601   Training iter 600, batch loss 0.7076, batch acc 0.1658
10:23:17.603 Training @ 14 epoch...
10:23:18.142   Training iter 50, batch loss 0.7062, batch acc 0.1734
10:23:18.687   Training iter 100, batch loss 0.7052, batch acc 0.1724
10:23:19.220   Training iter 150, batch loss 0.7036, batch acc 0.1780
10:23:19.754   Training iter 200, batch loss 0.7023, batch acc 0.1728
10:23:20.290   Training iter 250, batch loss 0.7003, batch acc 0.1782
10:23:20.799   Training iter 300, batch loss 0.6994, batch acc 0.1702
10:23:21.305   Training iter 350, batch loss 0.6977, batch acc 0.1746
10:23:21.808   Training iter 400, batch loss 0.6977, batch acc 0.1758
10:23:22.320   Training iter 450, batch loss 0.6952, batch acc 0.1830
10:23:22.836   Training iter 500, batch loss 0.6936, batch acc 0.1688
10:23:23.363   Training iter 550, batch loss 0.6921, batch acc 0.1830
10:23:23.874   Training iter 600, batch loss 0.6912, batch acc 0.1822
10:23:23.876 Training @ 15 epoch...
10:23:24.411   Training iter 50, batch loss 0.6889, batch acc 0.1842
10:23:24.936   Training iter 100, batch loss 0.6881, batch acc 0.1758
10:23:25.469   Training iter 150, batch loss 0.6872, batch acc 0.1792
10:23:25.998   Training iter 200, batch loss 0.6857, batch acc 0.1838
10:23:26.534   Training iter 250, batch loss 0.6846, batch acc 0.1766
10:23:27.096   Training iter 300, batch loss 0.6832, batch acc 0.1828
10:23:27.651   Training iter 350, batch loss 0.6821, batch acc 0.1870
10:23:28.230   Training iter 400, batch loss 0.6805, batch acc 0.1884
10:23:28.779   Training iter 450, batch loss 0.6788, batch acc 0.1926
10:23:29.376   Training iter 500, batch loss 0.6779, batch acc 0.1920
10:23:29.949   Training iter 550, batch loss 0.6776, batch acc 0.1824
10:23:30.533   Training iter 600, batch loss 0.6758, batch acc 0.1938
10:23:30.535 Testing @ 15 epoch...
10:23:30.571     Testing, total mean loss 0.67515, total acc 0.18710
10:23:30.571 Training @ 16 epoch...
10:23:31.142   Training iter 50, batch loss 0.6737, batch acc 0.1988
10:23:31.715   Training iter 100, batch loss 0.6727, batch acc 0.1876
10:23:32.290   Training iter 150, batch loss 0.6729, batch acc 0.1816
10:23:32.851   Training iter 200, batch loss 0.6710, batch acc 0.1910
10:23:33.413   Training iter 250, batch loss 0.6696, batch acc 0.1860
10:23:33.970   Training iter 300, batch loss 0.6683, batch acc 0.1942
10:23:34.533   Training iter 350, batch loss 0.6680, batch acc 0.1906
10:23:35.111   Training iter 400, batch loss 0.6659, batch acc 0.1996
10:23:35.678   Training iter 450, batch loss 0.6651, batch acc 0.1942
10:23:36.228   Training iter 500, batch loss 0.6640, batch acc 0.1982
10:23:36.741   Training iter 550, batch loss 0.6623, batch acc 0.2050
10:23:37.229   Training iter 600, batch loss 0.6616, batch acc 0.2042
10:23:37.231 Training @ 17 epoch...
10:23:37.693   Training iter 50, batch loss 0.6605, batch acc 0.1978
10:23:38.156   Training iter 100, batch loss 0.6589, batch acc 0.1946
10:23:38.629   Training iter 150, batch loss 0.6578, batch acc 0.2018
10:23:39.112   Training iter 200, batch loss 0.6571, batch acc 0.2042
10:23:39.575   Training iter 250, batch loss 0.6560, batch acc 0.2052
10:23:40.035   Training iter 300, batch loss 0.6556, batch acc 0.2000
10:23:40.517   Training iter 350, batch loss 0.6546, batch acc 0.1996
10:23:41.036   Training iter 400, batch loss 0.6530, batch acc 0.2076
10:23:41.545   Training iter 450, batch loss 0.6524, batch acc 0.2072
10:23:42.044   Training iter 500, batch loss 0.6509, batch acc 0.2092
10:23:42.516   Training iter 550, batch loss 0.6502, batch acc 0.1986
10:23:42.985   Training iter 600, batch loss 0.6492, batch acc 0.2058
10:23:42.987 Training @ 18 epoch...
10:23:43.456   Training iter 50, batch loss 0.6481, batch acc 0.2066
10:23:43.920   Training iter 100, batch loss 0.6474, batch acc 0.2086
10:23:44.391   Training iter 150, batch loss 0.6456, batch acc 0.2088
10:23:44.853   Training iter 200, batch loss 0.6456, batch acc 0.2094
10:23:45.308   Training iter 250, batch loss 0.6438, batch acc 0.2016
10:23:45.759   Training iter 300, batch loss 0.6434, batch acc 0.2108
10:23:46.223   Training iter 350, batch loss 0.6423, batch acc 0.2042
10:23:46.689   Training iter 400, batch loss 0.6404, batch acc 0.2216
10:23:47.151   Training iter 450, batch loss 0.6402, batch acc 0.2152
10:23:47.616   Training iter 500, batch loss 0.6390, batch acc 0.2194
10:23:48.079   Training iter 550, batch loss 0.6382, batch acc 0.2126
10:23:48.539   Training iter 600, batch loss 0.6377, batch acc 0.2158
10:23:48.540 Training @ 19 epoch...
10:23:49.013   Training iter 50, batch loss 0.6366, batch acc 0.2124
10:23:49.483   Training iter 100, batch loss 0.6359, batch acc 0.2174
10:23:49.947   Training iter 150, batch loss 0.6339, batch acc 0.2206
10:23:50.410   Training iter 200, batch loss 0.6346, batch acc 0.2098
10:23:50.876   Training iter 250, batch loss 0.6325, batch acc 0.2252
10:23:51.358   Training iter 300, batch loss 0.6328, batch acc 0.2192
10:23:51.872   Training iter 350, batch loss 0.6319, batch acc 0.2116
10:23:52.368   Training iter 400, batch loss 0.6298, batch acc 0.2140
10:23:52.870   Training iter 450, batch loss 0.6295, batch acc 0.2166
10:23:53.366   Training iter 500, batch loss 0.6283, batch acc 0.2336
10:23:53.820   Training iter 550, batch loss 0.6273, batch acc 0.2292
10:23:54.284   Training iter 600, batch loss 0.6269, batch acc 0.2224
10:23:54.286 Training @ 20 epoch...
10:23:54.765   Training iter 50, batch loss 0.6262, batch acc 0.2214
10:23:55.259   Training iter 100, batch loss 0.6256, batch acc 0.2290
10:23:55.751   Training iter 150, batch loss 0.6247, batch acc 0.2216
10:23:56.246   Training iter 200, batch loss 0.6235, batch acc 0.2256
10:23:56.714   Training iter 250, batch loss 0.6231, batch acc 0.2238
10:23:57.172   Training iter 300, batch loss 0.6217, batch acc 0.2266
10:23:57.614   Training iter 350, batch loss 0.6218, batch acc 0.2222
10:23:58.068   Training iter 400, batch loss 0.6201, batch acc 0.2324
10:23:58.564   Training iter 450, batch loss 0.6198, batch acc 0.2154
10:23:59.076   Training iter 500, batch loss 0.6179, batch acc 0.2356
10:23:59.589   Training iter 550, batch loss 0.6181, batch acc 0.2320
10:24:00.092   Training iter 600, batch loss 0.6174, batch acc 0.2230
10:24:00.094 Testing @ 20 epoch...
10:24:00.130     Testing, total mean loss 0.61703, total acc 0.22830
10:24:00.130 Training @ 21 epoch...
10:24:00.650   Training iter 50, batch loss 0.6165, batch acc 0.2340
10:24:01.168   Training iter 100, batch loss 0.6155, batch acc 0.2198
10:24:01.649   Training iter 150, batch loss 0.6151, batch acc 0.2274
10:24:02.183   Training iter 200, batch loss 0.6148, batch acc 0.2292
10:24:02.749   Training iter 250, batch loss 0.6137, batch acc 0.2332
10:24:03.310   Training iter 300, batch loss 0.6123, batch acc 0.2398
10:24:03.866   Training iter 350, batch loss 0.6122, batch acc 0.2198
10:24:04.420   Training iter 400, batch loss 0.6111, batch acc 0.2358
10:24:04.976   Training iter 450, batch loss 0.6101, batch acc 0.2462
10:24:05.538   Training iter 500, batch loss 0.6104, batch acc 0.2350
10:24:06.077   Training iter 550, batch loss 0.6091, batch acc 0.2370
10:24:06.596   Training iter 600, batch loss 0.6089, batch acc 0.2256
10:24:06.598 Training @ 22 epoch...
10:24:07.158   Training iter 50, batch loss 0.6081, batch acc 0.2396
10:24:07.723   Training iter 100, batch loss 0.6070, batch acc 0.2406
10:24:08.262   Training iter 150, batch loss 0.6063, batch acc 0.2360
10:24:08.749   Training iter 200, batch loss 0.6061, batch acc 0.2308
10:24:09.237   Training iter 250, batch loss 0.6049, batch acc 0.2416
10:24:09.721   Training iter 300, batch loss 0.6042, batch acc 0.2408
10:24:10.206   Training iter 350, batch loss 0.6034, batch acc 0.2350
10:24:10.698   Training iter 400, batch loss 0.6027, batch acc 0.2314
10:24:11.186   Training iter 450, batch loss 0.6022, batch acc 0.2386
10:24:11.683   Training iter 500, batch loss 0.6021, batch acc 0.2306
10:24:12.194   Training iter 550, batch loss 0.6013, batch acc 0.2384
10:24:12.696   Training iter 600, batch loss 0.6004, batch acc 0.2460
10:24:12.697 Training @ 23 epoch...
10:24:13.211   Training iter 50, batch loss 0.5996, batch acc 0.2470
10:24:13.679   Training iter 100, batch loss 0.5994, batch acc 0.2364
10:24:14.166   Training iter 150, batch loss 0.5982, batch acc 0.2360
10:24:14.665   Training iter 200, batch loss 0.5986, batch acc 0.2428
10:24:15.149   Training iter 250, batch loss 0.5976, batch acc 0.2308
10:24:15.657   Training iter 300, batch loss 0.5966, batch acc 0.2496
10:24:16.152   Training iter 350, batch loss 0.5952, batch acc 0.2428
10:24:16.624   Training iter 400, batch loss 0.5953, batch acc 0.2510
10:24:17.101   Training iter 450, batch loss 0.5949, batch acc 0.2412
10:24:17.574   Training iter 500, batch loss 0.5944, batch acc 0.2486
10:24:18.054   Training iter 550, batch loss 0.5929, batch acc 0.2506
10:24:18.571   Training iter 600, batch loss 0.5931, batch acc 0.2420
10:24:18.572 Training @ 24 epoch...
10:24:19.114   Training iter 50, batch loss 0.5921, batch acc 0.2480
10:24:19.639   Training iter 100, batch loss 0.5916, batch acc 0.2548
10:24:20.175   Training iter 150, batch loss 0.5910, batch acc 0.2500
10:24:20.702   Training iter 200, batch loss 0.5910, batch acc 0.2502
10:24:21.233   Training iter 250, batch loss 0.5904, batch acc 0.2464
10:24:21.757   Training iter 300, batch loss 0.5894, batch acc 0.2550
10:24:22.295   Training iter 350, batch loss 0.5890, batch acc 0.2498
10:24:22.845   Training iter 400, batch loss 0.5878, batch acc 0.2492
10:24:23.392   Training iter 450, batch loss 0.5881, batch acc 0.2430
10:24:23.920   Training iter 500, batch loss 0.5872, batch acc 0.2508
10:24:24.421   Training iter 550, batch loss 0.5867, batch acc 0.2420
10:24:24.913   Training iter 600, batch loss 0.5862, batch acc 0.2436
10:24:24.915 Training @ 25 epoch...
10:24:25.425   Training iter 50, batch loss 0.5856, batch acc 0.2468
10:24:25.896   Training iter 100, batch loss 0.5849, batch acc 0.2498
10:24:26.380   Training iter 150, batch loss 0.5844, batch acc 0.2508
10:24:26.849   Training iter 200, batch loss 0.5843, batch acc 0.2454
10:24:27.347   Training iter 250, batch loss 0.5838, batch acc 0.2514
10:24:27.858   Training iter 300, batch loss 0.5825, batch acc 0.2546
10:24:28.356   Training iter 350, batch loss 0.5818, batch acc 0.2676
10:24:28.843   Training iter 400, batch loss 0.5818, batch acc 0.2598
10:24:29.324   Training iter 450, batch loss 0.5810, batch acc 0.2570
10:24:29.825   Training iter 500, batch loss 0.5809, batch acc 0.2536
10:24:30.348   Training iter 550, batch loss 0.5808, batch acc 0.2546
10:24:30.869   Training iter 600, batch loss 0.5799, batch acc 0.2546
10:24:30.871 Testing @ 25 epoch...
10:24:30.908     Testing, total mean loss 0.57960, total acc 0.25890
10:24:30.908 Training @ 26 epoch...
10:24:31.420   Training iter 50, batch loss 0.5800, batch acc 0.2356
10:24:31.942   Training iter 100, batch loss 0.5789, batch acc 0.2560
10:24:32.469   Training iter 150, batch loss 0.5785, batch acc 0.2560
10:24:32.992   Training iter 200, batch loss 0.5776, batch acc 0.2704
10:24:33.517   Training iter 250, batch loss 0.5769, batch acc 0.2638
10:24:34.028   Training iter 300, batch loss 0.5767, batch acc 0.2706
10:24:34.584   Training iter 350, batch loss 0.5762, batch acc 0.2534
10:24:35.136   Training iter 400, batch loss 0.5761, batch acc 0.2502
10:24:35.682   Training iter 450, batch loss 0.5748, batch acc 0.2684
10:24:36.228   Training iter 500, batch loss 0.5752, batch acc 0.2506
10:24:36.779   Training iter 550, batch loss 0.5739, batch acc 0.2656
10:24:37.336   Training iter 600, batch loss 0.5741, batch acc 0.2622
10:24:37.338 Training @ 27 epoch...
10:24:37.898   Training iter 50, batch loss 0.5736, batch acc 0.2582
10:24:38.456   Training iter 100, batch loss 0.5729, batch acc 0.2606
10:24:39.004   Training iter 150, batch loss 0.5729, batch acc 0.2680
10:24:39.538   Training iter 200, batch loss 0.5723, batch acc 0.2582
10:24:40.070   Training iter 250, batch loss 0.5717, batch acc 0.2638
10:24:40.588   Training iter 300, batch loss 0.5713, batch acc 0.2612
10:24:41.075   Training iter 350, batch loss 0.5709, batch acc 0.2490
10:24:41.564   Training iter 400, batch loss 0.5700, batch acc 0.2702
10:24:42.056   Training iter 450, batch loss 0.5694, batch acc 0.2658
10:24:42.557   Training iter 500, batch loss 0.5694, batch acc 0.2564
10:24:43.064   Training iter 550, batch loss 0.5685, batch acc 0.2762
10:24:43.553   Training iter 600, batch loss 0.5685, batch acc 0.2716
10:24:43.554 Training @ 28 epoch...
10:24:44.059   Training iter 50, batch loss 0.5676, batch acc 0.2674
10:24:44.582   Training iter 100, batch loss 0.5676, batch acc 0.2624
10:24:45.108   Training iter 150, batch loss 0.5672, batch acc 0.2510
10:24:45.641   Training iter 200, batch loss 0.5666, batch acc 0.2656
10:24:46.174   Training iter 250, batch loss 0.5660, batch acc 0.2756
10:24:46.691   Training iter 300, batch loss 0.5664, batch acc 0.2702
10:24:47.177   Training iter 350, batch loss 0.5662, batch acc 0.2640
10:24:47.667   Training iter 400, batch loss 0.5645, batch acc 0.2756
10:24:48.170   Training iter 450, batch loss 0.5646, batch acc 0.2632
10:24:48.677   Training iter 500, batch loss 0.5649, batch acc 0.2674
10:24:49.177   Training iter 550, batch loss 0.5638, batch acc 0.2800
10:24:49.680   Training iter 600, batch loss 0.5634, batch acc 0.2732
10:24:49.682 Training @ 29 epoch...
10:24:50.194   Training iter 50, batch loss 0.5634, batch acc 0.2612
10:24:50.726   Training iter 100, batch loss 0.5627, batch acc 0.2686
10:24:51.288   Training iter 150, batch loss 0.5621, batch acc 0.2778
10:24:51.842   Training iter 200, batch loss 0.5613, batch acc 0.2762
10:24:52.399   Training iter 250, batch loss 0.5608, batch acc 0.2794
10:24:52.956   Training iter 300, batch loss 0.5612, batch acc 0.2758
10:24:53.514   Training iter 350, batch loss 0.5613, batch acc 0.2572
10:24:54.060   Training iter 400, batch loss 0.5601, batch acc 0.2728
10:24:54.614   Training iter 450, batch loss 0.5602, batch acc 0.2650
10:24:55.165   Training iter 500, batch loss 0.5590, batch acc 0.2862
10:24:55.717   Training iter 550, batch loss 0.5594, batch acc 0.2722
10:24:56.274   Training iter 600, batch loss 0.5590, batch acc 0.2718
10:24:56.276 Training @ 30 epoch...
10:24:56.838   Training iter 50, batch loss 0.5580, batch acc 0.2820
10:24:57.397   Training iter 100, batch loss 0.5577, batch acc 0.2732
10:24:57.941   Training iter 150, batch loss 0.5577, batch acc 0.2648
10:24:58.481   Training iter 200, batch loss 0.5578, batch acc 0.2708
10:24:59.016   Training iter 250, batch loss 0.5570, batch acc 0.2724
10:24:59.545   Training iter 300, batch loss 0.5564, batch acc 0.2752
10:25:00.070   Training iter 350, batch loss 0.5567, batch acc 0.2804
10:25:00.608   Training iter 400, batch loss 0.5559, batch acc 0.2788
10:25:01.155   Training iter 450, batch loss 0.5559, batch acc 0.2724
10:25:01.728   Training iter 500, batch loss 0.5548, batch acc 0.2776
10:25:02.273   Training iter 550, batch loss 0.5542, batch acc 0.2822
10:25:02.819   Training iter 600, batch loss 0.5543, batch acc 0.2814
10:25:02.821 Testing @ 30 epoch...
10:25:02.858     Testing, total mean loss 0.55425, total acc 0.28090
10:25:02.858 Training @ 31 epoch...
10:25:03.423   Training iter 50, batch loss 0.5541, batch acc 0.2642
10:25:03.987   Training iter 100, batch loss 0.5539, batch acc 0.2806
10:25:04.540   Training iter 150, batch loss 0.5539, batch acc 0.2754
10:25:05.102   Training iter 200, batch loss 0.5529, batch acc 0.2834
10:25:05.629   Training iter 250, batch loss 0.5524, batch acc 0.2810
10:25:06.140   Training iter 300, batch loss 0.5523, batch acc 0.2780
10:25:06.642   Training iter 350, batch loss 0.5516, batch acc 0.2838
10:25:07.201   Training iter 400, batch loss 0.5520, batch acc 0.2810
10:25:07.754   Training iter 450, batch loss 0.5510, batch acc 0.2856
10:25:08.280   Training iter 500, batch loss 0.5505, batch acc 0.2942
10:25:08.811   Training iter 550, batch loss 0.5509, batch acc 0.2780
10:25:09.344   Training iter 600, batch loss 0.5505, batch acc 0.2764
10:25:09.346 Training @ 32 epoch...
10:25:09.888   Training iter 50, batch loss 0.5498, batch acc 0.2854
10:25:10.411   Training iter 100, batch loss 0.5498, batch acc 0.2736
10:25:10.915   Training iter 150, batch loss 0.5494, batch acc 0.2860
10:25:11.417   Training iter 200, batch loss 0.5489, batch acc 0.2806
10:25:11.932   Training iter 250, batch loss 0.5480, batch acc 0.2892
10:25:12.441   Training iter 300, batch loss 0.5477, batch acc 0.3058
10:25:12.938   Training iter 350, batch loss 0.5484, batch acc 0.2802
10:25:13.428   Training iter 400, batch loss 0.5478, batch acc 0.2772
10:25:13.919   Training iter 450, batch loss 0.5475, batch acc 0.2802
10:25:14.407   Training iter 500, batch loss 0.5471, batch acc 0.2868
10:25:14.907   Training iter 550, batch loss 0.5470, batch acc 0.2872
10:25:15.434   Training iter 600, batch loss 0.5474, batch acc 0.2766
10:25:15.436 Training @ 33 epoch...
10:25:15.980   Training iter 50, batch loss 0.5462, batch acc 0.2812
10:25:16.520   Training iter 100, batch loss 0.5458, batch acc 0.2920
10:25:17.050   Training iter 150, batch loss 0.5455, batch acc 0.2760
10:25:17.583   Training iter 200, batch loss 0.5453, batch acc 0.2946
10:25:18.130   Training iter 250, batch loss 0.5452, batch acc 0.2894
10:25:18.665   Training iter 300, batch loss 0.5452, batch acc 0.2870
10:25:19.199   Training iter 350, batch loss 0.5440, batch acc 0.2956
10:25:19.739   Training iter 400, batch loss 0.5434, batch acc 0.2972
10:25:20.285   Training iter 450, batch loss 0.5440, batch acc 0.2852
10:25:20.824   Training iter 500, batch loss 0.5437, batch acc 0.2780
10:25:21.349   Training iter 550, batch loss 0.5433, batch acc 0.2812
10:25:21.851   Training iter 600, batch loss 0.5430, batch acc 0.2962
10:25:21.853 Training @ 34 epoch...
10:25:22.372   Training iter 50, batch loss 0.5423, batch acc 0.2890
10:25:22.907   Training iter 100, batch loss 0.5426, batch acc 0.2882
10:25:23.461   Training iter 150, batch loss 0.5419, batch acc 0.2890
10:25:24.002   Training iter 200, batch loss 0.5414, batch acc 0.2926
10:25:24.539   Training iter 250, batch loss 0.5414, batch acc 0.2836
10:25:25.092   Training iter 300, batch loss 0.5407, batch acc 0.3014
10:25:25.645   Training iter 350, batch loss 0.5411, batch acc 0.2892
10:25:26.201   Training iter 400, batch loss 0.5407, batch acc 0.2898
10:25:26.744   Training iter 450, batch loss 0.5404, batch acc 0.2942
10:25:27.299   Training iter 500, batch loss 0.5400, batch acc 0.2948
10:25:27.882   Training iter 550, batch loss 0.5407, batch acc 0.2946
10:25:28.478   Training iter 600, batch loss 0.5402, batch acc 0.2900
10:25:28.479 Training @ 35 epoch...
10:25:29.076   Training iter 50, batch loss 0.5394, batch acc 0.2980
10:25:29.618   Training iter 100, batch loss 0.5394, batch acc 0.2976
10:25:30.128   Training iter 150, batch loss 0.5388, batch acc 0.2914
10:25:30.660   Training iter 200, batch loss 0.5383, batch acc 0.2934
10:25:31.188   Training iter 250, batch loss 0.5379, batch acc 0.2950
10:25:31.721   Training iter 300, batch loss 0.5380, batch acc 0.2986
10:25:32.256   Training iter 350, batch loss 0.5379, batch acc 0.2962
10:25:32.788   Training iter 400, batch loss 0.5377, batch acc 0.2890
10:25:33.323   Training iter 450, batch loss 0.5372, batch acc 0.2932
10:25:33.846   Training iter 500, batch loss 0.5370, batch acc 0.3048
10:25:34.360   Training iter 550, batch loss 0.5364, batch acc 0.3036
10:25:34.906   Training iter 600, batch loss 0.5366, batch acc 0.2864
10:25:34.908 Testing @ 35 epoch...
10:25:34.946     Testing, total mean loss 0.53635, total acc 0.30220
10:25:34.946 Training @ 36 epoch...
10:25:35.516   Training iter 50, batch loss 0.5360, batch acc 0.3060
10:25:36.061   Training iter 100, batch loss 0.5365, batch acc 0.2926
10:25:36.584   Training iter 150, batch loss 0.5355, batch acc 0.3006
10:25:37.127   Training iter 200, batch loss 0.5357, batch acc 0.3042
10:25:37.668   Training iter 250, batch loss 0.5354, batch acc 0.2916
10:25:38.234   Training iter 300, batch loss 0.5347, batch acc 0.2998
10:25:38.778   Training iter 350, batch loss 0.5348, batch acc 0.2996
10:25:39.321   Training iter 400, batch loss 0.5346, batch acc 0.2986
10:25:39.889   Training iter 450, batch loss 0.5335, batch acc 0.3100
10:25:40.482   Training iter 500, batch loss 0.5346, batch acc 0.2922
10:25:41.061   Training iter 550, batch loss 0.5340, batch acc 0.2958
10:25:41.637   Training iter 600, batch loss 0.5331, batch acc 0.3056
10:25:41.639 Training @ 37 epoch...
10:25:42.228   Training iter 50, batch loss 0.5331, batch acc 0.2992
10:25:42.776   Training iter 100, batch loss 0.5333, batch acc 0.3072
10:25:43.334   Training iter 150, batch loss 0.5331, batch acc 0.3036
10:25:43.874   Training iter 200, batch loss 0.5324, batch acc 0.3044
10:25:44.425   Training iter 250, batch loss 0.5329, batch acc 0.2908
10:25:44.982   Training iter 300, batch loss 0.5316, batch acc 0.3004
10:25:45.527   Training iter 350, batch loss 0.5312, batch acc 0.3040
10:25:46.043   Training iter 400, batch loss 0.5320, batch acc 0.3030
10:25:46.562   Training iter 450, batch loss 0.5316, batch acc 0.3060
10:25:47.069   Training iter 500, batch loss 0.5311, batch acc 0.3070
10:25:47.587   Training iter 550, batch loss 0.5309, batch acc 0.3044
10:25:48.124   Training iter 600, batch loss 0.5311, batch acc 0.3082
10:25:48.126 Training @ 38 epoch...
10:25:48.642   Training iter 50, batch loss 0.5304, batch acc 0.3116
10:25:49.142   Training iter 100, batch loss 0.5301, batch acc 0.3042
10:25:49.647   Training iter 150, batch loss 0.5305, batch acc 0.3082
10:25:50.154   Training iter 200, batch loss 0.5300, batch acc 0.3034
10:25:50.698   Training iter 250, batch loss 0.5291, batch acc 0.3184
10:25:51.243   Training iter 300, batch loss 0.5294, batch acc 0.3064
10:25:51.766   Training iter 350, batch loss 0.5288, batch acc 0.3038
10:25:52.309   Training iter 400, batch loss 0.5288, batch acc 0.3128
10:25:52.834   Training iter 450, batch loss 0.5290, batch acc 0.3052
10:25:53.382   Training iter 500, batch loss 0.5290, batch acc 0.3000
10:25:53.903   Training iter 550, batch loss 0.5285, batch acc 0.3074
10:25:54.424   Training iter 600, batch loss 0.5284, batch acc 0.3022
10:25:54.426 Training @ 39 epoch...
10:25:54.947   Training iter 50, batch loss 0.5275, batch acc 0.3066
10:25:55.476   Training iter 100, batch loss 0.5278, batch acc 0.3076
10:25:56.027   Training iter 150, batch loss 0.5273, batch acc 0.3090
10:25:56.569   Training iter 200, batch loss 0.5275, batch acc 0.3166
10:25:57.122   Training iter 250, batch loss 0.5275, batch acc 0.3054
10:25:57.679   Training iter 300, batch loss 0.5267, batch acc 0.3106
10:25:58.258   Training iter 350, batch loss 0.5269, batch acc 0.3092
10:25:58.804   Training iter 400, batch loss 0.5267, batch acc 0.3144
10:25:59.353   Training iter 450, batch loss 0.5265, batch acc 0.3052
10:25:59.900   Training iter 500, batch loss 0.5260, batch acc 0.3248
10:26:00.457   Training iter 550, batch loss 0.5259, batch acc 0.3120
10:26:01.023   Training iter 600, batch loss 0.5255, batch acc 0.3130
10:26:01.024 Training @ 40 epoch...
10:26:01.556   Training iter 50, batch loss 0.5253, batch acc 0.3258
10:26:02.088   Training iter 100, batch loss 0.5246, batch acc 0.3244
10:26:02.635   Training iter 150, batch loss 0.5251, batch acc 0.3112
10:26:03.373   Training iter 200, batch loss 0.5250, batch acc 0.3064
10:26:04.106   Training iter 250, batch loss 0.5243, batch acc 0.3138
10:26:04.842   Training iter 300, batch loss 0.5249, batch acc 0.3208
10:26:05.459   Training iter 350, batch loss 0.5243, batch acc 0.3180
10:26:06.006   Training iter 400, batch loss 0.5242, batch acc 0.3180
10:26:06.554   Training iter 450, batch loss 0.5238, batch acc 0.3140
10:26:07.101   Training iter 500, batch loss 0.5240, batch acc 0.3024
10:26:07.616   Training iter 550, batch loss 0.5241, batch acc 0.3118
10:26:08.133   Training iter 600, batch loss 0.5235, batch acc 0.3060
10:26:08.135 Testing @ 40 epoch...
10:26:08.171     Testing, total mean loss 0.52326, total acc 0.32250
10:26:08.171 Training @ 41 epoch...
10:26:08.677   Training iter 50, batch loss 0.5231, batch acc 0.3172
10:26:09.186   Training iter 100, batch loss 0.5232, batch acc 0.3228
10:26:09.701   Training iter 150, batch loss 0.5225, batch acc 0.3228
10:26:10.196   Training iter 200, batch loss 0.5223, batch acc 0.3206
10:26:10.682   Training iter 250, batch loss 0.5224, batch acc 0.3162
10:26:11.158   Training iter 300, batch loss 0.5225, batch acc 0.3142
10:26:11.652   Training iter 350, batch loss 0.5225, batch acc 0.3156
10:26:12.171   Training iter 400, batch loss 0.5218, batch acc 0.3226
10:26:12.687   Training iter 450, batch loss 0.5217, batch acc 0.3186
10:26:13.196   Training iter 500, batch loss 0.5217, batch acc 0.3182
10:26:13.708   Training iter 550, batch loss 0.5215, batch acc 0.3122
10:26:14.244   Training iter 600, batch loss 0.5208, batch acc 0.3168
10:26:14.246 Training @ 42 epoch...
10:26:14.776   Training iter 50, batch loss 0.5213, batch acc 0.3222
10:26:15.306   Training iter 100, batch loss 0.5209, batch acc 0.3162
10:26:15.828   Training iter 150, batch loss 0.5206, batch acc 0.3200
10:26:16.351   Training iter 200, batch loss 0.5207, batch acc 0.3244
10:26:16.862   Training iter 250, batch loss 0.5203, batch acc 0.3242
10:26:17.381   Training iter 300, batch loss 0.5201, batch acc 0.3218
10:26:17.911   Training iter 350, batch loss 0.5201, batch acc 0.3286
10:26:18.431   Training iter 400, batch loss 0.5197, batch acc 0.3222
10:26:18.953   Training iter 450, batch loss 0.5194, batch acc 0.3240
10:26:19.453   Training iter 500, batch loss 0.5194, batch acc 0.3240
10:26:19.973   Training iter 550, batch loss 0.5189, batch acc 0.3182
10:26:20.508   Training iter 600, batch loss 0.5194, batch acc 0.3158
10:26:20.510 Training @ 43 epoch...
10:26:21.027   Training iter 50, batch loss 0.5185, batch acc 0.3312
10:26:21.531   Training iter 100, batch loss 0.5193, batch acc 0.3112
10:26:22.035   Training iter 150, batch loss 0.5186, batch acc 0.3268
10:26:22.516   Training iter 200, batch loss 0.5183, batch acc 0.3280
10:26:23.002   Training iter 250, batch loss 0.5180, batch acc 0.3228
10:26:23.483   Training iter 300, batch loss 0.5183, batch acc 0.3258
10:26:23.939   Training iter 350, batch loss 0.5186, batch acc 0.3136
10:26:24.437   Training iter 400, batch loss 0.5176, batch acc 0.3338
10:26:24.988   Training iter 450, batch loss 0.5178, batch acc 0.3324
10:26:25.570   Training iter 500, batch loss 0.5177, batch acc 0.3244
10:26:26.158   Training iter 550, batch loss 0.5171, batch acc 0.3260
10:26:26.745   Training iter 600, batch loss 0.5168, batch acc 0.3326
10:26:26.746 Training @ 44 epoch...
10:26:27.341   Training iter 50, batch loss 0.5170, batch acc 0.3308
10:26:27.915   Training iter 100, batch loss 0.5165, batch acc 0.3230
10:26:28.506   Training iter 150, batch loss 0.5166, batch acc 0.3196
10:26:29.115   Training iter 200, batch loss 0.5163, batch acc 0.3360
10:26:29.698   Training iter 250, batch loss 0.5162, batch acc 0.3368
10:26:30.359   Training iter 300, batch loss 0.5164, batch acc 0.3240
10:26:30.945   Training iter 350, batch loss 0.5160, batch acc 0.3292
10:26:31.482   Training iter 400, batch loss 0.5159, batch acc 0.3224
10:26:32.015   Training iter 450, batch loss 0.5161, batch acc 0.3354
10:26:32.554   Training iter 500, batch loss 0.5158, batch acc 0.3364
10:26:33.077   Training iter 550, batch loss 0.5154, batch acc 0.3378
10:26:33.570   Training iter 600, batch loss 0.5153, batch acc 0.3192
10:26:33.571 Training @ 45 epoch...
10:26:34.058   Training iter 50, batch loss 0.5155, batch acc 0.3228
10:26:34.500   Training iter 100, batch loss 0.5147, batch acc 0.3348
10:26:34.955   Training iter 150, batch loss 0.5147, batch acc 0.3438
10:26:35.438   Training iter 200, batch loss 0.5147, batch acc 0.3302
10:26:35.906   Training iter 250, batch loss 0.5150, batch acc 0.3296
10:26:36.365   Training iter 300, batch loss 0.5141, batch acc 0.3380
10:26:36.813   Training iter 350, batch loss 0.5144, batch acc 0.3318
10:26:37.276   Training iter 400, batch loss 0.5141, batch acc 0.3276
10:26:37.737   Training iter 450, batch loss 0.5140, batch acc 0.3254
10:26:38.195   Training iter 500, batch loss 0.5136, batch acc 0.3388
10:26:38.653   Training iter 550, batch loss 0.5138, batch acc 0.3416
10:26:39.106   Training iter 600, batch loss 0.5133, batch acc 0.3334
10:26:39.108 Testing @ 45 epoch...
10:26:39.144     Testing, total mean loss 0.51341, total acc 0.34130
10:26:39.144 Training @ 46 epoch...
10:26:39.619   Training iter 50, batch loss 0.5136, batch acc 0.3408
10:26:40.116   Training iter 100, batch loss 0.5136, batch acc 0.3350
10:26:40.596   Training iter 150, batch loss 0.5135, batch acc 0.3178
10:26:41.101   Training iter 200, batch loss 0.5128, batch acc 0.3310
10:26:41.566   Training iter 250, batch loss 0.5128, batch acc 0.3288
10:26:42.046   Training iter 300, batch loss 0.5125, batch acc 0.3226
10:26:42.553   Training iter 350, batch loss 0.5121, batch acc 0.3486
10:26:43.047   Training iter 400, batch loss 0.5124, batch acc 0.3470
10:26:43.545   Training iter 450, batch loss 0.5122, batch acc 0.3346
10:26:44.062   Training iter 500, batch loss 0.5120, batch acc 0.3452
10:26:44.611   Training iter 550, batch loss 0.5122, batch acc 0.3460
10:26:45.155   Training iter 600, batch loss 0.5116, batch acc 0.3438
10:26:45.156 Training @ 47 epoch...
10:26:45.706   Training iter 50, batch loss 0.5117, batch acc 0.3350
10:26:46.254   Training iter 100, batch loss 0.5120, batch acc 0.3314
10:26:46.804   Training iter 150, batch loss 0.5110, batch acc 0.3426
10:26:47.332   Training iter 200, batch loss 0.5113, batch acc 0.3494
10:26:47.858   Training iter 250, batch loss 0.5114, batch acc 0.3346
10:26:48.387   Training iter 300, batch loss 0.5105, batch acc 0.3416
10:26:48.913   Training iter 350, batch loss 0.5113, batch acc 0.3402
10:26:49.424   Training iter 400, batch loss 0.5109, batch acc 0.3474
10:26:49.929   Training iter 450, batch loss 0.5105, batch acc 0.3444
10:26:50.444   Training iter 500, batch loss 0.5101, batch acc 0.3386
10:26:50.946   Training iter 550, batch loss 0.5108, batch acc 0.3354
10:26:51.428   Training iter 600, batch loss 0.5102, batch acc 0.3464
10:26:51.430 Training @ 48 epoch...
10:26:51.911   Training iter 50, batch loss 0.5105, batch acc 0.3404
10:26:52.393   Training iter 100, batch loss 0.5098, batch acc 0.3382
10:26:52.916   Training iter 150, batch loss 0.5096, batch acc 0.3448
10:26:53.460   Training iter 200, batch loss 0.5098, batch acc 0.3460
10:26:53.988   Training iter 250, batch loss 0.5102, batch acc 0.3340
10:26:54.498   Training iter 300, batch loss 0.5095, batch acc 0.3470
10:26:55.018   Training iter 350, batch loss 0.5096, batch acc 0.3460
10:26:55.526   Training iter 400, batch loss 0.5089, batch acc 0.3496
10:26:56.032   Training iter 450, batch loss 0.5090, batch acc 0.3454
10:26:56.549   Training iter 500, batch loss 0.5086, batch acc 0.3474
10:26:57.059   Training iter 550, batch loss 0.5089, batch acc 0.3476
10:26:57.560   Training iter 600, batch loss 0.5088, batch acc 0.3438
10:26:57.562 Training @ 49 epoch...
10:26:58.089   Training iter 50, batch loss 0.5088, batch acc 0.3372
10:26:58.613   Training iter 100, batch loss 0.5087, batch acc 0.3468
10:26:59.129   Training iter 150, batch loss 0.5081, batch acc 0.3528
10:26:59.642   Training iter 200, batch loss 0.5082, batch acc 0.3376
10:27:00.181   Training iter 250, batch loss 0.5080, batch acc 0.3412
10:27:00.743   Training iter 300, batch loss 0.5080, batch acc 0.3444
10:27:01.307   Training iter 350, batch loss 0.5082, batch acc 0.3570
10:27:01.900   Training iter 400, batch loss 0.5078, batch acc 0.3488
10:27:02.482   Training iter 450, batch loss 0.5075, batch acc 0.3532
10:27:03.043   Training iter 500, batch loss 0.5072, batch acc 0.3592
10:27:03.605   Training iter 550, batch loss 0.5078, batch acc 0.3482
10:27:04.187   Training iter 600, batch loss 0.5072, batch acc 0.3440
10:27:04.189 Training @ 50 epoch...
10:27:04.771   Training iter 50, batch loss 0.5071, batch acc 0.3536
10:27:05.349   Training iter 100, batch loss 0.5075, batch acc 0.3432
10:27:05.899   Training iter 150, batch loss 0.5067, batch acc 0.3614
10:27:06.423   Training iter 200, batch loss 0.5070, batch acc 0.3472
10:27:07.004   Training iter 250, batch loss 0.5067, batch acc 0.3524
10:27:07.594   Training iter 300, batch loss 0.5067, batch acc 0.3404
10:27:08.186   Training iter 350, batch loss 0.5063, batch acc 0.3582
10:27:08.789   Training iter 400, batch loss 0.5065, batch acc 0.3534
10:27:09.398   Training iter 450, batch loss 0.5065, batch acc 0.3422
10:27:09.990   Training iter 500, batch loss 0.5062, batch acc 0.3438
10:27:10.592   Training iter 550, batch loss 0.5059, batch acc 0.3574
10:27:11.163   Training iter 600, batch loss 0.5056, batch acc 0.3584
10:27:11.165 Testing @ 50 epoch...
10:27:11.207     Testing, total mean loss 0.50581, total acc 0.35940
10:27:11.207 Training @ 51 epoch...
10:27:11.782   Training iter 50, batch loss 0.5061, batch acc 0.3548
10:27:12.348   Training iter 100, batch loss 0.5057, batch acc 0.3516
10:27:12.895   Training iter 150, batch loss 0.5052, batch acc 0.3668
10:27:13.424   Training iter 200, batch loss 0.5060, batch acc 0.3576
10:27:13.970   Training iter 250, batch loss 0.5057, batch acc 0.3436
10:27:14.503   Training iter 300, batch loss 0.5052, batch acc 0.3626
10:27:15.037   Training iter 350, batch loss 0.5051, batch acc 0.3530
10:27:15.609   Training iter 400, batch loss 0.5048, batch acc 0.3626
10:27:16.191   Training iter 450, batch loss 0.5048, batch acc 0.3456
10:27:16.788   Training iter 500, batch loss 0.5046, batch acc 0.3662
10:27:17.405   Training iter 550, batch loss 0.5046, batch acc 0.3444
10:27:18.016   Training iter 600, batch loss 0.5048, batch acc 0.3430
10:27:18.017 Training @ 52 epoch...
10:27:18.665   Training iter 50, batch loss 0.5049, batch acc 0.3510
10:27:19.302   Training iter 100, batch loss 0.5046, batch acc 0.3456
10:27:19.918   Training iter 150, batch loss 0.5042, batch acc 0.3620
10:27:20.511   Training iter 200, batch loss 0.5044, batch acc 0.3572
10:27:21.112   Training iter 250, batch loss 0.5035, batch acc 0.3638
10:27:21.727   Training iter 300, batch loss 0.5039, batch acc 0.3610
10:27:22.351   Training iter 350, batch loss 0.5038, batch acc 0.3534
10:27:22.972   Training iter 400, batch loss 0.5035, batch acc 0.3570
10:27:23.555   Training iter 450, batch loss 0.5037, batch acc 0.3646
10:27:24.140   Training iter 500, batch loss 0.5039, batch acc 0.3662
10:27:24.699   Training iter 550, batch loss 0.5033, batch acc 0.3564
10:27:25.280   Training iter 600, batch loss 0.5036, batch acc 0.3536
10:27:25.281 Training @ 53 epoch...
10:27:25.830   Training iter 50, batch loss 0.5029, batch acc 0.3608
10:27:26.392   Training iter 100, batch loss 0.5032, batch acc 0.3582
10:27:26.958   Training iter 150, batch loss 0.5026, batch acc 0.3626
10:27:27.536   Training iter 200, batch loss 0.5031, batch acc 0.3632
10:27:28.078   Training iter 250, batch loss 0.5032, batch acc 0.3438
10:27:28.611   Training iter 300, batch loss 0.5024, batch acc 0.3690
10:27:29.140   Training iter 350, batch loss 0.5029, batch acc 0.3642
10:27:29.671   Training iter 400, batch loss 0.5030, batch acc 0.3580
10:27:30.198   Training iter 450, batch loss 0.5026, batch acc 0.3588
10:27:30.740   Training iter 500, batch loss 0.5021, batch acc 0.3648
10:27:31.278   Training iter 550, batch loss 0.5028, batch acc 0.3596
10:27:31.804   Training iter 600, batch loss 0.5019, batch acc 0.3658
10:27:31.806 Training @ 54 epoch...
10:27:32.335   Training iter 50, batch loss 0.5023, batch acc 0.3590
10:27:32.866   Training iter 100, batch loss 0.5019, batch acc 0.3662
10:27:33.408   Training iter 150, batch loss 0.5019, batch acc 0.3640
10:27:33.938   Training iter 200, batch loss 0.5014, batch acc 0.3666
10:27:34.463   Training iter 250, batch loss 0.5017, batch acc 0.3610
10:27:34.997   Training iter 300, batch loss 0.5019, batch acc 0.3502
10:27:35.558   Training iter 350, batch loss 0.5016, batch acc 0.3746
10:27:36.106   Training iter 400, batch loss 0.5012, batch acc 0.3718
10:27:36.641   Training iter 450, batch loss 0.5011, batch acc 0.3656
10:27:37.183   Training iter 500, batch loss 0.5016, batch acc 0.3618
10:27:37.745   Training iter 550, batch loss 0.5009, batch acc 0.3700
10:27:38.324   Training iter 600, batch loss 0.5012, batch acc 0.3588
10:27:38.326 Training @ 55 epoch...
10:27:38.902   Training iter 50, batch loss 0.5008, batch acc 0.3678
10:27:39.456   Training iter 100, batch loss 0.5009, batch acc 0.3656
10:27:40.014   Training iter 150, batch loss 0.5009, batch acc 0.3578
10:27:40.578   Training iter 200, batch loss 0.5006, batch acc 0.3686
10:27:41.055   Training iter 250, batch loss 0.5008, batch acc 0.3730
10:27:41.538   Training iter 300, batch loss 0.5002, batch acc 0.3716
10:27:42.056   Training iter 350, batch loss 0.5001, batch acc 0.3748
10:27:42.557   Training iter 400, batch loss 0.5005, batch acc 0.3742
10:27:43.038   Training iter 450, batch loss 0.5005, batch acc 0.3712
10:27:43.504   Training iter 500, batch loss 0.5003, batch acc 0.3526
10:27:44.003   Training iter 550, batch loss 0.5004, batch acc 0.3630
10:27:44.504   Training iter 600, batch loss 0.4995, batch acc 0.3716
10:27:44.506 Testing @ 55 epoch...
10:27:44.543     Testing, total mean loss 0.49983, total acc 0.37490
10:27:44.543 Training @ 56 epoch...
10:27:45.048   Training iter 50, batch loss 0.5003, batch acc 0.3650
10:27:45.538   Training iter 100, batch loss 0.4995, batch acc 0.3772
10:27:46.015   Training iter 150, batch loss 0.4998, batch acc 0.3648
10:27:46.499   Training iter 200, batch loss 0.4999, batch acc 0.3798
10:27:46.980   Training iter 250, batch loss 0.4992, batch acc 0.3710
10:27:47.476   Training iter 300, batch loss 0.5000, batch acc 0.3756
10:27:47.971   Training iter 350, batch loss 0.4992, batch acc 0.3656
10:27:48.448   Training iter 400, batch loss 0.4989, batch acc 0.3672
10:27:48.956   Training iter 450, batch loss 0.4991, batch acc 0.3724
10:27:49.476   Training iter 500, batch loss 0.4989, batch acc 0.3680
10:27:50.013   Training iter 550, batch loss 0.4989, batch acc 0.3640
10:27:50.563   Training iter 600, batch loss 0.4991, batch acc 0.3730
10:27:50.565 Training @ 57 epoch...
10:27:51.111   Training iter 50, batch loss 0.4990, batch acc 0.3712
10:27:51.633   Training iter 100, batch loss 0.4989, batch acc 0.3704
10:27:52.153   Training iter 150, batch loss 0.4980, batch acc 0.3726
10:27:52.688   Training iter 200, batch loss 0.4987, batch acc 0.3706
10:27:53.244   Training iter 250, batch loss 0.4986, batch acc 0.3756
10:27:53.782   Training iter 300, batch loss 0.4984, batch acc 0.3728
10:27:54.319   Training iter 350, batch loss 0.4984, batch acc 0.3660
10:27:54.844   Training iter 400, batch loss 0.4982, batch acc 0.3740
10:27:55.361   Training iter 450, batch loss 0.4981, batch acc 0.3742
10:27:55.893   Training iter 500, batch loss 0.4984, batch acc 0.3758
10:27:56.428   Training iter 550, batch loss 0.4982, batch acc 0.3744
10:27:56.959   Training iter 600, batch loss 0.4976, batch acc 0.3858
10:27:56.960 Training @ 58 epoch...
10:27:57.492   Training iter 50, batch loss 0.4981, batch acc 0.3758
10:27:58.021   Training iter 100, batch loss 0.4977, batch acc 0.3706
10:27:58.538   Training iter 150, batch loss 0.4978, batch acc 0.3832
10:27:59.059   Training iter 200, batch loss 0.4979, batch acc 0.3748
10:27:59.581   Training iter 250, batch loss 0.4977, batch acc 0.3750
10:28:00.108   Training iter 300, batch loss 0.4974, batch acc 0.3788
10:28:00.654   Training iter 350, batch loss 0.4971, batch acc 0.3824
10:28:01.196   Training iter 400, batch loss 0.4973, batch acc 0.3746
10:28:01.732   Training iter 450, batch loss 0.4972, batch acc 0.3736
10:28:02.440   Training iter 500, batch loss 0.4971, batch acc 0.3768
10:28:03.108   Training iter 550, batch loss 0.4965, batch acc 0.3876
10:28:03.661   Training iter 600, batch loss 0.4970, batch acc 0.3684
10:28:03.663 Training @ 59 epoch...
10:28:04.240   Training iter 50, batch loss 0.4968, batch acc 0.3816
10:28:04.803   Training iter 100, batch loss 0.4968, batch acc 0.3880
10:28:05.363   Training iter 150, batch loss 0.4965, batch acc 0.3860
10:28:05.910   Training iter 200, batch loss 0.4966, batch acc 0.3824
10:28:06.461   Training iter 250, batch loss 0.4964, batch acc 0.3826
10:28:07.021   Training iter 300, batch loss 0.4966, batch acc 0.3724
10:28:07.566   Training iter 350, batch loss 0.4966, batch acc 0.3784
10:28:08.090   Training iter 400, batch loss 0.4961, batch acc 0.3850
10:28:08.617   Training iter 450, batch loss 0.4961, batch acc 0.3762
10:28:09.140   Training iter 500, batch loss 0.4962, batch acc 0.3764
10:28:09.643   Training iter 550, batch loss 0.4963, batch acc 0.3748
10:28:10.156   Training iter 600, batch loss 0.4965, batch acc 0.3812
10:28:10.158 Training @ 60 epoch...
10:28:10.671   Training iter 50, batch loss 0.4958, batch acc 0.3768
10:28:11.187   Training iter 100, batch loss 0.4961, batch acc 0.3898
10:28:11.692   Training iter 150, batch loss 0.4955, batch acc 0.3842
10:28:12.207   Training iter 200, batch loss 0.4956, batch acc 0.3810
10:28:12.729   Training iter 250, batch loss 0.4954, batch acc 0.3810
10:28:13.256   Training iter 300, batch loss 0.4957, batch acc 0.3756
10:28:13.780   Training iter 350, batch loss 0.4959, batch acc 0.3836
10:28:14.348   Training iter 400, batch loss 0.4958, batch acc 0.3796
10:28:14.915   Training iter 450, batch loss 0.4955, batch acc 0.3852
10:28:15.470   Training iter 500, batch loss 0.4957, batch acc 0.3800
10:28:15.976   Training iter 550, batch loss 0.4948, batch acc 0.3906
10:28:16.456   Training iter 600, batch loss 0.4951, batch acc 0.3950
10:28:16.458 Testing @ 60 epoch...
10:28:16.492     Testing, total mean loss 0.49504, total acc 0.39300
10:28:16.492 Training @ 61 epoch...
10:28:17.004   Training iter 50, batch loss 0.4956, batch acc 0.3770
10:28:17.517   Training iter 100, batch loss 0.4947, batch acc 0.3814
10:28:18.027   Training iter 150, batch loss 0.4950, batch acc 0.3836
10:28:18.536   Training iter 200, batch loss 0.4943, batch acc 0.3818
10:28:19.020   Training iter 250, batch loss 0.4949, batch acc 0.3872
10:28:19.488   Training iter 300, batch loss 0.4946, batch acc 0.3916
10:28:19.958   Training iter 350, batch loss 0.4949, batch acc 0.3914
10:28:20.440   Training iter 400, batch loss 0.4944, batch acc 0.3828
10:28:20.942   Training iter 450, batch loss 0.4947, batch acc 0.3842
10:28:21.427   Training iter 500, batch loss 0.4944, batch acc 0.3868
10:28:21.926   Training iter 550, batch loss 0.4944, batch acc 0.3970
10:28:22.393   Training iter 600, batch loss 0.4946, batch acc 0.3896
10:28:22.395 Training @ 62 epoch...
10:28:22.894   Training iter 50, batch loss 0.4942, batch acc 0.3846
10:28:23.407   Training iter 100, batch loss 0.4937, batch acc 0.3864
10:28:23.939   Training iter 150, batch loss 0.4933, batch acc 0.3930
10:28:24.405   Training iter 200, batch loss 0.4938, batch acc 0.4008
10:28:24.859   Training iter 250, batch loss 0.4944, batch acc 0.3774
10:28:25.355   Training iter 300, batch loss 0.4945, batch acc 0.3776
10:28:25.854   Training iter 350, batch loss 0.4939, batch acc 0.3932
10:28:26.352   Training iter 400, batch loss 0.4942, batch acc 0.3874
10:28:26.811   Training iter 450, batch loss 0.4941, batch acc 0.4042
10:28:27.283   Training iter 500, batch loss 0.4936, batch acc 0.3870
10:28:27.764   Training iter 550, batch loss 0.4939, batch acc 0.3770
10:28:28.255   Training iter 600, batch loss 0.4931, batch acc 0.3986
10:28:28.257 Training @ 63 epoch...
10:28:28.752   Training iter 50, batch loss 0.4937, batch acc 0.3960
10:28:29.231   Training iter 100, batch loss 0.4933, batch acc 0.3882
10:28:29.701   Training iter 150, batch loss 0.4929, batch acc 0.3930
10:28:30.182   Training iter 200, batch loss 0.4936, batch acc 0.3878
10:28:30.723   Training iter 250, batch loss 0.4932, batch acc 0.3828
10:28:31.260   Training iter 300, batch loss 0.4929, batch acc 0.3900
10:28:31.765   Training iter 350, batch loss 0.4929, batch acc 0.3866
10:28:32.303   Training iter 400, batch loss 0.4929, batch acc 0.4022
10:28:32.820   Training iter 450, batch loss 0.4928, batch acc 0.3966
10:28:33.349   Training iter 500, batch loss 0.4930, batch acc 0.3926
10:28:33.881   Training iter 550, batch loss 0.4926, batch acc 0.3918
10:28:34.415   Training iter 600, batch loss 0.4933, batch acc 0.3958
10:28:34.417 Training @ 64 epoch...
10:28:34.995   Training iter 50, batch loss 0.4927, batch acc 0.3882
10:28:35.544   Training iter 100, batch loss 0.4928, batch acc 0.3958
10:28:36.041   Training iter 150, batch loss 0.4923, batch acc 0.4068
10:28:36.544   Training iter 200, batch loss 0.4920, batch acc 0.4006
10:28:37.080   Training iter 250, batch loss 0.4920, batch acc 0.3932
10:28:37.607   Training iter 300, batch loss 0.4925, batch acc 0.3884
10:28:38.137   Training iter 350, batch loss 0.4928, batch acc 0.3956
10:28:38.669   Training iter 400, batch loss 0.4921, batch acc 0.3918
10:28:39.202   Training iter 450, batch loss 0.4924, batch acc 0.3924
10:28:39.720   Training iter 500, batch loss 0.4923, batch acc 0.3880
10:28:40.250   Training iter 550, batch loss 0.4918, batch acc 0.3990
10:28:40.771   Training iter 600, batch loss 0.4924, batch acc 0.3972
10:28:40.773 Training @ 65 epoch...
10:28:41.291   Training iter 50, batch loss 0.4917, batch acc 0.4078
10:28:41.827   Training iter 100, batch loss 0.4917, batch acc 0.3980
10:28:42.385   Training iter 150, batch loss 0.4920, batch acc 0.3976
10:28:42.901   Training iter 200, batch loss 0.4915, batch acc 0.3952
10:28:43.357   Training iter 250, batch loss 0.4916, batch acc 0.3976
10:28:43.802   Training iter 300, batch loss 0.4916, batch acc 0.3914
10:28:44.281   Training iter 350, batch loss 0.4918, batch acc 0.4004
10:28:44.745   Training iter 400, batch loss 0.4916, batch acc 0.4028
10:28:45.220   Training iter 450, batch loss 0.4915, batch acc 0.3990
10:28:45.679   Training iter 500, batch loss 0.4912, batch acc 0.4010
10:28:46.133   Training iter 550, batch loss 0.4915, batch acc 0.3888
10:28:46.585   Training iter 600, batch loss 0.4915, batch acc 0.3894
10:28:46.587 Testing @ 65 epoch...
10:28:46.624     Testing, total mean loss 0.49114, total acc 0.40970
10:28:46.624 Training @ 66 epoch...
10:28:47.100   Training iter 50, batch loss 0.4912, batch acc 0.4048
10:28:47.551   Training iter 100, batch loss 0.4915, batch acc 0.3974
10:28:48.009   Training iter 150, batch loss 0.4908, batch acc 0.4060
10:28:48.494   Training iter 200, batch loss 0.4912, batch acc 0.3962
10:28:49.004   Training iter 250, batch loss 0.4912, batch acc 0.3974
10:28:49.494   Training iter 300, batch loss 0.4908, batch acc 0.3950
10:28:50.003   Training iter 350, batch loss 0.4909, batch acc 0.4008
10:28:50.506   Training iter 400, batch loss 0.4909, batch acc 0.4074
10:28:51.024   Training iter 450, batch loss 0.4906, batch acc 0.3942
10:28:51.539   Training iter 500, batch loss 0.4904, batch acc 0.4114
10:28:52.052   Training iter 550, batch loss 0.4908, batch acc 0.3968
10:28:52.573   Training iter 600, batch loss 0.4905, batch acc 0.4006
10:28:52.575 Training @ 67 epoch...
10:28:53.120   Training iter 50, batch loss 0.4908, batch acc 0.4000
10:28:53.674   Training iter 100, batch loss 0.4905, batch acc 0.4080
10:28:54.235   Training iter 150, batch loss 0.4906, batch acc 0.4012
10:28:54.787   Training iter 200, batch loss 0.4902, batch acc 0.4004
10:28:55.345   Training iter 250, batch loss 0.4903, batch acc 0.4094
10:28:55.894   Training iter 300, batch loss 0.4902, batch acc 0.4012
10:28:56.452   Training iter 350, batch loss 0.4898, batch acc 0.4094
10:28:56.985   Training iter 400, batch loss 0.4901, batch acc 0.4004
10:28:57.514   Training iter 450, batch loss 0.4902, batch acc 0.4120
10:28:58.052   Training iter 500, batch loss 0.4902, batch acc 0.4052
10:28:58.595   Training iter 550, batch loss 0.4898, batch acc 0.4020
10:28:59.129   Training iter 600, batch loss 0.4900, batch acc 0.3962
10:28:59.131 Training @ 68 epoch...
10:28:59.645   Training iter 50, batch loss 0.4899, batch acc 0.4106
10:29:00.158   Training iter 100, batch loss 0.4899, batch acc 0.4122
10:29:00.698   Training iter 150, batch loss 0.4898, batch acc 0.3978
10:29:01.211   Training iter 200, batch loss 0.4895, batch acc 0.4164
10:29:01.850   Training iter 250, batch loss 0.4898, batch acc 0.4036
10:29:02.388   Training iter 300, batch loss 0.4899, batch acc 0.3922
10:29:02.902   Training iter 350, batch loss 0.4892, batch acc 0.4142
10:29:03.406   Training iter 400, batch loss 0.4894, batch acc 0.4092
10:29:03.925   Training iter 450, batch loss 0.4893, batch acc 0.4108
10:29:04.476   Training iter 500, batch loss 0.4896, batch acc 0.4006
10:29:04.999   Training iter 550, batch loss 0.4896, batch acc 0.4066
10:29:05.516   Training iter 600, batch loss 0.4889, batch acc 0.4044
10:29:05.518 Training @ 69 epoch...
10:29:06.037   Training iter 50, batch loss 0.4894, batch acc 0.4116
10:29:06.554   Training iter 100, batch loss 0.4894, batch acc 0.4026
10:29:07.112   Training iter 150, batch loss 0.4891, batch acc 0.4078
10:29:07.632   Training iter 200, batch loss 0.4887, batch acc 0.4148
10:29:08.116   Training iter 250, batch loss 0.4890, batch acc 0.4058
10:29:08.598   Training iter 300, batch loss 0.4889, batch acc 0.4138
10:29:09.093   Training iter 350, batch loss 0.4895, batch acc 0.3954
10:29:09.608   Training iter 400, batch loss 0.4887, batch acc 0.4054
10:29:10.120   Training iter 450, batch loss 0.4886, batch acc 0.4178
10:29:10.633   Training iter 500, batch loss 0.4890, batch acc 0.4104
10:29:11.157   Training iter 550, batch loss 0.4886, batch acc 0.4198
10:29:11.685   Training iter 600, batch loss 0.4886, batch acc 0.4040
10:29:11.686 Training @ 70 epoch...
10:29:12.212   Training iter 50, batch loss 0.4885, batch acc 0.4166
10:29:12.754   Training iter 100, batch loss 0.4887, batch acc 0.4122
10:29:13.311   Training iter 150, batch loss 0.4888, batch acc 0.3994
10:29:13.850   Training iter 200, batch loss 0.4884, batch acc 0.4148
10:29:14.391   Training iter 250, batch loss 0.4880, batch acc 0.4144
10:29:14.926   Training iter 300, batch loss 0.4883, batch acc 0.4110
10:29:15.451   Training iter 350, batch loss 0.4885, batch acc 0.4100
10:29:15.984   Training iter 400, batch loss 0.4879, batch acc 0.4130
10:29:16.506   Training iter 450, batch loss 0.4884, batch acc 0.4160
10:29:17.014   Training iter 500, batch loss 0.4886, batch acc 0.4094
10:29:17.497   Training iter 550, batch loss 0.4878, batch acc 0.4192
10:29:17.990   Training iter 600, batch loss 0.4882, batch acc 0.4042
10:29:17.992 Testing @ 70 epoch...
10:29:18.028     Testing, total mean loss 0.48793, total acc 0.42320
10:29:18.028 Training @ 71 epoch...
10:29:18.532   Training iter 50, batch loss 0.4880, batch acc 0.4168
10:29:19.067   Training iter 100, batch loss 0.4878, batch acc 0.4080
10:29:19.596   Training iter 150, batch loss 0.4885, batch acc 0.4064
10:29:20.125   Training iter 200, batch loss 0.4874, batch acc 0.4132
10:29:20.669   Training iter 250, batch loss 0.4882, batch acc 0.4100
10:29:21.200   Training iter 300, batch loss 0.4875, batch acc 0.4238
10:29:21.730   Training iter 350, batch loss 0.4879, batch acc 0.4040
10:29:22.263   Training iter 400, batch loss 0.4878, batch acc 0.4118
10:29:22.814   Training iter 450, batch loss 0.4878, batch acc 0.4182
10:29:23.354   Training iter 500, batch loss 0.4872, batch acc 0.4222
10:29:23.872   Training iter 550, batch loss 0.4876, batch acc 0.4076
10:29:24.398   Training iter 600, batch loss 0.4875, batch acc 0.4268
10:29:24.400 Training @ 72 epoch...
10:29:24.958   Training iter 50, batch loss 0.4881, batch acc 0.4120
10:29:25.496   Training iter 100, batch loss 0.4873, batch acc 0.4202
10:29:26.029   Training iter 150, batch loss 0.4872, batch acc 0.4182
10:29:26.567   Training iter 200, batch loss 0.4871, batch acc 0.4184
10:29:27.099   Training iter 250, batch loss 0.4875, batch acc 0.4044
10:29:27.648   Training iter 300, batch loss 0.4870, batch acc 0.4222
10:29:28.213   Training iter 350, batch loss 0.4875, batch acc 0.4128
10:29:28.748   Training iter 400, batch loss 0.4875, batch acc 0.4152
10:29:29.266   Training iter 450, batch loss 0.4868, batch acc 0.4226
10:29:29.775   Training iter 500, batch loss 0.4871, batch acc 0.4122
10:29:30.314   Training iter 550, batch loss 0.4868, batch acc 0.4254
10:29:30.851   Training iter 600, batch loss 0.4865, batch acc 0.4200
10:29:30.853 Training @ 73 epoch...
10:29:31.361   Training iter 50, batch loss 0.4867, batch acc 0.4162
10:29:31.822   Training iter 100, batch loss 0.4866, batch acc 0.4254
10:29:32.272   Training iter 150, batch loss 0.4869, batch acc 0.4174
10:29:32.726   Training iter 200, batch loss 0.4866, batch acc 0.4306
10:29:33.216   Training iter 250, batch loss 0.4869, batch acc 0.4122
10:29:33.684   Training iter 300, batch loss 0.4867, batch acc 0.4232
10:29:34.148   Training iter 350, batch loss 0.4868, batch acc 0.4190
10:29:34.617   Training iter 400, batch loss 0.4864, batch acc 0.4182
10:29:35.061   Training iter 450, batch loss 0.4864, batch acc 0.4204
10:29:35.514   Training iter 500, batch loss 0.4863, batch acc 0.4160
10:29:35.968   Training iter 550, batch loss 0.4866, batch acc 0.4230
10:29:36.418   Training iter 600, batch loss 0.4870, batch acc 0.4138
10:29:36.419 Training @ 74 epoch...
10:29:36.876   Training iter 50, batch loss 0.4867, batch acc 0.4146
10:29:37.335   Training iter 100, batch loss 0.4862, batch acc 0.4190
10:29:37.842   Training iter 150, batch loss 0.4863, batch acc 0.4206
10:29:38.315   Training iter 200, batch loss 0.4865, batch acc 0.4224
10:29:38.785   Training iter 250, batch loss 0.4861, batch acc 0.4218
10:29:39.247   Training iter 300, batch loss 0.4861, batch acc 0.4212
10:29:39.702   Training iter 350, batch loss 0.4858, batch acc 0.4302
10:29:40.158   Training iter 400, batch loss 0.4860, batch acc 0.4232
10:29:40.627   Training iter 450, batch loss 0.4858, batch acc 0.4288
10:29:41.114   Training iter 500, batch loss 0.4863, batch acc 0.4124
10:29:41.599   Training iter 550, batch loss 0.4857, batch acc 0.4308
10:29:42.112   Training iter 600, batch loss 0.4861, batch acc 0.4218
10:29:42.113 Training @ 75 epoch...
10:29:42.648   Training iter 50, batch loss 0.4857, batch acc 0.4288
10:29:43.197   Training iter 100, batch loss 0.4861, batch acc 0.4150
10:29:43.735   Training iter 150, batch loss 0.4858, batch acc 0.4230
10:29:44.303   Training iter 200, batch loss 0.4860, batch acc 0.4224
10:29:44.864   Training iter 250, batch loss 0.4854, batch acc 0.4292
10:29:45.436   Training iter 300, batch loss 0.4855, batch acc 0.4180
10:29:45.983   Training iter 350, batch loss 0.4856, batch acc 0.4254
10:29:46.534   Training iter 400, batch loss 0.4855, batch acc 0.4242
10:29:47.035   Training iter 450, batch loss 0.4854, batch acc 0.4386
10:29:47.535   Training iter 500, batch loss 0.4856, batch acc 0.4178
10:29:48.068   Training iter 550, batch loss 0.4855, batch acc 0.4208
10:29:48.628   Training iter 600, batch loss 0.4855, batch acc 0.4296
10:29:48.629 Testing @ 75 epoch...
10:29:48.666     Testing, total mean loss 0.48525, total acc 0.43550
10:29:48.666 Training @ 76 epoch...
10:29:49.184   Training iter 50, batch loss 0.4849, batch acc 0.4298
10:29:49.851   Training iter 100, batch loss 0.4852, batch acc 0.4228
10:29:50.480   Training iter 150, batch loss 0.4851, batch acc 0.4270
10:29:50.977   Training iter 200, batch loss 0.4856, batch acc 0.4312
10:29:51.465   Training iter 250, batch loss 0.4850, batch acc 0.4338
10:29:51.971   Training iter 300, batch loss 0.4852, batch acc 0.4246
10:29:52.458   Training iter 350, batch loss 0.4852, batch acc 0.4260
10:29:52.948   Training iter 400, batch loss 0.4854, batch acc 0.4254
10:29:53.452   Training iter 450, batch loss 0.4855, batch acc 0.4232
10:29:53.959   Training iter 500, batch loss 0.4854, batch acc 0.4238
10:29:54.472   Training iter 550, batch loss 0.4844, batch acc 0.4228
10:29:54.967   Training iter 600, batch loss 0.4848, batch acc 0.4268
10:29:54.968 Training @ 77 epoch...
10:29:55.482   Training iter 50, batch loss 0.4853, batch acc 0.4256
10:29:55.981   Training iter 100, batch loss 0.4849, batch acc 0.4342
10:29:56.484   Training iter 150, batch loss 0.4849, batch acc 0.4312
10:29:56.999   Training iter 200, batch loss 0.4851, batch acc 0.4142
10:29:57.525   Training iter 250, batch loss 0.4851, batch acc 0.4310
10:29:58.060   Training iter 300, batch loss 0.4848, batch acc 0.4234
10:29:58.592   Training iter 350, batch loss 0.4850, batch acc 0.4296
10:29:59.158   Training iter 400, batch loss 0.4846, batch acc 0.4314
10:29:59.714   Training iter 450, batch loss 0.4839, batch acc 0.4412
10:30:00.211   Training iter 500, batch loss 0.4842, batch acc 0.4278
10:30:00.718   Training iter 550, batch loss 0.4841, batch acc 0.4328
10:30:01.284   Training iter 600, batch loss 0.4842, batch acc 0.4262
10:30:01.286 Training @ 78 epoch...
10:30:01.885   Training iter 50, batch loss 0.4843, batch acc 0.4304
10:30:02.446   Training iter 100, batch loss 0.4841, batch acc 0.4352
10:30:02.986   Training iter 150, batch loss 0.4847, batch acc 0.4268
10:30:03.531   Training iter 200, batch loss 0.4844, batch acc 0.4238
10:30:04.047   Training iter 250, batch loss 0.4845, batch acc 0.4358
10:30:04.568   Training iter 300, batch loss 0.4841, batch acc 0.4388
10:30:05.089   Training iter 350, batch loss 0.4840, batch acc 0.4282
10:30:05.597   Training iter 400, batch loss 0.4841, batch acc 0.4326
10:30:06.116   Training iter 450, batch loss 0.4842, batch acc 0.4286
10:30:06.620   Training iter 500, batch loss 0.4840, batch acc 0.4378
10:30:07.130   Training iter 550, batch loss 0.4840, batch acc 0.4238
10:30:07.626   Training iter 600, batch loss 0.4841, batch acc 0.4356
10:30:07.628 Training @ 79 epoch...
10:30:08.119   Training iter 50, batch loss 0.4839, batch acc 0.4334
10:30:08.635   Training iter 100, batch loss 0.4838, batch acc 0.4360
10:30:09.155   Training iter 150, batch loss 0.4837, batch acc 0.4308
10:30:09.668   Training iter 200, batch loss 0.4840, batch acc 0.4258
10:30:10.182   Training iter 250, batch loss 0.4841, batch acc 0.4350
10:30:10.699   Training iter 300, batch loss 0.4838, batch acc 0.4400
10:30:11.213   Training iter 350, batch loss 0.4834, batch acc 0.4452
10:30:11.716   Training iter 400, batch loss 0.4837, batch acc 0.4300
10:30:12.222   Training iter 450, batch loss 0.4836, batch acc 0.4378
10:30:12.705   Training iter 500, batch loss 0.4833, batch acc 0.4370
10:30:13.221   Training iter 550, batch loss 0.4836, batch acc 0.4276
10:30:13.745   Training iter 600, batch loss 0.4842, batch acc 0.4224
10:30:13.747 Training @ 80 epoch...
10:30:14.283   Training iter 50, batch loss 0.4838, batch acc 0.4288
10:30:14.829   Training iter 100, batch loss 0.4833, batch acc 0.4368
10:30:15.389   Training iter 150, batch loss 0.4838, batch acc 0.4252
10:30:15.939   Training iter 200, batch loss 0.4835, batch acc 0.4296
10:30:16.491   Training iter 250, batch loss 0.4832, batch acc 0.4396
10:30:17.040   Training iter 300, batch loss 0.4835, batch acc 0.4308
10:30:17.596   Training iter 350, batch loss 0.4834, batch acc 0.4310
10:30:18.154   Training iter 400, batch loss 0.4830, batch acc 0.4490
10:30:18.732   Training iter 450, batch loss 0.4834, batch acc 0.4386
10:30:19.285   Training iter 500, batch loss 0.4829, batch acc 0.4460
10:30:19.842   Training iter 550, batch loss 0.4832, batch acc 0.4420
10:30:20.409   Training iter 600, batch loss 0.4832, batch acc 0.4314
10:30:20.411 Testing @ 80 epoch...
10:30:20.449     Testing, total mean loss 0.48299, total acc 0.44670
10:30:20.449 Training @ 81 epoch...
10:30:20.969   Training iter 50, batch loss 0.4827, batch acc 0.4396
10:30:21.446   Training iter 100, batch loss 0.4832, batch acc 0.4382
10:30:21.924   Training iter 150, batch loss 0.4828, batch acc 0.4282
10:30:22.424   Training iter 200, batch loss 0.4831, batch acc 0.4320
10:30:22.921   Training iter 250, batch loss 0.4832, batch acc 0.4428
10:30:23.416   Training iter 300, batch loss 0.4831, batch acc 0.4346
10:30:23.911   Training iter 350, batch loss 0.4829, batch acc 0.4422
10:30:24.394   Training iter 400, batch loss 0.4829, batch acc 0.4324
10:30:24.887   Training iter 450, batch loss 0.4832, batch acc 0.4260
10:30:25.420   Training iter 500, batch loss 0.4827, batch acc 0.4460
10:30:25.951   Training iter 550, batch loss 0.4824, batch acc 0.4468
10:30:26.490   Training iter 600, batch loss 0.4830, batch acc 0.4452
10:30:26.491 Training @ 82 epoch...
10:30:26.993   Training iter 50, batch loss 0.4825, batch acc 0.4380
10:30:27.502   Training iter 100, batch loss 0.4826, batch acc 0.4428
10:30:28.010   Training iter 150, batch loss 0.4824, batch acc 0.4424
10:30:28.501   Training iter 200, batch loss 0.4828, batch acc 0.4362
10:30:29.004   Training iter 250, batch loss 0.4830, batch acc 0.4242
10:30:29.531   Training iter 300, batch loss 0.4825, batch acc 0.4430
10:30:30.057   Training iter 350, batch loss 0.4825, batch acc 0.4358
10:30:30.579   Training iter 400, batch loss 0.4825, batch acc 0.4446
10:30:31.109   Training iter 450, batch loss 0.4826, batch acc 0.4314
10:30:31.640   Training iter 500, batch loss 0.4827, batch acc 0.4532
10:30:32.150   Training iter 550, batch loss 0.4820, batch acc 0.4498
10:30:32.669   Training iter 600, batch loss 0.4822, batch acc 0.4418
10:30:32.670 Training @ 83 epoch...
10:30:33.179   Training iter 50, batch loss 0.4822, batch acc 0.4386
10:30:33.671   Training iter 100, batch loss 0.4825, batch acc 0.4332
10:30:34.166   Training iter 150, batch loss 0.4824, batch acc 0.4316
10:30:34.656   Training iter 200, batch loss 0.4823, batch acc 0.4460
10:30:35.144   Training iter 250, batch loss 0.4822, batch acc 0.4334
10:30:35.606   Training iter 300, batch loss 0.4821, batch acc 0.4546
10:30:36.081   Training iter 350, batch loss 0.4819, batch acc 0.4472
10:30:36.556   Training iter 400, batch loss 0.4825, batch acc 0.4388
10:30:37.065   Training iter 450, batch loss 0.4821, batch acc 0.4460
10:30:37.578   Training iter 500, batch loss 0.4818, batch acc 0.4474
10:30:38.086   Training iter 550, batch loss 0.4816, batch acc 0.4470
10:30:38.584   Training iter 600, batch loss 0.4820, batch acc 0.4428
10:30:38.586 Training @ 84 epoch...
10:30:39.089   Training iter 50, batch loss 0.4820, batch acc 0.4450
10:30:39.578   Training iter 100, batch loss 0.4814, batch acc 0.4388
10:30:40.072   Training iter 150, batch loss 0.4821, batch acc 0.4416
10:30:40.583   Training iter 200, batch loss 0.4822, batch acc 0.4408
10:30:41.068   Training iter 250, batch loss 0.4814, batch acc 0.4490
10:30:41.550   Training iter 300, batch loss 0.4818, batch acc 0.4460
10:30:42.027   Training iter 350, batch loss 0.4818, batch acc 0.4442
10:30:42.551   Training iter 400, batch loss 0.4815, batch acc 0.4488
10:30:43.082   Training iter 450, batch loss 0.4821, batch acc 0.4332
10:30:43.602   Training iter 500, batch loss 0.4812, batch acc 0.4550
10:30:44.123   Training iter 550, batch loss 0.4820, batch acc 0.4478
10:30:44.581   Training iter 600, batch loss 0.4820, batch acc 0.4420
10:30:44.583 Training @ 85 epoch...
10:30:45.025   Training iter 50, batch loss 0.4816, batch acc 0.4458
10:30:45.484   Training iter 100, batch loss 0.4817, batch acc 0.4400
10:30:45.957   Training iter 150, batch loss 0.4812, batch acc 0.4520
10:30:46.440   Training iter 200, batch loss 0.4818, batch acc 0.4396
10:30:46.915   Training iter 250, batch loss 0.4817, batch acc 0.4484
10:30:47.392   Training iter 300, batch loss 0.4814, batch acc 0.4590
10:30:47.863   Training iter 350, batch loss 0.4813, batch acc 0.4524
10:30:48.345   Training iter 400, batch loss 0.4815, batch acc 0.4454
10:30:48.830   Training iter 450, batch loss 0.4809, batch acc 0.4444
10:30:49.334   Training iter 500, batch loss 0.4814, batch acc 0.4448
10:30:49.828   Training iter 550, batch loss 0.4812, batch acc 0.4432
10:30:50.341   Training iter 600, batch loss 0.4811, batch acc 0.4434
10:30:50.343 Testing @ 85 epoch...
10:30:50.378     Testing, total mean loss 0.48107, total acc 0.45670
10:30:50.378 Training @ 86 epoch...
10:30:50.906   Training iter 50, batch loss 0.4808, batch acc 0.4500
10:30:51.395   Training iter 100, batch loss 0.4814, batch acc 0.4476
10:30:51.864   Training iter 150, batch loss 0.4810, batch acc 0.4420
10:30:52.340   Training iter 200, batch loss 0.4815, batch acc 0.4370
10:30:52.815   Training iter 250, batch loss 0.4811, batch acc 0.4506
10:30:53.304   Training iter 300, batch loss 0.4815, batch acc 0.4352
10:30:53.806   Training iter 350, batch loss 0.4810, batch acc 0.4496
10:30:54.284   Training iter 400, batch loss 0.4806, batch acc 0.4650
10:30:54.753   Training iter 450, batch loss 0.4812, batch acc 0.4416
10:30:55.217   Training iter 500, batch loss 0.4811, batch acc 0.4528
10:30:55.661   Training iter 550, batch loss 0.4807, batch acc 0.4618
10:30:56.125   Training iter 600, batch loss 0.4807, batch acc 0.4526
10:30:56.126 Training @ 87 epoch...
10:30:56.582   Training iter 50, batch loss 0.4809, batch acc 0.4536
10:30:57.038   Training iter 100, batch loss 0.4809, batch acc 0.4426
10:30:57.486   Training iter 150, batch loss 0.4810, batch acc 0.4448
10:30:57.941   Training iter 200, batch loss 0.4808, batch acc 0.4432
10:30:58.401   Training iter 250, batch loss 0.4811, batch acc 0.4560
10:30:58.854   Training iter 300, batch loss 0.4808, batch acc 0.4464
10:30:59.302   Training iter 350, batch loss 0.4807, batch acc 0.4588
10:30:59.756   Training iter 400, batch loss 0.4802, batch acc 0.4516
10:31:00.224   Training iter 450, batch loss 0.4806, batch acc 0.4638
10:31:00.692   Training iter 500, batch loss 0.4803, batch acc 0.4554
10:31:01.193   Training iter 550, batch loss 0.4811, batch acc 0.4476
10:31:01.744   Training iter 600, batch loss 0.4804, batch acc 0.4462
10:31:01.746 Training @ 88 epoch...
10:31:02.300   Training iter 50, batch loss 0.4804, batch acc 0.4582
10:31:02.806   Training iter 100, batch loss 0.4802, batch acc 0.4526
10:31:03.330   Training iter 150, batch loss 0.4803, batch acc 0.4598
10:31:03.855   Training iter 200, batch loss 0.4805, batch acc 0.4514
10:31:04.384   Training iter 250, batch loss 0.4803, batch acc 0.4486
10:31:04.895   Training iter 300, batch loss 0.4808, batch acc 0.4416
10:31:05.389   Training iter 350, batch loss 0.4806, batch acc 0.4382
10:31:05.887   Training iter 400, batch loss 0.4802, batch acc 0.4554
10:31:06.388   Training iter 450, batch loss 0.4801, batch acc 0.4546
10:31:06.879   Training iter 500, batch loss 0.4807, batch acc 0.4522
10:31:07.383   Training iter 550, batch loss 0.4801, batch acc 0.4610
10:31:07.841   Training iter 600, batch loss 0.4804, batch acc 0.4580
10:31:07.843 Training @ 89 epoch...
10:31:08.314   Training iter 50, batch loss 0.4804, batch acc 0.4458
10:31:08.764   Training iter 100, batch loss 0.4804, batch acc 0.4648
10:31:09.242   Training iter 150, batch loss 0.4802, batch acc 0.4524
10:31:09.694   Training iter 200, batch loss 0.4801, batch acc 0.4560
10:31:10.155   Training iter 250, batch loss 0.4805, batch acc 0.4504
10:31:10.614   Training iter 300, batch loss 0.4801, batch acc 0.4486
10:31:11.066   Training iter 350, batch loss 0.4802, batch acc 0.4532
10:31:11.523   Training iter 400, batch loss 0.4797, batch acc 0.4466
10:31:11.976   Training iter 450, batch loss 0.4802, batch acc 0.4548
10:31:12.444   Training iter 500, batch loss 0.4795, batch acc 0.4686
10:31:12.902   Training iter 550, batch loss 0.4799, batch acc 0.4550
10:31:13.354   Training iter 600, batch loss 0.4797, batch acc 0.4628
10:31:13.356 Training @ 90 epoch...
10:31:13.805   Training iter 50, batch loss 0.4798, batch acc 0.4626
10:31:14.268   Training iter 100, batch loss 0.4802, batch acc 0.4526
10:31:14.738   Training iter 150, batch loss 0.4798, batch acc 0.4608
10:31:15.206   Training iter 200, batch loss 0.4799, batch acc 0.4554
10:31:15.647   Training iter 250, batch loss 0.4796, batch acc 0.4628
10:31:16.097   Training iter 300, batch loss 0.4802, batch acc 0.4474
10:31:16.552   Training iter 350, batch loss 0.4796, batch acc 0.4518
10:31:16.997   Training iter 400, batch loss 0.4798, batch acc 0.4586
10:31:17.488   Training iter 450, batch loss 0.4796, batch acc 0.4554
10:31:17.996   Training iter 500, batch loss 0.4793, batch acc 0.4618
10:31:18.511   Training iter 550, batch loss 0.4797, batch acc 0.4584
10:31:19.021   Training iter 600, batch loss 0.4796, batch acc 0.4572
10:31:19.023 Testing @ 90 epoch...
10:31:19.057     Testing, total mean loss 0.47943, total acc 0.46630
10:31:19.057 Training @ 91 epoch...
10:31:19.550   Training iter 50, batch loss 0.4794, batch acc 0.4660
10:31:20.050   Training iter 100, batch loss 0.4797, batch acc 0.4500
10:31:20.551   Training iter 150, batch loss 0.4796, batch acc 0.4636
10:31:21.052   Training iter 200, batch loss 0.4796, batch acc 0.4484
10:31:21.553   Training iter 250, batch loss 0.4789, batch acc 0.4838
10:31:22.057   Training iter 300, batch loss 0.4799, batch acc 0.4558
10:31:22.570   Training iter 350, batch loss 0.4793, batch acc 0.4566
10:31:23.075   Training iter 400, batch loss 0.4794, batch acc 0.4510
10:31:23.571   Training iter 450, batch loss 0.4795, batch acc 0.4584
10:31:24.045   Training iter 500, batch loss 0.4798, batch acc 0.4536
10:31:24.526   Training iter 550, batch loss 0.4791, batch acc 0.4698
10:31:25.001   Training iter 600, batch loss 0.4793, batch acc 0.4518
10:31:25.003 Training @ 92 epoch...
10:31:25.498   Training iter 50, batch loss 0.4795, batch acc 0.4578
10:31:26.008   Training iter 100, batch loss 0.4792, batch acc 0.4692
10:31:26.535   Training iter 150, batch loss 0.4795, batch acc 0.4524
10:31:27.042   Training iter 200, batch loss 0.4792, batch acc 0.4670
10:31:27.551   Training iter 250, batch loss 0.4793, batch acc 0.4588
10:31:28.051   Training iter 300, batch loss 0.4792, batch acc 0.4624
10:31:28.556   Training iter 350, batch loss 0.4789, batch acc 0.4604
10:31:29.069   Training iter 400, batch loss 0.4793, batch acc 0.4472
10:31:29.563   Training iter 450, batch loss 0.4786, batch acc 0.4664
10:31:30.062   Training iter 500, batch loss 0.4787, batch acc 0.4658
10:31:30.574   Training iter 550, batch loss 0.4795, batch acc 0.4648
10:31:31.086   Training iter 600, batch loss 0.4791, batch acc 0.4626
10:31:31.088 Training @ 93 epoch...
10:31:31.593   Training iter 50, batch loss 0.4789, batch acc 0.4662
10:31:32.105   Training iter 100, batch loss 0.4793, batch acc 0.4538
10:31:32.618   Training iter 150, batch loss 0.4790, batch acc 0.4576
10:31:33.123   Training iter 200, batch loss 0.4790, batch acc 0.4736
10:31:33.607   Training iter 250, batch loss 0.4788, batch acc 0.4638
10:31:34.110   Training iter 300, batch loss 0.4789, batch acc 0.4634
10:31:34.614   Training iter 350, batch loss 0.4787, batch acc 0.4694
10:31:35.127   Training iter 400, batch loss 0.4784, batch acc 0.4562
10:31:35.619   Training iter 450, batch loss 0.4788, batch acc 0.4658
10:31:36.116   Training iter 500, batch loss 0.4790, batch acc 0.4596
10:31:36.600   Training iter 550, batch loss 0.4788, batch acc 0.4730
10:31:37.098   Training iter 600, batch loss 0.4788, batch acc 0.4546
10:31:37.100 Training @ 94 epoch...
10:31:37.620   Training iter 50, batch loss 0.4784, batch acc 0.4672
10:31:38.140   Training iter 100, batch loss 0.4782, batch acc 0.4652
10:31:38.661   Training iter 150, batch loss 0.4784, batch acc 0.4614
10:31:39.155   Training iter 200, batch loss 0.4786, batch acc 0.4680
10:31:39.652   Training iter 250, batch loss 0.4789, batch acc 0.4698
10:31:40.126   Training iter 300, batch loss 0.4787, batch acc 0.4624
10:31:40.658   Training iter 350, batch loss 0.4788, batch acc 0.4706
10:31:41.191   Training iter 400, batch loss 0.4784, batch acc 0.4706
10:31:41.719   Training iter 450, batch loss 0.4784, batch acc 0.4690
10:31:42.224   Training iter 500, batch loss 0.4786, batch acc 0.4680
10:31:42.716   Training iter 550, batch loss 0.4788, batch acc 0.4534
10:31:43.216   Training iter 600, batch loss 0.4789, batch acc 0.4574
10:31:43.218 Training @ 95 epoch...
10:31:43.681   Training iter 50, batch loss 0.4782, batch acc 0.4678
10:31:44.143   Training iter 100, batch loss 0.4786, batch acc 0.4706
10:31:44.638   Training iter 150, batch loss 0.4785, batch acc 0.4668
10:31:45.142   Training iter 200, batch loss 0.4783, batch acc 0.4784
10:31:45.658   Training iter 250, batch loss 0.4781, batch acc 0.4650
10:31:46.160   Training iter 300, batch loss 0.4783, batch acc 0.4722
10:31:46.665   Training iter 350, batch loss 0.4789, batch acc 0.4546
10:31:47.157   Training iter 400, batch loss 0.4781, batch acc 0.4700
10:31:47.658   Training iter 450, batch loss 0.4780, batch acc 0.4684
10:31:48.169   Training iter 500, batch loss 0.4781, batch acc 0.4714
10:31:48.669   Training iter 550, batch loss 0.4784, batch acc 0.4690
10:31:49.177   Training iter 600, batch loss 0.4784, batch acc 0.4578
10:31:49.178 Testing @ 95 epoch...
10:31:49.213     Testing, total mean loss 0.47801, total acc 0.47610
10:31:49.213 Training @ 96 epoch...
10:31:49.741   Training iter 50, batch loss 0.4777, batch acc 0.4756
10:31:50.286   Training iter 100, batch loss 0.4782, batch acc 0.4768
10:31:50.827   Training iter 150, batch loss 0.4781, batch acc 0.4668
10:31:51.374   Training iter 200, batch loss 0.4779, batch acc 0.4664
10:31:51.926   Training iter 250, batch loss 0.4778, batch acc 0.4774
10:31:52.473   Training iter 300, batch loss 0.4784, batch acc 0.4616
10:31:53.019   Training iter 350, batch loss 0.4779, batch acc 0.4800
10:31:53.565   Training iter 400, batch loss 0.4781, batch acc 0.4616
10:31:54.107   Training iter 450, batch loss 0.4782, batch acc 0.4736
10:31:54.642   Training iter 500, batch loss 0.4780, batch acc 0.4630
10:31:55.175   Training iter 550, batch loss 0.4784, batch acc 0.4618
10:31:55.694   Training iter 600, batch loss 0.4781, batch acc 0.4672
10:31:55.696 Training @ 97 epoch...
10:31:56.186   Training iter 50, batch loss 0.4778, batch acc 0.4676
10:31:56.665   Training iter 100, batch loss 0.4782, batch acc 0.4758
10:31:57.151   Training iter 150, batch loss 0.4779, batch acc 0.4752
10:31:57.626   Training iter 200, batch loss 0.4780, batch acc 0.4742
10:31:58.110   Training iter 250, batch loss 0.4776, batch acc 0.4710
10:31:58.582   Training iter 300, batch loss 0.4779, batch acc 0.4748
10:31:59.055   Training iter 350, batch loss 0.4777, batch acc 0.4656
10:31:59.513   Training iter 400, batch loss 0.4775, batch acc 0.4720
10:31:59.964   Training iter 450, batch loss 0.4779, batch acc 0.4620
10:32:00.433   Training iter 500, batch loss 0.4776, batch acc 0.4726
10:32:00.892   Training iter 550, batch loss 0.4779, batch acc 0.4692
10:32:01.371   Training iter 600, batch loss 0.4776, batch acc 0.4740
10:32:01.373 Training @ 98 epoch...
10:32:01.891   Training iter 50, batch loss 0.4780, batch acc 0.4684
10:32:02.441   Training iter 100, batch loss 0.4780, batch acc 0.4592
10:32:02.978   Training iter 150, batch loss 0.4771, batch acc 0.4828
10:32:03.510   Training iter 200, batch loss 0.4771, batch acc 0.4910
10:32:04.042   Training iter 250, batch loss 0.4779, batch acc 0.4718
10:32:04.581   Training iter 300, batch loss 0.4777, batch acc 0.4700
10:32:05.123   Training iter 350, batch loss 0.4781, batch acc 0.4626
10:32:05.656   Training iter 400, batch loss 0.4773, batch acc 0.4770
10:32:06.165   Training iter 450, batch loss 0.4776, batch acc 0.4674
10:32:06.695   Training iter 500, batch loss 0.4770, batch acc 0.4776
10:32:07.246   Training iter 550, batch loss 0.4774, batch acc 0.4670
10:32:07.759   Training iter 600, batch loss 0.4774, batch acc 0.4812
10:32:07.761 Training @ 99 epoch...
10:32:08.254   Training iter 50, batch loss 0.4774, batch acc 0.4732
10:32:08.712   Training iter 100, batch loss 0.4772, batch acc 0.4790
10:32:09.187   Training iter 150, batch loss 0.4773, batch acc 0.4642
10:32:09.664   Training iter 200, batch loss 0.4771, batch acc 0.4952
10:32:10.135   Training iter 250, batch loss 0.4776, batch acc 0.4828
10:32:10.618   Training iter 300, batch loss 0.4769, batch acc 0.4834
10:32:11.119   Training iter 350, batch loss 0.4776, batch acc 0.4716
10:32:11.619   Training iter 400, batch loss 0.4772, batch acc 0.4722
10:32:12.122   Training iter 450, batch loss 0.4769, batch acc 0.4670
10:32:12.632   Training iter 500, batch loss 0.4776, batch acc 0.4666
10:32:13.135   Training iter 550, batch loss 0.4775, batch acc 0.4732
10:32:13.632   Training iter 600, batch loss 0.4775, batch acc 0.4684
10:32:13.634 Training @ 100 epoch...
10:32:14.130   Training iter 50, batch loss 0.4774, batch acc 0.4876
10:32:14.637   Training iter 100, batch loss 0.4774, batch acc 0.4692
10:32:15.132   Training iter 150, batch loss 0.4768, batch acc 0.4764
10:32:15.620   Training iter 200, batch loss 0.4773, batch acc 0.4760
10:32:16.104   Training iter 250, batch loss 0.4775, batch acc 0.4748
10:32:16.582   Training iter 300, batch loss 0.4770, batch acc 0.4710
10:32:17.052   Training iter 350, batch loss 0.4771, batch acc 0.4662
10:32:17.528   Training iter 400, batch loss 0.4769, batch acc 0.4818
10:32:18.010   Training iter 450, batch loss 0.4768, batch acc 0.4854
10:32:18.490   Training iter 500, batch loss 0.4771, batch acc 0.4754
10:32:18.964   Training iter 550, batch loss 0.4767, batch acc 0.4854
10:32:19.438   Training iter 600, batch loss 0.4770, batch acc 0.4672
10:32:19.439 Testing @ 100 epoch...
10:32:19.473     Testing, total mean loss 0.47677, total acc 0.48540
10:32:19.474 Plot @ 100 epoch...
10:32:19.474 Training @ 101 epoch...
10:32:19.941   Training iter 50, batch loss 0.4769, batch acc 0.4694
10:32:20.420   Training iter 100, batch loss 0.4769, batch acc 0.4766
10:32:20.895   Training iter 150, batch loss 0.4771, batch acc 0.4724
10:32:21.363   Training iter 200, batch loss 0.4770, batch acc 0.4742
10:32:21.841   Training iter 250, batch loss 0.4771, batch acc 0.4812
10:32:22.318   Training iter 300, batch loss 0.4768, batch acc 0.4826
10:32:22.791   Training iter 350, batch loss 0.4771, batch acc 0.4776
10:32:23.281   Training iter 400, batch loss 0.4766, batch acc 0.4762
10:32:23.760   Training iter 450, batch loss 0.4769, batch acc 0.4802
10:32:24.258   Training iter 500, batch loss 0.4764, batch acc 0.4734
10:32:24.747   Training iter 550, batch loss 0.4767, batch acc 0.4878
10:32:25.252   Training iter 600, batch loss 0.4767, batch acc 0.4836
10:32:25.254 Training @ 102 epoch...
10:32:25.742   Training iter 50, batch loss 0.4769, batch acc 0.4800
10:32:26.246   Training iter 100, batch loss 0.4766, batch acc 0.4692
10:32:26.742   Training iter 150, batch loss 0.4767, batch acc 0.4806
10:32:27.241   Training iter 200, batch loss 0.4764, batch acc 0.4878
10:32:27.745   Training iter 250, batch loss 0.4764, batch acc 0.4824
10:32:28.245   Training iter 300, batch loss 0.4768, batch acc 0.4792
10:32:28.734   Training iter 350, batch loss 0.4768, batch acc 0.4736
10:32:29.226   Training iter 400, batch loss 0.4767, batch acc 0.4736
10:32:29.723   Training iter 450, batch loss 0.4768, batch acc 0.4784
10:32:30.242   Training iter 500, batch loss 0.4765, batch acc 0.4794
10:32:30.741   Training iter 550, batch loss 0.4766, batch acc 0.4868
10:32:31.239   Training iter 600, batch loss 0.4764, batch acc 0.4820
10:32:31.240 Training @ 103 epoch...
10:32:31.744   Training iter 50, batch loss 0.4763, batch acc 0.4858
10:32:32.251   Training iter 100, batch loss 0.4767, batch acc 0.4706
10:32:32.734   Training iter 150, batch loss 0.4763, batch acc 0.4824
10:32:33.216   Training iter 200, batch loss 0.4767, batch acc 0.4748
10:32:33.682   Training iter 250, batch loss 0.4764, batch acc 0.4884
10:32:34.159   Training iter 300, batch loss 0.4765, batch acc 0.4886
10:32:34.651   Training iter 350, batch loss 0.4764, batch acc 0.4742
10:32:35.157   Training iter 400, batch loss 0.4761, batch acc 0.4828
10:32:35.706   Training iter 450, batch loss 0.4762, batch acc 0.4760
10:32:36.258   Training iter 500, batch loss 0.4768, batch acc 0.4806
10:32:36.808   Training iter 550, batch loss 0.4765, batch acc 0.4766
10:32:37.317   Training iter 600, batch loss 0.4761, batch acc 0.4904
10:32:37.319 Training @ 104 epoch...
10:32:37.810   Training iter 50, batch loss 0.4762, batch acc 0.4824
10:32:38.312   Training iter 100, batch loss 0.4763, batch acc 0.4652
10:32:38.825   Training iter 150, batch loss 0.4760, batch acc 0.4908
10:32:39.343   Training iter 200, batch loss 0.4765, batch acc 0.4806
10:32:39.850   Training iter 250, batch loss 0.4760, batch acc 0.4946
10:32:40.372   Training iter 300, batch loss 0.4763, batch acc 0.4772
10:32:40.865   Training iter 350, batch loss 0.4761, batch acc 0.4868
10:32:41.371   Training iter 400, batch loss 0.4766, batch acc 0.4786
10:32:41.871   Training iter 450, batch loss 0.4764, batch acc 0.4844
10:32:42.386   Training iter 500, batch loss 0.4763, batch acc 0.4812
10:32:42.891   Training iter 550, batch loss 0.4758, batch acc 0.4778
10:32:43.403   Training iter 600, batch loss 0.4759, batch acc 0.4886
10:32:43.405 Training @ 105 epoch...
10:32:43.889   Training iter 50, batch loss 0.4760, batch acc 0.4840
10:32:44.368   Training iter 100, batch loss 0.4761, batch acc 0.4878
10:32:44.845   Training iter 150, batch loss 0.4762, batch acc 0.4874
10:32:45.328   Training iter 200, batch loss 0.4761, batch acc 0.4836
10:32:45.792   Training iter 250, batch loss 0.4759, batch acc 0.4884
10:32:46.258   Training iter 300, batch loss 0.4759, batch acc 0.4798
10:32:46.718   Training iter 350, batch loss 0.4758, batch acc 0.4840
10:32:47.184   Training iter 400, batch loss 0.4760, batch acc 0.4740
10:32:47.665   Training iter 450, batch loss 0.4760, batch acc 0.4878
10:32:48.127   Training iter 500, batch loss 0.4755, batch acc 0.4872
10:32:48.557   Training iter 550, batch loss 0.4762, batch acc 0.4740
10:32:48.986   Training iter 600, batch loss 0.4762, batch acc 0.4866
10:32:48.988 Testing @ 105 epoch...
10:32:49.023     Testing, total mean loss 0.47569, total acc 0.49430
10:32:49.023 Training @ 106 epoch...
10:32:49.477   Training iter 50, batch loss 0.4759, batch acc 0.4804
10:32:49.925   Training iter 100, batch loss 0.4758, batch acc 0.4898
10:32:50.384   Training iter 150, batch loss 0.4757, batch acc 0.4820
10:32:50.832   Training iter 200, batch loss 0.4759, batch acc 0.4874
10:32:51.274   Training iter 250, batch loss 0.4759, batch acc 0.4844
10:32:51.723   Training iter 300, batch loss 0.4759, batch acc 0.4792
10:32:52.203   Training iter 350, batch loss 0.4759, batch acc 0.4906
10:32:52.673   Training iter 400, batch loss 0.4759, batch acc 0.4698
10:32:53.135   Training iter 450, batch loss 0.4757, batch acc 0.4820
10:32:53.610   Training iter 500, batch loss 0.4758, batch acc 0.4876
10:32:54.113   Training iter 550, batch loss 0.4757, batch acc 0.4932
10:32:54.629   Training iter 600, batch loss 0.4754, batch acc 0.4944
10:32:54.631 Training @ 107 epoch...
10:32:55.154   Training iter 50, batch loss 0.4756, batch acc 0.4780
10:32:55.655   Training iter 100, batch loss 0.4756, batch acc 0.4850
10:32:56.165   Training iter 150, batch loss 0.4757, batch acc 0.4742
10:32:56.696   Training iter 200, batch loss 0.4753, batch acc 0.4838
10:32:57.235   Training iter 250, batch loss 0.4757, batch acc 0.4880
10:32:57.771   Training iter 300, batch loss 0.4761, batch acc 0.4814
10:32:58.313   Training iter 350, batch loss 0.4756, batch acc 0.4898
10:32:58.845   Training iter 400, batch loss 0.4756, batch acc 0.4790
10:32:59.376   Training iter 450, batch loss 0.4757, batch acc 0.4924
10:32:59.914   Training iter 500, batch loss 0.4757, batch acc 0.4984
10:33:00.436   Training iter 550, batch loss 0.4757, batch acc 0.4970
10:33:00.939   Training iter 600, batch loss 0.4749, batch acc 0.4932
10:33:00.941 Training @ 108 epoch...
10:33:01.471   Training iter 50, batch loss 0.4757, batch acc 0.4794
10:33:02.001   Training iter 100, batch loss 0.4754, batch acc 0.4856
10:33:02.544   Training iter 150, batch loss 0.4752, batch acc 0.4940
10:33:03.082   Training iter 200, batch loss 0.4752, batch acc 0.4890
10:33:03.622   Training iter 250, batch loss 0.4755, batch acc 0.4906
10:33:04.160   Training iter 300, batch loss 0.4758, batch acc 0.4860
10:33:04.698   Training iter 350, batch loss 0.4750, batch acc 0.4874
10:33:05.231   Training iter 400, batch loss 0.4753, batch acc 0.4920
10:33:05.724   Training iter 450, batch loss 0.4754, batch acc 0.4892
10:33:06.217   Training iter 500, batch loss 0.4756, batch acc 0.4796
10:33:06.717   Training iter 550, batch loss 0.4753, batch acc 0.4962
10:33:07.263   Training iter 600, batch loss 0.4755, batch acc 0.4898
10:33:07.265 Training @ 109 epoch...
10:33:07.818   Training iter 50, batch loss 0.4751, batch acc 0.4994
10:33:08.342   Training iter 100, batch loss 0.4759, batch acc 0.4884
10:33:08.836   Training iter 150, batch loss 0.4751, batch acc 0.4928
10:33:09.328   Training iter 200, batch loss 0.4752, batch acc 0.4934
10:33:09.834   Training iter 250, batch loss 0.4754, batch acc 0.4904
10:33:10.369   Training iter 300, batch loss 0.4751, batch acc 0.4802
10:33:10.892   Training iter 350, batch loss 0.4753, batch acc 0.4854
10:33:11.425   Training iter 400, batch loss 0.4750, batch acc 0.4848
10:33:11.944   Training iter 450, batch loss 0.4750, batch acc 0.4894
10:33:12.484   Training iter 500, batch loss 0.4748, batch acc 0.4878
10:33:13.015   Training iter 550, batch loss 0.4753, batch acc 0.4906
10:33:13.550   Training iter 600, batch loss 0.4754, batch acc 0.4952
10:33:13.551 Training @ 110 epoch...
10:33:14.092   Training iter 50, batch loss 0.4749, batch acc 0.4996
10:33:14.627   Training iter 100, batch loss 0.4747, batch acc 0.4904
10:33:15.168   Training iter 150, batch loss 0.4751, batch acc 0.4824
10:33:15.704   Training iter 200, batch loss 0.4754, batch acc 0.4842
10:33:16.231   Training iter 250, batch loss 0.4754, batch acc 0.4874
10:33:16.738   Training iter 300, batch loss 0.4751, batch acc 0.5012
10:33:17.247   Training iter 350, batch loss 0.4750, batch acc 0.4910
10:33:17.751   Training iter 400, batch loss 0.4750, batch acc 0.4968
10:33:18.259   Training iter 450, batch loss 0.4752, batch acc 0.4954
10:33:18.765   Training iter 500, batch loss 0.4749, batch acc 0.4850
10:33:19.264   Training iter 550, batch loss 0.4747, batch acc 0.4942
10:33:19.767   Training iter 600, batch loss 0.4750, batch acc 0.4886
10:33:19.769 Testing @ 110 epoch...
10:33:19.803     Testing, total mean loss 0.47473, total acc 0.50370
10:33:19.803 Training @ 111 epoch...
10:33:20.316   Training iter 50, batch loss 0.4745, batch acc 0.4988
10:33:20.806   Training iter 100, batch loss 0.4750, batch acc 0.4946
10:33:21.300   Training iter 150, batch loss 0.4750, batch acc 0.4838
10:33:21.818   Training iter 200, batch loss 0.4749, batch acc 0.4982
10:33:22.341   Training iter 250, batch loss 0.4746, batch acc 0.4946
10:33:22.841   Training iter 300, batch loss 0.4748, batch acc 0.4850
10:33:23.331   Training iter 350, batch loss 0.4747, batch acc 0.4872
10:33:23.826   Training iter 400, batch loss 0.4751, batch acc 0.4958
10:33:24.327   Training iter 450, batch loss 0.4750, batch acc 0.4980
10:33:24.824   Training iter 500, batch loss 0.4750, batch acc 0.4928
10:33:25.328   Training iter 550, batch loss 0.4748, batch acc 0.4846
10:33:25.819   Training iter 600, batch loss 0.4748, batch acc 0.4952
10:33:25.820 Training @ 112 epoch...
10:33:26.331   Training iter 50, batch loss 0.4745, batch acc 0.4978
10:33:26.864   Training iter 100, batch loss 0.4748, batch acc 0.4980
10:33:27.409   Training iter 150, batch loss 0.4747, batch acc 0.4940
10:33:27.936   Training iter 200, batch loss 0.4751, batch acc 0.4970
10:33:28.462   Training iter 250, batch loss 0.4747, batch acc 0.4874
10:33:28.989   Training iter 300, batch loss 0.4742, batch acc 0.4976
10:33:29.525   Training iter 350, batch loss 0.4752, batch acc 0.4892
10:33:30.058   Training iter 400, batch loss 0.4747, batch acc 0.4842
10:33:30.566   Training iter 450, batch loss 0.4745, batch acc 0.4934
10:33:31.073   Training iter 500, batch loss 0.4747, batch acc 0.5036
10:33:31.596   Training iter 550, batch loss 0.4745, batch acc 0.4986
10:33:32.159   Training iter 600, batch loss 0.4747, batch acc 0.4890
10:33:32.161 Training @ 113 epoch...
10:33:32.726   Training iter 50, batch loss 0.4749, batch acc 0.4992
10:33:33.272   Training iter 100, batch loss 0.4745, batch acc 0.4922
10:33:33.793   Training iter 150, batch loss 0.4747, batch acc 0.4934
10:33:34.312   Training iter 200, batch loss 0.4742, batch acc 0.4896
10:33:34.833   Training iter 250, batch loss 0.4746, batch acc 0.4964
10:33:35.351   Training iter 300, batch loss 0.4743, batch acc 0.4896
10:33:35.863   Training iter 350, batch loss 0.4741, batch acc 0.5042
10:33:36.369   Training iter 400, batch loss 0.4745, batch acc 0.5056
10:33:36.866   Training iter 450, batch loss 0.4746, batch acc 0.4872
10:33:37.365   Training iter 500, batch loss 0.4745, batch acc 0.5008
10:33:37.864   Training iter 550, batch loss 0.4747, batch acc 0.4906
10:33:38.377   Training iter 600, batch loss 0.4746, batch acc 0.4940
10:33:38.379 Training @ 114 epoch...
10:33:38.874   Training iter 50, batch loss 0.4744, batch acc 0.5072
10:33:39.375   Training iter 100, batch loss 0.4746, batch acc 0.4878
10:33:39.869   Training iter 150, batch loss 0.4742, batch acc 0.5002
10:33:40.355   Training iter 200, batch loss 0.4748, batch acc 0.4848
10:33:40.828   Training iter 250, batch loss 0.4744, batch acc 0.5004
10:33:41.302   Training iter 300, batch loss 0.4747, batch acc 0.4888
10:33:41.775   Training iter 350, batch loss 0.4744, batch acc 0.4940
10:33:42.262   Training iter 400, batch loss 0.4745, batch acc 0.4970
10:33:42.762   Training iter 450, batch loss 0.4740, batch acc 0.4954
10:33:43.264   Training iter 500, batch loss 0.4743, batch acc 0.5022
10:33:43.732   Training iter 550, batch loss 0.4740, batch acc 0.5072
10:33:44.188   Training iter 600, batch loss 0.4741, batch acc 0.4964
10:33:44.190 Training @ 115 epoch...
10:33:44.642   Training iter 50, batch loss 0.4742, batch acc 0.4882
10:33:45.098   Training iter 100, batch loss 0.4742, batch acc 0.5018
10:33:45.558   Training iter 150, batch loss 0.4741, batch acc 0.5032
10:33:46.037   Training iter 200, batch loss 0.4745, batch acc 0.4868
10:33:46.504   Training iter 250, batch loss 0.4743, batch acc 0.5084
10:33:46.972   Training iter 300, batch loss 0.4739, batch acc 0.5032
10:33:47.454   Training iter 350, batch loss 0.4740, batch acc 0.4914
10:33:47.944   Training iter 400, batch loss 0.4742, batch acc 0.4862
10:33:48.445   Training iter 450, batch loss 0.4743, batch acc 0.4872
10:33:48.933   Training iter 500, batch loss 0.4744, batch acc 0.5092
10:33:49.417   Training iter 550, batch loss 0.4743, batch acc 0.4986
10:33:49.898   Training iter 600, batch loss 0.4738, batch acc 0.5118
10:33:49.900 Testing @ 115 epoch...
10:33:49.934     Testing, total mean loss 0.47389, total acc 0.50980
10:33:49.934 Training @ 116 epoch...
10:33:50.430   Training iter 50, batch loss 0.4743, batch acc 0.5080
10:33:50.917   Training iter 100, batch loss 0.4741, batch acc 0.5002
10:33:51.391   Training iter 150, batch loss 0.4738, batch acc 0.5022
10:33:51.867   Training iter 200, batch loss 0.4741, batch acc 0.4978
10:33:52.336   Training iter 250, batch loss 0.4739, batch acc 0.4990
10:33:52.780   Training iter 300, batch loss 0.4740, batch acc 0.4934
10:33:53.247   Training iter 350, batch loss 0.4742, batch acc 0.5058
10:33:53.707   Training iter 400, batch loss 0.4740, batch acc 0.5052
10:33:54.158   Training iter 450, batch loss 0.4740, batch acc 0.4886
10:33:54.615   Training iter 500, batch loss 0.4738, batch acc 0.5002
10:33:55.078   Training iter 550, batch loss 0.4741, batch acc 0.4948
10:33:55.566   Training iter 600, batch loss 0.4740, batch acc 0.4980
10:33:55.568 Training @ 117 epoch...
10:33:56.052   Training iter 50, batch loss 0.4740, batch acc 0.4986
10:33:56.538   Training iter 100, batch loss 0.4740, batch acc 0.4902
10:33:57.022   Training iter 150, batch loss 0.4737, batch acc 0.5064
10:33:57.518   Training iter 200, batch loss 0.4744, batch acc 0.4796
10:33:58.012   Training iter 250, batch loss 0.4737, batch acc 0.5108
10:33:58.505   Training iter 300, batch loss 0.4739, batch acc 0.5070
10:33:59.009   Training iter 350, batch loss 0.4741, batch acc 0.5034
10:33:59.490   Training iter 400, batch loss 0.4737, batch acc 0.5054
10:33:59.981   Training iter 450, batch loss 0.4739, batch acc 0.5006
10:34:00.484   Training iter 500, batch loss 0.4738, batch acc 0.5042
10:34:01.001   Training iter 550, batch loss 0.4737, batch acc 0.5054
10:34:01.535   Training iter 600, batch loss 0.4737, batch acc 0.4968
10:34:01.536 Training @ 118 epoch...
10:34:02.082   Training iter 50, batch loss 0.4737, batch acc 0.4942
10:34:02.582   Training iter 100, batch loss 0.4737, batch acc 0.4970
10:34:03.078   Training iter 150, batch loss 0.4740, batch acc 0.5040
10:34:03.590   Training iter 200, batch loss 0.4741, batch acc 0.4970
10:34:04.090   Training iter 250, batch loss 0.4741, batch acc 0.5074
10:34:04.581   Training iter 300, batch loss 0.4739, batch acc 0.5008
10:34:05.078   Training iter 350, batch loss 0.4736, batch acc 0.4968
10:34:05.553   Training iter 400, batch loss 0.4734, batch acc 0.5144
10:34:06.035   Training iter 450, batch loss 0.4732, batch acc 0.5080
10:34:06.519   Training iter 500, batch loss 0.4739, batch acc 0.4954
10:34:06.999   Training iter 550, batch loss 0.4737, batch acc 0.5080
10:34:07.447   Training iter 600, batch loss 0.4737, batch acc 0.5002
10:34:07.449 Training @ 119 epoch...
10:34:07.904   Training iter 50, batch loss 0.4737, batch acc 0.5024
10:34:08.407   Training iter 100, batch loss 0.4733, batch acc 0.5068
10:34:08.884   Training iter 150, batch loss 0.4737, batch acc 0.5084
10:34:09.370   Training iter 200, batch loss 0.4731, batch acc 0.5080
10:34:09.847   Training iter 250, batch loss 0.4738, batch acc 0.4988
10:34:10.309   Training iter 300, batch loss 0.4737, batch acc 0.5036
10:34:10.760   Training iter 350, batch loss 0.4736, batch acc 0.5028
10:34:11.213   Training iter 400, batch loss 0.4734, batch acc 0.5072
10:34:11.665   Training iter 450, batch loss 0.4734, batch acc 0.5046
10:34:12.130   Training iter 500, batch loss 0.4736, batch acc 0.4928
10:34:12.599   Training iter 550, batch loss 0.4741, batch acc 0.5018
10:34:13.079   Training iter 600, batch loss 0.4735, batch acc 0.4994
10:34:13.081 Training @ 120 epoch...
10:34:13.556   Training iter 50, batch loss 0.4731, batch acc 0.5078
10:34:14.017   Training iter 100, batch loss 0.4736, batch acc 0.5060
10:34:14.492   Training iter 150, batch loss 0.4737, batch acc 0.4996
10:34:14.955   Training iter 200, batch loss 0.4733, batch acc 0.5010
10:34:15.433   Training iter 250, batch loss 0.4735, batch acc 0.5086
10:34:15.900   Training iter 300, batch loss 0.4732, batch acc 0.5158
10:34:16.336   Training iter 350, batch loss 0.4733, batch acc 0.5044
10:34:16.779   Training iter 400, batch loss 0.4735, batch acc 0.5090
10:34:17.258   Training iter 450, batch loss 0.4737, batch acc 0.4948
10:34:17.744   Training iter 500, batch loss 0.4735, batch acc 0.5100
10:34:18.253   Training iter 550, batch loss 0.4733, batch acc 0.5010
10:34:18.758   Training iter 600, batch loss 0.4736, batch acc 0.5002
10:34:18.760 Testing @ 120 epoch...
10:34:18.794     Testing, total mean loss 0.47313, total acc 0.51660
10:34:18.794 Training @ 121 epoch...
10:34:19.301   Training iter 50, batch loss 0.4739, batch acc 0.4948
10:34:19.802   Training iter 100, batch loss 0.4735, batch acc 0.5020
10:34:20.297   Training iter 150, batch loss 0.4732, batch acc 0.5082
10:34:20.763   Training iter 200, batch loss 0.4735, batch acc 0.4990
10:34:21.236   Training iter 250, batch loss 0.4736, batch acc 0.5020
10:34:21.698   Training iter 300, batch loss 0.4734, batch acc 0.5086
10:34:22.169   Training iter 350, batch loss 0.4729, batch acc 0.4964
10:34:22.644   Training iter 400, batch loss 0.4735, batch acc 0.5004
10:34:23.110   Training iter 450, batch loss 0.4730, batch acc 0.5118
10:34:23.569   Training iter 500, batch loss 0.4730, batch acc 0.5174
10:34:24.025   Training iter 550, batch loss 0.4728, batch acc 0.5242
10:34:24.477   Training iter 600, batch loss 0.4733, batch acc 0.5116
10:34:24.479 Training @ 122 epoch...
10:34:24.952   Training iter 50, batch loss 0.4730, batch acc 0.5116
10:34:25.429   Training iter 100, batch loss 0.4734, batch acc 0.4996
10:34:25.899   Training iter 150, batch loss 0.4733, batch acc 0.5064
10:34:26.373   Training iter 200, batch loss 0.4730, batch acc 0.5144
10:34:26.838   Training iter 250, batch loss 0.4731, batch acc 0.5036
10:34:27.321   Training iter 300, batch loss 0.4730, batch acc 0.5072
10:34:27.784   Training iter 350, batch loss 0.4734, batch acc 0.5030
10:34:28.249   Training iter 400, batch loss 0.4728, batch acc 0.5160
10:34:28.692   Training iter 450, batch loss 0.4734, batch acc 0.5138
10:34:29.165   Training iter 500, batch loss 0.4733, batch acc 0.5112
10:34:29.702   Training iter 550, batch loss 0.4732, batch acc 0.5030
10:34:30.260   Training iter 600, batch loss 0.4728, batch acc 0.4978
10:34:30.262 Training @ 123 epoch...
10:34:30.804   Training iter 50, batch loss 0.4733, batch acc 0.4994
10:34:31.281   Training iter 100, batch loss 0.4729, batch acc 0.5106
10:34:31.729   Training iter 150, batch loss 0.4729, batch acc 0.5088
10:34:32.172   Training iter 200, batch loss 0.4727, batch acc 0.5110
10:34:32.614   Training iter 250, batch loss 0.4730, batch acc 0.5056
10:34:33.095   Training iter 300, batch loss 0.4732, batch acc 0.5220
10:34:33.582   Training iter 350, batch loss 0.4732, batch acc 0.4962
10:34:34.078   Training iter 400, batch loss 0.4732, batch acc 0.5004
10:34:34.561   Training iter 450, batch loss 0.4730, batch acc 0.5196
10:34:35.038   Training iter 500, batch loss 0.4733, batch acc 0.5156
10:34:35.526   Training iter 550, batch loss 0.4729, batch acc 0.5086
10:34:35.995   Training iter 600, batch loss 0.4729, batch acc 0.5056
10:34:35.997 Training @ 124 epoch...
10:34:36.481   Training iter 50, batch loss 0.4727, batch acc 0.5252
10:34:36.949   Training iter 100, batch loss 0.4730, batch acc 0.5064
10:34:37.429   Training iter 150, batch loss 0.4728, batch acc 0.5086
10:34:37.912   Training iter 200, batch loss 0.4731, batch acc 0.5136
10:34:38.425   Training iter 250, batch loss 0.4731, batch acc 0.5078
10:34:38.915   Training iter 300, batch loss 0.4725, batch acc 0.5092
10:34:39.391   Training iter 350, batch loss 0.4730, batch acc 0.4950
10:34:39.875   Training iter 400, batch loss 0.4730, batch acc 0.5044
10:34:40.385   Training iter 450, batch loss 0.4727, batch acc 0.5106
10:34:40.881   Training iter 500, batch loss 0.4725, batch acc 0.5184
10:34:41.367   Training iter 550, batch loss 0.4732, batch acc 0.5110
10:34:41.855   Training iter 600, batch loss 0.4730, batch acc 0.5076
10:34:41.856 Training @ 125 epoch...
10:34:42.362   Training iter 50, batch loss 0.4729, batch acc 0.5062
10:34:42.852   Training iter 100, batch loss 0.4727, batch acc 0.5154
10:34:43.342   Training iter 150, batch loss 0.4726, batch acc 0.5126
10:34:43.829   Training iter 200, batch loss 0.4727, batch acc 0.5002
10:34:44.309   Training iter 250, batch loss 0.4726, batch acc 0.5012
10:34:44.792   Training iter 300, batch loss 0.4725, batch acc 0.5098
10:34:45.282   Training iter 350, batch loss 0.4730, batch acc 0.5098
10:34:45.774   Training iter 400, batch loss 0.4728, batch acc 0.5066
10:34:46.249   Training iter 450, batch loss 0.4727, batch acc 0.5270
10:34:46.728   Training iter 500, batch loss 0.4731, batch acc 0.5144
10:34:47.194   Training iter 550, batch loss 0.4729, batch acc 0.5112
10:34:47.652   Training iter 600, batch loss 0.4727, batch acc 0.5182
10:34:47.653 Testing @ 125 epoch...
10:34:47.688     Testing, total mean loss 0.47246, total acc 0.52240
10:34:47.688 Training @ 126 epoch...
10:34:48.166   Training iter 50, batch loss 0.4726, batch acc 0.5198
10:34:48.610   Training iter 100, batch loss 0.4731, batch acc 0.5120
10:34:49.044   Training iter 150, batch loss 0.4724, batch acc 0.5094
10:34:49.506   Training iter 200, batch loss 0.4724, batch acc 0.5064
10:34:49.969   Training iter 250, batch loss 0.4729, batch acc 0.5134
10:34:50.439   Training iter 300, batch loss 0.4723, batch acc 0.5242
10:34:50.898   Training iter 350, batch loss 0.4729, batch acc 0.4938
10:34:51.362   Training iter 400, batch loss 0.4725, batch acc 0.5132
10:34:51.826   Training iter 450, batch loss 0.4729, batch acc 0.5106
10:34:52.286   Training iter 500, batch loss 0.4728, batch acc 0.5144
10:34:52.757   Training iter 550, batch loss 0.4723, batch acc 0.5196
10:34:53.229   Training iter 600, batch loss 0.4726, batch acc 0.5094
10:34:53.231 Training @ 127 epoch...
10:34:53.706   Training iter 50, batch loss 0.4727, batch acc 0.5090
10:34:54.169   Training iter 100, batch loss 0.4728, batch acc 0.5094
10:34:54.631   Training iter 150, batch loss 0.4723, batch acc 0.5148
10:34:55.121   Training iter 200, batch loss 0.4723, batch acc 0.5250
10:34:55.601   Training iter 250, batch loss 0.4726, batch acc 0.5024
10:34:56.096   Training iter 300, batch loss 0.4729, batch acc 0.5034
10:34:56.581   Training iter 350, batch loss 0.4725, batch acc 0.5018
10:34:57.068   Training iter 400, batch loss 0.4727, batch acc 0.5024
10:34:57.559   Training iter 450, batch loss 0.4725, batch acc 0.5142
10:34:58.044   Training iter 500, batch loss 0.4724, batch acc 0.5244
10:34:58.514   Training iter 550, batch loss 0.4723, batch acc 0.5176
10:34:58.982   Training iter 600, batch loss 0.4721, batch acc 0.5348
10:34:58.984 Training @ 128 epoch...
10:34:59.463   Training iter 50, batch loss 0.4722, batch acc 0.5246
10:34:59.944   Training iter 100, batch loss 0.4725, batch acc 0.5126
10:35:00.452   Training iter 150, batch loss 0.4724, batch acc 0.5100
10:35:00.947   Training iter 200, batch loss 0.4722, batch acc 0.5194
10:35:01.448   Training iter 250, batch loss 0.4727, batch acc 0.5176
10:35:01.936   Training iter 300, batch loss 0.4724, batch acc 0.5084
10:35:02.451   Training iter 350, batch loss 0.4725, batch acc 0.5172
10:35:02.964   Training iter 400, batch loss 0.4722, batch acc 0.5172
10:35:03.493   Training iter 450, batch loss 0.4724, batch acc 0.5170
10:35:04.015   Training iter 500, batch loss 0.4723, batch acc 0.5166
10:35:04.543   Training iter 550, batch loss 0.4722, batch acc 0.5044
10:35:05.075   Training iter 600, batch loss 0.4726, batch acc 0.5072
10:35:05.077 Training @ 129 epoch...
10:35:05.607   Training iter 50, batch loss 0.4720, batch acc 0.5210
10:35:06.136   Training iter 100, batch loss 0.4724, batch acc 0.5230
10:35:06.650   Training iter 150, batch loss 0.4721, batch acc 0.5202
10:35:07.197   Training iter 200, batch loss 0.4724, batch acc 0.5168
10:35:07.730   Training iter 250, batch loss 0.4723, batch acc 0.5052
10:35:08.261   Training iter 300, batch loss 0.4721, batch acc 0.5140
10:35:08.765   Training iter 350, batch loss 0.4723, batch acc 0.5160
10:35:09.262   Training iter 400, batch loss 0.4723, batch acc 0.5092
10:35:09.746   Training iter 450, batch loss 0.4725, batch acc 0.5064
10:35:10.243   Training iter 500, batch loss 0.4719, batch acc 0.5222
10:35:10.737   Training iter 550, batch loss 0.4722, batch acc 0.5122
10:35:11.240   Training iter 600, batch loss 0.4726, batch acc 0.5166
10:35:11.242 Training @ 130 epoch...
10:35:11.749   Training iter 50, batch loss 0.4721, batch acc 0.5222
10:35:12.241   Training iter 100, batch loss 0.4723, batch acc 0.5164
10:35:12.731   Training iter 150, batch loss 0.4724, batch acc 0.5162
10:35:13.230   Training iter 200, batch loss 0.4727, batch acc 0.4976
10:35:13.722   Training iter 250, batch loss 0.4722, batch acc 0.5134
10:35:14.221   Training iter 300, batch loss 0.4719, batch acc 0.5354
10:35:14.722   Training iter 350, batch loss 0.4721, batch acc 0.5072
10:35:15.243   Training iter 400, batch loss 0.4722, batch acc 0.5174
10:35:15.763   Training iter 450, batch loss 0.4721, batch acc 0.5164
10:35:16.285   Training iter 500, batch loss 0.4720, batch acc 0.5164
10:35:16.809   Training iter 550, batch loss 0.4718, batch acc 0.5216
10:35:17.337   Training iter 600, batch loss 0.4721, batch acc 0.5212
10:35:17.339 Testing @ 130 epoch...
10:35:17.374     Testing, total mean loss 0.47185, total acc 0.52850
10:35:17.374 Training @ 131 epoch...
10:35:17.886   Training iter 50, batch loss 0.4722, batch acc 0.5088
10:35:18.420   Training iter 100, batch loss 0.4718, batch acc 0.5256
10:35:18.952   Training iter 150, batch loss 0.4722, batch acc 0.5082
10:35:19.471   Training iter 200, batch loss 0.4721, batch acc 0.5210
10:35:19.997   Training iter 250, batch loss 0.4719, batch acc 0.5132
10:35:20.514   Training iter 300, batch loss 0.4720, batch acc 0.5164
10:35:21.024   Training iter 350, batch loss 0.4719, batch acc 0.5258
10:35:21.533   Training iter 400, batch loss 0.4721, batch acc 0.5148
10:35:22.042   Training iter 450, batch loss 0.4719, batch acc 0.5270
10:35:22.555   Training iter 500, batch loss 0.4720, batch acc 0.5172
10:35:23.076   Training iter 550, batch loss 0.4724, batch acc 0.5210
10:35:23.589   Training iter 600, batch loss 0.4720, batch acc 0.5158
10:35:23.591 Training @ 132 epoch...
10:35:24.112   Training iter 50, batch loss 0.4717, batch acc 0.5308
10:35:24.651   Training iter 100, batch loss 0.4723, batch acc 0.5118
10:35:25.185   Training iter 150, batch loss 0.4721, batch acc 0.5092
10:35:25.713   Training iter 200, batch loss 0.4718, batch acc 0.5256
10:35:26.222   Training iter 250, batch loss 0.4712, batch acc 0.5258
10:35:26.682   Training iter 300, batch loss 0.4721, batch acc 0.5096
10:35:27.151   Training iter 350, batch loss 0.4724, batch acc 0.5054
10:35:27.631   Training iter 400, batch loss 0.4717, batch acc 0.5254
10:35:28.175   Training iter 450, batch loss 0.4720, batch acc 0.5218
10:35:28.721   Training iter 500, batch loss 0.4718, batch acc 0.5246
10:35:29.254   Training iter 550, batch loss 0.4717, batch acc 0.5266
10:35:29.799   Training iter 600, batch loss 0.4722, batch acc 0.5112
10:35:29.801 Training @ 133 epoch...
10:35:30.337   Training iter 50, batch loss 0.4718, batch acc 0.5196
10:35:30.888   Training iter 100, batch loss 0.4716, batch acc 0.5092
10:35:31.425   Training iter 150, batch loss 0.4718, batch acc 0.5244
10:35:31.950   Training iter 200, batch loss 0.4718, batch acc 0.5224
10:35:32.478   Training iter 250, batch loss 0.4715, batch acc 0.5340
10:35:32.980   Training iter 300, batch loss 0.4716, batch acc 0.5306
10:35:33.481   Training iter 350, batch loss 0.4723, batch acc 0.5098
10:35:33.998   Training iter 400, batch loss 0.4720, batch acc 0.5156
10:35:34.507   Training iter 450, batch loss 0.4721, batch acc 0.5224
10:35:35.020   Training iter 500, batch loss 0.4719, batch acc 0.5190
10:35:35.539   Training iter 550, batch loss 0.4719, batch acc 0.5094
10:35:36.052   Training iter 600, batch loss 0.4717, batch acc 0.5230
10:35:36.054 Training @ 134 epoch...
10:35:36.571   Training iter 50, batch loss 0.4713, batch acc 0.5288
10:35:37.095   Training iter 100, batch loss 0.4719, batch acc 0.5282
10:35:37.611   Training iter 150, batch loss 0.4713, batch acc 0.5162
10:35:38.126   Training iter 200, batch loss 0.4720, batch acc 0.5168
10:35:38.637   Training iter 250, batch loss 0.4720, batch acc 0.5124
10:35:39.153   Training iter 300, batch loss 0.4715, batch acc 0.5256
10:35:39.640   Training iter 350, batch loss 0.4715, batch acc 0.5182
10:35:40.131   Training iter 400, batch loss 0.4718, batch acc 0.5204
10:35:40.612   Training iter 450, batch loss 0.4721, batch acc 0.5036
10:35:41.096   Training iter 500, batch loss 0.4715, batch acc 0.5266
10:35:41.582   Training iter 550, batch loss 0.4714, batch acc 0.5272
10:35:42.072   Training iter 600, batch loss 0.4721, batch acc 0.5264
10:35:42.074 Training @ 135 epoch...
10:35:42.546   Training iter 50, batch loss 0.4719, batch acc 0.5094
10:35:43.027   Training iter 100, batch loss 0.4716, batch acc 0.5262
10:35:43.515   Training iter 150, batch loss 0.4719, batch acc 0.5180
10:35:43.995   Training iter 200, batch loss 0.4719, batch acc 0.5128
10:35:44.490   Training iter 250, batch loss 0.4720, batch acc 0.5146
10:35:45.006   Training iter 300, batch loss 0.4713, batch acc 0.5256
10:35:45.527   Training iter 350, batch loss 0.4708, batch acc 0.5346
10:35:46.035   Training iter 400, batch loss 0.4717, batch acc 0.5192
10:35:46.533   Training iter 450, batch loss 0.4718, batch acc 0.5222
10:35:47.043   Training iter 500, batch loss 0.4715, batch acc 0.5266
10:35:47.541   Training iter 550, batch loss 0.4717, batch acc 0.5216
10:35:48.049   Training iter 600, batch loss 0.4713, batch acc 0.5348
10:35:48.051 Testing @ 135 epoch...
10:35:48.086     Testing, total mean loss 0.47130, total acc 0.53340
10:35:48.086 Training @ 136 epoch...
10:35:48.593   Training iter 50, batch loss 0.4719, batch acc 0.5114
10:35:49.065   Training iter 100, batch loss 0.4715, batch acc 0.5192
10:35:49.572   Training iter 150, batch loss 0.4713, batch acc 0.5266
10:35:50.112   Training iter 200, batch loss 0.4718, batch acc 0.5270
10:35:50.638   Training iter 250, batch loss 0.4713, batch acc 0.5298
10:35:51.159   Training iter 300, batch loss 0.4715, batch acc 0.5156
10:35:51.666   Training iter 350, batch loss 0.4718, batch acc 0.5162
10:35:52.172   Training iter 400, batch loss 0.4716, batch acc 0.5228
10:35:52.682   Training iter 450, batch loss 0.4715, batch acc 0.5258
10:35:53.181   Training iter 500, batch loss 0.4710, batch acc 0.5338
10:35:53.677   Training iter 550, batch loss 0.4716, batch acc 0.5206
10:35:54.178   Training iter 600, batch loss 0.4714, batch acc 0.5268
10:35:54.179 Training @ 137 epoch...
10:35:54.706   Training iter 50, batch loss 0.4709, batch acc 0.5314
10:35:55.249   Training iter 100, batch loss 0.4714, batch acc 0.5288
10:35:55.765   Training iter 150, batch loss 0.4716, batch acc 0.5164
10:35:56.229   Training iter 200, batch loss 0.4715, batch acc 0.5174
10:35:56.686   Training iter 250, batch loss 0.4711, batch acc 0.5220
10:35:57.147   Training iter 300, batch loss 0.4715, batch acc 0.5260
10:35:57.619   Training iter 350, batch loss 0.4718, batch acc 0.5198
10:35:58.109   Training iter 400, batch loss 0.4715, batch acc 0.5280
10:35:58.612   Training iter 450, batch loss 0.4712, batch acc 0.5284
10:35:59.104   Training iter 500, batch loss 0.4718, batch acc 0.5140
10:35:59.615   Training iter 550, batch loss 0.4714, batch acc 0.5298
10:36:00.067   Training iter 600, batch loss 0.4712, batch acc 0.5256
10:36:00.069 Training @ 138 epoch...
10:36:00.552   Training iter 50, batch loss 0.4709, batch acc 0.5258
10:36:01.046   Training iter 100, batch loss 0.4712, batch acc 0.5306
10:36:01.568   Training iter 150, batch loss 0.4713, batch acc 0.5236
10:36:02.118   Training iter 200, batch loss 0.4714, batch acc 0.5238
10:36:02.662   Training iter 250, batch loss 0.4715, batch acc 0.5316
10:36:03.206   Training iter 300, batch loss 0.4715, batch acc 0.5268
10:36:03.755   Training iter 350, batch loss 0.4712, batch acc 0.5322
10:36:04.278   Training iter 400, batch loss 0.4716, batch acc 0.5186
10:36:04.812   Training iter 450, batch loss 0.4711, batch acc 0.5360
10:36:05.320   Training iter 500, batch loss 0.4713, batch acc 0.5240
10:36:05.872   Training iter 550, batch loss 0.4711, batch acc 0.5190
10:36:06.504   Training iter 600, batch loss 0.4716, batch acc 0.5096
10:36:06.506 Training @ 139 epoch...
10:36:07.095   Training iter 50, batch loss 0.4715, batch acc 0.5212
10:36:07.653   Training iter 100, batch loss 0.4714, batch acc 0.5114
10:36:08.208   Training iter 150, batch loss 0.4712, batch acc 0.5296
10:36:08.769   Training iter 200, batch loss 0.4713, batch acc 0.5264
10:36:09.314   Training iter 250, batch loss 0.4709, batch acc 0.5228
10:36:09.868   Training iter 300, batch loss 0.4712, batch acc 0.5230
10:36:10.439   Training iter 350, batch loss 0.4713, batch acc 0.5354
10:36:10.982   Training iter 400, batch loss 0.4713, batch acc 0.5294
10:36:11.527   Training iter 450, batch loss 0.4713, batch acc 0.5358
10:36:12.069   Training iter 500, batch loss 0.4710, batch acc 0.5174
10:36:12.592   Training iter 550, batch loss 0.4712, batch acc 0.5318
10:36:13.090   Training iter 600, batch loss 0.4708, batch acc 0.5336
10:36:13.092 Training @ 140 epoch...
10:36:13.585   Training iter 50, batch loss 0.4715, batch acc 0.5178
10:36:14.081   Training iter 100, batch loss 0.4711, batch acc 0.5426
10:36:14.575   Training iter 150, batch loss 0.4714, batch acc 0.5306
10:36:15.067   Training iter 200, batch loss 0.4711, batch acc 0.5242
10:36:15.577   Training iter 250, batch loss 0.4708, batch acc 0.5286
10:36:16.091   Training iter 300, batch loss 0.4712, batch acc 0.5318
10:36:16.595   Training iter 350, batch loss 0.4712, batch acc 0.5306
10:36:17.105   Training iter 400, batch loss 0.4709, batch acc 0.5284
10:36:17.602   Training iter 450, batch loss 0.4711, batch acc 0.5340
10:36:18.118   Training iter 500, batch loss 0.4707, batch acc 0.5284
10:36:18.632   Training iter 550, batch loss 0.4708, batch acc 0.5206
10:36:19.142   Training iter 600, batch loss 0.4715, batch acc 0.5120
10:36:19.144 Testing @ 140 epoch...
10:36:19.179     Testing, total mean loss 0.47080, total acc 0.53790
10:36:19.179 Training @ 141 epoch...
10:36:19.699   Training iter 50, batch loss 0.4710, batch acc 0.5252
10:36:20.208   Training iter 100, batch loss 0.4711, batch acc 0.5190
10:36:20.722   Training iter 150, batch loss 0.4708, batch acc 0.5278
10:36:21.224   Training iter 200, batch loss 0.4710, batch acc 0.5340
10:36:21.716   Training iter 250, batch loss 0.4710, batch acc 0.5248
10:36:22.235   Training iter 300, batch loss 0.4712, batch acc 0.5236
10:36:22.761   Training iter 350, batch loss 0.4709, batch acc 0.5364
10:36:23.290   Training iter 400, batch loss 0.4711, batch acc 0.5302
10:36:23.805   Training iter 450, batch loss 0.4710, batch acc 0.5326
10:36:24.331   Training iter 500, batch loss 0.4708, batch acc 0.5286
10:36:24.852   Training iter 550, batch loss 0.4713, batch acc 0.5284
10:36:25.386   Training iter 600, batch loss 0.4709, batch acc 0.5310
10:36:25.388 Training @ 142 epoch...
10:36:25.923   Training iter 50, batch loss 0.4709, batch acc 0.5340
10:36:26.460   Training iter 100, batch loss 0.4713, batch acc 0.5198
10:36:26.990   Training iter 150, batch loss 0.4714, batch acc 0.5200
10:36:27.522   Training iter 200, batch loss 0.4709, batch acc 0.5386
10:36:28.049   Training iter 250, batch loss 0.4713, batch acc 0.5214
10:36:28.558   Training iter 300, batch loss 0.4708, batch acc 0.5372
10:36:29.056   Training iter 350, batch loss 0.4706, batch acc 0.5330
10:36:29.540   Training iter 400, batch loss 0.4707, batch acc 0.5364
10:36:30.044   Training iter 450, batch loss 0.4710, batch acc 0.5356
10:36:30.559   Training iter 500, batch loss 0.4707, batch acc 0.5228
10:36:31.064   Training iter 550, batch loss 0.4707, batch acc 0.5282
10:36:31.585   Training iter 600, batch loss 0.4708, batch acc 0.5270
10:36:31.587 Training @ 143 epoch...
10:36:32.116   Training iter 50, batch loss 0.4713, batch acc 0.5240
10:36:32.607   Training iter 100, batch loss 0.4709, batch acc 0.5300
10:36:33.167   Training iter 150, batch loss 0.4709, batch acc 0.5264
10:36:33.664   Training iter 200, batch loss 0.4709, batch acc 0.5326
10:36:34.151   Training iter 250, batch loss 0.4709, batch acc 0.5240
10:36:34.641   Training iter 300, batch loss 0.4705, batch acc 0.5392
10:36:35.131   Training iter 350, batch loss 0.4707, batch acc 0.5386
10:36:35.639   Training iter 400, batch loss 0.4711, batch acc 0.5262
10:36:36.137   Training iter 450, batch loss 0.4703, batch acc 0.5378
10:36:36.640   Training iter 500, batch loss 0.4708, batch acc 0.5370
10:36:37.140   Training iter 550, batch loss 0.4709, batch acc 0.5174
10:36:37.638   Training iter 600, batch loss 0.4709, batch acc 0.5298
10:36:37.640 Training @ 144 epoch...
10:36:38.154   Training iter 50, batch loss 0.4707, batch acc 0.5298
10:36:38.682   Training iter 100, batch loss 0.4705, batch acc 0.5398
10:36:39.206   Training iter 150, batch loss 0.4707, batch acc 0.5234
10:36:39.737   Training iter 200, batch loss 0.4709, batch acc 0.5396
10:36:40.284   Training iter 250, batch loss 0.4707, batch acc 0.5310
10:36:40.807   Training iter 300, batch loss 0.4709, batch acc 0.5218
10:36:41.334   Training iter 350, batch loss 0.4704, batch acc 0.5314
10:36:41.867   Training iter 400, batch loss 0.4709, batch acc 0.5278
10:36:42.406   Training iter 450, batch loss 0.4709, batch acc 0.5344
10:36:42.943   Training iter 500, batch loss 0.4708, batch acc 0.5324
10:36:43.478   Training iter 550, batch loss 0.4708, batch acc 0.5274
10:36:43.983   Training iter 600, batch loss 0.4708, batch acc 0.5322
10:36:43.984 Training @ 145 epoch...
10:36:44.480   Training iter 50, batch loss 0.4706, batch acc 0.5388
10:36:44.970   Training iter 100, batch loss 0.4705, batch acc 0.5364
10:36:45.482   Training iter 150, batch loss 0.4706, batch acc 0.5330
10:36:45.987   Training iter 200, batch loss 0.4708, batch acc 0.5286
10:36:46.497   Training iter 250, batch loss 0.4709, batch acc 0.5364
10:36:46.988   Training iter 300, batch loss 0.4709, batch acc 0.5274
10:36:47.477   Training iter 350, batch loss 0.4705, batch acc 0.5346
10:36:47.966   Training iter 400, batch loss 0.4704, batch acc 0.5278
10:36:48.470   Training iter 450, batch loss 0.4704, batch acc 0.5288
10:36:48.948   Training iter 500, batch loss 0.4712, batch acc 0.5138
10:36:49.434   Training iter 550, batch loss 0.4705, batch acc 0.5462
10:36:49.905   Training iter 600, batch loss 0.4707, batch acc 0.5334
10:36:49.907 Testing @ 145 epoch...
10:36:49.942     Testing, total mean loss 0.47034, total acc 0.54150
10:36:49.942 Training @ 146 epoch...
10:36:50.418   Training iter 50, batch loss 0.4706, batch acc 0.5274
10:36:50.887   Training iter 100, batch loss 0.4709, batch acc 0.5354
10:36:51.337   Training iter 150, batch loss 0.4701, batch acc 0.5302
10:36:51.784   Training iter 200, batch loss 0.4712, batch acc 0.5176
10:36:52.252   Training iter 250, batch loss 0.4709, batch acc 0.5360
10:36:52.729   Training iter 300, batch loss 0.4705, batch acc 0.5304
10:36:53.198   Training iter 350, batch loss 0.4704, batch acc 0.5376
10:36:53.695   Training iter 400, batch loss 0.4704, batch acc 0.5308
10:36:54.194   Training iter 450, batch loss 0.4706, batch acc 0.5390
10:36:54.705   Training iter 500, batch loss 0.4702, batch acc 0.5406
10:36:55.199   Training iter 550, batch loss 0.4704, batch acc 0.5416
10:36:55.702   Training iter 600, batch loss 0.4705, batch acc 0.5296
10:36:55.704 Training @ 147 epoch...
10:36:56.250   Training iter 50, batch loss 0.4707, batch acc 0.5302
10:36:56.805   Training iter 100, batch loss 0.4706, batch acc 0.5246
10:36:57.351   Training iter 150, batch loss 0.4708, batch acc 0.5246
10:36:57.910   Training iter 200, batch loss 0.4705, batch acc 0.5428
10:36:58.477   Training iter 250, batch loss 0.4705, batch acc 0.5398
10:36:59.028   Training iter 300, batch loss 0.4704, batch acc 0.5400
10:36:59.561   Training iter 350, batch loss 0.4704, batch acc 0.5410
10:37:00.055   Training iter 400, batch loss 0.4704, batch acc 0.5386
10:37:00.566   Training iter 450, batch loss 0.4701, batch acc 0.5422
10:37:01.084   Training iter 500, batch loss 0.4704, batch acc 0.5224
10:37:01.605   Training iter 550, batch loss 0.4705, batch acc 0.5246
10:37:02.159   Training iter 600, batch loss 0.4705, batch acc 0.5340
10:37:02.161 Training @ 148 epoch...
10:37:02.684   Training iter 50, batch loss 0.4702, batch acc 0.5390
10:37:03.203   Training iter 100, batch loss 0.4702, batch acc 0.5424
10:37:03.730   Training iter 150, batch loss 0.4707, batch acc 0.5250
10:37:04.258   Training iter 200, batch loss 0.4704, batch acc 0.5280
10:37:04.784   Training iter 250, batch loss 0.4708, batch acc 0.5256
10:37:05.309   Training iter 300, batch loss 0.4701, batch acc 0.5340
10:37:05.837   Training iter 350, batch loss 0.4703, batch acc 0.5436
10:37:06.359   Training iter 400, batch loss 0.4705, batch acc 0.5362
10:37:06.866   Training iter 450, batch loss 0.4706, batch acc 0.5226
10:37:07.351   Training iter 500, batch loss 0.4703, batch acc 0.5368
10:37:07.800   Training iter 550, batch loss 0.4704, batch acc 0.5348
10:37:08.250   Training iter 600, batch loss 0.4702, batch acc 0.5434
10:37:08.251 Training @ 149 epoch...
10:37:08.708   Training iter 50, batch loss 0.4703, batch acc 0.5310
10:37:09.150   Training iter 100, batch loss 0.4705, batch acc 0.5324
10:37:09.595   Training iter 150, batch loss 0.4704, batch acc 0.5312
10:37:10.052   Training iter 200, batch loss 0.4702, batch acc 0.5392
10:37:10.508   Training iter 250, batch loss 0.4700, batch acc 0.5394
10:37:10.963   Training iter 300, batch loss 0.4703, batch acc 0.5340
10:37:11.436   Training iter 350, batch loss 0.4702, batch acc 0.5360
10:37:11.900   Training iter 400, batch loss 0.4705, batch acc 0.5328
10:37:12.386   Training iter 450, batch loss 0.4704, batch acc 0.5356
10:37:12.886   Training iter 500, batch loss 0.4704, batch acc 0.5320
10:37:13.378   Training iter 550, batch loss 0.4703, batch acc 0.5372
10:37:13.878   Training iter 600, batch loss 0.4704, batch acc 0.5378
10:37:13.880 Training @ 150 epoch...
10:37:14.380   Training iter 50, batch loss 0.4700, batch acc 0.5504
10:37:14.889   Training iter 100, batch loss 0.4702, batch acc 0.5368
10:37:15.389   Training iter 150, batch loss 0.4703, batch acc 0.5336
10:37:15.893   Training iter 200, batch loss 0.4704, batch acc 0.5220
10:37:16.391   Training iter 250, batch loss 0.4702, batch acc 0.5330
10:37:16.911   Training iter 300, batch loss 0.4700, batch acc 0.5406
10:37:17.433   Training iter 350, batch loss 0.4703, batch acc 0.5294
10:37:17.942   Training iter 400, batch loss 0.4703, batch acc 0.5256
10:37:18.470   Training iter 450, batch loss 0.4703, batch acc 0.5310
10:37:19.007   Training iter 500, batch loss 0.4702, batch acc 0.5468
10:37:19.535   Training iter 550, batch loss 0.4702, batch acc 0.5488
10:37:20.060   Training iter 600, batch loss 0.4706, batch acc 0.5326
10:37:20.062 Testing @ 150 epoch...
10:37:20.099     Testing, total mean loss 0.46992, total acc 0.54610
10:37:20.099 Training @ 151 epoch...
10:37:20.624   Training iter 50, batch loss 0.4701, batch acc 0.5298
10:37:21.121   Training iter 100, batch loss 0.4703, batch acc 0.5356
10:37:21.611   Training iter 150, batch loss 0.4703, batch acc 0.5302
10:37:22.129   Training iter 200, batch loss 0.4705, batch acc 0.5268
10:37:22.637   Training iter 250, batch loss 0.4699, batch acc 0.5524
10:37:23.137   Training iter 300, batch loss 0.4702, batch acc 0.5332
10:37:23.618   Training iter 350, batch loss 0.4699, batch acc 0.5456
10:37:24.093   Training iter 400, batch loss 0.4703, batch acc 0.5326
10:37:24.596   Training iter 450, batch loss 0.4699, batch acc 0.5426
10:37:25.099   Training iter 500, batch loss 0.4707, batch acc 0.5336
10:37:25.591   Training iter 550, batch loss 0.4697, batch acc 0.5374
10:37:26.089   Training iter 600, batch loss 0.4701, batch acc 0.5422
10:37:26.091 Training @ 152 epoch...
10:37:26.606   Training iter 50, batch loss 0.4699, batch acc 0.5386
10:37:27.110   Training iter 100, batch loss 0.4702, batch acc 0.5370
10:37:27.601   Training iter 150, batch loss 0.4703, batch acc 0.5314
10:37:28.198   Training iter 200, batch loss 0.4700, batch acc 0.5334
10:37:28.775   Training iter 250, batch loss 0.4704, batch acc 0.5402
10:37:29.249   Training iter 300, batch loss 0.4702, batch acc 0.5456
10:37:29.735   Training iter 350, batch loss 0.4699, batch acc 0.5390
10:37:30.232   Training iter 400, batch loss 0.4699, batch acc 0.5358
10:37:30.724   Training iter 450, batch loss 0.4699, batch acc 0.5326
10:37:31.231   Training iter 500, batch loss 0.4701, batch acc 0.5368
10:37:31.754   Training iter 550, batch loss 0.4701, batch acc 0.5478
10:37:32.247   Training iter 600, batch loss 0.4701, batch acc 0.5334
10:37:32.249 Training @ 153 epoch...
10:37:32.730   Training iter 50, batch loss 0.4701, batch acc 0.5276
10:37:33.240   Training iter 100, batch loss 0.4700, batch acc 0.5368
10:37:33.746   Training iter 150, batch loss 0.4701, batch acc 0.5288
10:37:34.252   Training iter 200, batch loss 0.4701, batch acc 0.5440
10:37:34.764   Training iter 250, batch loss 0.4700, batch acc 0.5464
10:37:35.263   Training iter 300, batch loss 0.4698, batch acc 0.5442
10:37:35.779   Training iter 350, batch loss 0.4699, batch acc 0.5582
10:37:36.280   Training iter 400, batch loss 0.4700, batch acc 0.5430
10:37:36.769   Training iter 450, batch loss 0.4699, batch acc 0.5454
10:37:37.245   Training iter 500, batch loss 0.4699, batch acc 0.5334
10:37:37.711   Training iter 550, batch loss 0.4700, batch acc 0.5360
10:37:38.195   Training iter 600, batch loss 0.4703, batch acc 0.5206
10:37:38.196 Training @ 154 epoch...
10:37:38.687   Training iter 50, batch loss 0.4698, batch acc 0.5424
10:37:39.156   Training iter 100, batch loss 0.4702, batch acc 0.5474
10:37:39.635   Training iter 150, batch loss 0.4698, batch acc 0.5394
10:37:40.139   Training iter 200, batch loss 0.4700, batch acc 0.5390
10:37:40.642   Training iter 250, batch loss 0.4698, batch acc 0.5330
10:37:41.139   Training iter 300, batch loss 0.4701, batch acc 0.5354
10:37:41.614   Training iter 350, batch loss 0.4699, batch acc 0.5366
10:37:42.099   Training iter 400, batch loss 0.4701, batch acc 0.5418
10:37:42.578   Training iter 450, batch loss 0.4695, batch acc 0.5410
10:37:43.063   Training iter 500, batch loss 0.4699, batch acc 0.5484
10:37:43.554   Training iter 550, batch loss 0.4702, batch acc 0.5336
10:37:44.039   Training iter 600, batch loss 0.4698, batch acc 0.5366
10:37:44.040 Training @ 155 epoch...
10:37:44.523   Training iter 50, batch loss 0.4698, batch acc 0.5474
10:37:45.008   Training iter 100, batch loss 0.4697, batch acc 0.5200
10:37:45.512   Training iter 150, batch loss 0.4703, batch acc 0.5346
10:37:46.017   Training iter 200, batch loss 0.4697, batch acc 0.5358
10:37:46.506   Training iter 250, batch loss 0.4698, batch acc 0.5472
10:37:46.982   Training iter 300, batch loss 0.4694, batch acc 0.5466
10:37:47.447   Training iter 350, batch loss 0.4702, batch acc 0.5364
10:37:47.890   Training iter 400, batch loss 0.4700, batch acc 0.5448
10:37:48.353   Training iter 450, batch loss 0.4699, batch acc 0.5378
10:37:48.811   Training iter 500, batch loss 0.4699, batch acc 0.5510
10:37:49.259   Training iter 550, batch loss 0.4699, batch acc 0.5360
10:37:49.710   Training iter 600, batch loss 0.4696, batch acc 0.5504
10:37:49.712 Testing @ 155 epoch...
10:37:49.747     Testing, total mean loss 0.46953, total acc 0.55030
10:37:49.747 Training @ 156 epoch...
10:37:50.197   Training iter 50, batch loss 0.4697, batch acc 0.5404
10:37:50.651   Training iter 100, batch loss 0.4694, batch acc 0.5494
10:37:51.123   Training iter 150, batch loss 0.4699, batch acc 0.5494
10:37:51.602   Training iter 200, batch loss 0.4699, batch acc 0.5268
10:37:52.061   Training iter 250, batch loss 0.4700, batch acc 0.5306
10:37:52.510   Training iter 300, batch loss 0.4697, batch acc 0.5480
10:37:52.948   Training iter 350, batch loss 0.4701, batch acc 0.5304
10:37:53.401   Training iter 400, batch loss 0.4701, batch acc 0.5344
10:37:53.862   Training iter 450, batch loss 0.4697, batch acc 0.5488
10:37:54.320   Training iter 500, batch loss 0.4695, batch acc 0.5440
10:37:54.786   Training iter 550, batch loss 0.4696, batch acc 0.5532
10:37:55.275   Training iter 600, batch loss 0.4698, batch acc 0.5368
10:37:55.277 Training @ 157 epoch...
10:37:55.820   Training iter 50, batch loss 0.4699, batch acc 0.5492
10:37:56.362   Training iter 100, batch loss 0.4700, batch acc 0.5376
10:37:56.921   Training iter 150, batch loss 0.4697, batch acc 0.5480
10:37:57.439   Training iter 200, batch loss 0.4694, batch acc 0.5340
10:37:57.926   Training iter 250, batch loss 0.4698, batch acc 0.5378
10:37:58.450   Training iter 300, batch loss 0.4696, batch acc 0.5454
10:37:58.972   Training iter 350, batch loss 0.4695, batch acc 0.5434
10:37:59.492   Training iter 400, batch loss 0.4698, batch acc 0.5382
10:37:59.995   Training iter 450, batch loss 0.4697, batch acc 0.5420
10:38:00.515   Training iter 500, batch loss 0.4696, batch acc 0.5440
10:38:01.046   Training iter 550, batch loss 0.4699, batch acc 0.5386
10:38:01.612   Training iter 600, batch loss 0.4696, batch acc 0.5426
10:38:01.614 Training @ 158 epoch...
10:38:02.173   Training iter 50, batch loss 0.4698, batch acc 0.5396
10:38:02.734   Training iter 100, batch loss 0.4696, batch acc 0.5464
10:38:03.344   Training iter 150, batch loss 0.4695, batch acc 0.5358
10:38:03.897   Training iter 200, batch loss 0.4693, batch acc 0.5506
10:38:04.427   Training iter 250, batch loss 0.4694, batch acc 0.5518
10:38:04.936   Training iter 300, batch loss 0.4696, batch acc 0.5452
10:38:05.476   Training iter 350, batch loss 0.4698, batch acc 0.5402
10:38:06.025   Training iter 400, batch loss 0.4695, batch acc 0.5396
10:38:06.543   Training iter 450, batch loss 0.4697, batch acc 0.5456
10:38:07.029   Training iter 500, batch loss 0.4699, batch acc 0.5480
10:38:07.529   Training iter 550, batch loss 0.4701, batch acc 0.5320
10:38:08.020   Training iter 600, batch loss 0.4696, batch acc 0.5352
10:38:08.022 Training @ 159 epoch...
10:38:08.527   Training iter 50, batch loss 0.4696, batch acc 0.5452
10:38:09.031   Training iter 100, batch loss 0.4697, batch acc 0.5316
10:38:09.536   Training iter 150, batch loss 0.4696, batch acc 0.5422
10:38:10.042   Training iter 200, batch loss 0.4697, batch acc 0.5412
10:38:10.558   Training iter 250, batch loss 0.4696, batch acc 0.5482
10:38:11.071   Training iter 300, batch loss 0.4699, batch acc 0.5386
10:38:11.581   Training iter 350, batch loss 0.4697, batch acc 0.5370
10:38:12.097   Training iter 400, batch loss 0.4695, batch acc 0.5380
10:38:12.602   Training iter 450, batch loss 0.4691, batch acc 0.5496
10:38:13.104   Training iter 500, batch loss 0.4695, batch acc 0.5476
10:38:13.603   Training iter 550, batch loss 0.4697, batch acc 0.5434
10:38:14.105   Training iter 600, batch loss 0.4694, batch acc 0.5538
10:38:14.107 Training @ 160 epoch...
10:38:14.645   Training iter 50, batch loss 0.4698, batch acc 0.5370
10:38:15.186   Training iter 100, batch loss 0.4695, batch acc 0.5440
10:38:15.726   Training iter 150, batch loss 0.4691, batch acc 0.5450
10:38:16.259   Training iter 200, batch loss 0.4694, batch acc 0.5424
10:38:16.806   Training iter 250, batch loss 0.4693, batch acc 0.5548
10:38:17.361   Training iter 300, batch loss 0.4696, batch acc 0.5484
10:38:17.908   Training iter 350, batch loss 0.4694, batch acc 0.5420
10:38:18.443   Training iter 400, batch loss 0.4696, batch acc 0.5450
10:38:18.959   Training iter 450, batch loss 0.4697, batch acc 0.5534
10:38:19.490   Training iter 500, batch loss 0.4697, batch acc 0.5386
10:38:20.027   Training iter 550, batch loss 0.4694, batch acc 0.5342
10:38:20.538   Training iter 600, batch loss 0.4695, batch acc 0.5418
10:38:20.540 Testing @ 160 epoch...
10:38:20.575     Testing, total mean loss 0.46917, total acc 0.55500
10:38:20.575 Training @ 161 epoch...
10:38:21.067   Training iter 50, batch loss 0.4693, batch acc 0.5500
10:38:21.527   Training iter 100, batch loss 0.4695, batch acc 0.5330
10:38:21.991   Training iter 150, batch loss 0.4693, batch acc 0.5490
10:38:22.483   Training iter 200, batch loss 0.4690, batch acc 0.5422
10:38:22.987   Training iter 250, batch loss 0.4695, batch acc 0.5382
10:38:23.494   Training iter 300, batch loss 0.4695, batch acc 0.5500
10:38:23.998   Training iter 350, batch loss 0.4698, batch acc 0.5444
10:38:24.503   Training iter 400, batch loss 0.4692, batch acc 0.5456
10:38:25.026   Training iter 450, batch loss 0.4700, batch acc 0.5360
10:38:25.565   Training iter 500, batch loss 0.4695, batch acc 0.5424
10:38:26.114   Training iter 550, batch loss 0.4692, batch acc 0.5540
10:38:26.642   Training iter 600, batch loss 0.4693, batch acc 0.5532
10:38:26.644 Training @ 162 epoch...
10:38:27.177   Training iter 50, batch loss 0.4699, batch acc 0.5498
10:38:27.716   Training iter 100, batch loss 0.4689, batch acc 0.5564
10:38:28.398   Training iter 150, batch loss 0.4694, batch acc 0.5420
10:38:28.981   Training iter 200, batch loss 0.4694, batch acc 0.5310
10:38:29.468   Training iter 250, batch loss 0.4693, batch acc 0.5462
10:38:29.967   Training iter 300, batch loss 0.4695, batch acc 0.5430
10:38:30.501   Training iter 350, batch loss 0.4693, batch acc 0.5434
10:38:31.035   Training iter 400, batch loss 0.4687, batch acc 0.5668
10:38:31.553   Training iter 450, batch loss 0.4694, batch acc 0.5426
10:38:32.057   Training iter 500, batch loss 0.4693, batch acc 0.5410
10:38:32.558   Training iter 550, batch loss 0.4696, batch acc 0.5418
10:38:33.075   Training iter 600, batch loss 0.4696, batch acc 0.5420
10:38:33.077 Training @ 163 epoch...
10:38:33.590   Training iter 50, batch loss 0.4691, batch acc 0.5562
10:38:34.099   Training iter 100, batch loss 0.4690, batch acc 0.5600
10:38:34.616   Training iter 150, batch loss 0.4692, batch acc 0.5542
10:38:35.126   Training iter 200, batch loss 0.4693, batch acc 0.5460
10:38:35.688   Training iter 250, batch loss 0.4693, batch acc 0.5428
10:38:36.258   Training iter 300, batch loss 0.4696, batch acc 0.5330
10:38:36.739   Training iter 350, batch loss 0.4696, batch acc 0.5402
10:38:37.229   Training iter 400, batch loss 0.4692, batch acc 0.5402
10:38:37.711   Training iter 450, batch loss 0.4694, batch acc 0.5492
10:38:38.260   Training iter 500, batch loss 0.4693, batch acc 0.5464
10:38:38.806   Training iter 550, batch loss 0.4694, batch acc 0.5366
10:38:39.296   Training iter 600, batch loss 0.4692, batch acc 0.5456
10:38:39.298 Training @ 164 epoch...
10:38:39.779   Training iter 50, batch loss 0.4694, batch acc 0.5440
10:38:40.246   Training iter 100, batch loss 0.4691, batch acc 0.5524
10:38:40.711   Training iter 150, batch loss 0.4691, batch acc 0.5442
10:38:41.192   Training iter 200, batch loss 0.4695, batch acc 0.5484
10:38:41.676   Training iter 250, batch loss 0.4691, batch acc 0.5558
10:38:42.190   Training iter 300, batch loss 0.4694, batch acc 0.5404
10:38:42.708   Training iter 350, batch loss 0.4690, batch acc 0.5502
10:38:43.231   Training iter 400, batch loss 0.4691, batch acc 0.5526
10:38:43.755   Training iter 450, batch loss 0.4688, batch acc 0.5490
10:38:44.267   Training iter 500, batch loss 0.4696, batch acc 0.5412
10:38:44.773   Training iter 550, batch loss 0.4693, batch acc 0.5370
10:38:45.300   Training iter 600, batch loss 0.4694, batch acc 0.5436
10:38:45.302 Training @ 165 epoch...
10:38:45.836   Training iter 50, batch loss 0.4692, batch acc 0.5514
10:38:46.356   Training iter 100, batch loss 0.4692, batch acc 0.5484
10:38:46.887   Training iter 150, batch loss 0.4692, batch acc 0.5494
10:38:47.407   Training iter 200, batch loss 0.4692, batch acc 0.5428
10:38:47.930   Training iter 250, batch loss 0.4692, batch acc 0.5472
10:38:48.470   Training iter 300, batch loss 0.4693, batch acc 0.5474
10:38:48.993   Training iter 350, batch loss 0.4692, batch acc 0.5438
10:38:49.512   Training iter 400, batch loss 0.4690, batch acc 0.5460
10:38:50.024   Training iter 450, batch loss 0.4692, batch acc 0.5538
10:38:50.549   Training iter 500, batch loss 0.4690, batch acc 0.5502
10:38:51.113   Training iter 550, batch loss 0.4692, batch acc 0.5416
10:38:51.671   Training iter 600, batch loss 0.4691, batch acc 0.5442
10:38:51.673 Testing @ 165 epoch...
10:38:51.709     Testing, total mean loss 0.46884, total acc 0.55800
10:38:51.709 Training @ 166 epoch...
10:38:52.260   Training iter 50, batch loss 0.4692, batch acc 0.5376
10:38:52.728   Training iter 100, batch loss 0.4691, batch acc 0.5524
10:38:53.261   Training iter 150, batch loss 0.4694, batch acc 0.5418
10:38:53.790   Training iter 200, batch loss 0.4693, batch acc 0.5512
10:38:54.319   Training iter 250, batch loss 0.4686, batch acc 0.5614
10:38:54.804   Training iter 300, batch loss 0.4692, batch acc 0.5416
10:38:55.295   Training iter 350, batch loss 0.4687, batch acc 0.5558
10:38:55.798   Training iter 400, batch loss 0.4694, batch acc 0.5396
10:38:56.306   Training iter 450, batch loss 0.4688, batch acc 0.5606
10:38:56.803   Training iter 500, batch loss 0.4691, batch acc 0.5408
10:38:57.312   Training iter 550, batch loss 0.4693, batch acc 0.5466
10:38:57.809   Training iter 600, batch loss 0.4694, batch acc 0.5450
10:38:57.811 Training @ 167 epoch...
10:38:58.426   Training iter 50, batch loss 0.4689, batch acc 0.5466
10:38:59.007   Training iter 100, batch loss 0.4691, batch acc 0.5428
10:38:59.505   Training iter 150, batch loss 0.4691, batch acc 0.5430
10:39:00.078   Training iter 200, batch loss 0.4689, batch acc 0.5458
10:39:00.633   Training iter 250, batch loss 0.4686, batch acc 0.5566
10:39:01.163   Training iter 300, batch loss 0.4690, batch acc 0.5492
10:39:01.728   Training iter 350, batch loss 0.4694, batch acc 0.5426
10:39:02.393   Training iter 400, batch loss 0.4690, batch acc 0.5454
10:39:02.922   Training iter 450, batch loss 0.4691, batch acc 0.5536
10:39:03.477   Training iter 500, batch loss 0.4696, batch acc 0.5382
10:39:04.051   Training iter 550, batch loss 0.4689, batch acc 0.5524
10:39:04.636   Training iter 600, batch loss 0.4690, batch acc 0.5642
10:39:04.638 Training @ 168 epoch...
10:39:05.219   Training iter 50, batch loss 0.4691, batch acc 0.5492
10:39:05.790   Training iter 100, batch loss 0.4689, batch acc 0.5536
10:39:06.346   Training iter 150, batch loss 0.4686, batch acc 0.5518
10:39:06.896   Training iter 200, batch loss 0.4691, batch acc 0.5526
10:39:07.464   Training iter 250, batch loss 0.4691, batch acc 0.5456
10:39:07.981   Training iter 300, batch loss 0.4690, batch acc 0.5462
10:39:08.476   Training iter 350, batch loss 0.4696, batch acc 0.5322
10:39:08.968   Training iter 400, batch loss 0.4684, batch acc 0.5566
10:39:09.418   Training iter 450, batch loss 0.4690, batch acc 0.5516
10:39:09.884   Training iter 500, batch loss 0.4688, batch acc 0.5482
10:39:10.378   Training iter 550, batch loss 0.4692, batch acc 0.5482
10:39:10.850   Training iter 600, batch loss 0.4689, batch acc 0.5550
10:39:10.852 Training @ 169 epoch...
10:39:11.329   Training iter 50, batch loss 0.4689, batch acc 0.5462
10:39:11.791   Training iter 100, batch loss 0.4684, batch acc 0.5574
10:39:12.266   Training iter 150, batch loss 0.4689, batch acc 0.5404
10:39:12.746   Training iter 200, batch loss 0.4683, batch acc 0.5686
10:39:13.265   Training iter 250, batch loss 0.4691, batch acc 0.5440
10:39:13.766   Training iter 300, batch loss 0.4690, batch acc 0.5474
10:39:14.259   Training iter 350, batch loss 0.4687, batch acc 0.5512
10:39:14.754   Training iter 400, batch loss 0.4693, batch acc 0.5410
10:39:15.267   Training iter 450, batch loss 0.4690, batch acc 0.5486
10:39:15.780   Training iter 500, batch loss 0.4692, batch acc 0.5510
10:39:16.280   Training iter 550, batch loss 0.4689, batch acc 0.5516
10:39:16.788   Training iter 600, batch loss 0.4693, batch acc 0.5522
10:39:16.789 Training @ 170 epoch...
10:39:17.306   Training iter 50, batch loss 0.4687, batch acc 0.5432
10:39:17.820   Training iter 100, batch loss 0.4685, batch acc 0.5628
10:39:18.330   Training iter 150, batch loss 0.4690, batch acc 0.5550
10:39:18.857   Training iter 200, batch loss 0.4693, batch acc 0.5362
10:39:19.355   Training iter 250, batch loss 0.4687, batch acc 0.5480
10:39:19.857   Training iter 300, batch loss 0.4686, batch acc 0.5592
10:39:20.372   Training iter 350, batch loss 0.4690, batch acc 0.5452
10:39:20.897   Training iter 400, batch loss 0.4691, batch acc 0.5412
10:39:21.411   Training iter 450, batch loss 0.4687, batch acc 0.5622
10:39:21.919   Training iter 500, batch loss 0.4687, batch acc 0.5508
10:39:22.451   Training iter 550, batch loss 0.4691, batch acc 0.5474
10:39:22.977   Training iter 600, batch loss 0.4690, batch acc 0.5516
10:39:22.979 Testing @ 170 epoch...
10:39:23.015     Testing, total mean loss 0.46853, total acc 0.56080
10:39:23.016 Training @ 171 epoch...
10:39:23.555   Training iter 50, batch loss 0.4688, batch acc 0.5546
10:39:24.095   Training iter 100, batch loss 0.4690, batch acc 0.5554
10:39:24.627   Training iter 150, batch loss 0.4689, batch acc 0.5456
10:39:25.136   Training iter 200, batch loss 0.4690, batch acc 0.5560
10:39:25.644   Training iter 250, batch loss 0.4685, batch acc 0.5544
10:39:26.171   Training iter 300, batch loss 0.4685, batch acc 0.5478
10:39:26.688   Training iter 350, batch loss 0.4689, batch acc 0.5546
10:39:27.184   Training iter 400, batch loss 0.4688, batch acc 0.5500
10:39:27.650   Training iter 450, batch loss 0.4686, batch acc 0.5596
10:39:28.118   Training iter 500, batch loss 0.4689, batch acc 0.5384
10:39:28.611   Training iter 550, batch loss 0.4689, batch acc 0.5532
10:39:29.113   Training iter 600, batch loss 0.4689, batch acc 0.5418
10:39:29.115 Training @ 172 epoch...
10:39:29.620   Training iter 50, batch loss 0.4690, batch acc 0.5460
10:39:30.122   Training iter 100, batch loss 0.4686, batch acc 0.5550
10:39:30.623   Training iter 150, batch loss 0.4684, batch acc 0.5538
10:39:31.116   Training iter 200, batch loss 0.4688, batch acc 0.5498
10:39:31.575   Training iter 250, batch loss 0.4692, batch acc 0.5472
10:39:32.036   Training iter 300, batch loss 0.4690, batch acc 0.5468
10:39:32.493   Training iter 350, batch loss 0.4688, batch acc 0.5506
10:39:32.952   Training iter 400, batch loss 0.4685, batch acc 0.5478
10:39:33.407   Training iter 450, batch loss 0.4688, batch acc 0.5582
10:39:33.857   Training iter 500, batch loss 0.4681, batch acc 0.5650
10:39:34.317   Training iter 550, batch loss 0.4690, batch acc 0.5392
10:39:34.762   Training iter 600, batch loss 0.4687, batch acc 0.5574
10:39:34.763 Training @ 173 epoch...
10:39:35.217   Training iter 50, batch loss 0.4687, batch acc 0.5460
10:39:35.665   Training iter 100, batch loss 0.4686, batch acc 0.5482
10:39:36.106   Training iter 150, batch loss 0.4687, batch acc 0.5488
10:39:36.560   Training iter 200, batch loss 0.4689, batch acc 0.5500
10:39:37.003   Training iter 250, batch loss 0.4688, batch acc 0.5500
10:39:37.450   Training iter 300, batch loss 0.4687, batch acc 0.5508
10:39:37.897   Training iter 350, batch loss 0.4685, batch acc 0.5594
10:39:38.368   Training iter 400, batch loss 0.4691, batch acc 0.5508
10:39:38.846   Training iter 450, batch loss 0.4683, batch acc 0.5586
10:39:39.315   Training iter 500, batch loss 0.4686, batch acc 0.5550
10:39:39.784   Training iter 550, batch loss 0.4689, batch acc 0.5502
10:39:40.254   Training iter 600, batch loss 0.4684, batch acc 0.5600
10:39:40.256 Training @ 174 epoch...
10:39:40.737   Training iter 50, batch loss 0.4688, batch acc 0.5482
10:39:41.205   Training iter 100, batch loss 0.4683, batch acc 0.5598
10:39:41.680   Training iter 150, batch loss 0.4687, batch acc 0.5432
10:39:42.175   Training iter 200, batch loss 0.4688, batch acc 0.5518
10:39:42.654   Training iter 250, batch loss 0.4688, batch acc 0.5452
10:39:43.168   Training iter 300, batch loss 0.4684, batch acc 0.5578
10:39:43.676   Training iter 350, batch loss 0.4682, batch acc 0.5654
10:39:44.184   Training iter 400, batch loss 0.4687, batch acc 0.5672
10:39:44.677   Training iter 450, batch loss 0.4686, batch acc 0.5564
10:39:45.182   Training iter 500, batch loss 0.4687, batch acc 0.5476
10:39:45.685   Training iter 550, batch loss 0.4689, batch acc 0.5392
10:39:46.235   Training iter 600, batch loss 0.4687, batch acc 0.5482
10:39:46.236 Training @ 175 epoch...
10:39:46.782   Training iter 50, batch loss 0.4688, batch acc 0.5484
10:39:47.330   Training iter 100, batch loss 0.4684, batch acc 0.5726
10:39:47.854   Training iter 150, batch loss 0.4686, batch acc 0.5496
10:39:48.385   Training iter 200, batch loss 0.4685, batch acc 0.5560
10:39:48.912   Training iter 250, batch loss 0.4684, batch acc 0.5514
10:39:49.430   Training iter 300, batch loss 0.4685, batch acc 0.5598
10:39:49.908   Training iter 350, batch loss 0.4688, batch acc 0.5414
10:39:50.379   Training iter 400, batch loss 0.4690, batch acc 0.5386
10:39:50.875   Training iter 450, batch loss 0.4687, batch acc 0.5524
10:39:51.368   Training iter 500, batch loss 0.4687, batch acc 0.5560
10:39:51.883   Training iter 550, batch loss 0.4684, batch acc 0.5550
10:39:52.355   Training iter 600, batch loss 0.4684, batch acc 0.5546
10:39:52.357 Testing @ 175 epoch...
10:39:52.391     Testing, total mean loss 0.46824, total acc 0.56350
10:39:52.391 Training @ 176 epoch...
10:39:52.885   Training iter 50, batch loss 0.4685, batch acc 0.5630
10:39:53.372   Training iter 100, batch loss 0.4682, batch acc 0.5662
10:39:53.845   Training iter 150, batch loss 0.4689, batch acc 0.5448
10:39:54.335   Training iter 200, batch loss 0.4684, batch acc 0.5536
10:39:54.812   Training iter 250, batch loss 0.4685, batch acc 0.5488
10:39:55.303   Training iter 300, batch loss 0.4688, batch acc 0.5366
10:39:55.862   Training iter 350, batch loss 0.4687, batch acc 0.5618
10:39:56.345   Training iter 400, batch loss 0.4687, batch acc 0.5486
10:39:56.828   Training iter 450, batch loss 0.4685, batch acc 0.5486
10:39:57.362   Training iter 500, batch loss 0.4684, batch acc 0.5592
10:39:57.891   Training iter 550, batch loss 0.4681, batch acc 0.5534
10:39:58.390   Training iter 600, batch loss 0.4687, batch acc 0.5596
10:39:58.392 Training @ 177 epoch...
10:39:58.878   Training iter 50, batch loss 0.4686, batch acc 0.5496
10:39:59.401   Training iter 100, batch loss 0.4683, batch acc 0.5682
10:39:59.901   Training iter 150, batch loss 0.4683, batch acc 0.5540
10:40:00.415   Training iter 200, batch loss 0.4682, batch acc 0.5532
10:40:00.927   Training iter 250, batch loss 0.4686, batch acc 0.5436
10:40:01.429   Training iter 300, batch loss 0.4689, batch acc 0.5472
10:40:01.977   Training iter 350, batch loss 0.4688, batch acc 0.5464
10:40:02.522   Training iter 400, batch loss 0.4683, batch acc 0.5452
10:40:03.030   Training iter 450, batch loss 0.4685, batch acc 0.5554
10:40:03.562   Training iter 500, batch loss 0.4682, batch acc 0.5576
10:40:04.103   Training iter 550, batch loss 0.4685, batch acc 0.5572
10:40:04.621   Training iter 600, batch loss 0.4685, batch acc 0.5716
10:40:04.623 Training @ 178 epoch...
10:40:05.138   Training iter 50, batch loss 0.4686, batch acc 0.5538
10:40:05.647   Training iter 100, batch loss 0.4686, batch acc 0.5560
10:40:06.159   Training iter 150, batch loss 0.4689, batch acc 0.5464
10:40:06.699   Training iter 200, batch loss 0.4687, batch acc 0.5570
10:40:07.203   Training iter 250, batch loss 0.4686, batch acc 0.5564
10:40:07.714   Training iter 300, batch loss 0.4683, batch acc 0.5462
10:40:08.271   Training iter 350, batch loss 0.4681, batch acc 0.5500
10:40:08.820   Training iter 400, batch loss 0.4683, batch acc 0.5664
10:40:09.396   Training iter 450, batch loss 0.4683, batch acc 0.5604
10:40:09.933   Training iter 500, batch loss 0.4682, batch acc 0.5594
10:40:10.457   Training iter 550, batch loss 0.4682, batch acc 0.5536
10:40:10.943   Training iter 600, batch loss 0.4681, batch acc 0.5546
10:40:10.945 Training @ 179 epoch...
10:40:11.468   Training iter 50, batch loss 0.4687, batch acc 0.5562
10:40:11.989   Training iter 100, batch loss 0.4679, batch acc 0.5648
10:40:12.525   Training iter 150, batch loss 0.4683, batch acc 0.5566
10:40:13.044   Training iter 200, batch loss 0.4686, batch acc 0.5488
10:40:13.553   Training iter 250, batch loss 0.4683, batch acc 0.5472
10:40:14.072   Training iter 300, batch loss 0.4684, batch acc 0.5558
10:40:14.591   Training iter 350, batch loss 0.4684, batch acc 0.5466
10:40:15.080   Training iter 400, batch loss 0.4683, batch acc 0.5582
10:40:15.559   Training iter 450, batch loss 0.4681, batch acc 0.5538
10:40:16.032   Training iter 500, batch loss 0.4683, batch acc 0.5632
10:40:16.514   Training iter 550, batch loss 0.4687, batch acc 0.5590
10:40:16.980   Training iter 600, batch loss 0.4684, batch acc 0.5524
10:40:16.982 Training @ 180 epoch...
10:40:17.441   Training iter 50, batch loss 0.4682, batch acc 0.5582
10:40:17.895   Training iter 100, batch loss 0.4683, batch acc 0.5562
10:40:18.350   Training iter 150, batch loss 0.4684, batch acc 0.5620
10:40:18.812   Training iter 200, batch loss 0.4684, batch acc 0.5604
10:40:19.269   Training iter 250, batch loss 0.4679, batch acc 0.5636
10:40:19.734   Training iter 300, batch loss 0.4680, batch acc 0.5548
10:40:20.205   Training iter 350, batch loss 0.4684, batch acc 0.5560
10:40:20.668   Training iter 400, batch loss 0.4681, batch acc 0.5554
10:40:21.152   Training iter 450, batch loss 0.4686, batch acc 0.5448
10:40:21.647   Training iter 500, batch loss 0.4680, batch acc 0.5526
10:40:22.136   Training iter 550, batch loss 0.4686, batch acc 0.5520
10:40:22.643   Training iter 600, batch loss 0.4688, batch acc 0.5500
10:40:22.645 Testing @ 180 epoch...
10:40:22.680     Testing, total mean loss 0.46797, total acc 0.56700
10:40:22.680 Training @ 181 epoch...
10:40:23.219   Training iter 50, batch loss 0.4679, batch acc 0.5636
10:40:23.753   Training iter 100, batch loss 0.4684, batch acc 0.5514
10:40:24.287   Training iter 150, batch loss 0.4684, batch acc 0.5500
10:40:24.813   Training iter 200, batch loss 0.4688, batch acc 0.5408
10:40:25.359   Training iter 250, batch loss 0.4681, batch acc 0.5528
10:40:25.910   Training iter 300, batch loss 0.4683, batch acc 0.5562
10:40:26.458   Training iter 350, batch loss 0.4683, batch acc 0.5530
10:40:27.002   Training iter 400, batch loss 0.4681, batch acc 0.5550
10:40:27.570   Training iter 450, batch loss 0.4683, batch acc 0.5648
10:40:28.132   Training iter 500, batch loss 0.4683, batch acc 0.5666
10:40:28.684   Training iter 550, batch loss 0.4681, batch acc 0.5694
10:40:29.226   Training iter 600, batch loss 0.4682, batch acc 0.5500
10:40:29.228 Training @ 182 epoch...
10:40:29.769   Training iter 50, batch loss 0.4685, batch acc 0.5574
10:40:30.279   Training iter 100, batch loss 0.4681, batch acc 0.5598
10:40:30.773   Training iter 150, batch loss 0.4683, batch acc 0.5506
10:40:31.271   Training iter 200, batch loss 0.4681, batch acc 0.5564
10:40:31.771   Training iter 250, batch loss 0.4682, batch acc 0.5584
10:40:32.248   Training iter 300, batch loss 0.4684, batch acc 0.5486
10:40:32.710   Training iter 350, batch loss 0.4683, batch acc 0.5550
10:40:33.199   Training iter 400, batch loss 0.4682, batch acc 0.5628
10:40:33.689   Training iter 450, batch loss 0.4682, batch acc 0.5542
10:40:34.171   Training iter 500, batch loss 0.4681, batch acc 0.5564
10:40:34.688   Training iter 550, batch loss 0.4681, batch acc 0.5520
10:40:35.213   Training iter 600, batch loss 0.4679, batch acc 0.5668
10:40:35.215 Training @ 183 epoch...
10:40:35.752   Training iter 50, batch loss 0.4684, batch acc 0.5498
10:40:36.291   Training iter 100, batch loss 0.4683, batch acc 0.5654
10:40:36.814   Training iter 150, batch loss 0.4686, batch acc 0.5498
10:40:37.334   Training iter 200, batch loss 0.4681, batch acc 0.5512
10:40:37.852   Training iter 250, batch loss 0.4678, batch acc 0.5652
10:40:38.380   Training iter 300, batch loss 0.4684, batch acc 0.5564
10:40:38.940   Training iter 350, batch loss 0.4683, batch acc 0.5540
10:40:39.514   Training iter 400, batch loss 0.4682, batch acc 0.5438
10:40:40.082   Training iter 450, batch loss 0.4681, batch acc 0.5602
10:40:40.648   Training iter 500, batch loss 0.4677, batch acc 0.5622
10:40:41.198   Training iter 550, batch loss 0.4681, batch acc 0.5652
10:40:41.735   Training iter 600, batch loss 0.4680, batch acc 0.5618
10:40:41.737 Training @ 184 epoch...
10:40:42.273   Training iter 50, batch loss 0.4681, batch acc 0.5560
10:40:42.802   Training iter 100, batch loss 0.4678, batch acc 0.5606
10:40:43.351   Training iter 150, batch loss 0.4682, batch acc 0.5550
10:40:43.896   Training iter 200, batch loss 0.4680, batch acc 0.5584
10:40:44.446   Training iter 250, batch loss 0.4678, batch acc 0.5708
10:40:44.947   Training iter 300, batch loss 0.4680, batch acc 0.5660
10:40:45.443   Training iter 350, batch loss 0.4683, batch acc 0.5510
10:40:45.948   Training iter 400, batch loss 0.4681, batch acc 0.5508
10:40:46.443   Training iter 450, batch loss 0.4682, batch acc 0.5454
10:40:46.950   Training iter 500, batch loss 0.4684, batch acc 0.5592
10:40:47.459   Training iter 550, batch loss 0.4682, batch acc 0.5560
10:40:47.960   Training iter 600, batch loss 0.4682, batch acc 0.5582
10:40:47.962 Training @ 185 epoch...
10:40:48.464   Training iter 50, batch loss 0.4680, batch acc 0.5570
10:40:48.953   Training iter 100, batch loss 0.4680, batch acc 0.5632
10:40:49.467   Training iter 150, batch loss 0.4684, batch acc 0.5474
10:40:49.987   Training iter 200, batch loss 0.4681, batch acc 0.5524
10:40:50.475   Training iter 250, batch loss 0.4679, batch acc 0.5570
10:40:50.967   Training iter 300, batch loss 0.4678, batch acc 0.5592
10:40:51.464   Training iter 350, batch loss 0.4679, batch acc 0.5624
10:40:51.958   Training iter 400, batch loss 0.4679, batch acc 0.5622
10:40:52.466   Training iter 450, batch loss 0.4679, batch acc 0.5654
10:40:52.969   Training iter 500, batch loss 0.4683, batch acc 0.5484
10:40:53.476   Training iter 550, batch loss 0.4682, batch acc 0.5698
10:40:53.980   Training iter 600, batch loss 0.4684, batch acc 0.5466
10:40:53.981 Testing @ 185 epoch...
10:40:54.018     Testing, total mean loss 0.46771, total acc 0.56950
10:40:54.018 Training @ 186 epoch...
10:40:54.523   Training iter 50, batch loss 0.4680, batch acc 0.5522
10:40:55.052   Training iter 100, batch loss 0.4679, batch acc 0.5562
10:40:55.556   Training iter 150, batch loss 0.4685, batch acc 0.5512
10:40:56.063   Training iter 200, batch loss 0.4685, batch acc 0.5546
10:40:56.571   Training iter 250, batch loss 0.4678, batch acc 0.5598
10:40:57.095   Training iter 300, batch loss 0.4680, batch acc 0.5554
10:40:57.626   Training iter 350, batch loss 0.4678, batch acc 0.5632
10:40:58.160   Training iter 400, batch loss 0.4680, batch acc 0.5596
10:40:58.700   Training iter 450, batch loss 0.4680, batch acc 0.5590
10:40:59.257   Training iter 500, batch loss 0.4680, batch acc 0.5570
10:40:59.800   Training iter 550, batch loss 0.4681, batch acc 0.5600
10:41:00.332   Training iter 600, batch loss 0.4676, batch acc 0.5672
10:41:00.334 Training @ 187 epoch...
10:41:00.914   Training iter 50, batch loss 0.4680, batch acc 0.5618
10:41:01.492   Training iter 100, batch loss 0.4679, batch acc 0.5472
10:41:02.088   Training iter 150, batch loss 0.4680, batch acc 0.5584
10:41:02.654   Training iter 200, batch loss 0.4679, batch acc 0.5584
10:41:03.218   Training iter 250, batch loss 0.4680, batch acc 0.5680
10:41:03.763   Training iter 300, batch loss 0.4679, batch acc 0.5592
10:41:04.326   Training iter 350, batch loss 0.4680, batch acc 0.5634
10:41:04.885   Training iter 400, batch loss 0.4680, batch acc 0.5596
10:41:05.426   Training iter 450, batch loss 0.4678, batch acc 0.5598
10:41:05.957   Training iter 500, batch loss 0.4679, batch acc 0.5600
10:41:06.510   Training iter 550, batch loss 0.4682, batch acc 0.5548
10:41:07.059   Training iter 600, batch loss 0.4680, batch acc 0.5510
10:41:07.060 Training @ 188 epoch...
10:41:07.586   Training iter 50, batch loss 0.4680, batch acc 0.5590
10:41:08.109   Training iter 100, batch loss 0.4682, batch acc 0.5576
10:41:08.642   Training iter 150, batch loss 0.4682, batch acc 0.5634
10:41:09.168   Training iter 200, batch loss 0.4681, batch acc 0.5562
10:41:09.682   Training iter 250, batch loss 0.4678, batch acc 0.5610
10:41:10.210   Training iter 300, batch loss 0.4675, batch acc 0.5656
10:41:10.742   Training iter 350, batch loss 0.4678, batch acc 0.5582
10:41:11.309   Training iter 400, batch loss 0.4680, batch acc 0.5492
10:41:11.866   Training iter 450, batch loss 0.4673, batch acc 0.5662
10:41:12.420   Training iter 500, batch loss 0.4679, batch acc 0.5646
10:41:12.972   Training iter 550, batch loss 0.4680, batch acc 0.5550
10:41:13.507   Training iter 600, batch loss 0.4683, batch acc 0.5510
10:41:13.509 Training @ 189 epoch...
10:41:14.058   Training iter 50, batch loss 0.4675, batch acc 0.5642
10:41:14.610   Training iter 100, batch loss 0.4682, batch acc 0.5576
10:41:15.165   Training iter 150, batch loss 0.4678, batch acc 0.5606
10:41:15.732   Training iter 200, batch loss 0.4680, batch acc 0.5584
10:41:16.284   Training iter 250, batch loss 0.4680, batch acc 0.5616
10:41:16.807   Training iter 300, batch loss 0.4679, batch acc 0.5622
10:41:17.326   Training iter 350, batch loss 0.4678, batch acc 0.5594
10:41:17.813   Training iter 400, batch loss 0.4681, batch acc 0.5550
10:41:18.307   Training iter 450, batch loss 0.4680, batch acc 0.5522
10:41:18.808   Training iter 500, batch loss 0.4677, batch acc 0.5512
10:41:19.317   Training iter 550, batch loss 0.4679, batch acc 0.5620
10:41:19.829   Training iter 600, batch loss 0.4676, batch acc 0.5712
10:41:19.831 Training @ 190 epoch...
10:41:20.356   Training iter 50, batch loss 0.4673, batch acc 0.5688
10:41:20.883   Training iter 100, batch loss 0.4680, batch acc 0.5566
10:41:21.396   Training iter 150, batch loss 0.4678, batch acc 0.5570
10:41:21.919   Training iter 200, batch loss 0.4680, batch acc 0.5590
10:41:22.488   Training iter 250, batch loss 0.4676, batch acc 0.5522
10:41:23.050   Training iter 300, batch loss 0.4678, batch acc 0.5676
10:41:23.612   Training iter 350, batch loss 0.4678, batch acc 0.5626
10:41:24.132   Training iter 400, batch loss 0.4681, batch acc 0.5472
10:41:24.649   Training iter 450, batch loss 0.4678, batch acc 0.5678
10:41:25.176   Training iter 500, batch loss 0.4679, batch acc 0.5556
10:41:25.698   Training iter 550, batch loss 0.4679, batch acc 0.5578
10:41:26.216   Training iter 600, batch loss 0.4678, batch acc 0.5698
10:41:26.218 Testing @ 190 epoch...
10:41:26.252     Testing, total mean loss 0.46747, total acc 0.57100
10:41:26.252 Training @ 191 epoch...
10:41:26.771   Training iter 50, batch loss 0.4674, batch acc 0.5668
10:41:27.276   Training iter 100, batch loss 0.4677, batch acc 0.5524
10:41:27.762   Training iter 150, batch loss 0.4678, batch acc 0.5654
10:41:28.241   Training iter 200, batch loss 0.4681, batch acc 0.5462
10:41:28.698   Training iter 250, batch loss 0.4675, batch acc 0.5626
10:41:29.174   Training iter 300, batch loss 0.4678, batch acc 0.5568
10:41:29.640   Training iter 350, batch loss 0.4680, batch acc 0.5644
10:41:30.107   Training iter 400, batch loss 0.4678, batch acc 0.5626
10:41:30.582   Training iter 450, batch loss 0.4680, batch acc 0.5616
10:41:31.062   Training iter 500, batch loss 0.4680, batch acc 0.5596
10:41:31.541   Training iter 550, batch loss 0.4677, batch acc 0.5654
10:41:32.009   Training iter 600, batch loss 0.4676, batch acc 0.5596
10:41:32.011 Training @ 192 epoch...
10:41:32.483   Training iter 50, batch loss 0.4674, batch acc 0.5734
10:41:32.944   Training iter 100, batch loss 0.4676, batch acc 0.5640
10:41:33.437   Training iter 150, batch loss 0.4672, batch acc 0.5680
10:41:33.943   Training iter 200, batch loss 0.4679, batch acc 0.5568
10:41:34.442   Training iter 250, batch loss 0.4677, batch acc 0.5620
10:41:34.940   Training iter 300, batch loss 0.4680, batch acc 0.5610
10:41:35.438   Training iter 350, batch loss 0.4674, batch acc 0.5612
10:41:35.929   Training iter 400, batch loss 0.4679, batch acc 0.5574
10:41:36.433   Training iter 450, batch loss 0.4680, batch acc 0.5586
10:41:36.945   Training iter 500, batch loss 0.4677, batch acc 0.5602
10:41:37.468   Training iter 550, batch loss 0.4679, batch acc 0.5646
10:41:37.983   Training iter 600, batch loss 0.4682, batch acc 0.5426
10:41:37.985 Training @ 193 epoch...
10:41:38.504   Training iter 50, batch loss 0.4672, batch acc 0.5696
10:41:39.013   Training iter 100, batch loss 0.4673, batch acc 0.5672
10:41:39.516   Training iter 150, batch loss 0.4681, batch acc 0.5542
10:41:40.001   Training iter 200, batch loss 0.4677, batch acc 0.5662
10:41:40.508   Training iter 250, batch loss 0.4676, batch acc 0.5564
10:41:41.014   Training iter 300, batch loss 0.4679, batch acc 0.5666
10:41:41.517   Training iter 350, batch loss 0.4678, batch acc 0.5586
10:41:42.027   Training iter 400, batch loss 0.4678, batch acc 0.5618
10:41:42.540   Training iter 450, batch loss 0.4677, batch acc 0.5582
10:41:43.063   Training iter 500, batch loss 0.4675, batch acc 0.5632
10:41:43.565   Training iter 550, batch loss 0.4675, batch acc 0.5634
10:41:44.076   Training iter 600, batch loss 0.4680, batch acc 0.5502
10:41:44.078 Training @ 194 epoch...
10:41:44.638   Training iter 50, batch loss 0.4682, batch acc 0.5464
10:41:45.181   Training iter 100, batch loss 0.4678, batch acc 0.5662
10:41:45.726   Training iter 150, batch loss 0.4673, batch acc 0.5642
10:41:46.263   Training iter 200, batch loss 0.4675, batch acc 0.5666
10:41:46.811   Training iter 250, batch loss 0.4678, batch acc 0.5518
10:41:47.315   Training iter 300, batch loss 0.4676, batch acc 0.5578
10:41:47.838   Training iter 350, batch loss 0.4676, batch acc 0.5614
10:41:48.383   Training iter 400, batch loss 0.4678, batch acc 0.5558
10:41:48.899   Training iter 450, batch loss 0.4677, batch acc 0.5634
10:41:49.389   Training iter 500, batch loss 0.4674, batch acc 0.5638
10:41:49.869   Training iter 550, batch loss 0.4670, batch acc 0.5760
10:41:50.367   Training iter 600, batch loss 0.4679, batch acc 0.5678
10:41:50.369 Training @ 195 epoch...
10:41:50.865   Training iter 50, batch loss 0.4678, batch acc 0.5544
10:41:51.369   Training iter 100, batch loss 0.4676, batch acc 0.5682
10:41:51.866   Training iter 150, batch loss 0.4681, batch acc 0.5538
10:41:52.354   Training iter 200, batch loss 0.4672, batch acc 0.5580
10:41:52.852   Training iter 250, batch loss 0.4676, batch acc 0.5632
10:41:53.366   Training iter 300, batch loss 0.4672, batch acc 0.5762
10:41:53.880   Training iter 350, batch loss 0.4672, batch acc 0.5768
10:41:54.399   Training iter 400, batch loss 0.4681, batch acc 0.5524
10:41:54.911   Training iter 450, batch loss 0.4678, batch acc 0.5608
10:41:55.448   Training iter 500, batch loss 0.4677, batch acc 0.5600
10:41:55.943   Training iter 550, batch loss 0.4676, batch acc 0.5550
10:41:56.434   Training iter 600, batch loss 0.4674, batch acc 0.5668
10:41:56.436 Testing @ 195 epoch...
10:41:56.472     Testing, total mean loss 0.46724, total acc 0.57450
10:41:56.472 Training @ 196 epoch...
10:41:56.969   Training iter 50, batch loss 0.4676, batch acc 0.5508
10:41:57.467   Training iter 100, batch loss 0.4671, batch acc 0.5706
10:41:57.951   Training iter 150, batch loss 0.4676, batch acc 0.5510
10:41:58.431   Training iter 200, batch loss 0.4678, batch acc 0.5672
10:41:58.927   Training iter 250, batch loss 0.4679, batch acc 0.5538
10:41:59.448   Training iter 300, batch loss 0.4674, batch acc 0.5634
10:41:59.958   Training iter 350, batch loss 0.4679, batch acc 0.5592
10:42:00.501   Training iter 400, batch loss 0.4674, batch acc 0.5776
10:42:01.043   Training iter 450, batch loss 0.4674, batch acc 0.5640
10:42:01.613   Training iter 500, batch loss 0.4676, batch acc 0.5612
10:42:02.187   Training iter 550, batch loss 0.4673, batch acc 0.5706
10:42:02.745   Training iter 600, batch loss 0.4677, batch acc 0.5616
10:42:02.746 Training @ 197 epoch...
10:42:03.301   Training iter 50, batch loss 0.4675, batch acc 0.5618
10:42:03.840   Training iter 100, batch loss 0.4675, batch acc 0.5684
10:42:04.377   Training iter 150, batch loss 0.4676, batch acc 0.5630
10:42:04.902   Training iter 200, batch loss 0.4681, batch acc 0.5606
10:42:05.400   Training iter 250, batch loss 0.4675, batch acc 0.5642
10:42:05.888   Training iter 300, batch loss 0.4675, batch acc 0.5570
10:42:06.384   Training iter 350, batch loss 0.4674, batch acc 0.5588
10:42:06.915   Training iter 400, batch loss 0.4676, batch acc 0.5662
10:42:07.456   Training iter 450, batch loss 0.4672, batch acc 0.5698
10:42:07.934   Training iter 500, batch loss 0.4675, batch acc 0.5620
10:42:08.405   Training iter 550, batch loss 0.4672, batch acc 0.5668
10:42:08.889   Training iter 600, batch loss 0.4674, batch acc 0.5602
10:42:08.891 Training @ 198 epoch...
10:42:09.372   Training iter 50, batch loss 0.4676, batch acc 0.5634
10:42:09.834   Training iter 100, batch loss 0.4678, batch acc 0.5516
10:42:10.315   Training iter 150, batch loss 0.4674, batch acc 0.5734
10:42:10.790   Training iter 200, batch loss 0.4677, batch acc 0.5642
10:42:11.289   Training iter 250, batch loss 0.4676, batch acc 0.5718
10:42:11.782   Training iter 300, batch loss 0.4674, batch acc 0.5544
10:42:12.282   Training iter 350, batch loss 0.4672, batch acc 0.5756
10:42:12.799   Training iter 400, batch loss 0.4673, batch acc 0.5684
10:42:13.341   Training iter 450, batch loss 0.4671, batch acc 0.5716
10:42:13.886   Training iter 500, batch loss 0.4676, batch acc 0.5534
10:42:14.409   Training iter 550, batch loss 0.4672, batch acc 0.5628
10:42:14.912   Training iter 600, batch loss 0.4677, batch acc 0.5522
10:42:14.914 Training @ 199 epoch...
10:42:15.438   Training iter 50, batch loss 0.4677, batch acc 0.5624
10:42:15.959   Training iter 100, batch loss 0.4670, batch acc 0.5746
10:42:16.479   Training iter 150, batch loss 0.4676, batch acc 0.5628
10:42:17.007   Training iter 200, batch loss 0.4678, batch acc 0.5468
10:42:17.532   Training iter 250, batch loss 0.4674, batch acc 0.5652
10:42:18.049   Training iter 300, batch loss 0.4675, batch acc 0.5692
10:42:18.570   Training iter 350, batch loss 0.4676, batch acc 0.5484
10:42:19.062   Training iter 400, batch loss 0.4672, batch acc 0.5704
10:42:19.549   Training iter 450, batch loss 0.4677, batch acc 0.5516
10:42:20.037   Training iter 500, batch loss 0.4671, batch acc 0.5726
10:42:20.522   Training iter 550, batch loss 0.4669, batch acc 0.5644
10:42:21.025   Training iter 600, batch loss 0.4675, batch acc 0.5778
10:42:21.027 Training @ 200 epoch...
10:42:21.526   Training iter 50, batch loss 0.4676, batch acc 0.5664
10:42:22.018   Training iter 100, batch loss 0.4672, batch acc 0.5694
10:42:22.512   Training iter 150, batch loss 0.4675, batch acc 0.5628
10:42:22.996   Training iter 200, batch loss 0.4675, batch acc 0.5602
10:42:23.490   Training iter 250, batch loss 0.4671, batch acc 0.5622
10:42:23.970   Training iter 300, batch loss 0.4674, batch acc 0.5706
10:42:24.456   Training iter 350, batch loss 0.4673, batch acc 0.5638
10:42:24.941   Training iter 400, batch loss 0.4675, batch acc 0.5544
10:42:25.451   Training iter 450, batch loss 0.4672, batch acc 0.5682
10:42:25.968   Training iter 500, batch loss 0.4677, batch acc 0.5540
10:42:26.492   Training iter 550, batch loss 0.4673, batch acc 0.5702
10:42:27.018   Training iter 600, batch loss 0.4674, batch acc 0.5684
10:42:27.020 Testing @ 200 epoch...
10:42:27.055     Testing, total mean loss 0.46702, total acc 0.57680
10:42:27.055 Plot @ 200 epoch...
10:42:27.055 Training @ 201 epoch...
10:42:27.590   Training iter 50, batch loss 0.4672, batch acc 0.5626
10:42:28.115   Training iter 100, batch loss 0.4678, batch acc 0.5590
10:42:28.621   Training iter 150, batch loss 0.4674, batch acc 0.5658
10:42:29.145   Training iter 200, batch loss 0.4671, batch acc 0.5758
10:42:29.650   Training iter 250, batch loss 0.4671, batch acc 0.5716
10:42:30.159   Training iter 300, batch loss 0.4675, batch acc 0.5574
10:42:30.687   Training iter 350, batch loss 0.4671, batch acc 0.5716
10:42:31.191   Training iter 400, batch loss 0.4676, batch acc 0.5598
10:42:31.676   Training iter 450, batch loss 0.4673, batch acc 0.5704
10:42:32.165   Training iter 500, batch loss 0.4673, batch acc 0.5546
10:42:32.646   Training iter 550, batch loss 0.4675, batch acc 0.5562
10:42:33.136   Training iter 600, batch loss 0.4673, batch acc 0.5720
10:42:33.138 Training @ 202 epoch...
10:42:33.631   Training iter 50, batch loss 0.4674, batch acc 0.5586
10:42:34.129   Training iter 100, batch loss 0.4672, batch acc 0.5654
10:42:34.629   Training iter 150, batch loss 0.4674, batch acc 0.5686
10:42:35.158   Training iter 200, batch loss 0.4674, batch acc 0.5648
10:42:35.679   Training iter 250, batch loss 0.4672, batch acc 0.5752
10:42:36.203   Training iter 300, batch loss 0.4675, batch acc 0.5596
10:42:36.728   Training iter 350, batch loss 0.4673, batch acc 0.5576
10:42:37.258   Training iter 400, batch loss 0.4673, batch acc 0.5690
10:42:37.775   Training iter 450, batch loss 0.4671, batch acc 0.5656
10:42:38.284   Training iter 500, batch loss 0.4674, batch acc 0.5622
10:42:38.801   Training iter 550, batch loss 0.4674, batch acc 0.5594
10:42:39.326   Training iter 600, batch loss 0.4670, batch acc 0.5726
10:42:39.328 Training @ 203 epoch...
10:42:39.834   Training iter 50, batch loss 0.4671, batch acc 0.5618
10:42:40.350   Training iter 100, batch loss 0.4676, batch acc 0.5626
10:42:40.854   Training iter 150, batch loss 0.4671, batch acc 0.5606
10:42:41.382   Training iter 200, batch loss 0.4675, batch acc 0.5570
10:42:41.919   Training iter 250, batch loss 0.4673, batch acc 0.5656
10:42:42.464   Training iter 300, batch loss 0.4669, batch acc 0.5690
10:42:43.009   Training iter 350, batch loss 0.4675, batch acc 0.5680
10:42:43.548   Training iter 400, batch loss 0.4674, batch acc 0.5728
10:42:44.094   Training iter 450, batch loss 0.4672, batch acc 0.5734
10:42:44.640   Training iter 500, batch loss 0.4671, batch acc 0.5550
10:42:45.183   Training iter 550, batch loss 0.4672, batch acc 0.5694
10:42:45.730   Training iter 600, batch loss 0.4672, batch acc 0.5682
10:42:45.731 Training @ 204 epoch...
10:42:46.275   Training iter 50, batch loss 0.4671, batch acc 0.5708
10:42:46.816   Training iter 100, batch loss 0.4672, batch acc 0.5640
10:42:47.349   Training iter 150, batch loss 0.4673, batch acc 0.5554
10:42:47.885   Training iter 200, batch loss 0.4671, batch acc 0.5740
10:42:48.423   Training iter 250, batch loss 0.4672, batch acc 0.5670
10:42:48.955   Training iter 300, batch loss 0.4678, batch acc 0.5484
10:42:49.479   Training iter 350, batch loss 0.4672, batch acc 0.5672
10:42:50.038   Training iter 400, batch loss 0.4672, batch acc 0.5774
10:42:50.584   Training iter 450, batch loss 0.4670, batch acc 0.5764
10:42:51.109   Training iter 500, batch loss 0.4670, batch acc 0.5664
10:42:51.634   Training iter 550, batch loss 0.4674, batch acc 0.5608
10:42:52.184   Training iter 600, batch loss 0.4673, batch acc 0.5584
10:42:52.185 Training @ 205 epoch...
10:42:52.753   Training iter 50, batch loss 0.4673, batch acc 0.5630
10:42:53.277   Training iter 100, batch loss 0.4673, batch acc 0.5606
10:42:53.796   Training iter 150, batch loss 0.4672, batch acc 0.5704
10:42:54.311   Training iter 200, batch loss 0.4671, batch acc 0.5642
10:42:54.826   Training iter 250, batch loss 0.4674, batch acc 0.5590
10:42:55.358   Training iter 300, batch loss 0.4674, batch acc 0.5508
10:42:55.874   Training iter 350, batch loss 0.4670, batch acc 0.5684
10:42:56.379   Training iter 400, batch loss 0.4667, batch acc 0.5812
10:42:56.891   Training iter 450, batch loss 0.4673, batch acc 0.5652
10:42:57.412   Training iter 500, batch loss 0.4671, batch acc 0.5636
10:42:57.923   Training iter 550, batch loss 0.4674, batch acc 0.5652
10:42:58.433   Training iter 600, batch loss 0.4671, batch acc 0.5798
10:42:58.435 Testing @ 205 epoch...
10:42:58.470     Testing, total mean loss 0.46682, total acc 0.57870
10:42:58.470 Training @ 206 epoch...
10:42:58.997   Training iter 50, batch loss 0.4672, batch acc 0.5638
10:42:59.523   Training iter 100, batch loss 0.4671, batch acc 0.5660
10:43:00.047   Training iter 150, batch loss 0.4674, batch acc 0.5588
10:43:00.581   Training iter 200, batch loss 0.4669, batch acc 0.5648
10:43:01.117   Training iter 250, batch loss 0.4671, batch acc 0.5596
10:43:01.639   Training iter 300, batch loss 0.4669, batch acc 0.5630
10:43:02.174   Training iter 350, batch loss 0.4676, batch acc 0.5608
10:43:02.697   Training iter 400, batch loss 0.4671, batch acc 0.5686
10:43:03.253   Training iter 450, batch loss 0.4674, batch acc 0.5652
10:43:03.836   Training iter 500, batch loss 0.4673, batch acc 0.5788
10:43:04.435   Training iter 550, batch loss 0.4670, batch acc 0.5738
10:43:05.015   Training iter 600, batch loss 0.4668, batch acc 0.5746
10:43:05.017 Training @ 207 epoch...
10:43:05.603   Training iter 50, batch loss 0.4669, batch acc 0.5678
10:43:06.185   Training iter 100, batch loss 0.4670, batch acc 0.5488
10:43:06.756   Training iter 150, batch loss 0.4674, batch acc 0.5678
10:43:07.261   Training iter 200, batch loss 0.4671, batch acc 0.5622
10:43:07.753   Training iter 250, batch loss 0.4670, batch acc 0.5726
10:43:08.296   Training iter 300, batch loss 0.4673, batch acc 0.5782
10:43:08.870   Training iter 350, batch loss 0.4672, batch acc 0.5582
10:43:09.408   Training iter 400, batch loss 0.4670, batch acc 0.5744
10:43:09.922   Training iter 450, batch loss 0.4674, batch acc 0.5562
10:43:10.459   Training iter 500, batch loss 0.4669, batch acc 0.5732
10:43:11.078   Training iter 550, batch loss 0.4670, batch acc 0.5678
10:43:11.685   Training iter 600, batch loss 0.4670, batch acc 0.5722
10:43:11.687 Training @ 208 epoch...
10:43:12.281   Training iter 50, batch loss 0.4669, batch acc 0.5722
10:43:12.791   Training iter 100, batch loss 0.4670, batch acc 0.5628
10:43:13.304   Training iter 150, batch loss 0.4673, batch acc 0.5556
10:43:13.790   Training iter 200, batch loss 0.4671, batch acc 0.5702
10:43:14.278   Training iter 250, batch loss 0.4671, batch acc 0.5584
10:43:14.769   Training iter 300, batch loss 0.4677, batch acc 0.5614
10:43:15.281   Training iter 350, batch loss 0.4671, batch acc 0.5730
10:43:15.786   Training iter 400, batch loss 0.4668, batch acc 0.5716
10:43:16.297   Training iter 450, batch loss 0.4672, batch acc 0.5678
10:43:16.811   Training iter 500, batch loss 0.4669, batch acc 0.5686
10:43:17.323   Training iter 550, batch loss 0.4666, batch acc 0.5818
10:43:17.834   Training iter 600, batch loss 0.4672, batch acc 0.5598
10:43:17.836 Training @ 209 epoch...
10:43:18.348   Training iter 50, batch loss 0.4669, batch acc 0.5668
10:43:18.846   Training iter 100, batch loss 0.4670, batch acc 0.5584
10:43:19.355   Training iter 150, batch loss 0.4673, batch acc 0.5624
10:43:19.906   Training iter 200, batch loss 0.4666, batch acc 0.5768
10:43:20.462   Training iter 250, batch loss 0.4672, batch acc 0.5718
10:43:21.017   Training iter 300, batch loss 0.4666, batch acc 0.5750
10:43:21.571   Training iter 350, batch loss 0.4670, batch acc 0.5702
10:43:22.106   Training iter 400, batch loss 0.4668, batch acc 0.5702
10:43:22.652   Training iter 450, batch loss 0.4673, batch acc 0.5650
10:43:23.200   Training iter 500, batch loss 0.4676, batch acc 0.5606
10:43:23.760   Training iter 550, batch loss 0.4673, batch acc 0.5588
10:43:24.319   Training iter 600, batch loss 0.4670, batch acc 0.5706
10:43:24.321 Training @ 210 epoch...
10:43:24.861   Training iter 50, batch loss 0.4669, batch acc 0.5610
10:43:25.381   Training iter 100, batch loss 0.4672, batch acc 0.5698
10:43:25.878   Training iter 150, batch loss 0.4668, batch acc 0.5742
10:43:26.358   Training iter 200, batch loss 0.4670, batch acc 0.5642
10:43:26.867   Training iter 250, batch loss 0.4669, batch acc 0.5634
10:43:27.363   Training iter 300, batch loss 0.4673, batch acc 0.5612
10:43:27.849   Training iter 350, batch loss 0.4673, batch acc 0.5546
10:43:28.349   Training iter 400, batch loss 0.4666, batch acc 0.5848
10:43:28.868   Training iter 450, batch loss 0.4671, batch acc 0.5752
10:43:29.355   Training iter 500, batch loss 0.4671, batch acc 0.5710
10:43:29.843   Training iter 550, batch loss 0.4666, batch acc 0.5728
10:43:30.343   Training iter 600, batch loss 0.4672, batch acc 0.5590
10:43:30.344 Testing @ 210 epoch...
10:43:30.379     Testing, total mean loss 0.46662, total acc 0.58060
10:43:30.379 Training @ 211 epoch...
10:43:30.909   Training iter 50, batch loss 0.4670, batch acc 0.5710
10:43:31.423   Training iter 100, batch loss 0.4672, batch acc 0.5628
10:43:31.905   Training iter 150, batch loss 0.4672, batch acc 0.5542
10:43:32.395   Training iter 200, batch loss 0.4672, batch acc 0.5680
10:43:32.881   Training iter 250, batch loss 0.4669, batch acc 0.5590
10:43:33.385   Training iter 300, batch loss 0.4669, batch acc 0.5698
10:43:33.881   Training iter 350, batch loss 0.4668, batch acc 0.5650
10:43:34.377   Training iter 400, batch loss 0.4667, batch acc 0.5660
10:43:34.863   Training iter 450, batch loss 0.4666, batch acc 0.5778
10:43:35.393   Training iter 500, batch loss 0.4669, batch acc 0.5762
10:43:35.905   Training iter 550, batch loss 0.4667, batch acc 0.5746
10:43:36.417   Training iter 600, batch loss 0.4673, batch acc 0.5686
10:43:36.418 Training @ 212 epoch...
10:43:36.941   Training iter 50, batch loss 0.4662, batch acc 0.5852
10:43:37.452   Training iter 100, batch loss 0.4674, batch acc 0.5614
10:43:37.950   Training iter 150, batch loss 0.4672, batch acc 0.5692
10:43:38.443   Training iter 200, batch loss 0.4671, batch acc 0.5680
10:43:38.908   Training iter 250, batch loss 0.4671, batch acc 0.5564
10:43:39.381   Training iter 300, batch loss 0.4667, batch acc 0.5704
10:43:39.842   Training iter 350, batch loss 0.4670, batch acc 0.5642
10:43:40.327   Training iter 400, batch loss 0.4669, batch acc 0.5664
10:43:40.792   Training iter 450, batch loss 0.4663, batch acc 0.5712
10:43:41.278   Training iter 500, batch loss 0.4671, batch acc 0.5602
10:43:41.771   Training iter 550, batch loss 0.4668, batch acc 0.5766
10:43:42.257   Training iter 600, batch loss 0.4671, batch acc 0.5694
10:43:42.259 Training @ 213 epoch...
10:43:42.746   Training iter 50, batch loss 0.4666, batch acc 0.5774
10:43:43.275   Training iter 100, batch loss 0.4669, batch acc 0.5710
10:43:43.767   Training iter 150, batch loss 0.4667, batch acc 0.5742
10:43:44.255   Training iter 200, batch loss 0.4670, batch acc 0.5678
10:43:44.728   Training iter 250, batch loss 0.4665, batch acc 0.5664
10:43:45.197   Training iter 300, batch loss 0.4668, batch acc 0.5688
10:43:45.670   Training iter 350, batch loss 0.4676, batch acc 0.5538
10:43:46.177   Training iter 400, batch loss 0.4675, batch acc 0.5622
10:43:46.699   Training iter 450, batch loss 0.4665, batch acc 0.5678
10:43:47.197   Training iter 500, batch loss 0.4671, batch acc 0.5672
10:43:47.697   Training iter 550, batch loss 0.4665, batch acc 0.5790
10:43:48.196   Training iter 600, batch loss 0.4669, batch acc 0.5628
10:43:48.198 Training @ 214 epoch...
10:43:48.716   Training iter 50, batch loss 0.4672, batch acc 0.5590
10:43:49.213   Training iter 100, batch loss 0.4670, batch acc 0.5676
10:43:49.711   Training iter 150, batch loss 0.4669, batch acc 0.5734
10:43:50.199   Training iter 200, batch loss 0.4668, batch acc 0.5630
10:43:50.703   Training iter 250, batch loss 0.4676, batch acc 0.5576
10:43:51.214   Training iter 300, batch loss 0.4670, batch acc 0.5724
10:43:51.727   Training iter 350, batch loss 0.4665, batch acc 0.5782
10:43:52.261   Training iter 400, batch loss 0.4666, batch acc 0.5784
10:43:52.792   Training iter 450, batch loss 0.4666, batch acc 0.5662
10:43:53.322   Training iter 500, batch loss 0.4667, batch acc 0.5688
10:43:53.858   Training iter 550, batch loss 0.4669, batch acc 0.5678
10:43:54.404   Training iter 600, batch loss 0.4664, batch acc 0.5724
10:43:54.406 Training @ 215 epoch...
10:43:54.931   Training iter 50, batch loss 0.4662, batch acc 0.5838
10:43:55.427   Training iter 100, batch loss 0.4672, batch acc 0.5728
10:43:55.927   Training iter 150, batch loss 0.4667, batch acc 0.5674
10:43:56.422   Training iter 200, batch loss 0.4669, batch acc 0.5734
10:43:56.925   Training iter 250, batch loss 0.4663, batch acc 0.5742
10:43:57.403   Training iter 300, batch loss 0.4668, batch acc 0.5682
10:43:57.885   Training iter 350, batch loss 0.4665, batch acc 0.5662
10:43:58.352   Training iter 400, batch loss 0.4674, batch acc 0.5614
10:43:58.827   Training iter 450, batch loss 0.4670, batch acc 0.5624
10:43:59.295   Training iter 500, batch loss 0.4671, batch acc 0.5638
10:43:59.776   Training iter 550, batch loss 0.4666, batch acc 0.5672
10:44:00.265   Training iter 600, batch loss 0.4668, batch acc 0.5636
10:44:00.266 Testing @ 215 epoch...
10:44:00.301     Testing, total mean loss 0.46643, total acc 0.58280
10:44:00.301 Training @ 216 epoch...
10:44:00.809   Training iter 50, batch loss 0.4668, batch acc 0.5660
10:44:01.307   Training iter 100, batch loss 0.4667, batch acc 0.5662
10:44:01.818   Training iter 150, batch loss 0.4670, batch acc 0.5640
10:44:02.332   Training iter 200, batch loss 0.4674, batch acc 0.5628
10:44:02.824   Training iter 250, batch loss 0.4665, batch acc 0.5850
10:44:03.276   Training iter 300, batch loss 0.4667, batch acc 0.5656
10:44:03.729   Training iter 350, batch loss 0.4668, batch acc 0.5642
10:44:04.202   Training iter 400, batch loss 0.4665, batch acc 0.5702
10:44:04.680   Training iter 450, batch loss 0.4670, batch acc 0.5718
10:44:05.178   Training iter 500, batch loss 0.4666, batch acc 0.5708
10:44:05.715   Training iter 550, batch loss 0.4666, batch acc 0.5724
10:44:06.284   Training iter 600, batch loss 0.4668, batch acc 0.5698
10:44:06.285 Training @ 217 epoch...
10:44:06.853   Training iter 50, batch loss 0.4667, batch acc 0.5684
10:44:07.396   Training iter 100, batch loss 0.4667, batch acc 0.5776
10:44:07.941   Training iter 150, batch loss 0.4668, batch acc 0.5766
10:44:08.486   Training iter 200, batch loss 0.4674, batch acc 0.5594
10:44:09.015   Training iter 250, batch loss 0.4666, batch acc 0.5664
10:44:09.516   Training iter 300, batch loss 0.4673, batch acc 0.5626
10:44:10.026   Training iter 350, batch loss 0.4664, batch acc 0.5740
10:44:10.529   Training iter 400, batch loss 0.4664, batch acc 0.5612
10:44:11.025   Training iter 450, batch loss 0.4666, batch acc 0.5660
10:44:11.500   Training iter 500, batch loss 0.4667, batch acc 0.5786
10:44:11.978   Training iter 550, batch loss 0.4665, batch acc 0.5718
10:44:12.457   Training iter 600, batch loss 0.4667, batch acc 0.5680
10:44:12.459 Training @ 218 epoch...
10:44:12.963   Training iter 50, batch loss 0.4664, batch acc 0.5764
10:44:13.449   Training iter 100, batch loss 0.4670, batch acc 0.5626
10:44:13.925   Training iter 150, batch loss 0.4664, batch acc 0.5758
10:44:14.405   Training iter 200, batch loss 0.4666, batch acc 0.5694
10:44:14.882   Training iter 250, batch loss 0.4671, batch acc 0.5580
10:44:15.391   Training iter 300, batch loss 0.4663, batch acc 0.5696
10:44:15.887   Training iter 350, batch loss 0.4663, batch acc 0.5778
10:44:16.374   Training iter 400, batch loss 0.4669, batch acc 0.5656
10:44:16.877   Training iter 450, batch loss 0.4669, batch acc 0.5700
10:44:17.390   Training iter 500, batch loss 0.4671, batch acc 0.5754
10:44:17.879   Training iter 550, batch loss 0.4667, batch acc 0.5700
10:44:18.340   Training iter 600, batch loss 0.4667, batch acc 0.5602
10:44:18.341 Training @ 219 epoch...
10:44:18.823   Training iter 50, batch loss 0.4666, batch acc 0.5750
10:44:19.334   Training iter 100, batch loss 0.4670, batch acc 0.5644
10:44:19.845   Training iter 150, batch loss 0.4664, batch acc 0.5730
10:44:20.361   Training iter 200, batch loss 0.4668, batch acc 0.5628
10:44:20.875   Training iter 250, batch loss 0.4668, batch acc 0.5706
10:44:21.390   Training iter 300, batch loss 0.4668, batch acc 0.5682
10:44:21.891   Training iter 350, batch loss 0.4666, batch acc 0.5698
10:44:22.410   Training iter 400, batch loss 0.4671, batch acc 0.5634
10:44:22.920   Training iter 450, batch loss 0.4668, batch acc 0.5732
10:44:23.466   Training iter 500, batch loss 0.4664, batch acc 0.5636
10:44:24.017   Training iter 550, batch loss 0.4664, batch acc 0.5728
10:44:24.565   Training iter 600, batch loss 0.4665, batch acc 0.5804
10:44:24.566 Training @ 220 epoch...
10:44:25.116   Training iter 50, batch loss 0.4664, batch acc 0.5778
10:44:25.660   Training iter 100, batch loss 0.4669, batch acc 0.5742
10:44:26.191   Training iter 150, batch loss 0.4667, batch acc 0.5690
10:44:26.714   Training iter 200, batch loss 0.4669, batch acc 0.5662
10:44:27.215   Training iter 250, batch loss 0.4667, batch acc 0.5602
10:44:27.710   Training iter 300, batch loss 0.4668, batch acc 0.5750
10:44:28.256   Training iter 350, batch loss 0.4665, batch acc 0.5630
10:44:28.803   Training iter 400, batch loss 0.4668, batch acc 0.5752
10:44:29.328   Training iter 450, batch loss 0.4664, batch acc 0.5788
10:44:29.829   Training iter 500, batch loss 0.4664, batch acc 0.5690
10:44:30.331   Training iter 550, batch loss 0.4664, batch acc 0.5704
10:44:30.843   Training iter 600, batch loss 0.4667, batch acc 0.5594
10:44:30.844 Testing @ 220 epoch...
10:44:30.879     Testing, total mean loss 0.46625, total acc 0.58460
10:44:30.879 Training @ 221 epoch...
10:44:31.374   Training iter 50, batch loss 0.4670, batch acc 0.5700
10:44:31.837   Training iter 100, batch loss 0.4669, batch acc 0.5686
10:44:32.311   Training iter 150, batch loss 0.4663, batch acc 0.5758
10:44:32.760   Training iter 200, batch loss 0.4664, batch acc 0.5672
10:44:33.250   Training iter 250, batch loss 0.4667, batch acc 0.5710
10:44:33.744   Training iter 300, batch loss 0.4665, batch acc 0.5736
10:44:34.225   Training iter 350, batch loss 0.4666, batch acc 0.5714
10:44:34.725   Training iter 400, batch loss 0.4667, batch acc 0.5666
10:44:35.219   Training iter 450, batch loss 0.4666, batch acc 0.5620
10:44:35.688   Training iter 500, batch loss 0.4666, batch acc 0.5716
10:44:36.156   Training iter 550, batch loss 0.4665, batch acc 0.5696
10:44:36.638   Training iter 600, batch loss 0.4664, batch acc 0.5740
10:44:36.640 Training @ 222 epoch...
10:44:37.134   Training iter 50, batch loss 0.4670, batch acc 0.5626
10:44:37.623   Training iter 100, batch loss 0.4662, batch acc 0.5722
10:44:38.088   Training iter 150, batch loss 0.4665, batch acc 0.5666
10:44:38.550   Training iter 200, batch loss 0.4666, batch acc 0.5678
10:44:39.020   Training iter 250, batch loss 0.4665, batch acc 0.5722
10:44:39.504   Training iter 300, batch loss 0.4668, batch acc 0.5614
10:44:39.980   Training iter 350, batch loss 0.4662, batch acc 0.5676
10:44:40.480   Training iter 400, batch loss 0.4668, batch acc 0.5734
10:44:40.955   Training iter 450, batch loss 0.4669, batch acc 0.5806
10:44:41.441   Training iter 500, batch loss 0.4667, batch acc 0.5646
10:44:41.927   Training iter 550, batch loss 0.4661, batch acc 0.5788
10:44:42.463   Training iter 600, batch loss 0.4663, batch acc 0.5740
10:44:42.466 Training @ 223 epoch...
10:44:43.021   Training iter 50, batch loss 0.4667, batch acc 0.5644
10:44:43.510   Training iter 100, batch loss 0.4665, batch acc 0.5670
10:44:44.001   Training iter 150, batch loss 0.4665, batch acc 0.5664
10:44:44.504   Training iter 200, batch loss 0.4668, batch acc 0.5702
10:44:45.006   Training iter 250, batch loss 0.4659, batch acc 0.5846
10:44:45.492   Training iter 300, batch loss 0.4667, batch acc 0.5668
10:44:45.994   Training iter 350, batch loss 0.4664, batch acc 0.5698
10:44:46.496   Training iter 400, batch loss 0.4663, batch acc 0.5814
10:44:47.052   Training iter 450, batch loss 0.4666, batch acc 0.5740
10:44:47.588   Training iter 500, batch loss 0.4668, batch acc 0.5714
10:44:48.087   Training iter 550, batch loss 0.4665, batch acc 0.5708
10:44:48.575   Training iter 600, batch loss 0.4667, batch acc 0.5582
10:44:48.577 Training @ 224 epoch...
10:44:49.086   Training iter 50, batch loss 0.4662, batch acc 0.5798
10:44:49.611   Training iter 100, batch loss 0.4667, batch acc 0.5740
10:44:50.141   Training iter 150, batch loss 0.4662, batch acc 0.5806
10:44:50.646   Training iter 200, batch loss 0.4667, batch acc 0.5610
10:44:51.133   Training iter 250, batch loss 0.4667, batch acc 0.5628
10:44:51.622   Training iter 300, batch loss 0.4663, batch acc 0.5782
10:44:52.122   Training iter 350, batch loss 0.4665, batch acc 0.5704
10:44:52.629   Training iter 400, batch loss 0.4665, batch acc 0.5648
10:44:53.144   Training iter 450, batch loss 0.4667, batch acc 0.5650
10:44:53.666   Training iter 500, batch loss 0.4665, batch acc 0.5774
10:44:54.164   Training iter 550, batch loss 0.4663, batch acc 0.5662
10:44:54.653   Training iter 600, batch loss 0.4667, batch acc 0.5660
10:44:54.655 Training @ 225 epoch...
10:44:55.172   Training iter 50, batch loss 0.4664, batch acc 0.5660
10:44:55.675   Training iter 100, batch loss 0.4662, batch acc 0.5778
10:44:56.175   Training iter 150, batch loss 0.4669, batch acc 0.5618
10:44:56.664   Training iter 200, batch loss 0.4663, batch acc 0.5684
10:44:57.156   Training iter 250, batch loss 0.4665, batch acc 0.5732
10:44:57.663   Training iter 300, batch loss 0.4663, batch acc 0.5736
10:44:58.171   Training iter 350, batch loss 0.4663, batch acc 0.5768
10:44:58.658   Training iter 400, batch loss 0.4667, batch acc 0.5678
10:44:59.129   Training iter 450, batch loss 0.4667, batch acc 0.5716
10:44:59.621   Training iter 500, batch loss 0.4663, batch acc 0.5686
10:45:00.126   Training iter 550, batch loss 0.4666, batch acc 0.5712
10:45:00.642   Training iter 600, batch loss 0.4663, batch acc 0.5756
10:45:00.643 Testing @ 225 epoch...
10:45:00.680     Testing, total mean loss 0.46608, total acc 0.58550
10:45:00.680 Training @ 226 epoch...
10:45:01.287   Training iter 50, batch loss 0.4665, batch acc 0.5644
10:45:02.068   Training iter 100, batch loss 0.4663, batch acc 0.5748
10:45:02.855   Training iter 150, batch loss 0.4668, batch acc 0.5632
10:45:03.635   Training iter 200, batch loss 0.4662, batch acc 0.5812
10:45:04.373   Training iter 250, batch loss 0.4669, batch acc 0.5710
10:45:05.105   Training iter 300, batch loss 0.4663, batch acc 0.5702
10:45:05.827   Training iter 350, batch loss 0.4664, batch acc 0.5786
10:45:06.505   Training iter 400, batch loss 0.4664, batch acc 0.5710
10:45:07.009   Training iter 450, batch loss 0.4663, batch acc 0.5672
10:45:07.542   Training iter 500, batch loss 0.4659, batch acc 0.5744
10:45:08.096   Training iter 550, batch loss 0.4667, batch acc 0.5640
10:45:08.600   Training iter 600, batch loss 0.4664, batch acc 0.5726
10:45:08.602 Training @ 227 epoch...
10:45:09.077   Training iter 50, batch loss 0.4669, batch acc 0.5686
10:45:09.545   Training iter 100, batch loss 0.4664, batch acc 0.5718
10:45:10.011   Training iter 150, batch loss 0.4665, batch acc 0.5640
10:45:10.499   Training iter 200, batch loss 0.4664, batch acc 0.5668
10:45:10.957   Training iter 250, batch loss 0.4661, batch acc 0.5678
10:45:11.445   Training iter 300, batch loss 0.4664, batch acc 0.5758
10:45:11.932   Training iter 350, batch loss 0.4663, batch acc 0.5732
10:45:12.408   Training iter 400, batch loss 0.4664, batch acc 0.5698
10:45:12.895   Training iter 450, batch loss 0.4663, batch acc 0.5718
10:45:13.379   Training iter 500, batch loss 0.4663, batch acc 0.5770
10:45:13.860   Training iter 550, batch loss 0.4664, batch acc 0.5688
10:45:14.341   Training iter 600, batch loss 0.4664, batch acc 0.5798
10:45:14.342 Training @ 228 epoch...
10:45:14.861   Training iter 50, batch loss 0.4666, batch acc 0.5670
10:45:15.383   Training iter 100, batch loss 0.4663, batch acc 0.5716
10:45:15.891   Training iter 150, batch loss 0.4662, batch acc 0.5718
10:45:16.404   Training iter 200, batch loss 0.4664, batch acc 0.5676
10:45:16.938   Training iter 250, batch loss 0.4659, batch acc 0.5826
10:45:17.458   Training iter 300, batch loss 0.4663, batch acc 0.5756
10:45:17.970   Training iter 350, batch loss 0.4665, batch acc 0.5664
10:45:18.474   Training iter 400, batch loss 0.4666, batch acc 0.5670
10:45:18.973   Training iter 450, batch loss 0.4666, batch acc 0.5706
10:45:19.458   Training iter 500, batch loss 0.4660, batch acc 0.5872
10:45:19.953   Training iter 550, batch loss 0.4667, batch acc 0.5622
10:45:20.445   Training iter 600, batch loss 0.4662, batch acc 0.5680
10:45:20.446 Training @ 229 epoch...
10:45:20.923   Training iter 50, batch loss 0.4662, batch acc 0.5612
10:45:21.397   Training iter 100, batch loss 0.4668, batch acc 0.5688
10:45:21.866   Training iter 150, batch loss 0.4661, batch acc 0.5752
10:45:22.357   Training iter 200, batch loss 0.4664, batch acc 0.5682
10:45:22.876   Training iter 250, batch loss 0.4662, batch acc 0.5768
10:45:23.390   Training iter 300, batch loss 0.4659, batch acc 0.5792
10:45:23.875   Training iter 350, batch loss 0.4664, batch acc 0.5764
10:45:24.364   Training iter 400, batch loss 0.4666, batch acc 0.5570
10:45:24.869   Training iter 450, batch loss 0.4665, batch acc 0.5748
10:45:25.390   Training iter 500, batch loss 0.4665, batch acc 0.5624
10:45:25.886   Training iter 550, batch loss 0.4663, batch acc 0.5850
10:45:26.372   Training iter 600, batch loss 0.4662, batch acc 0.5746
10:45:26.374 Training @ 230 epoch...
10:45:26.900   Training iter 50, batch loss 0.4657, batch acc 0.5776
10:45:27.432   Training iter 100, batch loss 0.4667, batch acc 0.5628
10:45:27.987   Training iter 150, batch loss 0.4658, batch acc 0.5738
10:45:28.501   Training iter 200, batch loss 0.4669, batch acc 0.5666
10:45:29.009   Training iter 250, batch loss 0.4664, batch acc 0.5734
10:45:29.520   Training iter 300, batch loss 0.4665, batch acc 0.5686
10:45:30.039   Training iter 350, batch loss 0.4668, batch acc 0.5570
10:45:30.558   Training iter 400, batch loss 0.4662, batch acc 0.5758
10:45:31.078   Training iter 450, batch loss 0.4661, batch acc 0.5810
10:45:31.605   Training iter 500, batch loss 0.4662, batch acc 0.5736
10:45:32.113   Training iter 550, batch loss 0.4663, batch acc 0.5674
10:45:32.615   Training iter 600, batch loss 0.4661, batch acc 0.5848
10:45:32.617 Testing @ 230 epoch...
10:45:32.652     Testing, total mean loss 0.46592, total acc 0.58630
10:45:32.652 Training @ 231 epoch...
10:45:33.139   Training iter 50, batch loss 0.4663, batch acc 0.5720
10:45:33.615   Training iter 100, batch loss 0.4664, batch acc 0.5728
10:45:34.085   Training iter 150, batch loss 0.4662, batch acc 0.5712
10:45:34.558   Training iter 200, batch loss 0.4663, batch acc 0.5612
10:45:35.046   Training iter 250, batch loss 0.4659, batch acc 0.5776
10:45:35.533   Training iter 300, batch loss 0.4663, batch acc 0.5792
10:45:36.008   Training iter 350, batch loss 0.4666, batch acc 0.5612
10:45:36.530   Training iter 400, batch loss 0.4660, batch acc 0.5734
10:45:37.047   Training iter 450, batch loss 0.4664, batch acc 0.5786
10:45:37.578   Training iter 500, batch loss 0.4663, batch acc 0.5738
10:45:38.227   Training iter 550, batch loss 0.4662, batch acc 0.5796
10:45:38.751   Training iter 600, batch loss 0.4663, batch acc 0.5678
10:45:38.753 Training @ 232 epoch...
10:45:39.269   Training iter 50, batch loss 0.4662, batch acc 0.5746
10:45:39.784   Training iter 100, batch loss 0.4662, batch acc 0.5650
10:45:40.291   Training iter 150, batch loss 0.4660, batch acc 0.5800
10:45:40.802   Training iter 200, batch loss 0.4660, batch acc 0.5720
10:45:41.307   Training iter 250, batch loss 0.4663, batch acc 0.5626
10:45:41.815   Training iter 300, batch loss 0.4663, batch acc 0.5774
10:45:42.327   Training iter 350, batch loss 0.4664, batch acc 0.5756
10:45:42.886   Training iter 400, batch loss 0.4661, batch acc 0.5806
10:45:43.480   Training iter 450, batch loss 0.4661, batch acc 0.5788
10:45:44.032   Training iter 500, batch loss 0.4664, batch acc 0.5654
10:45:44.592   Training iter 550, batch loss 0.4662, batch acc 0.5698
10:45:45.165   Training iter 600, batch loss 0.4666, batch acc 0.5628
10:45:45.166 Training @ 233 epoch...
10:45:45.734   Training iter 50, batch loss 0.4663, batch acc 0.5702
10:45:46.283   Training iter 100, batch loss 0.4661, batch acc 0.5732
10:45:46.833   Training iter 150, batch loss 0.4660, batch acc 0.5784
10:45:47.362   Training iter 200, batch loss 0.4665, batch acc 0.5776
10:45:47.903   Training iter 250, batch loss 0.4664, batch acc 0.5610
10:45:48.438   Training iter 300, batch loss 0.4664, batch acc 0.5748
10:45:48.954   Training iter 350, batch loss 0.4665, batch acc 0.5776
10:45:49.459   Training iter 400, batch loss 0.4661, batch acc 0.5718
10:45:49.971   Training iter 450, batch loss 0.4658, batch acc 0.5748
10:45:50.491   Training iter 500, batch loss 0.4660, batch acc 0.5674
10:45:50.997   Training iter 550, batch loss 0.4666, batch acc 0.5536
10:45:51.500   Training iter 600, batch loss 0.4657, batch acc 0.5938
10:45:51.502 Training @ 234 epoch...
10:45:52.020   Training iter 50, batch loss 0.4660, batch acc 0.5664
10:45:52.544   Training iter 100, batch loss 0.4666, batch acc 0.5624
10:45:53.069   Training iter 150, batch loss 0.4665, batch acc 0.5752
10:45:53.583   Training iter 200, batch loss 0.4662, batch acc 0.5712
10:45:54.087   Training iter 250, batch loss 0.4658, batch acc 0.5868
10:45:54.587   Training iter 300, batch loss 0.4659, batch acc 0.5718
10:45:55.101   Training iter 350, batch loss 0.4662, batch acc 0.5732
10:45:55.598   Training iter 400, batch loss 0.4665, batch acc 0.5664
10:45:56.095   Training iter 450, batch loss 0.4661, batch acc 0.5858
10:45:56.595   Training iter 500, batch loss 0.4663, batch acc 0.5778
10:45:57.103   Training iter 550, batch loss 0.4658, batch acc 0.5764
10:45:57.614   Training iter 600, batch loss 0.4662, batch acc 0.5634
10:45:57.616 Training @ 235 epoch...
10:45:58.138   Training iter 50, batch loss 0.4658, batch acc 0.5728
10:45:58.652   Training iter 100, batch loss 0.4663, batch acc 0.5644
10:45:59.208   Training iter 150, batch loss 0.4665, batch acc 0.5690
10:45:59.784   Training iter 200, batch loss 0.4662, batch acc 0.5740
10:46:00.358   Training iter 250, batch loss 0.4662, batch acc 0.5718
10:46:00.928   Training iter 300, batch loss 0.4661, batch acc 0.5696
10:46:01.473   Training iter 350, batch loss 0.4663, batch acc 0.5706
10:46:02.027   Training iter 400, batch loss 0.4660, batch acc 0.5800
10:46:02.570   Training iter 450, batch loss 0.4662, batch acc 0.5814
10:46:03.098   Training iter 500, batch loss 0.4661, batch acc 0.5726
10:46:03.609   Training iter 550, batch loss 0.4661, batch acc 0.5704
10:46:04.107   Training iter 600, batch loss 0.4659, batch acc 0.5832
10:46:04.108 Testing @ 235 epoch...
10:46:04.143     Testing, total mean loss 0.46576, total acc 0.58760
10:46:04.144 Training @ 236 epoch...
10:46:04.655   Training iter 50, batch loss 0.4657, batch acc 0.5866
10:46:05.170   Training iter 100, batch loss 0.4659, batch acc 0.5814
10:46:05.673   Training iter 150, batch loss 0.4661, batch acc 0.5690
10:46:06.157   Training iter 200, batch loss 0.4661, batch acc 0.5742
10:46:06.648   Training iter 250, batch loss 0.4664, batch acc 0.5698
10:46:07.162   Training iter 300, batch loss 0.4663, batch acc 0.5668
10:46:07.673   Training iter 350, batch loss 0.4660, batch acc 0.5850
10:46:08.182   Training iter 400, batch loss 0.4661, batch acc 0.5720
10:46:08.681   Training iter 450, batch loss 0.4668, batch acc 0.5552
10:46:09.191   Training iter 500, batch loss 0.4665, batch acc 0.5678
10:46:09.690   Training iter 550, batch loss 0.4658, batch acc 0.5766
10:46:10.198   Training iter 600, batch loss 0.4659, batch acc 0.5766
10:46:10.200 Training @ 237 epoch...
10:46:10.720   Training iter 50, batch loss 0.4660, batch acc 0.5788
10:46:11.227   Training iter 100, batch loss 0.4661, batch acc 0.5626
10:46:11.734   Training iter 150, batch loss 0.4659, batch acc 0.5814
10:46:12.251   Training iter 200, batch loss 0.4662, batch acc 0.5790
10:46:12.759   Training iter 250, batch loss 0.4659, batch acc 0.5770
10:46:13.256   Training iter 300, batch loss 0.4658, batch acc 0.5776
10:46:13.720   Training iter 350, batch loss 0.4661, batch acc 0.5846
10:46:14.206   Training iter 400, batch loss 0.4661, batch acc 0.5778
10:46:14.692   Training iter 450, batch loss 0.4661, batch acc 0.5690
10:46:15.175   Training iter 500, batch loss 0.4664, batch acc 0.5646
10:46:15.679   Training iter 550, batch loss 0.4661, batch acc 0.5670
10:46:16.205   Training iter 600, batch loss 0.4665, batch acc 0.5640
10:46:16.207 Training @ 238 epoch...
10:46:16.736   Training iter 50, batch loss 0.4664, batch acc 0.5764
10:46:17.258   Training iter 100, batch loss 0.4660, batch acc 0.5692
10:46:17.785   Training iter 150, batch loss 0.4657, batch acc 0.5690
10:46:18.332   Training iter 200, batch loss 0.4664, batch acc 0.5744
10:46:18.861   Training iter 250, batch loss 0.4664, batch acc 0.5546
10:46:19.400   Training iter 300, batch loss 0.4662, batch acc 0.5692
10:46:19.945   Training iter 350, batch loss 0.4661, batch acc 0.5664
10:46:20.489   Training iter 400, batch loss 0.4659, batch acc 0.5812
10:46:21.031   Training iter 450, batch loss 0.4658, batch acc 0.5752
10:46:21.557   Training iter 500, batch loss 0.4657, batch acc 0.5854
10:46:22.057   Training iter 550, batch loss 0.4662, batch acc 0.5778
10:46:22.558   Training iter 600, batch loss 0.4659, batch acc 0.5890
10:46:22.560 Training @ 239 epoch...
10:46:23.037   Training iter 50, batch loss 0.4658, batch acc 0.5816
10:46:23.536   Training iter 100, batch loss 0.4657, batch acc 0.5854
10:46:24.034   Training iter 150, batch loss 0.4663, batch acc 0.5666
10:46:24.510   Training iter 200, batch loss 0.4661, batch acc 0.5764
10:46:24.996   Training iter 250, batch loss 0.4662, batch acc 0.5712
10:46:25.539   Training iter 300, batch loss 0.4659, batch acc 0.5780
10:46:26.034   Training iter 350, batch loss 0.4659, batch acc 0.5708
10:46:26.520   Training iter 400, batch loss 0.4656, batch acc 0.5836
10:46:27.023   Training iter 450, batch loss 0.4661, batch acc 0.5708
10:46:27.513   Training iter 500, batch loss 0.4662, batch acc 0.5720
10:46:28.017   Training iter 550, batch loss 0.4661, batch acc 0.5666
10:46:28.511   Training iter 600, batch loss 0.4664, batch acc 0.5664
10:46:28.512 Training @ 240 epoch...
10:46:28.996   Training iter 50, batch loss 0.4660, batch acc 0.5706
10:46:29.467   Training iter 100, batch loss 0.4655, batch acc 0.5742
10:46:29.939   Training iter 150, batch loss 0.4656, batch acc 0.5796
10:46:30.422   Training iter 200, batch loss 0.4662, batch acc 0.5744
10:46:30.908   Training iter 250, batch loss 0.4660, batch acc 0.5690
10:46:31.394   Training iter 300, batch loss 0.4665, batch acc 0.5594
10:46:31.897   Training iter 350, batch loss 0.4662, batch acc 0.5738
10:46:32.401   Training iter 400, batch loss 0.4660, batch acc 0.5822
10:46:32.908   Training iter 450, batch loss 0.4657, batch acc 0.5772
10:46:33.451   Training iter 500, batch loss 0.4659, batch acc 0.5752
10:46:33.981   Training iter 550, batch loss 0.4663, batch acc 0.5756
10:46:34.499   Training iter 600, batch loss 0.4660, batch acc 0.5782
10:46:34.501 Testing @ 240 epoch...
10:46:34.537     Testing, total mean loss 0.46561, total acc 0.58890
10:46:34.537 Training @ 241 epoch...
10:46:35.058   Training iter 50, batch loss 0.4657, batch acc 0.5880
10:46:35.555   Training iter 100, batch loss 0.4659, batch acc 0.5670
10:46:36.051   Training iter 150, batch loss 0.4660, batch acc 0.5838
10:46:36.544   Training iter 200, batch loss 0.4659, batch acc 0.5720
10:46:37.059   Training iter 250, batch loss 0.4660, batch acc 0.5742
10:46:37.594   Training iter 300, batch loss 0.4658, batch acc 0.5846
10:46:38.104   Training iter 350, batch loss 0.4661, batch acc 0.5594
10:46:38.574   Training iter 400, batch loss 0.4658, batch acc 0.5728
10:46:39.057   Training iter 450, batch loss 0.4662, batch acc 0.5744
10:46:39.556   Training iter 500, batch loss 0.4662, batch acc 0.5682
10:46:40.071   Training iter 550, batch loss 0.4661, batch acc 0.5706
10:46:40.576   Training iter 600, batch loss 0.4659, batch acc 0.5780
10:46:40.578 Training @ 242 epoch...
10:46:41.081   Training iter 50, batch loss 0.4658, batch acc 0.5910
10:46:41.564   Training iter 100, batch loss 0.4657, batch acc 0.5698
10:46:42.053   Training iter 150, batch loss 0.4659, batch acc 0.5762
10:46:42.598   Training iter 200, batch loss 0.4658, batch acc 0.5732
10:46:43.165   Training iter 250, batch loss 0.4662, batch acc 0.5696
10:46:43.724   Training iter 300, batch loss 0.4664, batch acc 0.5624
10:46:44.251   Training iter 350, batch loss 0.4662, batch acc 0.5688
10:46:44.755   Training iter 400, batch loss 0.4662, batch acc 0.5714
10:46:45.287   Training iter 450, batch loss 0.4660, batch acc 0.5726
10:46:45.836   Training iter 500, batch loss 0.4655, batch acc 0.5820
10:46:46.370   Training iter 550, batch loss 0.4658, batch acc 0.5800
10:46:46.897   Training iter 600, batch loss 0.4658, batch acc 0.5772
10:46:46.899 Training @ 243 epoch...
10:46:47.436   Training iter 50, batch loss 0.4665, batch acc 0.5726
10:46:47.983   Training iter 100, batch loss 0.4658, batch acc 0.5724
10:46:48.547   Training iter 150, batch loss 0.4661, batch acc 0.5772
10:46:49.106   Training iter 200, batch loss 0.4657, batch acc 0.5760
10:46:49.663   Training iter 250, batch loss 0.4654, batch acc 0.5784
10:46:50.229   Training iter 300, batch loss 0.4661, batch acc 0.5726
10:46:50.784   Training iter 350, batch loss 0.4655, batch acc 0.5870
10:46:51.324   Training iter 400, batch loss 0.4659, batch acc 0.5740
10:46:51.852   Training iter 450, batch loss 0.4658, batch acc 0.5708
10:46:52.404   Training iter 500, batch loss 0.4657, batch acc 0.5846
10:46:52.944   Training iter 550, batch loss 0.4664, batch acc 0.5620
10:46:53.486   Training iter 600, batch loss 0.4661, batch acc 0.5684
10:46:53.487 Training @ 244 epoch...
10:46:54.006   Training iter 50, batch loss 0.4657, batch acc 0.5698
10:46:54.506   Training iter 100, batch loss 0.4661, batch acc 0.5704
10:46:55.025   Training iter 150, batch loss 0.4654, batch acc 0.5702
10:46:55.549   Training iter 200, batch loss 0.4660, batch acc 0.5786
10:46:56.048   Training iter 250, batch loss 0.4661, batch acc 0.5826
10:46:56.543   Training iter 300, batch loss 0.4659, batch acc 0.5688
10:46:57.048   Training iter 350, batch loss 0.4660, batch acc 0.5712
10:46:57.560   Training iter 400, batch loss 0.4657, batch acc 0.5868
10:46:58.083   Training iter 450, batch loss 0.4663, batch acc 0.5628
10:46:58.626   Training iter 500, batch loss 0.4660, batch acc 0.5702
10:46:59.127   Training iter 550, batch loss 0.4658, batch acc 0.5854
10:46:59.632   Training iter 600, batch loss 0.4657, batch acc 0.5810
10:46:59.634 Training @ 245 epoch...
10:47:00.156   Training iter 50, batch loss 0.4662, batch acc 0.5714
10:47:00.691   Training iter 100, batch loss 0.4660, batch acc 0.5758
10:47:01.204   Training iter 150, batch loss 0.4653, batch acc 0.5750
10:47:01.733   Training iter 200, batch loss 0.4657, batch acc 0.5848
10:47:02.260   Training iter 250, batch loss 0.4660, batch acc 0.5756
10:47:02.761   Training iter 300, batch loss 0.4660, batch acc 0.5756
10:47:03.270   Training iter 350, batch loss 0.4658, batch acc 0.5756
10:47:03.788   Training iter 400, batch loss 0.4662, batch acc 0.5670
10:47:04.342   Training iter 450, batch loss 0.4658, batch acc 0.5692
10:47:04.901   Training iter 500, batch loss 0.4660, batch acc 0.5730
10:47:05.470   Training iter 550, batch loss 0.4656, batch acc 0.5794
10:47:06.031   Training iter 600, batch loss 0.4659, batch acc 0.5784
10:47:06.033 Testing @ 245 epoch...
10:47:06.069     Testing, total mean loss 0.46546, total acc 0.58900
10:47:06.069 Training @ 246 epoch...
10:47:06.638   Training iter 50, batch loss 0.4655, batch acc 0.5874
10:47:07.185   Training iter 100, batch loss 0.4656, batch acc 0.5626
10:47:07.748   Training iter 150, batch loss 0.4661, batch acc 0.5642
10:47:08.321   Training iter 200, batch loss 0.4662, batch acc 0.5654
10:47:08.876   Training iter 250, batch loss 0.4655, batch acc 0.5830
10:47:09.421   Training iter 300, batch loss 0.4662, batch acc 0.5716
10:47:09.959   Training iter 350, batch loss 0.4658, batch acc 0.5780
10:47:10.489   Training iter 400, batch loss 0.4663, batch acc 0.5640
10:47:10.994   Training iter 450, batch loss 0.4658, batch acc 0.5798
10:47:11.513   Training iter 500, batch loss 0.4657, batch acc 0.5840
10:47:12.015   Training iter 550, batch loss 0.4661, batch acc 0.5790
10:47:12.524   Training iter 600, batch loss 0.4652, batch acc 0.5842
10:47:12.526 Training @ 247 epoch...
10:47:13.056   Training iter 50, batch loss 0.4659, batch acc 0.5788
10:47:13.564   Training iter 100, batch loss 0.4662, batch acc 0.5640
10:47:14.061   Training iter 150, batch loss 0.4657, batch acc 0.5890
10:47:14.553   Training iter 200, batch loss 0.4659, batch acc 0.5794
10:47:15.055   Training iter 250, batch loss 0.4654, batch acc 0.5816
10:47:15.538   Training iter 300, batch loss 0.4657, batch acc 0.5696
10:47:16.026   Training iter 350, batch loss 0.4660, batch acc 0.5702
10:47:16.497   Training iter 400, batch loss 0.4655, batch acc 0.5810
10:47:16.963   Training iter 450, batch loss 0.4660, batch acc 0.5694
10:47:17.437   Training iter 500, batch loss 0.4659, batch acc 0.5714
10:47:17.920   Training iter 550, batch loss 0.4662, batch acc 0.5734
10:47:18.418   Training iter 600, batch loss 0.4654, batch acc 0.5760
10:47:18.420 Training @ 248 epoch...
10:47:18.903   Training iter 50, batch loss 0.4659, batch acc 0.5748
10:47:19.427   Training iter 100, batch loss 0.4656, batch acc 0.5696
10:47:19.981   Training iter 150, batch loss 0.4657, batch acc 0.5766
10:47:20.546   Training iter 200, batch loss 0.4654, batch acc 0.5852
10:47:21.121   Training iter 250, batch loss 0.4656, batch acc 0.5824
10:47:21.676   Training iter 300, batch loss 0.4658, batch acc 0.5800
10:47:22.241   Training iter 350, batch loss 0.4661, batch acc 0.5696
10:47:22.805   Training iter 400, batch loss 0.4661, batch acc 0.5716
10:47:23.361   Training iter 450, batch loss 0.4659, batch acc 0.5718
10:47:23.907   Training iter 500, batch loss 0.4658, batch acc 0.5714
10:47:24.461   Training iter 550, batch loss 0.4657, batch acc 0.5748
10:47:25.025   Training iter 600, batch loss 0.4657, batch acc 0.5798
10:47:25.026 Training @ 249 epoch...
10:47:25.596   Training iter 50, batch loss 0.4664, batch acc 0.5696
10:47:26.128   Training iter 100, batch loss 0.4657, batch acc 0.5816
10:47:26.645   Training iter 150, batch loss 0.4655, batch acc 0.5756
10:47:27.173   Training iter 200, batch loss 0.4656, batch acc 0.5814
10:47:27.687   Training iter 250, batch loss 0.4654, batch acc 0.5742
10:47:28.198   Training iter 300, batch loss 0.4658, batch acc 0.5808
10:47:28.702   Training iter 350, batch loss 0.4661, batch acc 0.5730
10:47:29.218   Training iter 400, batch loss 0.4656, batch acc 0.5744
10:47:29.725   Training iter 450, batch loss 0.4656, batch acc 0.5834
10:47:30.242   Training iter 500, batch loss 0.4663, batch acc 0.5692
10:47:30.804   Training iter 550, batch loss 0.4657, batch acc 0.5680
10:47:31.370   Training iter 600, batch loss 0.4652, batch acc 0.5794
10:47:31.372 Training @ 250 epoch...
10:47:31.947   Training iter 50, batch loss 0.4659, batch acc 0.5790
10:47:32.455   Training iter 100, batch loss 0.4653, batch acc 0.5792
10:47:32.977   Training iter 150, batch loss 0.4657, batch acc 0.5790
10:47:33.525   Training iter 200, batch loss 0.4659, batch acc 0.5674
10:47:34.057   Training iter 250, batch loss 0.4658, batch acc 0.5756
10:47:34.570   Training iter 300, batch loss 0.4660, batch acc 0.5754
10:47:35.065   Training iter 350, batch loss 0.4659, batch acc 0.5786
10:47:35.564   Training iter 400, batch loss 0.4658, batch acc 0.5722
10:47:36.044   Training iter 450, batch loss 0.4654, batch acc 0.5706
10:47:36.541   Training iter 500, batch loss 0.4658, batch acc 0.5770
10:47:37.041   Training iter 550, batch loss 0.4654, batch acc 0.5764
10:47:37.539   Training iter 600, batch loss 0.4658, batch acc 0.5792
10:47:37.541 Testing @ 250 epoch...
10:47:37.576     Testing, total mean loss 0.46532, total acc 0.59030
10:47:37.576 Training @ 251 epoch...
10:47:38.081   Training iter 50, batch loss 0.4652, batch acc 0.5916
10:47:38.582   Training iter 100, batch loss 0.4656, batch acc 0.5836
10:47:39.074   Training iter 150, batch loss 0.4656, batch acc 0.5800
10:47:39.576   Training iter 200, batch loss 0.4655, batch acc 0.5744
10:47:40.089   Training iter 250, batch loss 0.4658, batch acc 0.5732
10:47:40.596   Training iter 300, batch loss 0.4660, batch acc 0.5690
10:47:41.093   Training iter 350, batch loss 0.4657, batch acc 0.5752
10:47:41.590   Training iter 400, batch loss 0.4660, batch acc 0.5694
10:47:42.093   Training iter 450, batch loss 0.4655, batch acc 0.5802
10:47:42.595   Training iter 500, batch loss 0.4656, batch acc 0.5762
10:47:43.099   Training iter 550, batch loss 0.4661, batch acc 0.5668
10:47:43.605   Training iter 600, batch loss 0.4658, batch acc 0.5706
10:47:43.606 Training @ 252 epoch...
10:47:44.121   Training iter 50, batch loss 0.4661, batch acc 0.5732
10:47:44.645   Training iter 100, batch loss 0.4654, batch acc 0.5820
10:47:45.156   Training iter 150, batch loss 0.4654, batch acc 0.5808
10:47:45.673   Training iter 200, batch loss 0.4659, batch acc 0.5700
10:47:46.168   Training iter 250, batch loss 0.4657, batch acc 0.5714
10:47:46.642   Training iter 300, batch loss 0.4654, batch acc 0.5796
10:47:47.122   Training iter 350, batch loss 0.4659, batch acc 0.5754
10:47:47.599   Training iter 400, batch loss 0.4656, batch acc 0.5808
10:47:48.083   Training iter 450, batch loss 0.4655, batch acc 0.5768
10:47:48.557   Training iter 500, batch loss 0.4660, batch acc 0.5676
10:47:49.034   Training iter 550, batch loss 0.4658, batch acc 0.5788
10:47:49.514   Training iter 600, batch loss 0.4653, batch acc 0.5768
10:47:49.516 Training @ 253 epoch...
10:47:49.997   Training iter 50, batch loss 0.4660, batch acc 0.5684
10:47:50.496   Training iter 100, batch loss 0.4657, batch acc 0.5684
10:47:50.973   Training iter 150, batch loss 0.4654, batch acc 0.5738
10:47:51.459   Training iter 200, batch loss 0.4655, batch acc 0.5796
10:47:51.998   Training iter 250, batch loss 0.4657, batch acc 0.5654
10:47:52.552   Training iter 300, batch loss 0.4656, batch acc 0.5690
10:47:53.103   Training iter 350, batch loss 0.4656, batch acc 0.5906
10:47:53.637   Training iter 400, batch loss 0.4657, batch acc 0.5828
10:47:54.174   Training iter 450, batch loss 0.4655, batch acc 0.5784
10:47:54.708   Training iter 500, batch loss 0.4657, batch acc 0.5798
10:47:55.250   Training iter 550, batch loss 0.4661, batch acc 0.5746
10:47:55.787   Training iter 600, batch loss 0.4652, batch acc 0.5856
10:47:55.788 Training @ 254 epoch...
10:47:56.320   Training iter 50, batch loss 0.4658, batch acc 0.5702
10:47:56.824   Training iter 100, batch loss 0.4659, batch acc 0.5740
10:47:57.344   Training iter 150, batch loss 0.4659, batch acc 0.5690
10:47:57.851   Training iter 200, batch loss 0.4653, batch acc 0.5890
10:47:58.336   Training iter 250, batch loss 0.4655, batch acc 0.5834
10:47:58.823   Training iter 300, batch loss 0.4656, batch acc 0.5760
10:47:59.318   Training iter 350, batch loss 0.4649, batch acc 0.5844
10:47:59.815   Training iter 400, batch loss 0.4660, batch acc 0.5750
10:48:00.338   Training iter 450, batch loss 0.4657, batch acc 0.5752
10:48:00.872   Training iter 500, batch loss 0.4655, batch acc 0.5676
10:48:01.414   Training iter 550, batch loss 0.4653, batch acc 0.5810
10:48:01.981   Training iter 600, batch loss 0.4659, batch acc 0.5722
10:48:01.983 Training @ 255 epoch...
10:48:02.526   Training iter 50, batch loss 0.4657, batch acc 0.5752
10:48:03.059   Training iter 100, batch loss 0.4659, batch acc 0.5718
10:48:03.596   Training iter 150, batch loss 0.4652, batch acc 0.5742
10:48:04.132   Training iter 200, batch loss 0.4654, batch acc 0.5750
10:48:04.676   Training iter 250, batch loss 0.4656, batch acc 0.5806
10:48:05.217   Training iter 300, batch loss 0.4658, batch acc 0.5782
10:48:05.761   Training iter 350, batch loss 0.4654, batch acc 0.5796
10:48:06.296   Training iter 400, batch loss 0.4653, batch acc 0.5810
10:48:06.815   Training iter 450, batch loss 0.4659, batch acc 0.5716
10:48:07.297   Training iter 500, batch loss 0.4660, batch acc 0.5690
10:48:07.799   Training iter 550, batch loss 0.4651, batch acc 0.5880
10:48:08.345   Training iter 600, batch loss 0.4657, batch acc 0.5734
10:48:08.347 Testing @ 255 epoch...
10:48:08.382     Testing, total mean loss 0.46518, total acc 0.59210
10:48:08.383 Training @ 256 epoch...
10:48:08.934   Training iter 50, batch loss 0.4653, batch acc 0.5768
10:48:09.487   Training iter 100, batch loss 0.4657, batch acc 0.5814
10:48:10.038   Training iter 150, batch loss 0.4656, batch acc 0.5804
10:48:10.575   Training iter 200, batch loss 0.4657, batch acc 0.5754
10:48:11.123   Training iter 250, batch loss 0.4658, batch acc 0.5808
10:48:11.672   Training iter 300, batch loss 0.4658, batch acc 0.5746
10:48:12.224   Training iter 350, batch loss 0.4657, batch acc 0.5656
10:48:12.770   Training iter 400, batch loss 0.4656, batch acc 0.5764
10:48:13.310   Training iter 450, batch loss 0.4656, batch acc 0.5712
10:48:13.805   Training iter 500, batch loss 0.4657, batch acc 0.5654
10:48:14.284   Training iter 550, batch loss 0.4648, batch acc 0.5940
10:48:14.761   Training iter 600, batch loss 0.4654, batch acc 0.5826
10:48:14.763 Training @ 257 epoch...
10:48:15.273   Training iter 50, batch loss 0.4654, batch acc 0.5800
10:48:15.748   Training iter 100, batch loss 0.4656, batch acc 0.5706
10:48:16.219   Training iter 150, batch loss 0.4654, batch acc 0.5782
10:48:16.696   Training iter 200, batch loss 0.4656, batch acc 0.5712
10:48:17.226   Training iter 250, batch loss 0.4652, batch acc 0.5856
10:48:17.761   Training iter 300, batch loss 0.4657, batch acc 0.5678
10:48:18.286   Training iter 350, batch loss 0.4656, batch acc 0.5700
10:48:18.798   Training iter 400, batch loss 0.4658, batch acc 0.5706
10:48:19.326   Training iter 450, batch loss 0.4652, batch acc 0.5930
10:48:19.878   Training iter 500, batch loss 0.4657, batch acc 0.5794
10:48:20.450   Training iter 550, batch loss 0.4659, batch acc 0.5740
10:48:20.979   Training iter 600, batch loss 0.4655, batch acc 0.5836
10:48:20.980 Training @ 258 epoch...
10:48:21.498   Training iter 50, batch loss 0.4657, batch acc 0.5808
10:48:22.016   Training iter 100, batch loss 0.4653, batch acc 0.5876
10:48:22.539   Training iter 150, batch loss 0.4656, batch acc 0.5758
10:48:23.057   Training iter 200, batch loss 0.4653, batch acc 0.5812
10:48:23.587   Training iter 250, batch loss 0.4650, batch acc 0.5826
10:48:24.138   Training iter 300, batch loss 0.4656, batch acc 0.5754
10:48:24.688   Training iter 350, batch loss 0.4654, batch acc 0.5722
10:48:25.252   Training iter 400, batch loss 0.4655, batch acc 0.5760
10:48:25.803   Training iter 450, batch loss 0.4655, batch acc 0.5742
10:48:26.357   Training iter 500, batch loss 0.4657, batch acc 0.5760
10:48:26.912   Training iter 550, batch loss 0.4658, batch acc 0.5692
10:48:27.483   Training iter 600, batch loss 0.4657, batch acc 0.5738
10:48:27.485 Training @ 259 epoch...
10:48:28.071   Training iter 50, batch loss 0.4655, batch acc 0.5774
10:48:28.655   Training iter 100, batch loss 0.4651, batch acc 0.5900
10:48:29.239   Training iter 150, batch loss 0.4655, batch acc 0.5786
10:48:29.802   Training iter 200, batch loss 0.4655, batch acc 0.5740
10:48:30.312   Training iter 250, batch loss 0.4650, batch acc 0.5826
10:48:30.858   Training iter 300, batch loss 0.4657, batch acc 0.5716
10:48:31.388   Training iter 350, batch loss 0.4656, batch acc 0.5802
10:48:31.921   Training iter 400, batch loss 0.4658, batch acc 0.5700
10:48:32.440   Training iter 450, batch loss 0.4656, batch acc 0.5846
10:48:32.953   Training iter 500, batch loss 0.4655, batch acc 0.5776
10:48:33.452   Training iter 550, batch loss 0.4659, batch acc 0.5634
10:48:33.962   Training iter 600, batch loss 0.4652, batch acc 0.5766
10:48:33.964 Training @ 260 epoch...
10:48:34.478   Training iter 50, batch loss 0.4657, batch acc 0.5794
10:48:34.996   Training iter 100, batch loss 0.4657, batch acc 0.5682
10:48:35.518   Training iter 150, batch loss 0.4656, batch acc 0.5740
10:48:36.026   Training iter 200, batch loss 0.4652, batch acc 0.5776
10:48:36.531   Training iter 250, batch loss 0.4657, batch acc 0.5796
10:48:37.032   Training iter 300, batch loss 0.4658, batch acc 0.5780
10:48:37.545   Training iter 350, batch loss 0.4651, batch acc 0.5804
10:48:38.023   Training iter 400, batch loss 0.4654, batch acc 0.5746
10:48:38.517   Training iter 450, batch loss 0.4657, batch acc 0.5736
10:48:39.024   Training iter 500, batch loss 0.4649, batch acc 0.5876
10:48:39.519   Training iter 550, batch loss 0.4653, batch acc 0.5782
10:48:40.023   Training iter 600, batch loss 0.4655, batch acc 0.5802
10:48:40.025 Testing @ 260 epoch...
10:48:40.062     Testing, total mean loss 0.46505, total acc 0.59260
10:48:40.062 Training @ 261 epoch...
10:48:40.588   Training iter 50, batch loss 0.4655, batch acc 0.5736
10:48:41.110   Training iter 100, batch loss 0.4657, batch acc 0.5738
10:48:41.611   Training iter 150, batch loss 0.4657, batch acc 0.5844
10:48:42.121   Training iter 200, batch loss 0.4655, batch acc 0.5856
10:48:42.631   Training iter 250, batch loss 0.4655, batch acc 0.5682
10:48:43.131   Training iter 300, batch loss 0.4656, batch acc 0.5752
10:48:43.630   Training iter 350, batch loss 0.4651, batch acc 0.5858
10:48:44.148   Training iter 400, batch loss 0.4651, batch acc 0.5836
10:48:44.671   Training iter 450, batch loss 0.4652, batch acc 0.5846
10:48:45.189   Training iter 500, batch loss 0.4652, batch acc 0.5764
10:48:45.740   Training iter 550, batch loss 0.4653, batch acc 0.5694
10:48:46.282   Training iter 600, batch loss 0.4658, batch acc 0.5698
10:48:46.284 Training @ 262 epoch...
10:48:46.831   Training iter 50, batch loss 0.4656, batch acc 0.5706
10:48:47.345   Training iter 100, batch loss 0.4651, batch acc 0.5778
10:48:47.851   Training iter 150, batch loss 0.4654, batch acc 0.5800
10:48:48.337   Training iter 200, batch loss 0.4652, batch acc 0.5874
10:48:48.834   Training iter 250, batch loss 0.4654, batch acc 0.5780
10:48:49.353   Training iter 300, batch loss 0.4655, batch acc 0.5780
10:48:49.891   Training iter 350, batch loss 0.4646, batch acc 0.5760
10:48:50.433   Training iter 400, batch loss 0.4654, batch acc 0.5858
10:48:50.958   Training iter 450, batch loss 0.4658, batch acc 0.5790
10:48:51.474   Training iter 500, batch loss 0.4655, batch acc 0.5842
10:48:52.001   Training iter 550, batch loss 0.4658, batch acc 0.5606
10:48:52.529   Training iter 600, batch loss 0.4657, batch acc 0.5720
10:48:52.531 Training @ 263 epoch...
10:48:53.087   Training iter 50, batch loss 0.4653, batch acc 0.5772
10:48:53.625   Training iter 100, batch loss 0.4651, batch acc 0.5774
10:48:54.153   Training iter 150, batch loss 0.4656, batch acc 0.5690
10:48:54.677   Training iter 200, batch loss 0.4654, batch acc 0.5834
10:48:55.206   Training iter 250, batch loss 0.4652, batch acc 0.5878
10:48:55.729   Training iter 300, batch loss 0.4646, batch acc 0.5964
10:48:56.236   Training iter 350, batch loss 0.4656, batch acc 0.5738
10:48:56.739   Training iter 400, batch loss 0.4654, batch acc 0.5742
10:48:57.271   Training iter 450, batch loss 0.4652, batch acc 0.5784
10:48:57.825   Training iter 500, batch loss 0.4658, batch acc 0.5714
10:48:58.362   Training iter 550, batch loss 0.4658, batch acc 0.5642
10:48:58.890   Training iter 600, batch loss 0.4657, batch acc 0.5784
10:48:58.892 Training @ 264 epoch...
10:48:59.415   Training iter 50, batch loss 0.4656, batch acc 0.5622
10:48:59.926   Training iter 100, batch loss 0.4655, batch acc 0.5688
10:49:00.451   Training iter 150, batch loss 0.4655, batch acc 0.5790
10:49:00.986   Training iter 200, batch loss 0.4654, batch acc 0.5744
10:49:01.513   Training iter 250, batch loss 0.4652, batch acc 0.5868
10:49:02.061   Training iter 300, batch loss 0.4650, batch acc 0.5848
10:49:02.612   Training iter 350, batch loss 0.4655, batch acc 0.5838
10:49:03.110   Training iter 400, batch loss 0.4655, batch acc 0.5872
10:49:03.592   Training iter 450, batch loss 0.4654, batch acc 0.5706
10:49:04.095   Training iter 500, batch loss 0.4649, batch acc 0.5830
10:49:04.592   Training iter 550, batch loss 0.4659, batch acc 0.5738
10:49:05.091   Training iter 600, batch loss 0.4649, batch acc 0.5820
10:49:05.093 Training @ 265 epoch...
10:49:05.608   Training iter 50, batch loss 0.4655, batch acc 0.5736
10:49:06.113   Training iter 100, batch loss 0.4657, batch acc 0.5734
10:49:06.595   Training iter 150, batch loss 0.4653, batch acc 0.5802
10:49:07.113   Training iter 200, batch loss 0.4655, batch acc 0.5788
10:49:07.619   Training iter 250, batch loss 0.4653, batch acc 0.5816
10:49:08.089   Training iter 300, batch loss 0.4652, batch acc 0.5780
10:49:08.566   Training iter 350, batch loss 0.4652, batch acc 0.5852
10:49:09.078   Training iter 400, batch loss 0.4653, batch acc 0.5794
10:49:09.599   Training iter 450, batch loss 0.4650, batch acc 0.5712
10:49:10.144   Training iter 500, batch loss 0.4655, batch acc 0.5718
10:49:10.667   Training iter 550, batch loss 0.4653, batch acc 0.5832
10:49:11.175   Training iter 600, batch loss 0.4652, batch acc 0.5822
10:49:11.177 Testing @ 265 epoch...
10:49:11.213     Testing, total mean loss 0.46493, total acc 0.59340
10:49:11.213 Training @ 266 epoch...
10:49:11.739   Training iter 50, batch loss 0.4661, batch acc 0.5634
10:49:12.257   Training iter 100, batch loss 0.4655, batch acc 0.5770
10:49:12.787   Training iter 150, batch loss 0.4651, batch acc 0.5788
10:49:13.320   Training iter 200, batch loss 0.4652, batch acc 0.5774
10:49:13.812   Training iter 250, batch loss 0.4648, batch acc 0.5868
10:49:14.301   Training iter 300, batch loss 0.4652, batch acc 0.5888
10:49:14.791   Training iter 350, batch loss 0.4650, batch acc 0.5824
10:49:15.307   Training iter 400, batch loss 0.4649, batch acc 0.5806
10:49:15.820   Training iter 450, batch loss 0.4655, batch acc 0.5808
10:49:16.343   Training iter 500, batch loss 0.4656, batch acc 0.5736
10:49:16.853   Training iter 550, batch loss 0.4653, batch acc 0.5744
10:49:17.351   Training iter 600, batch loss 0.4656, batch acc 0.5754
10:49:17.353 Training @ 267 epoch...
10:49:17.838   Training iter 50, batch loss 0.4651, batch acc 0.5832
10:49:18.338   Training iter 100, batch loss 0.4654, batch acc 0.5678
10:49:18.840   Training iter 150, batch loss 0.4652, batch acc 0.5882
10:49:19.322   Training iter 200, batch loss 0.4651, batch acc 0.5732
10:49:19.834   Training iter 250, batch loss 0.4652, batch acc 0.5886
10:49:20.370   Training iter 300, batch loss 0.4657, batch acc 0.5698
10:49:20.890   Training iter 350, batch loss 0.4649, batch acc 0.5902
10:49:21.378   Training iter 400, batch loss 0.4653, batch acc 0.5780
10:49:21.875   Training iter 450, batch loss 0.4653, batch acc 0.5796
10:49:22.374   Training iter 500, batch loss 0.4656, batch acc 0.5704
10:49:22.875   Training iter 550, batch loss 0.4651, batch acc 0.5814
10:49:23.375   Training iter 600, batch loss 0.4655, batch acc 0.5710
10:49:23.377 Training @ 268 epoch...
10:49:23.858   Training iter 50, batch loss 0.4656, batch acc 0.5760
10:49:24.342   Training iter 100, batch loss 0.4649, batch acc 0.5730
10:49:24.834   Training iter 150, batch loss 0.4653, batch acc 0.5814
10:49:25.350   Training iter 200, batch loss 0.4656, batch acc 0.5732
10:49:25.860   Training iter 250, batch loss 0.4652, batch acc 0.5836
10:49:26.407   Training iter 300, batch loss 0.4652, batch acc 0.5738
10:49:26.968   Training iter 350, batch loss 0.4655, batch acc 0.5792
10:49:27.530   Training iter 400, batch loss 0.4660, batch acc 0.5656
10:49:28.105   Training iter 450, batch loss 0.4647, batch acc 0.5914
10:49:28.615   Training iter 500, batch loss 0.4650, batch acc 0.5888
10:49:29.155   Training iter 550, batch loss 0.4652, batch acc 0.5736
10:49:29.700   Training iter 600, batch loss 0.4650, batch acc 0.5862
10:49:29.702 Training @ 269 epoch...
10:49:30.266   Training iter 50, batch loss 0.4650, batch acc 0.5806
10:49:30.827   Training iter 100, batch loss 0.4651, batch acc 0.5806
10:49:31.372   Training iter 150, batch loss 0.4651, batch acc 0.5824
10:49:31.919   Training iter 200, batch loss 0.4654, batch acc 0.5798
10:49:32.476   Training iter 250, batch loss 0.4655, batch acc 0.5756
10:49:33.035   Training iter 300, batch loss 0.4652, batch acc 0.5758
10:49:33.587   Training iter 350, batch loss 0.4652, batch acc 0.5820
10:49:34.146   Training iter 400, batch loss 0.4653, batch acc 0.5870
10:49:34.702   Training iter 450, batch loss 0.4654, batch acc 0.5702
10:49:35.216   Training iter 500, batch loss 0.4653, batch acc 0.5778
10:49:35.730   Training iter 550, batch loss 0.4652, batch acc 0.5814
10:49:36.243   Training iter 600, batch loss 0.4653, batch acc 0.5746
10:49:36.245 Training @ 270 epoch...
10:49:36.747   Training iter 50, batch loss 0.4651, batch acc 0.5846
10:49:37.268   Training iter 100, batch loss 0.4653, batch acc 0.5746
10:49:37.808   Training iter 150, batch loss 0.4652, batch acc 0.5810
10:49:38.352   Training iter 200, batch loss 0.4654, batch acc 0.5786
10:49:38.890   Training iter 250, batch loss 0.4653, batch acc 0.5782
10:49:39.428   Training iter 300, batch loss 0.4651, batch acc 0.5898
10:49:39.953   Training iter 350, batch loss 0.4648, batch acc 0.5770
10:49:40.474   Training iter 400, batch loss 0.4651, batch acc 0.5856
10:49:40.990   Training iter 450, batch loss 0.4652, batch acc 0.5836
10:49:41.526   Training iter 500, batch loss 0.4650, batch acc 0.5838
10:49:42.083   Training iter 550, batch loss 0.4654, batch acc 0.5734
10:49:42.637   Training iter 600, batch loss 0.4655, batch acc 0.5588
10:49:42.639 Testing @ 270 epoch...
10:49:42.677     Testing, total mean loss 0.46480, total acc 0.59370
10:49:42.677 Training @ 271 epoch...
10:49:43.235   Training iter 50, batch loss 0.4652, batch acc 0.5780
10:49:43.732   Training iter 100, batch loss 0.4652, batch acc 0.5670
10:49:44.247   Training iter 150, batch loss 0.4656, batch acc 0.5710
10:49:44.767   Training iter 200, batch loss 0.4652, batch acc 0.5820
10:49:45.284   Training iter 250, batch loss 0.4651, batch acc 0.5826
10:49:45.786   Training iter 300, batch loss 0.4652, batch acc 0.5820
10:49:46.285   Training iter 350, batch loss 0.4651, batch acc 0.5796
10:49:46.782   Training iter 400, batch loss 0.4655, batch acc 0.5790
10:49:47.271   Training iter 450, batch loss 0.4647, batch acc 0.5834
10:49:47.779   Training iter 500, batch loss 0.4654, batch acc 0.5672
10:49:48.286   Training iter 550, batch loss 0.4651, batch acc 0.5918
10:49:48.794   Training iter 600, batch loss 0.4653, batch acc 0.5850
10:49:48.796 Training @ 272 epoch...
10:49:49.305   Training iter 50, batch loss 0.4653, batch acc 0.5732
10:49:49.809   Training iter 100, batch loss 0.4652, batch acc 0.5820
10:49:50.332   Training iter 150, batch loss 0.4647, batch acc 0.5822
10:49:50.843   Training iter 200, batch loss 0.4649, batch acc 0.5776
10:49:51.353   Training iter 250, batch loss 0.4654, batch acc 0.5834
10:49:51.857   Training iter 300, batch loss 0.4654, batch acc 0.5750
10:49:52.369   Training iter 350, batch loss 0.4658, batch acc 0.5668
10:49:52.816   Training iter 400, batch loss 0.4655, batch acc 0.5670
10:49:53.293   Training iter 450, batch loss 0.4651, batch acc 0.5908
10:49:53.767   Training iter 500, batch loss 0.4647, batch acc 0.5816
10:49:54.235   Training iter 550, batch loss 0.4648, batch acc 0.5898
10:49:54.719   Training iter 600, batch loss 0.4652, batch acc 0.5818
10:49:54.720 Training @ 273 epoch...
10:49:55.247   Training iter 50, batch loss 0.4653, batch acc 0.5668
10:49:55.738   Training iter 100, batch loss 0.4656, batch acc 0.5700
10:49:56.215   Training iter 150, batch loss 0.4654, batch acc 0.5786
10:49:56.678   Training iter 200, batch loss 0.4650, batch acc 0.5816
10:49:57.179   Training iter 250, batch loss 0.4652, batch acc 0.5850
10:49:57.704   Training iter 300, batch loss 0.4646, batch acc 0.5940
10:49:58.242   Training iter 350, batch loss 0.4648, batch acc 0.5808
10:49:58.734   Training iter 400, batch loss 0.4650, batch acc 0.5806
10:49:59.237   Training iter 450, batch loss 0.4656, batch acc 0.5662
10:49:59.720   Training iter 500, batch loss 0.4653, batch acc 0.5748
10:50:00.224   Training iter 550, batch loss 0.4651, batch acc 0.5838
10:50:00.743   Training iter 600, batch loss 0.4647, batch acc 0.5916
10:50:00.745 Training @ 274 epoch...
10:50:01.257   Training iter 50, batch loss 0.4650, batch acc 0.5704
10:50:01.799   Training iter 100, batch loss 0.4649, batch acc 0.5864
10:50:02.356   Training iter 150, batch loss 0.4654, batch acc 0.5830
10:50:02.895   Training iter 200, batch loss 0.4654, batch acc 0.5798
10:50:03.434   Training iter 250, batch loss 0.4652, batch acc 0.5788
10:50:03.957   Training iter 300, batch loss 0.4649, batch acc 0.5758
10:50:04.475   Training iter 350, batch loss 0.4654, batch acc 0.5760
10:50:05.006   Training iter 400, batch loss 0.4650, batch acc 0.5786
10:50:05.564   Training iter 450, batch loss 0.4650, batch acc 0.5876
10:50:06.123   Training iter 500, batch loss 0.4653, batch acc 0.5762
10:50:06.641   Training iter 550, batch loss 0.4651, batch acc 0.5720
10:50:07.169   Training iter 600, batch loss 0.4648, batch acc 0.5886
10:50:07.172 Training @ 275 epoch...
10:50:07.694   Training iter 50, batch loss 0.4648, batch acc 0.5922
10:50:08.172   Training iter 100, batch loss 0.4652, batch acc 0.5814
10:50:08.665   Training iter 150, batch loss 0.4655, batch acc 0.5742
10:50:09.150   Training iter 200, batch loss 0.4652, batch acc 0.5766
10:50:09.640   Training iter 250, batch loss 0.4651, batch acc 0.5838
10:50:10.154   Training iter 300, batch loss 0.4653, batch acc 0.5636
10:50:10.640   Training iter 350, batch loss 0.4649, batch acc 0.5748
10:50:11.124   Training iter 400, batch loss 0.4654, batch acc 0.5742
10:50:11.601   Training iter 450, batch loss 0.4654, batch acc 0.5754
10:50:12.055   Training iter 500, batch loss 0.4646, batch acc 0.5870
10:50:12.502   Training iter 550, batch loss 0.4649, batch acc 0.5862
10:50:12.975   Training iter 600, batch loss 0.4649, batch acc 0.5842
10:50:12.976 Testing @ 275 epoch...
10:50:13.012     Testing, total mean loss 0.46468, total acc 0.59450
10:50:13.013 Training @ 276 epoch...
10:50:13.486   Training iter 50, batch loss 0.4657, batch acc 0.5728
10:50:13.933   Training iter 100, batch loss 0.4649, batch acc 0.5840
10:50:14.408   Training iter 150, batch loss 0.4647, batch acc 0.5784
10:50:14.873   Training iter 200, batch loss 0.4652, batch acc 0.5888
10:50:15.347   Training iter 250, batch loss 0.4651, batch acc 0.5764
10:50:15.796   Training iter 300, batch loss 0.4649, batch acc 0.5840
10:50:16.269   Training iter 350, batch loss 0.4649, batch acc 0.5860
10:50:16.775   Training iter 400, batch loss 0.4653, batch acc 0.5718
10:50:17.300   Training iter 450, batch loss 0.4652, batch acc 0.5810
10:50:17.814   Training iter 500, batch loss 0.4652, batch acc 0.5742
10:50:18.346   Training iter 550, batch loss 0.4648, batch acc 0.5876
10:50:18.866   Training iter 600, batch loss 0.4651, batch acc 0.5696
10:50:18.868 Training @ 277 epoch...
10:50:19.380   Training iter 50, batch loss 0.4655, batch acc 0.5754
10:50:19.921   Training iter 100, batch loss 0.4653, batch acc 0.5732
10:50:20.479   Training iter 150, batch loss 0.4650, batch acc 0.5802
10:50:21.035   Training iter 200, batch loss 0.4651, batch acc 0.5814
10:50:21.592   Training iter 250, batch loss 0.4648, batch acc 0.5826
10:50:22.137   Training iter 300, batch loss 0.4653, batch acc 0.5794
10:50:22.663   Training iter 350, batch loss 0.4649, batch acc 0.5790
10:50:23.191   Training iter 400, batch loss 0.4647, batch acc 0.5858
10:50:23.692   Training iter 450, batch loss 0.4646, batch acc 0.5856
10:50:24.197   Training iter 500, batch loss 0.4653, batch acc 0.5666
10:50:24.698   Training iter 550, batch loss 0.4649, batch acc 0.5896
10:50:25.211   Training iter 600, batch loss 0.4651, batch acc 0.5796
10:50:25.212 Training @ 278 epoch...
10:50:25.708   Training iter 50, batch loss 0.4652, batch acc 0.5874
10:50:26.200   Training iter 100, batch loss 0.4656, batch acc 0.5790
10:50:26.688   Training iter 150, batch loss 0.4650, batch acc 0.5786
10:50:27.168   Training iter 200, batch loss 0.4649, batch acc 0.5812
10:50:27.618   Training iter 250, batch loss 0.4653, batch acc 0.5708
10:50:28.103   Training iter 300, batch loss 0.4647, batch acc 0.5820
10:50:28.605   Training iter 350, batch loss 0.4651, batch acc 0.5808
10:50:29.102   Training iter 400, batch loss 0.4652, batch acc 0.5724
10:50:29.603   Training iter 450, batch loss 0.4643, batch acc 0.5956
10:50:30.095   Training iter 500, batch loss 0.4652, batch acc 0.5758
10:50:30.600   Training iter 550, batch loss 0.4649, batch acc 0.5832
10:50:31.105   Training iter 600, batch loss 0.4651, batch acc 0.5726
10:50:31.107 Training @ 279 epoch...
10:50:31.609   Training iter 50, batch loss 0.4646, batch acc 0.5894
10:50:32.101   Training iter 100, batch loss 0.4646, batch acc 0.5816
10:50:32.593   Training iter 150, batch loss 0.4650, batch acc 0.5780
10:50:33.085   Training iter 200, batch loss 0.4649, batch acc 0.5860
10:50:33.582   Training iter 250, batch loss 0.4655, batch acc 0.5738
10:50:34.093   Training iter 300, batch loss 0.4650, batch acc 0.5774
10:50:34.614   Training iter 350, batch loss 0.4648, batch acc 0.5826
10:50:35.143   Training iter 400, batch loss 0.4650, batch acc 0.5776
10:50:35.656   Training iter 450, batch loss 0.4645, batch acc 0.5864
10:50:36.174   Training iter 500, batch loss 0.4655, batch acc 0.5724
10:50:36.691   Training iter 550, batch loss 0.4653, batch acc 0.5792
10:50:37.214   Training iter 600, batch loss 0.4653, batch acc 0.5748
10:50:37.216 Training @ 280 epoch...
10:50:37.729   Training iter 50, batch loss 0.4652, batch acc 0.5840
10:50:38.234   Training iter 100, batch loss 0.4650, batch acc 0.5858
10:50:38.736   Training iter 150, batch loss 0.4652, batch acc 0.5758
10:50:39.240   Training iter 200, batch loss 0.4653, batch acc 0.5746
10:50:39.716   Training iter 250, batch loss 0.4641, batch acc 0.5988
10:50:40.214   Training iter 300, batch loss 0.4652, batch acc 0.5754
10:50:40.698   Training iter 350, batch loss 0.4650, batch acc 0.5726
10:50:41.170   Training iter 400, batch loss 0.4653, batch acc 0.5676
10:50:41.653   Training iter 450, batch loss 0.4651, batch acc 0.5710
10:50:42.158   Training iter 500, batch loss 0.4649, batch acc 0.5852
10:50:42.671   Training iter 550, batch loss 0.4648, batch acc 0.5906
10:50:43.183   Training iter 600, batch loss 0.4648, batch acc 0.5810
10:50:43.185 Testing @ 280 epoch...
10:50:43.220     Testing, total mean loss 0.46457, total acc 0.59480
10:50:43.220 Training @ 281 epoch...
10:50:43.714   Training iter 50, batch loss 0.4654, batch acc 0.5818
10:50:44.217   Training iter 100, batch loss 0.4647, batch acc 0.5786
10:50:44.729   Training iter 150, batch loss 0.4647, batch acc 0.5716
10:50:45.244   Training iter 200, batch loss 0.4643, batch acc 0.5884
10:50:45.751   Training iter 250, batch loss 0.4644, batch acc 0.5830
10:50:46.262   Training iter 300, batch loss 0.4651, batch acc 0.5884
10:50:46.771   Training iter 350, batch loss 0.4653, batch acc 0.5780
10:50:47.279   Training iter 400, batch loss 0.4655, batch acc 0.5792
10:50:47.781   Training iter 450, batch loss 0.4655, batch acc 0.5754
10:50:48.277   Training iter 500, batch loss 0.4649, batch acc 0.5768
10:50:48.743   Training iter 550, batch loss 0.4649, batch acc 0.5736
10:50:49.209   Training iter 600, batch loss 0.4648, batch acc 0.5880
10:50:49.211 Training @ 282 epoch...
10:50:49.726   Training iter 50, batch loss 0.4650, batch acc 0.5774
10:50:50.245   Training iter 100, batch loss 0.4650, batch acc 0.5788
10:50:50.747   Training iter 150, batch loss 0.4649, batch acc 0.5812
10:50:51.269   Training iter 200, batch loss 0.4648, batch acc 0.5882
10:50:51.804   Training iter 250, batch loss 0.4644, batch acc 0.5880
10:50:52.325   Training iter 300, batch loss 0.4645, batch acc 0.5818
10:50:52.834   Training iter 350, batch loss 0.4651, batch acc 0.5812
10:50:53.347   Training iter 400, batch loss 0.4654, batch acc 0.5814
10:50:53.843   Training iter 450, batch loss 0.4650, batch acc 0.5726
10:50:54.352   Training iter 500, batch loss 0.4651, batch acc 0.5912
10:50:54.845   Training iter 550, batch loss 0.4652, batch acc 0.5760
10:50:55.348   Training iter 600, batch loss 0.4650, batch acc 0.5668
10:50:55.350 Training @ 283 epoch...
10:50:55.856   Training iter 50, batch loss 0.4652, batch acc 0.5736
10:50:56.352   Training iter 100, batch loss 0.4652, batch acc 0.5798
10:50:56.832   Training iter 150, batch loss 0.4651, batch acc 0.5792
10:50:57.330   Training iter 200, batch loss 0.4651, batch acc 0.5756
10:50:57.826   Training iter 250, batch loss 0.4646, batch acc 0.5768
10:50:58.321   Training iter 300, batch loss 0.4653, batch acc 0.5826
10:50:58.801   Training iter 350, batch loss 0.4647, batch acc 0.5782
10:50:59.285   Training iter 400, batch loss 0.4648, batch acc 0.5882
10:50:59.778   Training iter 450, batch loss 0.4649, batch acc 0.5786
10:51:00.278   Training iter 500, batch loss 0.4647, batch acc 0.5818
10:51:00.794   Training iter 550, batch loss 0.4644, batch acc 0.5890
10:51:01.302   Training iter 600, batch loss 0.4650, batch acc 0.5832
10:51:01.303 Training @ 284 epoch...
10:51:01.822   Training iter 50, batch loss 0.4655, batch acc 0.5794
10:51:02.349   Training iter 100, batch loss 0.4651, batch acc 0.5784
10:51:02.873   Training iter 150, batch loss 0.4647, batch acc 0.5828
10:51:03.415   Training iter 200, batch loss 0.4648, batch acc 0.5812
10:51:03.945   Training iter 250, batch loss 0.4648, batch acc 0.5834
10:51:04.468   Training iter 300, batch loss 0.4650, batch acc 0.5816
10:51:05.005   Training iter 350, batch loss 0.4647, batch acc 0.5934
10:51:05.569   Training iter 400, batch loss 0.4646, batch acc 0.5836
10:51:06.107   Training iter 450, batch loss 0.4650, batch acc 0.5746
10:51:06.651   Training iter 500, batch loss 0.4651, batch acc 0.5704
10:51:07.224   Training iter 550, batch loss 0.4649, batch acc 0.5716
10:51:07.803   Training iter 600, batch loss 0.4645, batch acc 0.5874
10:51:07.805 Training @ 285 epoch...
10:51:08.316   Training iter 50, batch loss 0.4652, batch acc 0.5748
10:51:08.807   Training iter 100, batch loss 0.4651, batch acc 0.5808
10:51:09.303   Training iter 150, batch loss 0.4653, batch acc 0.5814
10:51:09.789   Training iter 200, batch loss 0.4645, batch acc 0.5896
10:51:10.303   Training iter 250, batch loss 0.4651, batch acc 0.5804
10:51:10.802   Training iter 300, batch loss 0.4649, batch acc 0.5760
10:51:11.309   Training iter 350, batch loss 0.4653, batch acc 0.5696
10:51:11.816   Training iter 400, batch loss 0.4645, batch acc 0.5858
10:51:12.319   Training iter 450, batch loss 0.4646, batch acc 0.5908
10:51:12.827   Training iter 500, batch loss 0.4648, batch acc 0.5840
10:51:13.332   Training iter 550, batch loss 0.4648, batch acc 0.5712
10:51:13.826   Training iter 600, batch loss 0.4646, batch acc 0.5860
10:51:13.828 Testing @ 285 epoch...
10:51:13.862     Testing, total mean loss 0.46446, total acc 0.59540
10:51:13.862 Training @ 286 epoch...
10:51:14.367   Training iter 50, batch loss 0.4652, batch acc 0.5736
10:51:14.870   Training iter 100, batch loss 0.4650, batch acc 0.5810
10:51:15.377   Training iter 150, batch loss 0.4648, batch acc 0.5854
10:51:15.885   Training iter 200, batch loss 0.4644, batch acc 0.5900
10:51:16.417   Training iter 250, batch loss 0.4652, batch acc 0.5684
10:51:16.977   Training iter 300, batch loss 0.4651, batch acc 0.5728
10:51:17.541   Training iter 350, batch loss 0.4642, batch acc 0.5910
10:51:18.102   Training iter 400, batch loss 0.4651, batch acc 0.5726
10:51:18.592   Training iter 450, batch loss 0.4650, batch acc 0.5816
10:51:19.090   Training iter 500, batch loss 0.4646, batch acc 0.5998
10:51:19.574   Training iter 550, batch loss 0.4647, batch acc 0.5754
10:51:20.085   Training iter 600, batch loss 0.4651, batch acc 0.5750
10:51:20.087 Training @ 287 epoch...
10:51:20.593   Training iter 50, batch loss 0.4645, batch acc 0.5888
10:51:21.082   Training iter 100, batch loss 0.4649, batch acc 0.5788
10:51:21.564   Training iter 150, batch loss 0.4644, batch acc 0.5918
10:51:22.067   Training iter 200, batch loss 0.4649, batch acc 0.5870
10:51:22.555   Training iter 250, batch loss 0.4652, batch acc 0.5738
10:51:23.023   Training iter 300, batch loss 0.4652, batch acc 0.5696
10:51:23.496   Training iter 350, batch loss 0.4646, batch acc 0.5816
10:51:23.952   Training iter 400, batch loss 0.4651, batch acc 0.5842
10:51:24.412   Training iter 450, batch loss 0.4643, batch acc 0.5816
10:51:24.879   Training iter 500, batch loss 0.4649, batch acc 0.5862
10:51:25.370   Training iter 550, batch loss 0.4650, batch acc 0.5680
10:51:25.818   Training iter 600, batch loss 0.4650, batch acc 0.5768
10:51:25.819 Training @ 288 epoch...
10:51:26.295   Training iter 50, batch loss 0.4645, batch acc 0.5808
10:51:26.759   Training iter 100, batch loss 0.4649, batch acc 0.5828
10:51:27.215   Training iter 150, batch loss 0.4647, batch acc 0.5904
10:51:27.680   Training iter 200, batch loss 0.4646, batch acc 0.5832
10:51:28.134   Training iter 250, batch loss 0.4647, batch acc 0.5832
10:51:28.575   Training iter 300, batch loss 0.4648, batch acc 0.5740
10:51:29.034   Training iter 350, batch loss 0.4648, batch acc 0.5848
10:51:29.493   Training iter 400, batch loss 0.4652, batch acc 0.5834
10:51:29.948   Training iter 450, batch loss 0.4649, batch acc 0.5720
10:51:30.427   Training iter 500, batch loss 0.4649, batch acc 0.5728
10:51:30.917   Training iter 550, batch loss 0.4649, batch acc 0.5724
10:51:31.420   Training iter 600, batch loss 0.4650, batch acc 0.5910
10:51:31.422 Training @ 289 epoch...
10:51:31.925   Training iter 50, batch loss 0.4648, batch acc 0.5806
10:51:32.424   Training iter 100, batch loss 0.4650, batch acc 0.5814
10:51:32.858   Training iter 150, batch loss 0.4650, batch acc 0.5858
10:51:33.314   Training iter 200, batch loss 0.4648, batch acc 0.5890
10:51:33.770   Training iter 250, batch loss 0.4647, batch acc 0.5746
10:51:34.211   Training iter 300, batch loss 0.4649, batch acc 0.5818
10:51:34.675   Training iter 350, batch loss 0.4644, batch acc 0.5816
10:51:35.136   Training iter 400, batch loss 0.4652, batch acc 0.5750
10:51:35.610   Training iter 450, batch loss 0.4647, batch acc 0.5816
10:51:36.090   Training iter 500, batch loss 0.4648, batch acc 0.5892
10:51:36.562   Training iter 550, batch loss 0.4646, batch acc 0.5724
10:51:37.064   Training iter 600, batch loss 0.4649, batch acc 0.5804
10:51:37.066 Training @ 290 epoch...
10:51:37.562   Training iter 50, batch loss 0.4650, batch acc 0.5788
10:51:38.095   Training iter 100, batch loss 0.4649, batch acc 0.5800
10:51:38.609   Training iter 150, batch loss 0.4647, batch acc 0.5714
10:51:39.123   Training iter 200, batch loss 0.4643, batch acc 0.5934
10:51:39.636   Training iter 250, batch loss 0.4651, batch acc 0.5786
10:51:40.161   Training iter 300, batch loss 0.4648, batch acc 0.5722
10:51:40.673   Training iter 350, batch loss 0.4651, batch acc 0.5770
10:51:41.179   Training iter 400, batch loss 0.4640, batch acc 0.5910
10:51:41.689   Training iter 450, batch loss 0.4648, batch acc 0.5898
10:51:42.209   Training iter 500, batch loss 0.4649, batch acc 0.5842
10:51:42.731   Training iter 550, batch loss 0.4647, batch acc 0.5762
10:51:43.239   Training iter 600, batch loss 0.4651, batch acc 0.5814
10:51:43.241 Testing @ 290 epoch...
10:51:43.275     Testing, total mean loss 0.46435, total acc 0.59650
10:51:43.275 Training @ 291 epoch...
10:51:43.766   Training iter 50, batch loss 0.4646, batch acc 0.5774
10:51:44.305   Training iter 100, batch loss 0.4639, batch acc 0.6004
10:51:44.830   Training iter 150, batch loss 0.4649, batch acc 0.5804
10:51:45.356   Training iter 200, batch loss 0.4649, batch acc 0.5746
10:51:45.876   Training iter 250, batch loss 0.4645, batch acc 0.5814
10:51:46.369   Training iter 300, batch loss 0.4646, batch acc 0.5928
10:51:46.882   Training iter 350, batch loss 0.4649, batch acc 0.5798
10:51:47.415   Training iter 400, batch loss 0.4652, batch acc 0.5738
10:51:47.930   Training iter 450, batch loss 0.4649, batch acc 0.5704
10:51:48.451   Training iter 500, batch loss 0.4650, batch acc 0.5772
10:51:48.977   Training iter 550, batch loss 0.4648, batch acc 0.5868
10:51:49.491   Training iter 600, batch loss 0.4649, batch acc 0.5782
10:51:49.493 Training @ 292 epoch...
10:51:50.024   Training iter 50, batch loss 0.4648, batch acc 0.5796
10:51:50.505   Training iter 100, batch loss 0.4648, batch acc 0.5748
10:51:50.998   Training iter 150, batch loss 0.4652, batch acc 0.5752
10:51:51.458   Training iter 200, batch loss 0.4648, batch acc 0.5846
10:51:51.950   Training iter 250, batch loss 0.4645, batch acc 0.5838
10:51:52.444   Training iter 300, batch loss 0.4646, batch acc 0.5760
10:51:52.911   Training iter 350, batch loss 0.4644, batch acc 0.5906
10:51:53.402   Training iter 400, batch loss 0.4647, batch acc 0.5804
10:51:53.879   Training iter 450, batch loss 0.4644, batch acc 0.5882
10:51:54.350   Training iter 500, batch loss 0.4652, batch acc 0.5710
10:51:54.838   Training iter 550, batch loss 0.4646, batch acc 0.5836
10:51:55.341   Training iter 600, batch loss 0.4650, batch acc 0.5892
10:51:55.343 Training @ 293 epoch...
10:51:55.869   Training iter 50, batch loss 0.4646, batch acc 0.5786
10:51:56.406   Training iter 100, batch loss 0.4645, batch acc 0.5818
10:51:57.075   Training iter 150, batch loss 0.4644, batch acc 0.5876
10:51:57.653   Training iter 200, batch loss 0.4648, batch acc 0.5828
10:51:58.159   Training iter 250, batch loss 0.4643, batch acc 0.5888
10:51:58.624   Training iter 300, batch loss 0.4649, batch acc 0.5812
10:51:59.094   Training iter 350, batch loss 0.4649, batch acc 0.5726
10:51:59.563   Training iter 400, batch loss 0.4654, batch acc 0.5780
10:52:00.064   Training iter 450, batch loss 0.4649, batch acc 0.5768
10:52:00.576   Training iter 500, batch loss 0.4643, batch acc 0.5832
10:52:01.098   Training iter 550, batch loss 0.4644, batch acc 0.5862
10:52:01.655   Training iter 600, batch loss 0.4651, batch acc 0.5828
10:52:01.657 Training @ 294 epoch...
10:52:02.213   Training iter 50, batch loss 0.4646, batch acc 0.5772
10:52:02.771   Training iter 100, batch loss 0.4649, batch acc 0.5758
10:52:03.332   Training iter 150, batch loss 0.4649, batch acc 0.5858
10:52:03.898   Training iter 200, batch loss 0.4647, batch acc 0.5792
10:52:04.447   Training iter 250, batch loss 0.4648, batch acc 0.5818
10:52:04.976   Training iter 300, batch loss 0.4647, batch acc 0.5864
10:52:05.498   Training iter 350, batch loss 0.4647, batch acc 0.5856
10:52:06.050   Training iter 400, batch loss 0.4642, batch acc 0.5854
10:52:06.592   Training iter 450, batch loss 0.4649, batch acc 0.5786
10:52:07.117   Training iter 500, batch loss 0.4648, batch acc 0.5730
10:52:07.616   Training iter 550, batch loss 0.4646, batch acc 0.5856
10:52:08.118   Training iter 600, batch loss 0.4647, batch acc 0.5884
10:52:08.120 Training @ 295 epoch...
10:52:08.642   Training iter 50, batch loss 0.4643, batch acc 0.5904
10:52:09.166   Training iter 100, batch loss 0.4648, batch acc 0.5756
10:52:09.692   Training iter 150, batch loss 0.4647, batch acc 0.5768
10:52:10.217   Training iter 200, batch loss 0.4645, batch acc 0.5806
10:52:10.741   Training iter 250, batch loss 0.4643, batch acc 0.5998
10:52:11.270   Training iter 300, batch loss 0.4647, batch acc 0.5870
10:52:11.782   Training iter 350, batch loss 0.4647, batch acc 0.5824
10:52:12.307   Training iter 400, batch loss 0.4648, batch acc 0.5780
10:52:12.826   Training iter 450, batch loss 0.4651, batch acc 0.5744
10:52:13.349   Training iter 500, batch loss 0.4646, batch acc 0.5782
10:52:13.869   Training iter 550, batch loss 0.4649, batch acc 0.5812
10:52:14.383   Training iter 600, batch loss 0.4645, batch acc 0.5782
10:52:14.385 Testing @ 295 epoch...
10:52:14.420     Testing, total mean loss 0.46425, total acc 0.59690
10:52:14.420 Training @ 296 epoch...
10:52:14.934   Training iter 50, batch loss 0.4646, batch acc 0.5826
10:52:15.448   Training iter 100, batch loss 0.4648, batch acc 0.5766
10:52:15.963   Training iter 150, batch loss 0.4646, batch acc 0.5786
10:52:16.488   Training iter 200, batch loss 0.4644, batch acc 0.5884
10:52:17.003   Training iter 250, batch loss 0.4649, batch acc 0.5806
10:52:17.513   Training iter 300, batch loss 0.4647, batch acc 0.5790
10:52:18.039   Training iter 350, batch loss 0.4649, batch acc 0.5858
10:52:18.551   Training iter 400, batch loss 0.4644, batch acc 0.5852
10:52:19.075   Training iter 450, batch loss 0.4646, batch acc 0.5784
10:52:19.582   Training iter 500, batch loss 0.4644, batch acc 0.5866
10:52:20.090   Training iter 550, batch loss 0.4650, batch acc 0.5804
10:52:20.611   Training iter 600, batch loss 0.4646, batch acc 0.5828
10:52:20.613 Training @ 297 epoch...
10:52:21.132   Training iter 50, batch loss 0.4647, batch acc 0.5884
10:52:21.645   Training iter 100, batch loss 0.4647, batch acc 0.5888
10:52:22.178   Training iter 150, batch loss 0.4647, batch acc 0.5824
10:52:22.707   Training iter 200, batch loss 0.4653, batch acc 0.5732
10:52:23.234   Training iter 250, batch loss 0.4643, batch acc 0.5822
10:52:23.700   Training iter 300, batch loss 0.4643, batch acc 0.5818
10:52:24.167   Training iter 350, batch loss 0.4641, batch acc 0.5952
10:52:24.650   Training iter 400, batch loss 0.4647, batch acc 0.5826
10:52:25.160   Training iter 450, batch loss 0.4642, batch acc 0.5834
10:52:25.684   Training iter 500, batch loss 0.4646, batch acc 0.5760
10:52:26.219   Training iter 550, batch loss 0.4652, batch acc 0.5806
10:52:26.727   Training iter 600, batch loss 0.4648, batch acc 0.5708
10:52:26.729 Training @ 298 epoch...
10:52:27.277   Training iter 50, batch loss 0.4644, batch acc 0.5798
10:52:27.817   Training iter 100, batch loss 0.4646, batch acc 0.5756
10:52:28.342   Training iter 150, batch loss 0.4648, batch acc 0.5828
10:52:28.861   Training iter 200, batch loss 0.4645, batch acc 0.5870
10:52:29.379   Training iter 250, batch loss 0.4646, batch acc 0.5894
10:52:29.915   Training iter 300, batch loss 0.4644, batch acc 0.5838
10:52:30.475   Training iter 350, batch loss 0.4650, batch acc 0.5780
10:52:31.029   Training iter 400, batch loss 0.4651, batch acc 0.5728
10:52:31.497   Training iter 450, batch loss 0.4649, batch acc 0.5762
10:52:31.960   Training iter 500, batch loss 0.4646, batch acc 0.5768
10:52:32.411   Training iter 550, batch loss 0.4643, batch acc 0.5950
10:52:32.890   Training iter 600, batch loss 0.4642, batch acc 0.5868
10:52:32.891 Training @ 299 epoch...
10:52:33.386   Training iter 50, batch loss 0.4649, batch acc 0.5772
10:52:33.847   Training iter 100, batch loss 0.4646, batch acc 0.5804
10:52:34.307   Training iter 150, batch loss 0.4644, batch acc 0.5816
10:52:34.791   Training iter 200, batch loss 0.4649, batch acc 0.5808
10:52:35.257   Training iter 250, batch loss 0.4646, batch acc 0.5850
10:52:35.728   Training iter 300, batch loss 0.4648, batch acc 0.5786
10:52:36.183   Training iter 350, batch loss 0.4645, batch acc 0.5824
10:52:36.622   Training iter 400, batch loss 0.4643, batch acc 0.5946
10:52:37.056   Training iter 450, batch loss 0.4644, batch acc 0.5866
10:52:37.503   Training iter 500, batch loss 0.4646, batch acc 0.5788
10:52:37.967   Training iter 550, batch loss 0.4643, batch acc 0.5790
10:52:38.427   Training iter 600, batch loss 0.4648, batch acc 0.5824
10:52:38.428 Training @ 300 epoch...
10:52:38.879   Training iter 50, batch loss 0.4645, batch acc 0.5828
10:52:39.354   Training iter 100, batch loss 0.4649, batch acc 0.5658
10:52:39.829   Training iter 150, batch loss 0.4642, batch acc 0.6070
10:52:40.320   Training iter 200, batch loss 0.4646, batch acc 0.5834
10:52:40.782   Training iter 250, batch loss 0.4644, batch acc 0.5808
10:52:41.261   Training iter 300, batch loss 0.4647, batch acc 0.5818
10:52:41.730   Training iter 350, batch loss 0.4643, batch acc 0.5836
10:52:42.221   Training iter 400, batch loss 0.4647, batch acc 0.5802
10:52:42.722   Training iter 450, batch loss 0.4650, batch acc 0.5766
10:52:43.233   Training iter 500, batch loss 0.4644, batch acc 0.5796
10:52:43.722   Training iter 550, batch loss 0.4647, batch acc 0.5742
10:52:44.221   Training iter 600, batch loss 0.4647, batch acc 0.5910
10:52:44.223 Testing @ 300 epoch...
10:52:44.259     Testing, total mean loss 0.46415, total acc 0.59730
10:52:44.259 Plot @ 300 epoch...
10:52:44.259 Training @ 301 epoch...
10:52:44.754   Training iter 50, batch loss 0.4646, batch acc 0.5956
10:52:45.272   Training iter 100, batch loss 0.4642, batch acc 0.5756
10:52:45.753   Training iter 150, batch loss 0.4644, batch acc 0.5844
10:52:46.252   Training iter 200, batch loss 0.4646, batch acc 0.5850
10:52:46.738   Training iter 250, batch loss 0.4648, batch acc 0.5858
10:52:47.248   Training iter 300, batch loss 0.4636, batch acc 0.5878
10:52:47.771   Training iter 350, batch loss 0.4643, batch acc 0.5850
10:52:48.309   Training iter 400, batch loss 0.4648, batch acc 0.5846
10:52:48.805   Training iter 450, batch loss 0.4651, batch acc 0.5778
10:52:49.302   Training iter 500, batch loss 0.4648, batch acc 0.5706
10:52:49.767   Training iter 550, batch loss 0.4645, batch acc 0.5842
10:52:50.219   Training iter 600, batch loss 0.4650, batch acc 0.5700
10:52:50.220 Training @ 302 epoch...
10:52:50.700   Training iter 50, batch loss 0.4647, batch acc 0.5778
10:52:51.167   Training iter 100, batch loss 0.4646, batch acc 0.5898
10:52:51.658   Training iter 150, batch loss 0.4645, batch acc 0.5844
10:52:52.151   Training iter 200, batch loss 0.4642, batch acc 0.5938
10:52:52.636   Training iter 250, batch loss 0.4646, batch acc 0.5780
10:52:53.137   Training iter 300, batch loss 0.4643, batch acc 0.5934
10:52:53.613   Training iter 350, batch loss 0.4649, batch acc 0.5724
10:52:54.116   Training iter 400, batch loss 0.4646, batch acc 0.5832
10:52:54.605   Training iter 450, batch loss 0.4650, batch acc 0.5758
10:52:55.091   Training iter 500, batch loss 0.4645, batch acc 0.5696
10:52:55.571   Training iter 550, batch loss 0.4645, batch acc 0.5744
10:52:56.058   Training iter 600, batch loss 0.4641, batch acc 0.5982
10:52:56.060 Training @ 303 epoch...
10:52:56.536   Training iter 50, batch loss 0.4643, batch acc 0.5832
10:52:57.062   Training iter 100, batch loss 0.4646, batch acc 0.5830
10:52:57.651   Training iter 150, batch loss 0.4645, batch acc 0.5808
10:52:58.244   Training iter 200, batch loss 0.4646, batch acc 0.5802
10:52:58.841   Training iter 250, batch loss 0.4648, batch acc 0.5712
10:52:59.386   Training iter 300, batch loss 0.4639, batch acc 0.5830
10:52:59.913   Training iter 350, batch loss 0.4648, batch acc 0.5736
10:53:00.444   Training iter 400, batch loss 0.4649, batch acc 0.5914
10:53:00.976   Training iter 450, batch loss 0.4647, batch acc 0.5828
10:53:01.526   Training iter 500, batch loss 0.4644, batch acc 0.5888
10:53:02.089   Training iter 550, batch loss 0.4642, batch acc 0.5900
10:53:02.637   Training iter 600, batch loss 0.4646, batch acc 0.5812
10:53:02.639 Training @ 304 epoch...
10:53:03.189   Training iter 50, batch loss 0.4644, batch acc 0.5814
10:53:03.726   Training iter 100, batch loss 0.4642, batch acc 0.5828
10:53:04.270   Training iter 150, batch loss 0.4646, batch acc 0.5718
10:53:04.810   Training iter 200, batch loss 0.4642, batch acc 0.5906
10:53:05.347   Training iter 250, batch loss 0.4646, batch acc 0.5768
10:53:05.863   Training iter 300, batch loss 0.4641, batch acc 0.5960
10:53:06.374   Training iter 350, batch loss 0.4644, batch acc 0.5774
10:53:06.865   Training iter 400, batch loss 0.4645, batch acc 0.5890
10:53:07.328   Training iter 450, batch loss 0.4648, batch acc 0.5766
10:53:07.778   Training iter 500, batch loss 0.4646, batch acc 0.5896
10:53:08.229   Training iter 550, batch loss 0.4645, batch acc 0.5824
10:53:08.682   Training iter 600, batch loss 0.4652, batch acc 0.5744
10:53:08.684 Training @ 305 epoch...
10:53:09.147   Training iter 50, batch loss 0.4644, batch acc 0.5876
10:53:09.602   Training iter 100, batch loss 0.4642, batch acc 0.5836
10:53:10.047   Training iter 150, batch loss 0.4643, batch acc 0.5898
10:53:10.517   Training iter 200, batch loss 0.4642, batch acc 0.5820
10:53:10.961   Training iter 250, batch loss 0.4643, batch acc 0.5846
10:53:11.421   Training iter 300, batch loss 0.4650, batch acc 0.5692
10:53:11.873   Training iter 350, batch loss 0.4642, batch acc 0.5882
10:53:12.340   Training iter 400, batch loss 0.4646, batch acc 0.5844
10:53:12.831   Training iter 450, batch loss 0.4647, batch acc 0.5852
10:53:13.324   Training iter 500, batch loss 0.4646, batch acc 0.5782
10:53:13.796   Training iter 550, batch loss 0.4646, batch acc 0.5788
10:53:14.296   Training iter 600, batch loss 0.4647, batch acc 0.5778
10:53:14.298 Testing @ 305 epoch...
10:53:14.334     Testing, total mean loss 0.46405, total acc 0.59760
10:53:14.334 Training @ 306 epoch...
10:53:14.868   Training iter 50, batch loss 0.4643, batch acc 0.5876
10:53:15.411   Training iter 100, batch loss 0.4641, batch acc 0.5880
10:53:15.950   Training iter 150, batch loss 0.4645, batch acc 0.5872
10:53:16.504   Training iter 200, batch loss 0.4649, batch acc 0.5700
10:53:17.050   Training iter 250, batch loss 0.4648, batch acc 0.5840
10:53:17.583   Training iter 300, batch loss 0.4640, batch acc 0.5906
10:53:18.121   Training iter 350, batch loss 0.4645, batch acc 0.5808
10:53:18.637   Training iter 400, batch loss 0.4643, batch acc 0.5834
10:53:19.153   Training iter 450, batch loss 0.4646, batch acc 0.5826
10:53:19.672   Training iter 500, batch loss 0.4647, batch acc 0.5726
10:53:20.209   Training iter 550, batch loss 0.4646, batch acc 0.5794
10:53:20.750   Training iter 600, batch loss 0.4643, batch acc 0.5838
10:53:20.751 Training @ 307 epoch...
10:53:21.285   Training iter 50, batch loss 0.4648, batch acc 0.5708
10:53:21.802   Training iter 100, batch loss 0.4639, batch acc 0.6024
10:53:22.328   Training iter 150, batch loss 0.4642, batch acc 0.5766
10:53:22.842   Training iter 200, batch loss 0.4646, batch acc 0.5864
10:53:23.360   Training iter 250, batch loss 0.4647, batch acc 0.5828
10:53:23.863   Training iter 300, batch loss 0.4642, batch acc 0.5892
10:53:24.365   Training iter 350, batch loss 0.4639, batch acc 0.5802
10:53:24.860   Training iter 400, batch loss 0.4644, batch acc 0.5850
10:53:25.356   Training iter 450, batch loss 0.4648, batch acc 0.5772
10:53:25.865   Training iter 500, batch loss 0.4647, batch acc 0.5838
10:53:26.389   Training iter 550, batch loss 0.4648, batch acc 0.5766
10:53:26.916   Training iter 600, batch loss 0.4643, batch acc 0.5810
10:53:26.918 Training @ 308 epoch...
10:53:27.455   Training iter 50, batch loss 0.4641, batch acc 0.5918
10:53:27.987   Training iter 100, batch loss 0.4645, batch acc 0.5844
10:53:28.509   Training iter 150, batch loss 0.4644, batch acc 0.5852
10:53:29.028   Training iter 200, batch loss 0.4644, batch acc 0.5938
10:53:29.575   Training iter 250, batch loss 0.4645, batch acc 0.5758
10:53:30.111   Training iter 300, batch loss 0.4645, batch acc 0.5860
10:53:30.637   Training iter 350, batch loss 0.4653, batch acc 0.5678
10:53:31.167   Training iter 400, batch loss 0.4647, batch acc 0.5718
10:53:31.697   Training iter 450, batch loss 0.4641, batch acc 0.5876
10:53:32.248   Training iter 500, batch loss 0.4641, batch acc 0.5876
10:53:32.783   Training iter 550, batch loss 0.4642, batch acc 0.5792
10:53:33.310   Training iter 600, batch loss 0.4645, batch acc 0.5810
10:53:33.312 Training @ 309 epoch...
10:53:33.835   Training iter 50, batch loss 0.4647, batch acc 0.5776
10:53:34.358   Training iter 100, batch loss 0.4642, batch acc 0.5762
10:53:34.886   Training iter 150, batch loss 0.4643, batch acc 0.5856
10:53:35.406   Training iter 200, batch loss 0.4647, batch acc 0.5824
10:53:35.905   Training iter 250, batch loss 0.4640, batch acc 0.5970
10:53:36.399   Training iter 300, batch loss 0.4642, batch acc 0.5908
10:53:36.888   Training iter 350, batch loss 0.4643, batch acc 0.5798
10:53:37.395   Training iter 400, batch loss 0.4642, batch acc 0.5770
10:53:37.886   Training iter 450, batch loss 0.4645, batch acc 0.5750
10:53:38.364   Training iter 500, batch loss 0.4645, batch acc 0.5838
10:53:38.835   Training iter 550, batch loss 0.4648, batch acc 0.5760
10:53:39.299   Training iter 600, batch loss 0.4646, batch acc 0.5904
10:53:39.300 Training @ 310 epoch...
10:53:39.758   Training iter 50, batch loss 0.4639, batch acc 0.5880
10:53:40.220   Training iter 100, batch loss 0.4649, batch acc 0.5864
10:53:40.673   Training iter 150, batch loss 0.4638, batch acc 0.5850
10:53:41.125   Training iter 200, batch loss 0.4644, batch acc 0.5900
10:53:41.563   Training iter 250, batch loss 0.4645, batch acc 0.5912
10:53:41.997   Training iter 300, batch loss 0.4643, batch acc 0.5912
10:53:42.451   Training iter 350, batch loss 0.4645, batch acc 0.5754
10:53:42.915   Training iter 400, batch loss 0.4646, batch acc 0.5742
10:53:43.384   Training iter 450, batch loss 0.4644, batch acc 0.5710
10:53:43.827   Training iter 500, batch loss 0.4646, batch acc 0.5838
10:53:44.282   Training iter 550, batch loss 0.4646, batch acc 0.5806
10:53:44.746   Training iter 600, batch loss 0.4642, batch acc 0.5780
10:53:44.747 Testing @ 310 epoch...
10:53:44.784     Testing, total mean loss 0.46396, total acc 0.59760
10:53:44.784 Training @ 311 epoch...
10:53:45.268   Training iter 50, batch loss 0.4643, batch acc 0.5914
10:53:45.833   Training iter 100, batch loss 0.4642, batch acc 0.5812
10:53:46.398   Training iter 150, batch loss 0.4642, batch acc 0.5904
10:53:46.959   Training iter 200, batch loss 0.4643, batch acc 0.5850
10:53:47.445   Training iter 250, batch loss 0.4643, batch acc 0.5826
10:53:47.952   Training iter 300, batch loss 0.4648, batch acc 0.5838
10:53:48.451   Training iter 350, batch loss 0.4647, batch acc 0.5732
10:53:48.945   Training iter 400, batch loss 0.4645, batch acc 0.5862
10:53:49.453   Training iter 450, batch loss 0.4645, batch acc 0.5800
10:53:49.977   Training iter 500, batch loss 0.4643, batch acc 0.5806
10:53:50.484   Training iter 550, batch loss 0.4644, batch acc 0.5792
10:53:50.995   Training iter 600, batch loss 0.4640, batch acc 0.5818
10:53:50.996 Training @ 312 epoch...
10:53:51.494   Training iter 50, batch loss 0.4642, batch acc 0.5938
10:53:51.985   Training iter 100, batch loss 0.4646, batch acc 0.5762
10:53:52.447   Training iter 150, batch loss 0.4638, batch acc 0.5840
10:53:52.917   Training iter 200, batch loss 0.4642, batch acc 0.5978
10:53:53.382   Training iter 250, batch loss 0.4653, batch acc 0.5626
10:53:53.832   Training iter 300, batch loss 0.4643, batch acc 0.5794
10:53:54.287   Training iter 350, batch loss 0.4644, batch acc 0.5836
10:53:54.774   Training iter 400, batch loss 0.4642, batch acc 0.5802
10:53:55.272   Training iter 450, batch loss 0.4642, batch acc 0.5932
10:53:55.769   Training iter 500, batch loss 0.4641, batch acc 0.5814
10:53:56.278   Training iter 550, batch loss 0.4648, batch acc 0.5830
10:53:56.763   Training iter 600, batch loss 0.4641, batch acc 0.5818
10:53:56.765 Training @ 313 epoch...
10:53:57.270   Training iter 50, batch loss 0.4645, batch acc 0.5770
10:53:57.776   Training iter 100, batch loss 0.4639, batch acc 0.5790
10:53:58.286   Training iter 150, batch loss 0.4644, batch acc 0.5878
10:53:58.789   Training iter 200, batch loss 0.4646, batch acc 0.5752
10:53:59.305   Training iter 250, batch loss 0.4646, batch acc 0.5866
10:53:59.802   Training iter 300, batch loss 0.4639, batch acc 0.5914
10:54:00.325   Training iter 350, batch loss 0.4641, batch acc 0.5840
10:54:00.851   Training iter 400, batch loss 0.4643, batch acc 0.5728
10:54:01.388   Training iter 450, batch loss 0.4645, batch acc 0.5780
10:54:01.899   Training iter 500, batch loss 0.4648, batch acc 0.5800
10:54:02.441   Training iter 550, batch loss 0.4644, batch acc 0.5972
10:54:03.007   Training iter 600, batch loss 0.4642, batch acc 0.5878
10:54:03.009 Training @ 314 epoch...
10:54:03.583   Training iter 50, batch loss 0.4644, batch acc 0.5790
10:54:04.157   Training iter 100, batch loss 0.4644, batch acc 0.5808
10:54:04.706   Training iter 150, batch loss 0.4646, batch acc 0.5862
10:54:05.216   Training iter 200, batch loss 0.4638, batch acc 0.5904
10:54:05.768   Training iter 250, batch loss 0.4645, batch acc 0.5796
10:54:06.294   Training iter 300, batch loss 0.4644, batch acc 0.5820
10:54:06.850   Training iter 350, batch loss 0.4642, batch acc 0.5738
10:54:07.386   Training iter 400, batch loss 0.4642, batch acc 0.5972
10:54:07.899   Training iter 450, batch loss 0.4645, batch acc 0.5826
10:54:08.424   Training iter 500, batch loss 0.4647, batch acc 0.5804
10:54:08.950   Training iter 550, batch loss 0.4643, batch acc 0.5872
10:54:09.472   Training iter 600, batch loss 0.4638, batch acc 0.5820
10:54:09.473 Training @ 315 epoch...
10:54:09.967   Training iter 50, batch loss 0.4645, batch acc 0.5828
10:54:10.469   Training iter 100, batch loss 0.4644, batch acc 0.5786
10:54:10.993   Training iter 150, batch loss 0.4648, batch acc 0.5746
10:54:11.520   Training iter 200, batch loss 0.4644, batch acc 0.5782
10:54:12.014   Training iter 250, batch loss 0.4637, batch acc 0.5994
10:54:12.525   Training iter 300, batch loss 0.4642, batch acc 0.5874
10:54:13.036   Training iter 350, batch loss 0.4643, batch acc 0.5864
10:54:13.536   Training iter 400, batch loss 0.4640, batch acc 0.5856
10:54:14.038   Training iter 450, batch loss 0.4645, batch acc 0.5802
10:54:14.543   Training iter 500, batch loss 0.4641, batch acc 0.5958
10:54:15.049   Training iter 550, batch loss 0.4648, batch acc 0.5740
10:54:15.559   Training iter 600, batch loss 0.4640, batch acc 0.5752
10:54:15.561 Testing @ 315 epoch...
10:54:15.595     Testing, total mean loss 0.46386, total acc 0.59810
10:54:15.595 Training @ 316 epoch...
10:54:16.113   Training iter 50, batch loss 0.4647, batch acc 0.5816
10:54:16.616   Training iter 100, batch loss 0.4637, batch acc 0.5926
10:54:17.105   Training iter 150, batch loss 0.4643, batch acc 0.5984
10:54:17.608   Training iter 200, batch loss 0.4643, batch acc 0.5800
10:54:18.107   Training iter 250, batch loss 0.4645, batch acc 0.5836
10:54:18.619   Training iter 300, batch loss 0.4640, batch acc 0.5898
10:54:19.133   Training iter 350, batch loss 0.4641, batch acc 0.5802
10:54:19.645   Training iter 400, batch loss 0.4645, batch acc 0.5774
10:54:20.182   Training iter 450, batch loss 0.4644, batch acc 0.5784
10:54:20.724   Training iter 500, batch loss 0.4643, batch acc 0.5734
10:54:21.270   Training iter 550, batch loss 0.4646, batch acc 0.5770
10:54:21.814   Training iter 600, batch loss 0.4641, batch acc 0.5878
10:54:21.815 Training @ 317 epoch...
10:54:22.363   Training iter 50, batch loss 0.4641, batch acc 0.5834
10:54:22.901   Training iter 100, batch loss 0.4642, batch acc 0.5772
10:54:23.441   Training iter 150, batch loss 0.4641, batch acc 0.5860
10:54:23.955   Training iter 200, batch loss 0.4644, batch acc 0.5728
10:54:24.449   Training iter 250, batch loss 0.4642, batch acc 0.5892
10:54:24.954   Training iter 300, batch loss 0.4643, batch acc 0.5870
10:54:25.484   Training iter 350, batch loss 0.4645, batch acc 0.5804
10:54:25.998   Training iter 400, batch loss 0.4643, batch acc 0.5824
10:54:26.521   Training iter 450, batch loss 0.4646, batch acc 0.5874
10:54:27.054   Training iter 500, batch loss 0.4637, batch acc 0.5956
10:54:27.596   Training iter 550, batch loss 0.4645, batch acc 0.5722
10:54:28.124   Training iter 600, batch loss 0.4644, batch acc 0.5856
10:54:28.125 Training @ 318 epoch...
10:54:28.647   Training iter 50, batch loss 0.4644, batch acc 0.5818
10:54:29.184   Training iter 100, batch loss 0.4648, batch acc 0.5748
10:54:29.709   Training iter 150, batch loss 0.4643, batch acc 0.5830
10:54:30.232   Training iter 200, batch loss 0.4641, batch acc 0.5848
10:54:30.704   Training iter 250, batch loss 0.4644, batch acc 0.5824
10:54:31.195   Training iter 300, batch loss 0.4641, batch acc 0.5838
10:54:31.687   Training iter 350, batch loss 0.4643, batch acc 0.5796
10:54:32.148   Training iter 400, batch loss 0.4638, batch acc 0.5990
10:54:32.610   Training iter 450, batch loss 0.4641, batch acc 0.5800
10:54:33.078   Training iter 500, batch loss 0.4638, batch acc 0.5908
10:54:33.551   Training iter 550, batch loss 0.4646, batch acc 0.5792
10:54:34.060   Training iter 600, batch loss 0.4644, batch acc 0.5844
10:54:34.062 Training @ 319 epoch...
10:54:34.577   Training iter 50, batch loss 0.4647, batch acc 0.5736
10:54:35.100   Training iter 100, batch loss 0.4643, batch acc 0.5822
10:54:35.627   Training iter 150, batch loss 0.4644, batch acc 0.5816
10:54:36.154   Training iter 200, batch loss 0.4643, batch acc 0.5772
10:54:36.668   Training iter 250, batch loss 0.4639, batch acc 0.6002
10:54:37.193   Training iter 300, batch loss 0.4642, batch acc 0.5792
10:54:37.717   Training iter 350, batch loss 0.4640, batch acc 0.5834
10:54:38.258   Training iter 400, batch loss 0.4640, batch acc 0.5880
10:54:38.772   Training iter 450, batch loss 0.4644, batch acc 0.5820
10:54:39.267   Training iter 500, batch loss 0.4644, batch acc 0.5832
10:54:39.770   Training iter 550, batch loss 0.4642, batch acc 0.5846
10:54:40.252   Training iter 600, batch loss 0.4640, batch acc 0.5878
10:54:40.253 Training @ 320 epoch...
10:54:40.722   Training iter 50, batch loss 0.4640, batch acc 0.5806
10:54:41.184   Training iter 100, batch loss 0.4640, batch acc 0.5918
10:54:41.651   Training iter 150, batch loss 0.4648, batch acc 0.5788
10:54:42.127   Training iter 200, batch loss 0.4642, batch acc 0.5920
10:54:42.597   Training iter 250, batch loss 0.4640, batch acc 0.5916
10:54:43.091   Training iter 300, batch loss 0.4639, batch acc 0.5896
10:54:43.623   Training iter 350, batch loss 0.4643, batch acc 0.5712
10:54:44.153   Training iter 400, batch loss 0.4641, batch acc 0.5878
10:54:44.673   Training iter 450, batch loss 0.4643, batch acc 0.5856
10:54:45.178   Training iter 500, batch loss 0.4639, batch acc 0.5870
10:54:45.694   Training iter 550, batch loss 0.4644, batch acc 0.5742
10:54:46.200   Training iter 600, batch loss 0.4647, batch acc 0.5738
10:54:46.202 Testing @ 320 epoch...
10:54:46.237     Testing, total mean loss 0.46378, total acc 0.59840
10:54:46.237 Training @ 321 epoch...
10:54:46.737   Training iter 50, batch loss 0.4642, batch acc 0.5926
10:54:47.240   Training iter 100, batch loss 0.4639, batch acc 0.5886
10:54:47.747   Training iter 150, batch loss 0.4644, batch acc 0.5780
10:54:48.257   Training iter 200, batch loss 0.4640, batch acc 0.5778
10:54:48.776   Training iter 250, batch loss 0.4639, batch acc 0.5868
10:54:49.302   Training iter 300, batch loss 0.4640, batch acc 0.5928
10:54:49.818   Training iter 350, batch loss 0.4640, batch acc 0.5892
10:54:50.352   Training iter 400, batch loss 0.4648, batch acc 0.5770
10:54:50.901   Training iter 450, batch loss 0.4645, batch acc 0.5748
10:54:51.440   Training iter 500, batch loss 0.4641, batch acc 0.5888
10:54:51.978   Training iter 550, batch loss 0.4643, batch acc 0.5764
10:54:52.528   Training iter 600, batch loss 0.4642, batch acc 0.5830
10:54:52.529 Training @ 322 epoch...
10:54:53.080   Training iter 50, batch loss 0.4649, batch acc 0.5614
10:54:53.625   Training iter 100, batch loss 0.4639, batch acc 0.5890
10:54:54.152   Training iter 150, batch loss 0.4644, batch acc 0.5766
10:54:54.679   Training iter 200, batch loss 0.4641, batch acc 0.5894
10:54:55.214   Training iter 250, batch loss 0.4646, batch acc 0.5894
10:54:55.717   Training iter 300, batch loss 0.4646, batch acc 0.5856
10:54:56.189   Training iter 350, batch loss 0.4642, batch acc 0.5892
10:54:56.614   Training iter 400, batch loss 0.4643, batch acc 0.5894
10:54:57.047   Training iter 450, batch loss 0.4634, batch acc 0.6034
10:54:57.496   Training iter 500, batch loss 0.4640, batch acc 0.5790
10:54:57.961   Training iter 550, batch loss 0.4641, batch acc 0.5770
10:54:58.440   Training iter 600, batch loss 0.4639, batch acc 0.5790
10:54:58.442 Training @ 323 epoch...
10:54:58.930   Training iter 50, batch loss 0.4649, batch acc 0.5742
10:54:59.420   Training iter 100, batch loss 0.4637, batch acc 0.5864
10:54:59.921   Training iter 150, batch loss 0.4638, batch acc 0.5848
10:55:00.440   Training iter 200, batch loss 0.4642, batch acc 0.5830
10:55:00.950   Training iter 250, batch loss 0.4640, batch acc 0.5864
10:55:01.510   Training iter 300, batch loss 0.4640, batch acc 0.5906
10:55:02.072   Training iter 350, batch loss 0.4639, batch acc 0.5938
10:55:02.584   Training iter 400, batch loss 0.4645, batch acc 0.5794
10:55:03.079   Training iter 450, batch loss 0.4644, batch acc 0.5708
10:55:03.583   Training iter 500, batch loss 0.4645, batch acc 0.5864
10:55:04.092   Training iter 550, batch loss 0.4642, batch acc 0.5804
10:55:04.606   Training iter 600, batch loss 0.4640, batch acc 0.5904
10:55:04.608 Training @ 324 epoch...
10:55:05.116   Training iter 50, batch loss 0.4644, batch acc 0.5818
10:55:05.621   Training iter 100, batch loss 0.4643, batch acc 0.5848
10:55:06.136   Training iter 150, batch loss 0.4638, batch acc 0.5958
10:55:06.661   Training iter 200, batch loss 0.4642, batch acc 0.5874
10:55:07.179   Training iter 250, batch loss 0.4635, batch acc 0.5908
10:55:07.696   Training iter 300, batch loss 0.4638, batch acc 0.5808
10:55:08.205   Training iter 350, batch loss 0.4647, batch acc 0.5724
10:55:08.722   Training iter 400, batch loss 0.4641, batch acc 0.5882
10:55:09.247   Training iter 450, batch loss 0.4644, batch acc 0.5792
10:55:09.780   Training iter 500, batch loss 0.4642, batch acc 0.5822
10:55:10.313   Training iter 550, batch loss 0.4641, batch acc 0.5844
10:55:10.843   Training iter 600, batch loss 0.4643, batch acc 0.5790
10:55:10.845 Training @ 325 epoch...
10:55:11.391   Training iter 50, batch loss 0.4642, batch acc 0.5898
10:55:11.923   Training iter 100, batch loss 0.4641, batch acc 0.5800
10:55:12.423   Training iter 150, batch loss 0.4639, batch acc 0.5944
10:55:12.916   Training iter 200, batch loss 0.4643, batch acc 0.5844
10:55:13.433   Training iter 250, batch loss 0.4646, batch acc 0.5816
10:55:13.939   Training iter 300, batch loss 0.4639, batch acc 0.5896
10:55:14.439   Training iter 350, batch loss 0.4644, batch acc 0.5810
10:55:14.935   Training iter 400, batch loss 0.4639, batch acc 0.5918
10:55:15.423   Training iter 450, batch loss 0.4643, batch acc 0.5728
10:55:15.943   Training iter 500, batch loss 0.4636, batch acc 0.5850
10:55:16.495   Training iter 550, batch loss 0.4641, batch acc 0.5784
10:55:16.981   Training iter 600, batch loss 0.4643, batch acc 0.5810
10:55:16.983 Testing @ 325 epoch...
10:55:17.019     Testing, total mean loss 0.46369, total acc 0.59890
10:55:17.019 Training @ 326 epoch...
10:55:17.513   Training iter 50, batch loss 0.4636, batch acc 0.5942
10:55:18.015   Training iter 100, batch loss 0.4640, batch acc 0.5908
10:55:18.514   Training iter 150, batch loss 0.4648, batch acc 0.5650
10:55:19.004   Training iter 200, batch loss 0.4644, batch acc 0.5842
10:55:19.479   Training iter 250, batch loss 0.4639, batch acc 0.5714
10:55:19.967   Training iter 300, batch loss 0.4644, batch acc 0.5806
10:55:20.483   Training iter 350, batch loss 0.4644, batch acc 0.5810
10:55:20.996   Training iter 400, batch loss 0.4639, batch acc 0.5898
10:55:21.508   Training iter 450, batch loss 0.4640, batch acc 0.5900
10:55:22.009   Training iter 500, batch loss 0.4644, batch acc 0.5764
10:55:22.512   Training iter 550, batch loss 0.4638, batch acc 0.5982
10:55:23.035   Training iter 600, batch loss 0.4638, batch acc 0.5884
10:55:23.037 Training @ 327 epoch...
10:55:23.555   Training iter 50, batch loss 0.4641, batch acc 0.5904
10:55:24.064   Training iter 100, batch loss 0.4639, batch acc 0.5808
10:55:24.571   Training iter 150, batch loss 0.4637, batch acc 0.5910
10:55:25.094   Training iter 200, batch loss 0.4642, batch acc 0.5906
10:55:25.606   Training iter 250, batch loss 0.4643, batch acc 0.5956
10:55:26.123   Training iter 300, batch loss 0.4643, batch acc 0.5730
10:55:26.638   Training iter 350, batch loss 0.4640, batch acc 0.5778
10:55:27.165   Training iter 400, batch loss 0.4644, batch acc 0.5738
10:55:27.691   Training iter 450, batch loss 0.4640, batch acc 0.5886
10:55:28.216   Training iter 500, batch loss 0.4641, batch acc 0.5826
10:55:28.741   Training iter 550, batch loss 0.4639, batch acc 0.5782
10:55:29.257   Training iter 600, batch loss 0.4642, batch acc 0.5872
10:55:29.259 Training @ 328 epoch...
10:55:29.763   Training iter 50, batch loss 0.4642, batch acc 0.5790
10:55:30.270   Training iter 100, batch loss 0.4642, batch acc 0.5922
10:55:30.774   Training iter 150, batch loss 0.4640, batch acc 0.5832
10:55:31.277   Training iter 200, batch loss 0.4639, batch acc 0.5950
10:55:31.768   Training iter 250, batch loss 0.4643, batch acc 0.5794
10:55:32.285   Training iter 300, batch loss 0.4645, batch acc 0.5720
10:55:32.801   Training iter 350, batch loss 0.4639, batch acc 0.5788
10:55:33.331   Training iter 400, batch loss 0.4640, batch acc 0.5852
10:55:33.844   Training iter 450, batch loss 0.4637, batch acc 0.5994
10:55:34.354   Training iter 500, batch loss 0.4642, batch acc 0.5790
10:55:34.880   Training iter 550, batch loss 0.4639, batch acc 0.5904
10:55:35.389   Training iter 600, batch loss 0.4644, batch acc 0.5790
10:55:35.391 Training @ 329 epoch...
10:55:35.911   Training iter 50, batch loss 0.4641, batch acc 0.5810
10:55:36.420   Training iter 100, batch loss 0.4645, batch acc 0.5888
10:55:36.925   Training iter 150, batch loss 0.4641, batch acc 0.5812
10:55:37.432   Training iter 200, batch loss 0.4639, batch acc 0.5852
10:55:37.946   Training iter 250, batch loss 0.4640, batch acc 0.5866
10:55:38.461   Training iter 300, batch loss 0.4647, batch acc 0.5750
10:55:38.958   Training iter 350, batch loss 0.4641, batch acc 0.5802
10:55:39.459   Training iter 400, batch loss 0.4635, batch acc 0.5878
10:55:39.946   Training iter 450, batch loss 0.4642, batch acc 0.5822
10:55:40.464   Training iter 500, batch loss 0.4639, batch acc 0.5814
10:55:41.014   Training iter 550, batch loss 0.4644, batch acc 0.5826
10:55:41.571   Training iter 600, batch loss 0.4634, batch acc 0.6018
10:55:41.572 Training @ 330 epoch...
10:55:42.144   Training iter 50, batch loss 0.4642, batch acc 0.5852
10:55:42.676   Training iter 100, batch loss 0.4636, batch acc 0.5900
10:55:43.195   Training iter 150, batch loss 0.4637, batch acc 0.5950
10:55:43.690   Training iter 200, batch loss 0.4640, batch acc 0.5772
10:55:44.177   Training iter 250, batch loss 0.4642, batch acc 0.5812
10:55:44.662   Training iter 300, batch loss 0.4642, batch acc 0.5850
10:55:45.160   Training iter 350, batch loss 0.4642, batch acc 0.5900
10:55:45.644   Training iter 400, batch loss 0.4640, batch acc 0.5836
10:55:46.141   Training iter 450, batch loss 0.4642, batch acc 0.5882
10:55:46.623   Training iter 500, batch loss 0.4640, batch acc 0.5900
10:55:47.108   Training iter 550, batch loss 0.4640, batch acc 0.5766
10:55:47.593   Training iter 600, batch loss 0.4643, batch acc 0.5720
10:55:47.595 Testing @ 330 epoch...
10:55:47.629     Testing, total mean loss 0.46361, total acc 0.59920
10:55:47.629 Training @ 331 epoch...
10:55:48.130   Training iter 50, batch loss 0.4644, batch acc 0.5800
10:55:48.626   Training iter 100, batch loss 0.4638, batch acc 0.5794
10:55:49.102   Training iter 150, batch loss 0.4640, batch acc 0.5896
10:55:49.577   Training iter 200, batch loss 0.4638, batch acc 0.5988
10:55:50.056   Training iter 250, batch loss 0.4640, batch acc 0.5868
10:55:50.489   Training iter 300, batch loss 0.4641, batch acc 0.5850
10:55:50.929   Training iter 350, batch loss 0.4642, batch acc 0.5834
10:55:51.366   Training iter 400, batch loss 0.4639, batch acc 0.5802
10:55:51.801   Training iter 450, batch loss 0.4637, batch acc 0.5902
10:55:52.268   Training iter 500, batch loss 0.4643, batch acc 0.5764
10:55:52.721   Training iter 550, batch loss 0.4641, batch acc 0.5832
10:55:53.193   Training iter 600, batch loss 0.4641, batch acc 0.5816
10:55:53.195 Training @ 332 epoch...
10:55:53.667   Training iter 50, batch loss 0.4642, batch acc 0.5862
10:55:54.122   Training iter 100, batch loss 0.4643, batch acc 0.5764
10:55:54.614   Training iter 150, batch loss 0.4638, batch acc 0.5852
10:55:55.132   Training iter 200, batch loss 0.4635, batch acc 0.6062
10:55:55.648   Training iter 250, batch loss 0.4638, batch acc 0.5986
10:55:56.156   Training iter 300, batch loss 0.4639, batch acc 0.5854
10:55:56.650   Training iter 350, batch loss 0.4648, batch acc 0.5804
10:55:57.153   Training iter 400, batch loss 0.4639, batch acc 0.5814
10:55:57.659   Training iter 450, batch loss 0.4642, batch acc 0.5764
10:55:58.177   Training iter 500, batch loss 0.4639, batch acc 0.5796
10:55:58.681   Training iter 550, batch loss 0.4640, batch acc 0.5730
10:55:59.186   Training iter 600, batch loss 0.4640, batch acc 0.5882
10:55:59.188 Training @ 333 epoch...
10:55:59.686   Training iter 50, batch loss 0.4639, batch acc 0.5892
10:56:00.188   Training iter 100, batch loss 0.4640, batch acc 0.5904
10:56:00.691   Training iter 150, batch loss 0.4639, batch acc 0.5888
10:56:01.208   Training iter 200, batch loss 0.4639, batch acc 0.5818
10:56:01.747   Training iter 250, batch loss 0.4639, batch acc 0.5878
10:56:02.266   Training iter 300, batch loss 0.4637, batch acc 0.5836
10:56:02.778   Training iter 350, batch loss 0.4643, batch acc 0.5844
10:56:03.303   Training iter 400, batch loss 0.4641, batch acc 0.5836
10:56:03.829   Training iter 450, batch loss 0.4641, batch acc 0.5816
10:56:04.336   Training iter 500, batch loss 0.4643, batch acc 0.5748
10:56:04.854   Training iter 550, batch loss 0.4640, batch acc 0.5802
10:56:05.389   Training iter 600, batch loss 0.4638, batch acc 0.5910
10:56:05.391 Training @ 334 epoch...
10:56:05.901   Training iter 50, batch loss 0.4640, batch acc 0.5792
10:56:06.434   Training iter 100, batch loss 0.4638, batch acc 0.5900
10:56:06.956   Training iter 150, batch loss 0.4637, batch acc 0.5812
10:56:07.483   Training iter 200, batch loss 0.4648, batch acc 0.5730
10:56:08.010   Training iter 250, batch loss 0.4641, batch acc 0.5848
10:56:08.525   Training iter 300, batch loss 0.4638, batch acc 0.5914
10:56:09.029   Training iter 350, batch loss 0.4635, batch acc 0.5904
10:56:09.515   Training iter 400, batch loss 0.4635, batch acc 0.5916
10:56:10.010   Training iter 450, batch loss 0.4640, batch acc 0.5768
10:56:10.491   Training iter 500, batch loss 0.4644, batch acc 0.5814
10:56:10.979   Training iter 550, batch loss 0.4641, batch acc 0.5868
10:56:11.474   Training iter 600, batch loss 0.4641, batch acc 0.5926
10:56:11.475 Training @ 335 epoch...
10:56:11.972   Training iter 50, batch loss 0.4638, batch acc 0.5952
10:56:12.483   Training iter 100, batch loss 0.4642, batch acc 0.5834
10:56:12.986   Training iter 150, batch loss 0.4634, batch acc 0.5924
10:56:13.475   Training iter 200, batch loss 0.4641, batch acc 0.5816
10:56:13.979   Training iter 250, batch loss 0.4642, batch acc 0.5810
10:56:14.482   Training iter 300, batch loss 0.4642, batch acc 0.5826
10:56:14.993   Training iter 350, batch loss 0.4641, batch acc 0.5766
10:56:15.497   Training iter 400, batch loss 0.4643, batch acc 0.5746
10:56:16.005   Training iter 450, batch loss 0.4636, batch acc 0.5854
10:56:16.552   Training iter 500, batch loss 0.4643, batch acc 0.5832
10:56:17.074   Training iter 550, batch loss 0.4637, batch acc 0.5932
10:56:17.580   Training iter 600, batch loss 0.4637, batch acc 0.5922
10:56:17.582 Testing @ 335 epoch...
10:56:17.621     Testing, total mean loss 0.46352, total acc 0.59970
10:56:17.621 Training @ 336 epoch...
10:56:18.182   Training iter 50, batch loss 0.4641, batch acc 0.5892
10:56:18.726   Training iter 100, batch loss 0.4637, batch acc 0.5946
10:56:19.283   Training iter 150, batch loss 0.4635, batch acc 0.5914
10:56:19.813   Training iter 200, batch loss 0.4644, batch acc 0.5796
10:56:20.329   Training iter 250, batch loss 0.4639, batch acc 0.5854
10:56:20.825   Training iter 300, batch loss 0.4638, batch acc 0.5782
10:56:21.336   Training iter 350, batch loss 0.4645, batch acc 0.5748
10:56:21.821   Training iter 400, batch loss 0.4643, batch acc 0.5872
10:56:22.310   Training iter 450, batch loss 0.4638, batch acc 0.5856
10:56:22.815   Training iter 500, batch loss 0.4639, batch acc 0.5816
10:56:23.326   Training iter 550, batch loss 0.4635, batch acc 0.5910
10:56:23.822   Training iter 600, batch loss 0.4640, batch acc 0.5846
10:56:23.824 Training @ 337 epoch...
10:56:24.327   Training iter 50, batch loss 0.4637, batch acc 0.5896
10:56:24.832   Training iter 100, batch loss 0.4640, batch acc 0.5944
10:56:25.373   Training iter 150, batch loss 0.4645, batch acc 0.5798
10:56:25.923   Training iter 200, batch loss 0.4640, batch acc 0.5830
10:56:26.461   Training iter 250, batch loss 0.4637, batch acc 0.5958
10:56:26.994   Training iter 300, batch loss 0.4641, batch acc 0.5796
10:56:27.556   Training iter 350, batch loss 0.4639, batch acc 0.5864
10:56:28.114   Training iter 400, batch loss 0.4636, batch acc 0.5910
10:56:28.669   Training iter 450, batch loss 0.4637, batch acc 0.5870
10:56:29.219   Training iter 500, batch loss 0.4640, batch acc 0.5792
10:56:29.790   Training iter 550, batch loss 0.4639, batch acc 0.5796
10:56:30.360   Training iter 600, batch loss 0.4640, batch acc 0.5784
10:56:30.362 Training @ 338 epoch...
10:56:30.927   Training iter 50, batch loss 0.4636, batch acc 0.5982
10:56:31.470   Training iter 100, batch loss 0.4645, batch acc 0.5868
10:56:31.971   Training iter 150, batch loss 0.4642, batch acc 0.5798
10:56:32.472   Training iter 200, batch loss 0.4637, batch acc 0.5942
10:56:32.976   Training iter 250, batch loss 0.4637, batch acc 0.5880
10:56:33.470   Training iter 300, batch loss 0.4634, batch acc 0.5878
10:56:33.940   Training iter 350, batch loss 0.4642, batch acc 0.5808
10:56:34.453   Training iter 400, batch loss 0.4639, batch acc 0.5816
10:56:35.011   Training iter 450, batch loss 0.4642, batch acc 0.5768
10:56:35.563   Training iter 500, batch loss 0.4638, batch acc 0.5838
10:56:36.122   Training iter 550, batch loss 0.4639, batch acc 0.5816
10:56:36.583   Training iter 600, batch loss 0.4640, batch acc 0.5868
10:56:36.584 Training @ 339 epoch...
10:56:37.029   Training iter 50, batch loss 0.4638, batch acc 0.5842
10:56:37.490   Training iter 100, batch loss 0.4641, batch acc 0.5802
10:56:37.938   Training iter 150, batch loss 0.4637, batch acc 0.5820
10:56:38.383   Training iter 200, batch loss 0.4637, batch acc 0.5968
10:56:38.840   Training iter 250, batch loss 0.4637, batch acc 0.5802
10:56:39.306   Training iter 300, batch loss 0.4639, batch acc 0.5892
10:56:39.762   Training iter 350, batch loss 0.4638, batch acc 0.5868
10:56:40.219   Training iter 400, batch loss 0.4638, batch acc 0.5854
10:56:40.667   Training iter 450, batch loss 0.4642, batch acc 0.5834
10:56:41.110   Training iter 500, batch loss 0.4642, batch acc 0.5834
10:56:41.586   Training iter 550, batch loss 0.4638, batch acc 0.5886
10:56:42.093   Training iter 600, batch loss 0.4642, batch acc 0.5870
10:56:42.095 Training @ 340 epoch...
10:56:42.599   Training iter 50, batch loss 0.4640, batch acc 0.5862
10:56:43.093   Training iter 100, batch loss 0.4644, batch acc 0.5682
10:56:43.591   Training iter 150, batch loss 0.4636, batch acc 0.5960
10:56:44.088   Training iter 200, batch loss 0.4638, batch acc 0.5850
10:56:44.583   Training iter 250, batch loss 0.4635, batch acc 0.5902
10:56:45.093   Training iter 300, batch loss 0.4635, batch acc 0.5942
10:56:45.620   Training iter 350, batch loss 0.4639, batch acc 0.5846
10:56:46.263   Training iter 400, batch loss 0.4642, batch acc 0.5760
10:56:46.778   Training iter 450, batch loss 0.4644, batch acc 0.5802
10:56:47.299   Training iter 500, batch loss 0.4639, batch acc 0.5878
10:56:47.773   Training iter 550, batch loss 0.4640, batch acc 0.5920
10:56:48.256   Training iter 600, batch loss 0.4636, batch acc 0.5852
10:56:48.258 Testing @ 340 epoch...
10:56:48.295     Testing, total mean loss 0.46345, total acc 0.59990
10:56:48.295 Training @ 341 epoch...
10:56:48.767   Training iter 50, batch loss 0.4640, batch acc 0.5794
10:56:49.240   Training iter 100, batch loss 0.4638, batch acc 0.5814
10:56:49.733   Training iter 150, batch loss 0.4642, batch acc 0.5882
10:56:50.218   Training iter 200, batch loss 0.4637, batch acc 0.5934
10:56:50.724   Training iter 250, batch loss 0.4640, batch acc 0.5860
10:56:51.253   Training iter 300, batch loss 0.4638, batch acc 0.5848
10:56:51.780   Training iter 350, batch loss 0.4640, batch acc 0.5764
10:56:52.309   Training iter 400, batch loss 0.4641, batch acc 0.5826
10:56:52.822   Training iter 450, batch loss 0.4639, batch acc 0.5928
10:56:53.326   Training iter 500, batch loss 0.4636, batch acc 0.5956
10:56:53.842   Training iter 550, batch loss 0.4637, batch acc 0.5872
10:56:54.366   Training iter 600, batch loss 0.4638, batch acc 0.5812
10:56:54.368 Training @ 342 epoch...
10:56:54.903   Training iter 50, batch loss 0.4643, batch acc 0.5776
10:56:55.440   Training iter 100, batch loss 0.4638, batch acc 0.5840
10:56:55.962   Training iter 150, batch loss 0.4636, batch acc 0.5882
10:56:56.479   Training iter 200, batch loss 0.4640, batch acc 0.5854
10:56:56.967   Training iter 250, batch loss 0.4640, batch acc 0.5956
10:56:57.452   Training iter 300, batch loss 0.4635, batch acc 0.5926
10:56:57.965   Training iter 350, batch loss 0.4639, batch acc 0.5820
10:56:58.465   Training iter 400, batch loss 0.4641, batch acc 0.5794
10:56:58.971   Training iter 450, batch loss 0.4639, batch acc 0.5760
10:56:59.467   Training iter 500, batch loss 0.4639, batch acc 0.5892
10:56:59.973   Training iter 550, batch loss 0.4639, batch acc 0.5824
10:57:00.478   Training iter 600, batch loss 0.4637, batch acc 0.5954
10:57:00.480 Training @ 343 epoch...
10:57:00.996   Training iter 50, batch loss 0.4640, batch acc 0.5898
10:57:01.526   Training iter 100, batch loss 0.4634, batch acc 0.5832
10:57:02.060   Training iter 150, batch loss 0.4636, batch acc 0.5934
10:57:02.584   Training iter 200, batch loss 0.4641, batch acc 0.5860
10:57:03.127   Training iter 250, batch loss 0.4636, batch acc 0.5962
10:57:03.664   Training iter 300, batch loss 0.4635, batch acc 0.5880
10:57:04.190   Training iter 350, batch loss 0.4639, batch acc 0.5908
10:57:04.720   Training iter 400, batch loss 0.4639, batch acc 0.5858
10:57:05.250   Training iter 450, batch loss 0.4639, batch acc 0.5844
10:57:05.770   Training iter 500, batch loss 0.4639, batch acc 0.5842
10:57:06.295   Training iter 550, batch loss 0.4641, batch acc 0.5726
10:57:06.817   Training iter 600, batch loss 0.4642, batch acc 0.5740
10:57:06.819 Training @ 344 epoch...
10:57:07.358   Training iter 50, batch loss 0.4638, batch acc 0.5812
10:57:07.873   Training iter 100, batch loss 0.4641, batch acc 0.5876
10:57:08.385   Training iter 150, batch loss 0.4641, batch acc 0.5830
10:57:08.899   Training iter 200, batch loss 0.4634, batch acc 0.5848
10:57:09.414   Training iter 250, batch loss 0.4636, batch acc 0.5940
10:57:09.930   Training iter 300, batch loss 0.4636, batch acc 0.5840
10:57:10.509   Training iter 350, batch loss 0.4638, batch acc 0.5840
10:57:11.052   Training iter 400, batch loss 0.4638, batch acc 0.5908
10:57:11.554   Training iter 450, batch loss 0.4636, batch acc 0.5890
10:57:12.062   Training iter 500, batch loss 0.4643, batch acc 0.5794
10:57:12.599   Training iter 550, batch loss 0.4643, batch acc 0.5876
10:57:13.155   Training iter 600, batch loss 0.4635, batch acc 0.5816
10:57:13.157 Training @ 345 epoch...
10:57:13.758   Training iter 50, batch loss 0.4635, batch acc 0.5832
10:57:14.352   Training iter 100, batch loss 0.4640, batch acc 0.5824
10:57:14.940   Training iter 150, batch loss 0.4638, batch acc 0.5952
10:57:15.552   Training iter 200, batch loss 0.4642, batch acc 0.5798
10:57:16.129   Training iter 250, batch loss 0.4639, batch acc 0.5872
10:57:16.687   Training iter 300, batch loss 0.4637, batch acc 0.5762
10:57:17.238   Training iter 350, batch loss 0.4639, batch acc 0.5828
10:57:17.807   Training iter 400, batch loss 0.4640, batch acc 0.5870
10:57:18.372   Training iter 450, batch loss 0.4635, batch acc 0.5978
10:57:18.952   Training iter 500, batch loss 0.4636, batch acc 0.5848
10:57:19.502   Training iter 550, batch loss 0.4641, batch acc 0.5928
10:57:20.070   Training iter 600, batch loss 0.4636, batch acc 0.5794
10:57:20.072 Testing @ 345 epoch...
10:57:20.114     Testing, total mean loss 0.46337, total acc 0.60020
10:57:20.114 Training @ 346 epoch...
10:57:20.671   Training iter 50, batch loss 0.4638, batch acc 0.5736
10:57:21.209   Training iter 100, batch loss 0.4640, batch acc 0.5896
10:57:21.722   Training iter 150, batch loss 0.4635, batch acc 0.5946
10:57:22.225   Training iter 200, batch loss 0.4634, batch acc 0.5968
10:57:22.738   Training iter 250, batch loss 0.4636, batch acc 0.5918
10:57:23.274   Training iter 300, batch loss 0.4638, batch acc 0.5816
10:57:23.823   Training iter 350, batch loss 0.4634, batch acc 0.5948
10:57:24.372   Training iter 400, batch loss 0.4642, batch acc 0.5794
10:57:24.949   Training iter 450, batch loss 0.4638, batch acc 0.5862
10:57:25.538   Training iter 500, batch loss 0.4637, batch acc 0.5892
10:57:26.117   Training iter 550, batch loss 0.4642, batch acc 0.5766
10:57:26.687   Training iter 600, batch loss 0.4642, batch acc 0.5720
10:57:26.689 Training @ 347 epoch...
10:57:27.282   Training iter 50, batch loss 0.4641, batch acc 0.5754
10:57:27.868   Training iter 100, batch loss 0.4637, batch acc 0.5922
10:57:28.457   Training iter 150, batch loss 0.4641, batch acc 0.5850
10:57:29.034   Training iter 200, batch loss 0.4636, batch acc 0.5964
10:57:29.627   Training iter 250, batch loss 0.4640, batch acc 0.5844
10:57:30.250   Training iter 300, batch loss 0.4633, batch acc 0.5854
10:57:30.875   Training iter 350, batch loss 0.4640, batch acc 0.5886
10:57:31.479   Training iter 400, batch loss 0.4639, batch acc 0.5836
10:57:32.084   Training iter 450, batch loss 0.4638, batch acc 0.5876
10:57:32.699   Training iter 500, batch loss 0.4639, batch acc 0.5702
10:57:33.306   Training iter 550, batch loss 0.4636, batch acc 0.5908
10:57:33.908   Training iter 600, batch loss 0.4636, batch acc 0.5908
10:57:33.910 Training @ 348 epoch...
10:57:34.520   Training iter 50, batch loss 0.4635, batch acc 0.5940
10:57:35.131   Training iter 100, batch loss 0.4637, batch acc 0.5828
10:57:35.686   Training iter 150, batch loss 0.4637, batch acc 0.5878
10:57:36.237   Training iter 200, batch loss 0.4638, batch acc 0.5870
10:57:36.770   Training iter 250, batch loss 0.4638, batch acc 0.5854
10:57:37.324   Training iter 300, batch loss 0.4633, batch acc 0.5924
10:57:37.886   Training iter 350, batch loss 0.4643, batch acc 0.5744
10:57:38.449   Training iter 400, batch loss 0.4638, batch acc 0.5818
10:57:38.994   Training iter 450, batch loss 0.4639, batch acc 0.5856
10:57:39.530   Training iter 500, batch loss 0.4640, batch acc 0.5806
10:57:40.084   Training iter 550, batch loss 0.4639, batch acc 0.5854
10:57:40.637   Training iter 600, batch loss 0.4635, batch acc 0.5938
10:57:40.639 Training @ 349 epoch...
10:57:41.200   Training iter 50, batch loss 0.4637, batch acc 0.5874
10:57:41.714   Training iter 100, batch loss 0.4638, batch acc 0.5848
10:57:42.277   Training iter 150, batch loss 0.4639, batch acc 0.5718
10:57:42.814   Training iter 200, batch loss 0.4643, batch acc 0.5810
10:57:43.364   Training iter 250, batch loss 0.4637, batch acc 0.5848
10:57:43.909   Training iter 300, batch loss 0.4632, batch acc 0.5930
10:57:44.476   Training iter 350, batch loss 0.4640, batch acc 0.5904
10:57:45.031   Training iter 400, batch loss 0.4639, batch acc 0.5830
10:57:45.615   Training iter 450, batch loss 0.4639, batch acc 0.5926
10:57:46.206   Training iter 500, batch loss 0.4634, batch acc 0.5942
10:57:46.788   Training iter 550, batch loss 0.4637, batch acc 0.5794
10:57:47.387   Training iter 600, batch loss 0.4636, batch acc 0.5876
10:57:47.389 Training @ 350 epoch...
10:57:47.980   Training iter 50, batch loss 0.4637, batch acc 0.5876
10:57:48.580   Training iter 100, batch loss 0.4636, batch acc 0.5932
10:57:49.178   Training iter 150, batch loss 0.4635, batch acc 0.5924
10:57:49.778   Training iter 200, batch loss 0.4640, batch acc 0.5862
10:57:50.383   Training iter 250, batch loss 0.4640, batch acc 0.5818
10:57:50.991   Training iter 300, batch loss 0.4638, batch acc 0.5848
10:57:51.602   Training iter 350, batch loss 0.4639, batch acc 0.5892
10:57:52.181   Training iter 400, batch loss 0.4636, batch acc 0.5906
10:57:52.777   Training iter 450, batch loss 0.4639, batch acc 0.5800
10:57:53.373   Training iter 500, batch loss 0.4640, batch acc 0.5802
10:57:53.948   Training iter 550, batch loss 0.4634, batch acc 0.5890
10:57:54.523   Training iter 600, batch loss 0.4636, batch acc 0.5778
10:57:54.525 Testing @ 350 epoch...
10:57:54.567     Testing, total mean loss 0.46330, total acc 0.59990
10:57:54.567 Training @ 351 epoch...
10:57:55.171   Training iter 50, batch loss 0.4640, batch acc 0.5768
10:57:55.760   Training iter 100, batch loss 0.4635, batch acc 0.5952
10:57:56.345   Training iter 150, batch loss 0.4640, batch acc 0.5874
10:57:56.934   Training iter 200, batch loss 0.4637, batch acc 0.5864
10:57:57.550   Training iter 250, batch loss 0.4634, batch acc 0.5972
10:57:58.146   Training iter 300, batch loss 0.4635, batch acc 0.5842
10:57:58.739   Training iter 350, batch loss 0.4642, batch acc 0.5790
10:57:59.310   Training iter 400, batch loss 0.4637, batch acc 0.5920
10:57:59.888   Training iter 450, batch loss 0.4634, batch acc 0.5900
10:58:00.490   Training iter 500, batch loss 0.4640, batch acc 0.5836
10:58:01.078   Training iter 550, batch loss 0.4637, batch acc 0.5740
10:58:01.654   Training iter 600, batch loss 0.4637, batch acc 0.5866
10:58:01.656 Training @ 352 epoch...
10:58:02.301   Training iter 50, batch loss 0.4637, batch acc 0.5862
10:58:02.923   Training iter 100, batch loss 0.4639, batch acc 0.5848
10:58:03.555   Training iter 150, batch loss 0.4637, batch acc 0.5836
10:58:04.191   Training iter 200, batch loss 0.4638, batch acc 0.5818
10:58:04.828   Training iter 250, batch loss 0.4643, batch acc 0.5718
10:58:05.461   Training iter 300, batch loss 0.4632, batch acc 0.5966
10:58:06.102   Training iter 350, batch loss 0.4635, batch acc 0.5938
10:58:06.735   Training iter 400, batch loss 0.4635, batch acc 0.5882
10:58:07.375   Training iter 450, batch loss 0.4636, batch acc 0.5886
10:58:08.013   Training iter 500, batch loss 0.4642, batch acc 0.5806
10:58:08.608   Training iter 550, batch loss 0.4633, batch acc 0.6000
10:58:09.182   Training iter 600, batch loss 0.4640, batch acc 0.5776
10:58:09.184 Training @ 353 epoch...
10:58:09.767   Training iter 50, batch loss 0.4635, batch acc 0.5914
10:58:10.352   Training iter 100, batch loss 0.4640, batch acc 0.5932
10:58:10.929   Training iter 150, batch loss 0.4639, batch acc 0.5868
10:58:11.495   Training iter 200, batch loss 0.4637, batch acc 0.5844
10:58:12.071   Training iter 250, batch loss 0.4635, batch acc 0.5878
10:58:12.674   Training iter 300, batch loss 0.4636, batch acc 0.5932
10:58:13.267   Training iter 350, batch loss 0.4635, batch acc 0.5862
10:58:13.850   Training iter 400, batch loss 0.4637, batch acc 0.5842
10:58:14.393   Training iter 450, batch loss 0.4639, batch acc 0.5816
10:58:14.918   Training iter 500, batch loss 0.4641, batch acc 0.5792
10:58:15.449   Training iter 550, batch loss 0.4634, batch acc 0.5908
10:58:15.964   Training iter 600, batch loss 0.4636, batch acc 0.5756
10:58:15.966 Training @ 354 epoch...
10:58:16.488   Training iter 50, batch loss 0.4632, batch acc 0.5940
10:58:17.012   Training iter 100, batch loss 0.4639, batch acc 0.5740
10:58:17.533   Training iter 150, batch loss 0.4640, batch acc 0.5856
10:58:18.054   Training iter 200, batch loss 0.4640, batch acc 0.5898
10:58:18.583   Training iter 250, batch loss 0.4640, batch acc 0.5816
10:58:19.127   Training iter 300, batch loss 0.4634, batch acc 0.5952
10:58:19.673   Training iter 350, batch loss 0.4635, batch acc 0.5878
10:58:20.215   Training iter 400, batch loss 0.4637, batch acc 0.5848
10:58:20.754   Training iter 450, batch loss 0.4632, batch acc 0.5934
10:58:21.293   Training iter 500, batch loss 0.4638, batch acc 0.5802
10:58:21.828   Training iter 550, batch loss 0.4635, batch acc 0.5860
10:58:22.370   Training iter 600, batch loss 0.4641, batch acc 0.5820
10:58:22.372 Training @ 355 epoch...
10:58:22.923   Training iter 50, batch loss 0.4636, batch acc 0.5902
10:58:23.445   Training iter 100, batch loss 0.4638, batch acc 0.5738
10:58:23.982   Training iter 150, batch loss 0.4631, batch acc 0.5886
10:58:24.508   Training iter 200, batch loss 0.4642, batch acc 0.5754
10:58:25.009   Training iter 250, batch loss 0.4637, batch acc 0.5886
10:58:25.524   Training iter 300, batch loss 0.4637, batch acc 0.5830
10:58:26.035   Training iter 350, batch loss 0.4634, batch acc 0.5856
10:58:26.546   Training iter 400, batch loss 0.4638, batch acc 0.5858
10:58:27.064   Training iter 450, batch loss 0.4640, batch acc 0.5880
10:58:27.612   Training iter 500, batch loss 0.4636, batch acc 0.5894
10:58:28.145   Training iter 550, batch loss 0.4636, batch acc 0.5910
10:58:28.660   Training iter 600, batch loss 0.4636, batch acc 0.5960
10:58:28.661 Testing @ 355 epoch...
10:58:28.697     Testing, total mean loss 0.46322, total acc 0.60000
10:58:28.697 Training @ 356 epoch...
10:58:29.237   Training iter 50, batch loss 0.4635, batch acc 0.5936
10:58:29.756   Training iter 100, batch loss 0.4636, batch acc 0.5836
10:58:30.277   Training iter 150, batch loss 0.4638, batch acc 0.5776
10:58:30.787   Training iter 200, batch loss 0.4642, batch acc 0.5846
10:58:31.300   Training iter 250, batch loss 0.4634, batch acc 0.5840
10:58:31.801   Training iter 300, batch loss 0.4634, batch acc 0.5960
10:58:32.303   Training iter 350, batch loss 0.4642, batch acc 0.5806
10:58:32.797   Training iter 400, batch loss 0.4637, batch acc 0.5888
10:58:33.294   Training iter 450, batch loss 0.4638, batch acc 0.5786
10:58:33.785   Training iter 500, batch loss 0.4628, batch acc 0.5946
10:58:34.279   Training iter 550, batch loss 0.4640, batch acc 0.5892
10:58:34.776   Training iter 600, batch loss 0.4635, batch acc 0.5830
10:58:34.778 Training @ 357 epoch...
10:58:35.280   Training iter 50, batch loss 0.4638, batch acc 0.5864
10:58:35.772   Training iter 100, batch loss 0.4634, batch acc 0.5940
10:58:36.276   Training iter 150, batch loss 0.4637, batch acc 0.5852
10:58:36.772   Training iter 200, batch loss 0.4634, batch acc 0.5924
10:58:37.281   Training iter 250, batch loss 0.4639, batch acc 0.5908
10:58:37.776   Training iter 300, batch loss 0.4636, batch acc 0.5870
10:58:38.264   Training iter 350, batch loss 0.4635, batch acc 0.5820
10:58:38.740   Training iter 400, batch loss 0.4637, batch acc 0.5942
10:58:39.236   Training iter 450, batch loss 0.4631, batch acc 0.5920
10:58:39.707   Training iter 500, batch loss 0.4645, batch acc 0.5688
10:58:40.196   Training iter 550, batch loss 0.4630, batch acc 0.5920
10:58:40.667   Training iter 600, batch loss 0.4642, batch acc 0.5724
10:58:40.669 Training @ 358 epoch...
10:58:41.149   Training iter 50, batch loss 0.4636, batch acc 0.5852
10:58:41.628   Training iter 100, batch loss 0.4635, batch acc 0.6016
10:58:42.119   Training iter 150, batch loss 0.4635, batch acc 0.5858
10:58:42.583   Training iter 200, batch loss 0.4636, batch acc 0.5842
10:58:43.040   Training iter 250, batch loss 0.4638, batch acc 0.5828
10:58:43.502   Training iter 300, batch loss 0.4640, batch acc 0.5840
10:58:43.981   Training iter 350, batch loss 0.4636, batch acc 0.5848
10:58:44.451   Training iter 400, batch loss 0.4639, batch acc 0.5770
10:58:44.941   Training iter 450, batch loss 0.4634, batch acc 0.5870
10:58:45.451   Training iter 500, batch loss 0.4638, batch acc 0.5850
10:58:45.942   Training iter 550, batch loss 0.4639, batch acc 0.5922
10:58:46.432   Training iter 600, batch loss 0.4632, batch acc 0.5914
10:58:46.433 Training @ 359 epoch...
10:58:46.926   Training iter 50, batch loss 0.4632, batch acc 0.5974
10:58:47.422   Training iter 100, batch loss 0.4636, batch acc 0.5856
10:58:47.916   Training iter 150, batch loss 0.4638, batch acc 0.5828
10:58:48.401   Training iter 200, batch loss 0.4641, batch acc 0.5844
10:58:48.879   Training iter 250, batch loss 0.4637, batch acc 0.5832
10:58:49.325   Training iter 300, batch loss 0.4640, batch acc 0.5734
10:58:49.826   Training iter 350, batch loss 0.4636, batch acc 0.5848
10:58:50.339   Training iter 400, batch loss 0.4633, batch acc 0.5882
10:58:50.823   Training iter 450, batch loss 0.4638, batch acc 0.5768
10:58:51.313   Training iter 500, batch loss 0.4640, batch acc 0.5858
10:58:51.801   Training iter 550, batch loss 0.4632, batch acc 0.6032
10:58:52.297   Training iter 600, batch loss 0.4634, batch acc 0.5950
10:58:52.299 Training @ 360 epoch...
10:58:52.811   Training iter 50, batch loss 0.4635, batch acc 0.5882
10:58:53.319   Training iter 100, batch loss 0.4636, batch acc 0.5824
10:58:53.822   Training iter 150, batch loss 0.4639, batch acc 0.5896
10:58:54.288   Training iter 200, batch loss 0.4635, batch acc 0.5870
10:58:54.759   Training iter 250, batch loss 0.4638, batch acc 0.5800
10:58:55.265   Training iter 300, batch loss 0.4633, batch acc 0.5884
10:58:55.749   Training iter 350, batch loss 0.4637, batch acc 0.5868
10:58:56.229   Training iter 400, batch loss 0.4634, batch acc 0.5936
10:58:56.709   Training iter 450, batch loss 0.4633, batch acc 0.5966
10:58:57.204   Training iter 500, batch loss 0.4642, batch acc 0.5774
10:58:57.692   Training iter 550, batch loss 0.4638, batch acc 0.5824
10:58:58.167   Training iter 600, batch loss 0.4633, batch acc 0.5870
10:58:58.169 Testing @ 360 epoch...
10:58:58.205     Testing, total mean loss 0.46315, total acc 0.60000
10:58:58.205 Training @ 361 epoch...
10:58:58.677   Training iter 50, batch loss 0.4636, batch acc 0.5908
10:58:59.187   Training iter 100, batch loss 0.4634, batch acc 0.5912
10:58:59.735   Training iter 150, batch loss 0.4639, batch acc 0.5776
10:59:00.286   Training iter 200, batch loss 0.4637, batch acc 0.5858
10:59:00.832   Training iter 250, batch loss 0.4637, batch acc 0.5840
10:59:01.350   Training iter 300, batch loss 0.4631, batch acc 0.5926
10:59:01.885   Training iter 350, batch loss 0.4637, batch acc 0.5834
10:59:02.423   Training iter 400, batch loss 0.4640, batch acc 0.5850
10:59:02.925   Training iter 450, batch loss 0.4633, batch acc 0.5940
10:59:03.436   Training iter 500, batch loss 0.4634, batch acc 0.5886
10:59:03.946   Training iter 550, batch loss 0.4636, batch acc 0.5744
10:59:04.452   Training iter 600, batch loss 0.4637, batch acc 0.5918
10:59:04.454 Training @ 362 epoch...
10:59:04.958   Training iter 50, batch loss 0.4639, batch acc 0.5810
10:59:05.459   Training iter 100, batch loss 0.4637, batch acc 0.5798
10:59:05.964   Training iter 150, batch loss 0.4638, batch acc 0.5820
10:59:06.500   Training iter 200, batch loss 0.4632, batch acc 0.5906
10:59:07.025   Training iter 250, batch loss 0.4634, batch acc 0.5892
10:59:07.552   Training iter 300, batch loss 0.4635, batch acc 0.6000
10:59:08.074   Training iter 350, batch loss 0.4635, batch acc 0.5902
10:59:08.595   Training iter 400, batch loss 0.4638, batch acc 0.5788
10:59:09.099   Training iter 450, batch loss 0.4635, batch acc 0.5922
10:59:09.606   Training iter 500, batch loss 0.4637, batch acc 0.5854
10:59:10.108   Training iter 550, batch loss 0.4638, batch acc 0.5868
10:59:10.629   Training iter 600, batch loss 0.4633, batch acc 0.5868
10:59:10.631 Training @ 363 epoch...
10:59:11.179   Training iter 50, batch loss 0.4635, batch acc 0.5902
10:59:11.726   Training iter 100, batch loss 0.4639, batch acc 0.5802
10:59:12.272   Training iter 150, batch loss 0.4636, batch acc 0.5824
10:59:12.804   Training iter 200, batch loss 0.4631, batch acc 0.6014
10:59:13.326   Training iter 250, batch loss 0.4639, batch acc 0.5720
10:59:13.839   Training iter 300, batch loss 0.4636, batch acc 0.5744
10:59:14.347   Training iter 350, batch loss 0.4641, batch acc 0.5840
10:59:14.868   Training iter 400, batch loss 0.4632, batch acc 0.5928
10:59:15.355   Training iter 450, batch loss 0.4639, batch acc 0.5852
10:59:15.817   Training iter 500, batch loss 0.4631, batch acc 0.5924
10:59:16.284   Training iter 550, batch loss 0.4638, batch acc 0.5864
10:59:16.747   Training iter 600, batch loss 0.4633, batch acc 0.6006
10:59:16.749 Training @ 364 epoch...
10:59:17.238   Training iter 50, batch loss 0.4633, batch acc 0.5962
10:59:17.724   Training iter 100, batch loss 0.4635, batch acc 0.5920
10:59:18.207   Training iter 150, batch loss 0.4638, batch acc 0.5834
10:59:18.694   Training iter 200, batch loss 0.4635, batch acc 0.5908
10:59:19.168   Training iter 250, batch loss 0.4632, batch acc 0.5916
10:59:19.649   Training iter 300, batch loss 0.4636, batch acc 0.5898
10:59:20.129   Training iter 350, batch loss 0.4634, batch acc 0.5792
10:59:20.648   Training iter 400, batch loss 0.4636, batch acc 0.5862
10:59:21.174   Training iter 450, batch loss 0.4641, batch acc 0.5786
10:59:21.698   Training iter 500, batch loss 0.4633, batch acc 0.5790
10:59:22.224   Training iter 550, batch loss 0.4638, batch acc 0.5826
10:59:22.720   Training iter 600, batch loss 0.4636, batch acc 0.5912
10:59:22.722 Training @ 365 epoch...
10:59:23.225   Training iter 50, batch loss 0.4640, batch acc 0.5806
10:59:23.717   Training iter 100, batch loss 0.4631, batch acc 0.5964
10:59:24.228   Training iter 150, batch loss 0.4637, batch acc 0.5788
10:59:24.733   Training iter 200, batch loss 0.4635, batch acc 0.5890
10:59:25.223   Training iter 250, batch loss 0.4641, batch acc 0.5734
10:59:25.762   Training iter 300, batch loss 0.4630, batch acc 0.5990
10:59:26.281   Training iter 350, batch loss 0.4631, batch acc 0.5908
10:59:26.796   Training iter 400, batch loss 0.4637, batch acc 0.5884
10:59:27.327   Training iter 450, batch loss 0.4638, batch acc 0.5916
10:59:27.837   Training iter 500, batch loss 0.4638, batch acc 0.5768
10:59:28.332   Training iter 550, batch loss 0.4636, batch acc 0.5914
10:59:28.838   Training iter 600, batch loss 0.4632, batch acc 0.5876
10:59:28.840 Testing @ 365 epoch...
10:59:28.876     Testing, total mean loss 0.46309, total acc 0.59960
10:59:28.876 Training @ 366 epoch...
10:59:29.417   Training iter 50, batch loss 0.4637, batch acc 0.5730
10:59:29.940   Training iter 100, batch loss 0.4634, batch acc 0.5880
10:59:30.474   Training iter 150, batch loss 0.4638, batch acc 0.5886
10:59:30.988   Training iter 200, batch loss 0.4630, batch acc 0.5946
10:59:31.512   Training iter 250, batch loss 0.4637, batch acc 0.5924
10:59:32.010   Training iter 300, batch loss 0.4636, batch acc 0.5950
10:59:32.530   Training iter 350, batch loss 0.4636, batch acc 0.5840
10:59:32.993   Training iter 400, batch loss 0.4631, batch acc 0.5982
10:59:33.476   Training iter 450, batch loss 0.4632, batch acc 0.5890
10:59:33.953   Training iter 500, batch loss 0.4635, batch acc 0.5810
10:59:34.425   Training iter 550, batch loss 0.4640, batch acc 0.5830
10:59:34.911   Training iter 600, batch loss 0.4639, batch acc 0.5752
10:59:34.912 Training @ 367 epoch...
10:59:35.400   Training iter 50, batch loss 0.4636, batch acc 0.5794
10:59:35.876   Training iter 100, batch loss 0.4634, batch acc 0.5936
10:59:36.364   Training iter 150, batch loss 0.4638, batch acc 0.5802
10:59:36.867   Training iter 200, batch loss 0.4636, batch acc 0.5886
10:59:37.378   Training iter 250, batch loss 0.4631, batch acc 0.5988
10:59:37.882   Training iter 300, batch loss 0.4638, batch acc 0.5758
10:59:38.395   Training iter 350, batch loss 0.4637, batch acc 0.5784
10:59:38.913   Training iter 400, batch loss 0.4635, batch acc 0.5906
10:59:39.409   Training iter 450, batch loss 0.4638, batch acc 0.5864
10:59:39.925   Training iter 500, batch loss 0.4632, batch acc 0.5898
10:59:40.460   Training iter 550, batch loss 0.4628, batch acc 0.5936
10:59:40.974   Training iter 600, batch loss 0.4638, batch acc 0.5884
10:59:40.975 Training @ 368 epoch...
10:59:41.482   Training iter 50, batch loss 0.4632, batch acc 0.5978
10:59:41.996   Training iter 100, batch loss 0.4636, batch acc 0.5836
10:59:42.528   Training iter 150, batch loss 0.4638, batch acc 0.5824
10:59:43.055   Training iter 200, batch loss 0.4635, batch acc 0.5828
10:59:43.600   Training iter 250, batch loss 0.4632, batch acc 0.5858
10:59:44.138   Training iter 300, batch loss 0.4629, batch acc 0.6030
10:59:44.608   Training iter 350, batch loss 0.4634, batch acc 0.5872
10:59:45.060   Training iter 400, batch loss 0.4638, batch acc 0.5820
10:59:45.530   Training iter 450, batch loss 0.4637, batch acc 0.5944
10:59:45.987   Training iter 500, batch loss 0.4637, batch acc 0.5722
10:59:46.464   Training iter 550, batch loss 0.4637, batch acc 0.5868
10:59:46.939   Training iter 600, batch loss 0.4636, batch acc 0.5850
10:59:46.941 Training @ 369 epoch...
10:59:47.428   Training iter 50, batch loss 0.4636, batch acc 0.5842
10:59:47.932   Training iter 100, batch loss 0.4635, batch acc 0.5922
10:59:48.414   Training iter 150, batch loss 0.4637, batch acc 0.5844
10:59:48.892   Training iter 200, batch loss 0.4632, batch acc 0.5984
10:59:49.340   Training iter 250, batch loss 0.4636, batch acc 0.5816
10:59:49.795   Training iter 300, batch loss 0.4634, batch acc 0.5872
10:59:50.276   Training iter 350, batch loss 0.4638, batch acc 0.5854
10:59:50.734   Training iter 400, batch loss 0.4630, batch acc 0.5940
10:59:51.204   Training iter 450, batch loss 0.4635, batch acc 0.5922
10:59:51.667   Training iter 500, batch loss 0.4638, batch acc 0.5748
10:59:52.129   Training iter 550, batch loss 0.4640, batch acc 0.5754
10:59:52.588   Training iter 600, batch loss 0.4630, batch acc 0.5940
10:59:52.589 Training @ 370 epoch...
10:59:53.048   Training iter 50, batch loss 0.4635, batch acc 0.5886
10:59:53.501   Training iter 100, batch loss 0.4636, batch acc 0.5862
10:59:53.934   Training iter 150, batch loss 0.4634, batch acc 0.5826
10:59:54.407   Training iter 200, batch loss 0.4634, batch acc 0.5888
10:59:54.920   Training iter 250, batch loss 0.4634, batch acc 0.5792
10:59:55.445   Training iter 300, batch loss 0.4637, batch acc 0.5822
10:59:55.953   Training iter 350, batch loss 0.4634, batch acc 0.5950
10:59:56.468   Training iter 400, batch loss 0.4633, batch acc 0.5898
10:59:57.008   Training iter 450, batch loss 0.4632, batch acc 0.5914
10:59:57.578   Training iter 500, batch loss 0.4641, batch acc 0.5860
10:59:58.152   Training iter 550, batch loss 0.4635, batch acc 0.5794
10:59:58.720   Training iter 600, batch loss 0.4632, batch acc 0.5962
10:59:58.721 Testing @ 370 epoch...
10:59:58.757     Testing, total mean loss 0.46302, total acc 0.60000
10:59:58.757 Training @ 371 epoch...
10:59:59.287   Training iter 50, batch loss 0.4636, batch acc 0.5900
10:59:59.768   Training iter 100, batch loss 0.4628, batch acc 0.6054
11:00:00.288   Training iter 150, batch loss 0.4634, batch acc 0.5810
11:00:00.795   Training iter 200, batch loss 0.4633, batch acc 0.5836
11:00:01.309   Training iter 250, batch loss 0.4634, batch acc 0.5924
11:00:01.890   Training iter 300, batch loss 0.4636, batch acc 0.5808
11:00:02.445   Training iter 350, batch loss 0.4640, batch acc 0.5782
11:00:02.951   Training iter 400, batch loss 0.4633, batch acc 0.5984
11:00:03.477   Training iter 450, batch loss 0.4634, batch acc 0.5848
11:00:04.002   Training iter 500, batch loss 0.4631, batch acc 0.5962
11:00:04.509   Training iter 550, batch loss 0.4639, batch acc 0.5806
11:00:05.023   Training iter 600, batch loss 0.4639, batch acc 0.5760
11:00:05.025 Training @ 372 epoch...
11:00:05.540   Training iter 50, batch loss 0.4635, batch acc 0.5878
11:00:06.070   Training iter 100, batch loss 0.4635, batch acc 0.5844
11:00:06.594   Training iter 150, batch loss 0.4635, batch acc 0.5838
11:00:07.140   Training iter 200, batch loss 0.4631, batch acc 0.5844
11:00:07.692   Training iter 250, batch loss 0.4638, batch acc 0.5834
11:00:08.213   Training iter 300, batch loss 0.4637, batch acc 0.5866
11:00:08.753   Training iter 350, batch loss 0.4633, batch acc 0.5904
11:00:09.294   Training iter 400, batch loss 0.4635, batch acc 0.5778
11:00:09.823   Training iter 450, batch loss 0.4632, batch acc 0.5966
11:00:10.355   Training iter 500, batch loss 0.4633, batch acc 0.5928
11:00:10.879   Training iter 550, batch loss 0.4634, batch acc 0.5822
11:00:11.402   Training iter 600, batch loss 0.4636, batch acc 0.5952
11:00:11.404 Training @ 373 epoch...
11:00:11.954   Training iter 50, batch loss 0.4638, batch acc 0.5836
11:00:12.499   Training iter 100, batch loss 0.4636, batch acc 0.5928
11:00:13.039   Training iter 150, batch loss 0.4633, batch acc 0.5992
11:00:13.569   Training iter 200, batch loss 0.4641, batch acc 0.5798
11:00:14.091   Training iter 250, batch loss 0.4633, batch acc 0.5830
11:00:14.620   Training iter 300, batch loss 0.4635, batch acc 0.5874
11:00:15.155   Training iter 350, batch loss 0.4635, batch acc 0.5908
11:00:15.704   Training iter 400, batch loss 0.4633, batch acc 0.5882
11:00:16.243   Training iter 450, batch loss 0.4633, batch acc 0.5876
11:00:16.781   Training iter 500, batch loss 0.4634, batch acc 0.5836
11:00:17.329   Training iter 550, batch loss 0.4632, batch acc 0.5858
11:00:17.864   Training iter 600, batch loss 0.4631, batch acc 0.5876
11:00:17.866 Training @ 374 epoch...
11:00:18.382   Training iter 50, batch loss 0.4635, batch acc 0.5922
11:00:18.876   Training iter 100, batch loss 0.4632, batch acc 0.5888
11:00:19.362   Training iter 150, batch loss 0.4634, batch acc 0.5926
11:00:19.860   Training iter 200, batch loss 0.4637, batch acc 0.5864
11:00:20.370   Training iter 250, batch loss 0.4642, batch acc 0.5830
11:00:20.900   Training iter 300, batch loss 0.4632, batch acc 0.5850
11:00:21.441   Training iter 350, batch loss 0.4634, batch acc 0.5864
11:00:21.971   Training iter 400, batch loss 0.4634, batch acc 0.5784
11:00:22.503   Training iter 450, batch loss 0.4635, batch acc 0.5866
11:00:23.047   Training iter 500, batch loss 0.4630, batch acc 0.5942
11:00:23.578   Training iter 550, batch loss 0.4634, batch acc 0.5892
11:00:24.106   Training iter 600, batch loss 0.4634, batch acc 0.5850
11:00:24.107 Training @ 375 epoch...
11:00:24.634   Training iter 50, batch loss 0.4635, batch acc 0.5892
11:00:25.163   Training iter 100, batch loss 0.4636, batch acc 0.5882
11:00:25.683   Training iter 150, batch loss 0.4630, batch acc 0.5946
11:00:26.204   Training iter 200, batch loss 0.4633, batch acc 0.5890
11:00:26.755   Training iter 250, batch loss 0.4633, batch acc 0.5872
11:00:27.311   Training iter 300, batch loss 0.4637, batch acc 0.5906
11:00:27.879   Training iter 350, batch loss 0.4631, batch acc 0.5898
11:00:28.433   Training iter 400, batch loss 0.4635, batch acc 0.5758
11:00:28.963   Training iter 450, batch loss 0.4634, batch acc 0.5834
11:00:29.499   Training iter 500, batch loss 0.4636, batch acc 0.5810
11:00:30.041   Training iter 550, batch loss 0.4634, batch acc 0.5952
11:00:30.588   Training iter 600, batch loss 0.4636, batch acc 0.5838
11:00:30.590 Testing @ 375 epoch...
11:00:30.626     Testing, total mean loss 0.46296, total acc 0.60020
11:00:30.626 Training @ 376 epoch...
11:00:31.183   Training iter 50, batch loss 0.4632, batch acc 0.5890
11:00:31.726   Training iter 100, batch loss 0.4633, batch acc 0.5926
11:00:32.282   Training iter 150, batch loss 0.4635, batch acc 0.5852
11:00:32.826   Training iter 200, batch loss 0.4632, batch acc 0.5894
11:00:33.350   Training iter 250, batch loss 0.4635, batch acc 0.5822
11:00:33.865   Training iter 300, batch loss 0.4638, batch acc 0.5830
11:00:34.400   Training iter 350, batch loss 0.4629, batch acc 0.5988
11:00:34.958   Training iter 400, batch loss 0.4632, batch acc 0.5926
11:00:35.506   Training iter 450, batch loss 0.4639, batch acc 0.5768
11:00:36.059   Training iter 500, batch loss 0.4633, batch acc 0.5876
11:00:36.606   Training iter 550, batch loss 0.4638, batch acc 0.5868
11:00:37.167   Training iter 600, batch loss 0.4634, batch acc 0.5826
11:00:37.169 Training @ 377 epoch...
11:00:37.708   Training iter 50, batch loss 0.4632, batch acc 0.5904
11:00:38.235   Training iter 100, batch loss 0.4633, batch acc 0.5922
11:00:38.758   Training iter 150, batch loss 0.4632, batch acc 0.5884
11:00:39.298   Training iter 200, batch loss 0.4633, batch acc 0.5898
11:00:39.814   Training iter 250, batch loss 0.4637, batch acc 0.5792
11:00:40.338   Training iter 300, batch loss 0.4633, batch acc 0.5816
11:00:40.835   Training iter 350, batch loss 0.4639, batch acc 0.5836
11:00:41.364   Training iter 400, batch loss 0.4632, batch acc 0.5958
11:00:41.915   Training iter 450, batch loss 0.4634, batch acc 0.5884
11:00:42.477   Training iter 500, batch loss 0.4633, batch acc 0.5914
11:00:43.029   Training iter 550, batch loss 0.4634, batch acc 0.5858
11:00:43.586   Training iter 600, batch loss 0.4635, batch acc 0.5812
11:00:43.587 Training @ 378 epoch...
11:00:44.136   Training iter 50, batch loss 0.4631, batch acc 0.5920
11:00:44.685   Training iter 100, batch loss 0.4632, batch acc 0.5784
11:00:45.236   Training iter 150, batch loss 0.4634, batch acc 0.5846
11:00:45.775   Training iter 200, batch loss 0.4637, batch acc 0.5764
11:00:46.314   Training iter 250, batch loss 0.4634, batch acc 0.5982
11:00:46.832   Training iter 300, batch loss 0.4636, batch acc 0.5730
11:00:47.397   Training iter 350, batch loss 0.4630, batch acc 0.5882
11:00:47.978   Training iter 400, batch loss 0.4635, batch acc 0.6002
11:00:48.492   Training iter 450, batch loss 0.4637, batch acc 0.5866
11:00:48.978   Training iter 500, batch loss 0.4636, batch acc 0.5872
11:00:49.498   Training iter 550, batch loss 0.4632, batch acc 0.5878
11:00:50.122   Training iter 600, batch loss 0.4631, batch acc 0.5980
11:00:50.124 Training @ 379 epoch...
11:00:50.751   Training iter 50, batch loss 0.4634, batch acc 0.5886
11:00:51.364   Training iter 100, batch loss 0.4634, batch acc 0.5810
11:00:51.828   Training iter 150, batch loss 0.4635, batch acc 0.5888
11:00:52.294   Training iter 200, batch loss 0.4631, batch acc 0.5884
11:00:52.759   Training iter 250, batch loss 0.4635, batch acc 0.5976
11:00:53.231   Training iter 300, batch loss 0.4632, batch acc 0.5932
11:00:53.694   Training iter 350, batch loss 0.4632, batch acc 0.5958
11:00:54.130   Training iter 400, batch loss 0.4635, batch acc 0.5924
11:00:54.583   Training iter 450, batch loss 0.4632, batch acc 0.5792
11:00:55.039   Training iter 500, batch loss 0.4630, batch acc 0.5934
11:00:55.518   Training iter 550, batch loss 0.4637, batch acc 0.5782
11:00:56.021   Training iter 600, batch loss 0.4637, batch acc 0.5744
11:00:56.023 Training @ 380 epoch...
11:00:56.511   Training iter 50, batch loss 0.4635, batch acc 0.5950
11:00:57.003   Training iter 100, batch loss 0.4633, batch acc 0.5886
11:00:57.501   Training iter 150, batch loss 0.4631, batch acc 0.5930
11:00:58.002   Training iter 200, batch loss 0.4631, batch acc 0.5906
11:00:58.503   Training iter 250, batch loss 0.4633, batch acc 0.5812
11:00:59.031   Training iter 300, batch loss 0.4632, batch acc 0.5968
11:00:59.569   Training iter 350, batch loss 0.4637, batch acc 0.5706
11:01:00.137   Training iter 400, batch loss 0.4632, batch acc 0.5896
11:01:00.681   Training iter 450, batch loss 0.4631, batch acc 0.5944
11:01:01.242   Training iter 500, batch loss 0.4637, batch acc 0.5798
11:01:01.813   Training iter 550, batch loss 0.4636, batch acc 0.5854
11:01:02.393   Training iter 600, batch loss 0.4635, batch acc 0.5828
11:01:02.395 Testing @ 380 epoch...
11:01:02.431     Testing, total mean loss 0.46290, total acc 0.60070
11:01:02.431 Training @ 381 epoch...
11:01:03.006   Training iter 50, batch loss 0.4633, batch acc 0.5950
11:01:03.547   Training iter 100, batch loss 0.4638, batch acc 0.5782
11:01:04.095   Training iter 150, batch loss 0.4631, batch acc 0.5924
11:01:04.617   Training iter 200, batch loss 0.4634, batch acc 0.5860
11:01:05.102   Training iter 250, batch loss 0.4632, batch acc 0.5934
11:01:05.604   Training iter 300, batch loss 0.4629, batch acc 0.5894
11:01:06.093   Training iter 350, batch loss 0.4632, batch acc 0.5892
11:01:06.583   Training iter 400, batch loss 0.4634, batch acc 0.5804
11:01:07.116   Training iter 450, batch loss 0.4637, batch acc 0.5782
11:01:07.637   Training iter 500, batch loss 0.4635, batch acc 0.5888
11:01:08.137   Training iter 550, batch loss 0.4635, batch acc 0.5796
11:01:08.632   Training iter 600, batch loss 0.4631, batch acc 0.6002
11:01:08.634 Training @ 382 epoch...
11:01:09.109   Training iter 50, batch loss 0.4630, batch acc 0.5928
11:01:09.589   Training iter 100, batch loss 0.4634, batch acc 0.5868
11:01:10.093   Training iter 150, batch loss 0.4633, batch acc 0.5964
11:01:10.624   Training iter 200, batch loss 0.4638, batch acc 0.5848
11:01:11.165   Training iter 250, batch loss 0.4633, batch acc 0.5810
11:01:11.710   Training iter 300, batch loss 0.4635, batch acc 0.5758
11:01:12.244   Training iter 350, batch loss 0.4632, batch acc 0.5962
11:01:12.763   Training iter 400, batch loss 0.4633, batch acc 0.5966
11:01:13.303   Training iter 450, batch loss 0.4633, batch acc 0.5764
11:01:13.833   Training iter 500, batch loss 0.4631, batch acc 0.5992
11:01:14.369   Training iter 550, batch loss 0.4634, batch acc 0.5806
11:01:14.931   Training iter 600, batch loss 0.4633, batch acc 0.5840
11:01:14.933 Training @ 383 epoch...
11:01:15.494   Training iter 50, batch loss 0.4628, batch acc 0.5846
11:01:16.049   Training iter 100, batch loss 0.4636, batch acc 0.5880
11:01:16.605   Training iter 150, batch loss 0.4637, batch acc 0.5812
11:01:17.158   Training iter 200, batch loss 0.4629, batch acc 0.5990
11:01:17.716   Training iter 250, batch loss 0.4637, batch acc 0.5770
11:01:18.270   Training iter 300, batch loss 0.4634, batch acc 0.5954
11:01:18.817   Training iter 350, batch loss 0.4637, batch acc 0.5816
11:01:19.372   Training iter 400, batch loss 0.4636, batch acc 0.5884
11:01:19.929   Training iter 450, batch loss 0.4637, batch acc 0.5832
11:01:20.491   Training iter 500, batch loss 0.4628, batch acc 0.6014
11:01:21.023   Training iter 550, batch loss 0.4629, batch acc 0.5834
11:01:21.552   Training iter 600, batch loss 0.4631, batch acc 0.5880
11:01:21.554 Training @ 384 epoch...
11:01:22.070   Training iter 50, batch loss 0.4636, batch acc 0.5750
11:01:22.600   Training iter 100, batch loss 0.4638, batch acc 0.5772
11:01:23.134   Training iter 150, batch loss 0.4631, batch acc 0.5846
11:01:23.650   Training iter 200, batch loss 0.4629, batch acc 0.5916
11:01:24.158   Training iter 250, batch loss 0.4633, batch acc 0.5922
11:01:24.675   Training iter 300, batch loss 0.4632, batch acc 0.5890
11:01:25.196   Training iter 350, batch loss 0.4631, batch acc 0.6012
11:01:25.700   Training iter 400, batch loss 0.4638, batch acc 0.5890
11:01:26.219   Training iter 450, batch loss 0.4632, batch acc 0.5886
11:01:26.714   Training iter 500, batch loss 0.4630, batch acc 0.5904
11:01:27.231   Training iter 550, batch loss 0.4631, batch acc 0.5930
11:01:27.753   Training iter 600, batch loss 0.4636, batch acc 0.5788
11:01:27.754 Training @ 385 epoch...
11:01:28.277   Training iter 50, batch loss 0.4628, batch acc 0.5976
11:01:28.784   Training iter 100, batch loss 0.4635, batch acc 0.5868
11:01:29.303   Training iter 150, batch loss 0.4635, batch acc 0.5816
11:01:29.811   Training iter 200, batch loss 0.4629, batch acc 0.5944
11:01:30.337   Training iter 250, batch loss 0.4630, batch acc 0.5914
11:01:30.849   Training iter 300, batch loss 0.4635, batch acc 0.5838
11:01:31.335   Training iter 350, batch loss 0.4632, batch acc 0.5950
11:01:31.793   Training iter 400, batch loss 0.4630, batch acc 0.5904
11:01:32.298   Training iter 450, batch loss 0.4634, batch acc 0.5898
11:01:32.797   Training iter 500, batch loss 0.4640, batch acc 0.5796
11:01:33.310   Training iter 550, batch loss 0.4637, batch acc 0.5754
11:01:33.803   Training iter 600, batch loss 0.4632, batch acc 0.5854
11:01:33.805 Testing @ 385 epoch...
11:01:33.840     Testing, total mean loss 0.46284, total acc 0.60070
11:01:33.841 Training @ 386 epoch...
11:01:34.329   Training iter 50, batch loss 0.4633, batch acc 0.5918
11:01:34.829   Training iter 100, batch loss 0.4632, batch acc 0.5918
11:01:35.307   Training iter 150, batch loss 0.4630, batch acc 0.5986
11:01:35.776   Training iter 200, batch loss 0.4634, batch acc 0.5796
11:01:36.250   Training iter 250, batch loss 0.4637, batch acc 0.5810
11:01:36.724   Training iter 300, batch loss 0.4632, batch acc 0.5874
11:01:37.230   Training iter 350, batch loss 0.4636, batch acc 0.5836
11:01:37.741   Training iter 400, batch loss 0.4632, batch acc 0.5856
11:01:38.254   Training iter 450, batch loss 0.4631, batch acc 0.5912
11:01:38.756   Training iter 500, batch loss 0.4633, batch acc 0.5920
11:01:39.249   Training iter 550, batch loss 0.4632, batch acc 0.5840
11:01:39.747   Training iter 600, batch loss 0.4632, batch acc 0.5860
11:01:39.749 Training @ 387 epoch...
11:01:40.270   Training iter 50, batch loss 0.4629, batch acc 0.6014
11:01:40.761   Training iter 100, batch loss 0.4630, batch acc 0.5956
11:01:41.262   Training iter 150, batch loss 0.4635, batch acc 0.5828
11:01:41.742   Training iter 200, batch loss 0.4637, batch acc 0.5710
11:01:42.239   Training iter 250, batch loss 0.4635, batch acc 0.5728
11:01:42.739   Training iter 300, batch loss 0.4629, batch acc 0.5924
11:01:43.233   Training iter 350, batch loss 0.4633, batch acc 0.5870
11:01:43.699   Training iter 400, batch loss 0.4635, batch acc 0.5952
11:01:44.177   Training iter 450, batch loss 0.4636, batch acc 0.5908
11:01:44.704   Training iter 500, batch loss 0.4632, batch acc 0.5870
11:01:45.231   Training iter 550, batch loss 0.4633, batch acc 0.5828
11:01:45.738   Training iter 600, batch loss 0.4630, batch acc 0.5934
11:01:45.740 Training @ 388 epoch...
11:01:46.262   Training iter 50, batch loss 0.4634, batch acc 0.5924
11:01:46.763   Training iter 100, batch loss 0.4634, batch acc 0.5908
11:01:47.280   Training iter 150, batch loss 0.4637, batch acc 0.5882
11:01:47.795   Training iter 200, batch loss 0.4634, batch acc 0.5818
11:01:48.315   Training iter 250, batch loss 0.4629, batch acc 0.6000
11:01:48.831   Training iter 300, batch loss 0.4628, batch acc 0.5992
11:01:49.331   Training iter 350, batch loss 0.4637, batch acc 0.5854
11:01:49.841   Training iter 400, batch loss 0.4630, batch acc 0.5808
11:01:50.357   Training iter 450, batch loss 0.4631, batch acc 0.5884
11:01:50.931   Training iter 500, batch loss 0.4633, batch acc 0.5816
11:01:51.499   Training iter 550, batch loss 0.4630, batch acc 0.5844
11:01:52.042   Training iter 600, batch loss 0.4636, batch acc 0.5794
11:01:52.043 Training @ 389 epoch...
11:01:52.498   Training iter 50, batch loss 0.4631, batch acc 0.5828
11:01:52.956   Training iter 100, batch loss 0.4640, batch acc 0.5730
11:01:53.426   Training iter 150, batch loss 0.4636, batch acc 0.5908
11:01:53.883   Training iter 200, batch loss 0.4630, batch acc 0.5960
11:01:54.373   Training iter 250, batch loss 0.4631, batch acc 0.5920
11:01:54.855   Training iter 300, batch loss 0.4631, batch acc 0.5886
11:01:55.361   Training iter 350, batch loss 0.4632, batch acc 0.5878
11:01:55.848   Training iter 400, batch loss 0.4635, batch acc 0.5846
11:01:56.334   Training iter 450, batch loss 0.4632, batch acc 0.5882
11:01:56.823   Training iter 500, batch loss 0.4631, batch acc 0.5928
11:01:57.324   Training iter 550, batch loss 0.4630, batch acc 0.5924
11:01:57.833   Training iter 600, batch loss 0.4632, batch acc 0.5858
11:01:57.835 Training @ 390 epoch...
11:01:58.352   Training iter 50, batch loss 0.4637, batch acc 0.5866
11:01:58.869   Training iter 100, batch loss 0.4631, batch acc 0.5840
11:01:59.388   Training iter 150, batch loss 0.4633, batch acc 0.5854
11:01:59.904   Training iter 200, batch loss 0.4631, batch acc 0.5874
11:02:00.443   Training iter 250, batch loss 0.4632, batch acc 0.5890
11:02:00.989   Training iter 300, batch loss 0.4631, batch acc 0.5882
11:02:01.560   Training iter 350, batch loss 0.4632, batch acc 0.5870
11:02:02.106   Training iter 400, batch loss 0.4635, batch acc 0.5788
11:02:02.680   Training iter 450, batch loss 0.4631, batch acc 0.5940
11:02:03.242   Training iter 500, batch loss 0.4627, batch acc 0.6054
11:02:03.834   Training iter 550, batch loss 0.4632, batch acc 0.5894
11:02:04.417   Training iter 600, batch loss 0.4638, batch acc 0.5770
11:02:04.418 Testing @ 390 epoch...
11:02:04.458     Testing, total mean loss 0.46278, total acc 0.60110
11:02:04.458 Training @ 391 epoch...
11:02:05.042   Training iter 50, batch loss 0.4628, batch acc 0.5910
11:02:05.622   Training iter 100, batch loss 0.4636, batch acc 0.5904
11:02:06.196   Training iter 150, batch loss 0.4633, batch acc 0.5912
11:02:06.774   Training iter 200, batch loss 0.4634, batch acc 0.5804
11:02:07.323   Training iter 250, batch loss 0.4633, batch acc 0.5928
11:02:07.878   Training iter 300, batch loss 0.4635, batch acc 0.5828
11:02:08.407   Training iter 350, batch loss 0.4633, batch acc 0.5838
11:02:08.916   Training iter 400, batch loss 0.4628, batch acc 0.5906
11:02:09.431   Training iter 450, batch loss 0.4636, batch acc 0.5868
11:02:09.923   Training iter 500, batch loss 0.4635, batch acc 0.5772
11:02:10.451   Training iter 550, batch loss 0.4627, batch acc 0.5962
11:02:10.954   Training iter 600, batch loss 0.4629, batch acc 0.5904
11:02:10.956 Training @ 392 epoch...
11:02:11.470   Training iter 50, batch loss 0.4632, batch acc 0.5852
11:02:11.950   Training iter 100, batch loss 0.4637, batch acc 0.5782
11:02:12.464   Training iter 150, batch loss 0.4635, batch acc 0.5806
11:02:12.965   Training iter 200, batch loss 0.4632, batch acc 0.5956
11:02:13.444   Training iter 250, batch loss 0.4630, batch acc 0.5912
11:02:13.920   Training iter 300, batch loss 0.4631, batch acc 0.5930
11:02:14.403   Training iter 350, batch loss 0.4635, batch acc 0.5798
11:02:14.893   Training iter 400, batch loss 0.4632, batch acc 0.5936
11:02:15.412   Training iter 450, batch loss 0.4630, batch acc 0.5800
11:02:15.929   Training iter 500, batch loss 0.4630, batch acc 0.5938
11:02:16.468   Training iter 550, batch loss 0.4632, batch acc 0.5876
11:02:17.011   Training iter 600, batch loss 0.4631, batch acc 0.5946
11:02:17.013 Training @ 393 epoch...
11:02:17.566   Training iter 50, batch loss 0.4631, batch acc 0.5890
11:02:18.100   Training iter 100, batch loss 0.4636, batch acc 0.5854
11:02:18.673   Training iter 150, batch loss 0.4633, batch acc 0.5844
11:02:19.251   Training iter 200, batch loss 0.4627, batch acc 0.6014
11:02:19.822   Training iter 250, batch loss 0.4630, batch acc 0.6026
11:02:20.408   Training iter 300, batch loss 0.4628, batch acc 0.5934
11:02:20.985   Training iter 350, batch loss 0.4629, batch acc 0.5908
11:02:21.546   Training iter 400, batch loss 0.4638, batch acc 0.5784
11:02:22.111   Training iter 450, batch loss 0.4631, batch acc 0.5882
11:02:22.701   Training iter 500, batch loss 0.4634, batch acc 0.5814
11:02:23.296   Training iter 550, batch loss 0.4638, batch acc 0.5816
11:02:23.840   Training iter 600, batch loss 0.4631, batch acc 0.5782
11:02:23.842 Training @ 394 epoch...
11:02:24.382   Training iter 50, batch loss 0.4630, batch acc 0.5838
11:02:24.908   Training iter 100, batch loss 0.4635, batch acc 0.5818
11:02:25.424   Training iter 150, batch loss 0.4629, batch acc 0.6026
11:02:25.892   Training iter 200, batch loss 0.4635, batch acc 0.5798
11:02:26.368   Training iter 250, batch loss 0.4633, batch acc 0.5918
11:02:26.841   Training iter 300, batch loss 0.4633, batch acc 0.5742
11:02:27.332   Training iter 350, batch loss 0.4629, batch acc 0.5970
11:02:27.825   Training iter 400, batch loss 0.4633, batch acc 0.5800
11:02:28.307   Training iter 450, batch loss 0.4630, batch acc 0.5870
11:02:28.767   Training iter 500, batch loss 0.4634, batch acc 0.5876
11:02:29.239   Training iter 550, batch loss 0.4629, batch acc 0.5944
11:02:29.771   Training iter 600, batch loss 0.4634, batch acc 0.5934
11:02:29.773 Training @ 395 epoch...
11:02:30.321   Training iter 50, batch loss 0.4635, batch acc 0.5844
11:02:30.851   Training iter 100, batch loss 0.4635, batch acc 0.5860
11:02:31.363   Training iter 150, batch loss 0.4629, batch acc 0.6002
11:02:31.876   Training iter 200, batch loss 0.4631, batch acc 0.5962
11:02:32.413   Training iter 250, batch loss 0.4631, batch acc 0.5978
11:02:32.971   Training iter 300, batch loss 0.4628, batch acc 0.5896
11:02:33.514   Training iter 350, batch loss 0.4630, batch acc 0.5856
11:02:34.046   Training iter 400, batch loss 0.4637, batch acc 0.5808
11:02:34.602   Training iter 450, batch loss 0.4629, batch acc 0.5892
11:02:35.175   Training iter 500, batch loss 0.4632, batch acc 0.5816
11:02:35.753   Training iter 550, batch loss 0.4635, batch acc 0.5798
11:02:36.331   Training iter 600, batch loss 0.4630, batch acc 0.5836
11:02:36.333 Testing @ 395 epoch...
11:02:36.369     Testing, total mean loss 0.46272, total acc 0.60170
11:02:36.369 Training @ 396 epoch...
11:02:36.926   Training iter 50, batch loss 0.4633, batch acc 0.5932
11:02:37.491   Training iter 100, batch loss 0.4636, batch acc 0.5718
11:02:38.084   Training iter 150, batch loss 0.4632, batch acc 0.5844
11:02:38.696   Training iter 200, batch loss 0.4628, batch acc 0.5982
11:02:39.292   Training iter 250, batch loss 0.4636, batch acc 0.5856
11:02:39.871   Training iter 300, batch loss 0.4630, batch acc 0.5868
11:02:40.438   Training iter 350, batch loss 0.4628, batch acc 0.5900
11:02:40.963   Training iter 400, batch loss 0.4632, batch acc 0.5846
11:02:41.496   Training iter 450, batch loss 0.4627, batch acc 0.6060
11:02:42.135   Training iter 500, batch loss 0.4635, batch acc 0.5758
11:02:42.785   Training iter 550, batch loss 0.4630, batch acc 0.5900
11:02:43.323   Training iter 600, batch loss 0.4635, batch acc 0.5896
11:02:43.324 Training @ 397 epoch...
11:02:43.866   Training iter 50, batch loss 0.4632, batch acc 0.5892
11:02:44.388   Training iter 100, batch loss 0.4631, batch acc 0.5896
11:02:44.854   Training iter 150, batch loss 0.4629, batch acc 0.5918
11:02:45.327   Training iter 200, batch loss 0.4633, batch acc 0.5848
11:02:45.802   Training iter 250, batch loss 0.4631, batch acc 0.5844
11:02:46.267   Training iter 300, batch loss 0.4633, batch acc 0.5926
11:02:46.728   Training iter 350, batch loss 0.4631, batch acc 0.5902
11:02:47.226   Training iter 400, batch loss 0.4635, batch acc 0.5802
11:02:47.721   Training iter 450, batch loss 0.4636, batch acc 0.5814
11:02:48.217   Training iter 500, batch loss 0.4630, batch acc 0.5980
11:02:48.705   Training iter 550, batch loss 0.4632, batch acc 0.5836
11:02:49.207   Training iter 600, batch loss 0.4629, batch acc 0.5902
11:02:49.209 Training @ 398 epoch...
11:02:49.742   Training iter 50, batch loss 0.4632, batch acc 0.5904
11:02:50.265   Training iter 100, batch loss 0.4631, batch acc 0.5908
11:02:50.765   Training iter 150, batch loss 0.4630, batch acc 0.5864
11:02:51.252   Training iter 200, batch loss 0.4632, batch acc 0.6022
11:02:51.765   Training iter 250, batch loss 0.4632, batch acc 0.5842
11:02:52.294   Training iter 300, batch loss 0.4631, batch acc 0.5876
11:02:52.830   Training iter 350, batch loss 0.4627, batch acc 0.5904
11:02:53.366   Training iter 400, batch loss 0.4633, batch acc 0.5878
11:02:53.901   Training iter 450, batch loss 0.4629, batch acc 0.5822
11:02:54.428   Training iter 500, batch loss 0.4633, batch acc 0.5822
11:02:54.949   Training iter 550, batch loss 0.4628, batch acc 0.5976
11:02:55.491   Training iter 600, batch loss 0.4641, batch acc 0.5744
11:02:55.493 Training @ 399 epoch...
11:02:56.029   Training iter 50, batch loss 0.4629, batch acc 0.5914
11:02:56.541   Training iter 100, batch loss 0.4632, batch acc 0.5830
11:02:57.048   Training iter 150, batch loss 0.4630, batch acc 0.5884
11:02:57.567   Training iter 200, batch loss 0.4634, batch acc 0.5832
11:02:58.077   Training iter 250, batch loss 0.4631, batch acc 0.5856
11:02:58.576   Training iter 300, batch loss 0.4634, batch acc 0.5814
11:02:59.081   Training iter 350, batch loss 0.4630, batch acc 0.5900
11:02:59.583   Training iter 400, batch loss 0.4635, batch acc 0.5832
11:03:00.075   Training iter 450, batch loss 0.4629, batch acc 0.5970
11:03:00.524   Training iter 500, batch loss 0.4630, batch acc 0.5966
11:03:00.995   Training iter 550, batch loss 0.4631, batch acc 0.5904
11:03:01.514   Training iter 600, batch loss 0.4633, batch acc 0.5862
11:03:01.515 Training @ 400 epoch...
11:03:02.022   Training iter 50, batch loss 0.4631, batch acc 0.5898
11:03:02.527   Training iter 100, batch loss 0.4634, batch acc 0.5760
11:03:02.994   Training iter 150, batch loss 0.4632, batch acc 0.5878
11:03:03.472   Training iter 200, batch loss 0.4632, batch acc 0.5834
11:03:03.971   Training iter 250, batch loss 0.4633, batch acc 0.5830
11:03:04.479   Training iter 300, batch loss 0.4635, batch acc 0.5870
11:03:04.992   Training iter 350, batch loss 0.4628, batch acc 0.5966
11:03:05.487   Training iter 400, batch loss 0.4632, batch acc 0.5958
11:03:05.977   Training iter 450, batch loss 0.4631, batch acc 0.5870
11:03:06.505   Training iter 500, batch loss 0.4631, batch acc 0.5850
11:03:07.054   Training iter 550, batch loss 0.4627, batch acc 0.5976
11:03:07.595   Training iter 600, batch loss 0.4630, batch acc 0.5912
11:03:07.597 Testing @ 400 epoch...
11:03:07.633     Testing, total mean loss 0.46267, total acc 0.60210
11:03:07.633 Plot @ 400 epoch...
11:03:07.633 Training @ 401 epoch...
11:03:08.170   Training iter 50, batch loss 0.4631, batch acc 0.5872
11:03:08.721   Training iter 100, batch loss 0.4632, batch acc 0.5892
11:03:09.265   Training iter 150, batch loss 0.4630, batch acc 0.5894
11:03:09.800   Training iter 200, batch loss 0.4626, batch acc 0.6006
11:03:10.352   Training iter 250, batch loss 0.4632, batch acc 0.5844
11:03:10.886   Training iter 300, batch loss 0.4633, batch acc 0.5894
11:03:11.429   Training iter 350, batch loss 0.4628, batch acc 0.5874
11:03:11.946   Training iter 400, batch loss 0.4632, batch acc 0.5780
11:03:12.473   Training iter 450, batch loss 0.4633, batch acc 0.5860
11:03:13.007   Training iter 500, batch loss 0.4630, batch acc 0.5894
11:03:13.542   Training iter 550, batch loss 0.4632, batch acc 0.5934
11:03:14.070   Training iter 600, batch loss 0.4637, batch acc 0.5828
11:03:14.072 Training @ 402 epoch...
11:03:14.593   Training iter 50, batch loss 0.4628, batch acc 0.6048
11:03:15.121   Training iter 100, batch loss 0.4632, batch acc 0.5842
11:03:15.650   Training iter 150, batch loss 0.4629, batch acc 0.5816
11:03:16.177   Training iter 200, batch loss 0.4633, batch acc 0.5846
11:03:16.659   Training iter 250, batch loss 0.4629, batch acc 0.5940
11:03:17.127   Training iter 300, batch loss 0.4630, batch acc 0.5930
11:03:17.605   Training iter 350, batch loss 0.4633, batch acc 0.5860
11:03:18.102   Training iter 400, batch loss 0.4633, batch acc 0.5818
11:03:18.629   Training iter 450, batch loss 0.4628, batch acc 0.5942
11:03:19.139   Training iter 500, batch loss 0.4631, batch acc 0.5830
11:03:19.633   Training iter 550, batch loss 0.4634, batch acc 0.5814
11:03:20.146   Training iter 600, batch loss 0.4633, batch acc 0.5902
11:03:20.148 Training @ 403 epoch...
11:03:20.670   Training iter 50, batch loss 0.4630, batch acc 0.5868
11:03:21.211   Training iter 100, batch loss 0.4629, batch acc 0.5944
11:03:21.755   Training iter 150, batch loss 0.4628, batch acc 0.5820
11:03:22.311   Training iter 200, batch loss 0.4635, batch acc 0.5980
11:03:22.858   Training iter 250, batch loss 0.4634, batch acc 0.5768
11:03:23.403   Training iter 300, batch loss 0.4635, batch acc 0.5780
11:03:23.950   Training iter 350, batch loss 0.4630, batch acc 0.5960
11:03:24.497   Training iter 400, batch loss 0.4630, batch acc 0.5908
11:03:25.043   Training iter 450, batch loss 0.4638, batch acc 0.5824
11:03:25.586   Training iter 500, batch loss 0.4626, batch acc 0.5998
11:03:26.130   Training iter 550, batch loss 0.4629, batch acc 0.5892
11:03:26.663   Training iter 600, batch loss 0.4631, batch acc 0.5842
11:03:26.665 Training @ 404 epoch...
11:03:27.200   Training iter 50, batch loss 0.4626, batch acc 0.5982
11:03:27.747   Training iter 100, batch loss 0.4632, batch acc 0.5882
11:03:28.289   Training iter 150, batch loss 0.4627, batch acc 0.5898
11:03:28.842   Training iter 200, batch loss 0.4629, batch acc 0.5952
11:03:29.381   Training iter 250, batch loss 0.4634, batch acc 0.5788
11:03:29.920   Training iter 300, batch loss 0.4632, batch acc 0.5824
11:03:30.460   Training iter 350, batch loss 0.4635, batch acc 0.5836
11:03:31.002   Training iter 400, batch loss 0.4634, batch acc 0.5774
11:03:31.541   Training iter 450, batch loss 0.4631, batch acc 0.5882
11:03:32.031   Training iter 500, batch loss 0.4630, batch acc 0.5958
11:03:32.525   Training iter 550, batch loss 0.4628, batch acc 0.5916
11:03:33.020   Training iter 600, batch loss 0.4632, batch acc 0.5884
11:03:33.022 Training @ 405 epoch...
11:03:33.513   Training iter 50, batch loss 0.4630, batch acc 0.5950
11:03:34.041   Training iter 100, batch loss 0.4629, batch acc 0.5882
11:03:34.571   Training iter 150, batch loss 0.4632, batch acc 0.5826
11:03:35.098   Training iter 200, batch loss 0.4632, batch acc 0.5940
11:03:35.618   Training iter 250, batch loss 0.4632, batch acc 0.5862
11:03:36.132   Training iter 300, batch loss 0.4632, batch acc 0.5802
11:03:36.644   Training iter 350, batch loss 0.4634, batch acc 0.5858
11:03:37.149   Training iter 400, batch loss 0.4632, batch acc 0.5856
11:03:37.671   Training iter 450, batch loss 0.4629, batch acc 0.5930
11:03:38.225   Training iter 500, batch loss 0.4627, batch acc 0.5864
11:03:38.779   Training iter 550, batch loss 0.4631, batch acc 0.5908
11:03:39.333   Training iter 600, batch loss 0.4628, batch acc 0.5916
11:03:39.335 Testing @ 405 epoch...
11:03:39.371     Testing, total mean loss 0.46261, total acc 0.60240
11:03:39.371 Training @ 406 epoch...
11:03:39.931   Training iter 50, batch loss 0.4640, batch acc 0.5804
11:03:40.486   Training iter 100, batch loss 0.4629, batch acc 0.5936
11:03:41.039   Training iter 150, batch loss 0.4628, batch acc 0.5932
11:03:41.585   Training iter 200, batch loss 0.4628, batch acc 0.5904
11:03:42.136   Training iter 250, batch loss 0.4630, batch acc 0.5924
11:03:42.688   Training iter 300, batch loss 0.4627, batch acc 0.5966
11:03:43.247   Training iter 350, batch loss 0.4631, batch acc 0.5840
11:03:43.799   Training iter 400, batch loss 0.4629, batch acc 0.5760
11:03:44.349   Training iter 450, batch loss 0.4633, batch acc 0.5806
11:03:44.866   Training iter 500, batch loss 0.4628, batch acc 0.5936
11:03:45.397   Training iter 550, batch loss 0.4632, batch acc 0.5950
11:03:45.923   Training iter 600, batch loss 0.4634, batch acc 0.5848
11:03:45.925 Training @ 407 epoch...
11:03:46.454   Training iter 50, batch loss 0.4629, batch acc 0.5906
11:03:46.984   Training iter 100, batch loss 0.4637, batch acc 0.5814
11:03:47.509   Training iter 150, batch loss 0.4633, batch acc 0.5804
11:03:48.040   Training iter 200, batch loss 0.4629, batch acc 0.5928
11:03:48.572   Training iter 250, batch loss 0.4630, batch acc 0.5932
11:03:49.108   Training iter 300, batch loss 0.4632, batch acc 0.5904
11:03:49.632   Training iter 350, batch loss 0.4630, batch acc 0.5954
11:03:50.155   Training iter 400, batch loss 0.4628, batch acc 0.5912
11:03:50.681   Training iter 450, batch loss 0.4633, batch acc 0.5846
11:03:51.218   Training iter 500, batch loss 0.4632, batch acc 0.5846
11:03:51.741   Training iter 550, batch loss 0.4627, batch acc 0.5862
11:03:52.284   Training iter 600, batch loss 0.4628, batch acc 0.5922
11:03:52.286 Training @ 408 epoch...
11:03:52.828   Training iter 50, batch loss 0.4631, batch acc 0.5852
11:03:53.371   Training iter 100, batch loss 0.4627, batch acc 0.5956
11:03:53.914   Training iter 150, batch loss 0.4631, batch acc 0.5832
11:03:54.457   Training iter 200, batch loss 0.4633, batch acc 0.5936
11:03:55.019   Training iter 250, batch loss 0.4632, batch acc 0.5840
11:03:55.572   Training iter 300, batch loss 0.4634, batch acc 0.5780
11:03:56.133   Training iter 350, batch loss 0.4629, batch acc 0.5960
11:03:56.686   Training iter 400, batch loss 0.4634, batch acc 0.6000
11:03:57.248   Training iter 450, batch loss 0.4627, batch acc 0.5908
11:03:57.812   Training iter 500, batch loss 0.4629, batch acc 0.5888
11:03:58.371   Training iter 550, batch loss 0.4632, batch acc 0.5786
11:03:58.913   Training iter 600, batch loss 0.4629, batch acc 0.5872
11:03:58.914 Training @ 409 epoch...
11:03:59.460   Training iter 50, batch loss 0.4633, batch acc 0.5772
11:03:59.995   Training iter 100, batch loss 0.4629, batch acc 0.5948
11:04:00.521   Training iter 150, batch loss 0.4628, batch acc 0.5914
11:04:01.025   Training iter 200, batch loss 0.4628, batch acc 0.5952
11:04:01.528   Training iter 250, batch loss 0.4630, batch acc 0.5858
11:04:02.054   Training iter 300, batch loss 0.4625, batch acc 0.5972
11:04:02.591   Training iter 350, batch loss 0.4633, batch acc 0.5968
11:04:03.110   Training iter 400, batch loss 0.4632, batch acc 0.5804
11:04:03.610   Training iter 450, batch loss 0.4629, batch acc 0.5900
11:04:04.113   Training iter 500, batch loss 0.4636, batch acc 0.5822
11:04:04.637   Training iter 550, batch loss 0.4633, batch acc 0.5822
11:04:05.169   Training iter 600, batch loss 0.4630, batch acc 0.5894
11:04:05.171 Training @ 410 epoch...
11:04:05.718   Training iter 50, batch loss 0.4630, batch acc 0.5926
11:04:06.263   Training iter 100, batch loss 0.4633, batch acc 0.5952
11:04:06.801   Training iter 150, batch loss 0.4637, batch acc 0.5766
11:04:07.347   Training iter 200, batch loss 0.4631, batch acc 0.5954
11:04:07.884   Training iter 250, batch loss 0.4628, batch acc 0.5966
11:04:08.420   Training iter 300, batch loss 0.4630, batch acc 0.5856
11:04:08.948   Training iter 350, batch loss 0.4633, batch acc 0.5788
11:04:09.480   Training iter 400, batch loss 0.4634, batch acc 0.5788
11:04:10.012   Training iter 450, batch loss 0.4628, batch acc 0.5992
11:04:10.567   Training iter 500, batch loss 0.4626, batch acc 0.5904
11:04:11.127   Training iter 550, batch loss 0.4629, batch acc 0.5826
11:04:11.676   Training iter 600, batch loss 0.4625, batch acc 0.5922
11:04:11.677 Testing @ 410 epoch...
11:04:11.714     Testing, total mean loss 0.46256, total acc 0.60250
11:04:11.714 Training @ 411 epoch...
11:04:12.278   Training iter 50, batch loss 0.4627, batch acc 0.5978
11:04:12.820   Training iter 100, batch loss 0.4630, batch acc 0.5748
11:04:13.359   Training iter 150, batch loss 0.4628, batch acc 0.5948
11:04:13.881   Training iter 200, batch loss 0.4631, batch acc 0.5878
11:04:14.401   Training iter 250, batch loss 0.4632, batch acc 0.5860
11:04:14.919   Training iter 300, batch loss 0.4629, batch acc 0.5942
11:04:15.447   Training iter 350, batch loss 0.4635, batch acc 0.5798
11:04:16.009   Training iter 400, batch loss 0.4630, batch acc 0.5818
11:04:16.592   Training iter 450, batch loss 0.4633, batch acc 0.5828
11:04:17.170   Training iter 500, batch loss 0.4629, batch acc 0.5970
11:04:17.684   Training iter 550, batch loss 0.4632, batch acc 0.5930
11:04:18.140   Training iter 600, batch loss 0.4628, batch acc 0.5920
11:04:18.141 Training @ 412 epoch...
11:04:18.616   Training iter 50, batch loss 0.4631, batch acc 0.5878
11:04:19.089   Training iter 100, batch loss 0.4629, batch acc 0.5902
11:04:19.580   Training iter 150, batch loss 0.4625, batch acc 0.5948
11:04:20.066   Training iter 200, batch loss 0.4627, batch acc 0.6032
11:04:20.570   Training iter 250, batch loss 0.4629, batch acc 0.5862
11:04:21.053   Training iter 300, batch loss 0.4630, batch acc 0.5822
11:04:21.537   Training iter 350, batch loss 0.4632, batch acc 0.5842
11:04:22.012   Training iter 400, batch loss 0.4637, batch acc 0.5840
11:04:22.492   Training iter 450, batch loss 0.4629, batch acc 0.5934
11:04:22.975   Training iter 500, batch loss 0.4626, batch acc 0.5916
11:04:23.496   Training iter 550, batch loss 0.4635, batch acc 0.5798
11:04:24.035   Training iter 600, batch loss 0.4632, batch acc 0.5856
11:04:24.037 Training @ 413 epoch...
11:04:24.568   Training iter 50, batch loss 0.4631, batch acc 0.5960
11:04:25.087   Training iter 100, batch loss 0.4629, batch acc 0.5846
11:04:25.625   Training iter 150, batch loss 0.4629, batch acc 0.5948
11:04:26.149   Training iter 200, batch loss 0.4633, batch acc 0.5848
11:04:26.693   Training iter 250, batch loss 0.4635, batch acc 0.5774
11:04:27.266   Training iter 300, batch loss 0.4629, batch acc 0.5912
11:04:27.846   Training iter 350, batch loss 0.4632, batch acc 0.5862
11:04:28.411   Training iter 400, batch loss 0.4629, batch acc 0.6004
11:04:28.981   Training iter 450, batch loss 0.4630, batch acc 0.5856
11:04:29.541   Training iter 500, batch loss 0.4628, batch acc 0.5852
11:04:30.110   Training iter 550, batch loss 0.4631, batch acc 0.5908
11:04:30.696   Training iter 600, batch loss 0.4625, batch acc 0.5882
11:04:30.698 Training @ 414 epoch...
11:04:31.287   Training iter 50, batch loss 0.4629, batch acc 0.5868
11:04:31.843   Training iter 100, batch loss 0.4626, batch acc 0.5942
11:04:32.396   Training iter 150, batch loss 0.4629, batch acc 0.5902
11:04:32.916   Training iter 200, batch loss 0.4628, batch acc 0.5904
11:04:33.440   Training iter 250, batch loss 0.4632, batch acc 0.5928
11:04:33.975   Training iter 300, batch loss 0.4630, batch acc 0.5860
11:04:34.502   Training iter 350, batch loss 0.4632, batch acc 0.5876
11:04:35.030   Training iter 400, batch loss 0.4626, batch acc 0.5904
11:04:35.546   Training iter 450, batch loss 0.4633, batch acc 0.5840
11:04:36.059   Training iter 500, batch loss 0.4632, batch acc 0.5850
11:04:36.586   Training iter 550, batch loss 0.4632, batch acc 0.5854
11:04:37.139   Training iter 600, batch loss 0.4631, batch acc 0.5888
11:04:37.141 Training @ 415 epoch...
11:04:37.700   Training iter 50, batch loss 0.4631, batch acc 0.5862
11:04:38.239   Training iter 100, batch loss 0.4634, batch acc 0.5820
11:04:38.780   Training iter 150, batch loss 0.4628, batch acc 0.5972
11:04:39.327   Training iter 200, batch loss 0.4636, batch acc 0.5852
11:04:39.875   Training iter 250, batch loss 0.4629, batch acc 0.5836
11:04:40.411   Training iter 300, batch loss 0.4624, batch acc 0.5956
11:04:40.934   Training iter 350, batch loss 0.4631, batch acc 0.5820
11:04:41.467   Training iter 400, batch loss 0.4632, batch acc 0.5890
11:04:42.011   Training iter 450, batch loss 0.4629, batch acc 0.5902
11:04:42.547   Training iter 500, batch loss 0.4626, batch acc 0.5934
11:04:43.106   Training iter 550, batch loss 0.4630, batch acc 0.5842
11:04:43.639   Training iter 600, batch loss 0.4627, batch acc 0.5972
11:04:43.641 Testing @ 415 epoch...
11:04:43.677     Testing, total mean loss 0.46251, total acc 0.60290
11:04:43.677 Training @ 416 epoch...
11:04:44.219   Training iter 50, batch loss 0.4628, batch acc 0.5806
11:04:44.754   Training iter 100, batch loss 0.4629, batch acc 0.5884
11:04:45.295   Training iter 150, batch loss 0.4624, batch acc 0.6122
11:04:45.845   Training iter 200, batch loss 0.4630, batch acc 0.5868
11:04:46.380   Training iter 250, batch loss 0.4626, batch acc 0.5964
11:04:46.905   Training iter 300, batch loss 0.4631, batch acc 0.5880
11:04:47.432   Training iter 350, batch loss 0.4627, batch acc 0.5974
11:04:47.956   Training iter 400, batch loss 0.4632, batch acc 0.5796
11:04:48.514   Training iter 450, batch loss 0.4631, batch acc 0.5876
11:04:49.044   Training iter 500, batch loss 0.4633, batch acc 0.5820
11:04:49.580   Training iter 550, batch loss 0.4633, batch acc 0.5792
11:04:50.115   Training iter 600, batch loss 0.4633, batch acc 0.5840
11:04:50.117 Training @ 417 epoch...
11:04:50.648   Training iter 50, batch loss 0.4627, batch acc 0.5994
11:04:51.176   Training iter 100, batch loss 0.4632, batch acc 0.5832
11:04:51.703   Training iter 150, batch loss 0.4632, batch acc 0.5854
11:04:52.224   Training iter 200, batch loss 0.4628, batch acc 0.5790
11:04:52.734   Training iter 250, batch loss 0.4630, batch acc 0.5904
11:04:53.257   Training iter 300, batch loss 0.4630, batch acc 0.5978
11:04:53.772   Training iter 350, batch loss 0.4631, batch acc 0.5886
11:04:54.283   Training iter 400, batch loss 0.4627, batch acc 0.5988
11:04:54.789   Training iter 450, batch loss 0.4627, batch acc 0.5904
11:04:55.308   Training iter 500, batch loss 0.4632, batch acc 0.5842
11:04:55.823   Training iter 550, batch loss 0.4634, batch acc 0.5796
11:04:56.345   Training iter 600, batch loss 0.4624, batch acc 0.5902
11:04:56.347 Training @ 418 epoch...
11:04:56.887   Training iter 50, batch loss 0.4633, batch acc 0.5826
11:04:57.411   Training iter 100, batch loss 0.4630, batch acc 0.5850
11:04:57.949   Training iter 150, batch loss 0.4627, batch acc 0.5878
11:04:58.482   Training iter 200, batch loss 0.4626, batch acc 0.5934
11:04:59.035   Training iter 250, batch loss 0.4633, batch acc 0.5802
11:04:59.586   Training iter 300, batch loss 0.4625, batch acc 0.6074
11:05:00.136   Training iter 350, batch loss 0.4632, batch acc 0.5854
11:05:00.703   Training iter 400, batch loss 0.4631, batch acc 0.5936
11:05:01.282   Training iter 450, batch loss 0.4629, batch acc 0.5844
11:05:01.876   Training iter 500, batch loss 0.4631, batch acc 0.5872
11:05:02.456   Training iter 550, batch loss 0.4631, batch acc 0.5974
11:05:03.041   Training iter 600, batch loss 0.4627, batch acc 0.5818
11:05:03.043 Training @ 419 epoch...
11:05:03.618   Training iter 50, batch loss 0.4626, batch acc 0.5974
11:05:04.179   Training iter 100, batch loss 0.4631, batch acc 0.5806
11:05:04.737   Training iter 150, batch loss 0.4625, batch acc 0.6012
11:05:05.307   Training iter 200, batch loss 0.4636, batch acc 0.5730
11:05:05.910   Training iter 250, batch loss 0.4631, batch acc 0.5904
11:05:06.455   Training iter 300, batch loss 0.4634, batch acc 0.5746
11:05:06.978   Training iter 350, batch loss 0.4627, batch acc 0.5924
11:05:07.508   Training iter 400, batch loss 0.4632, batch acc 0.5900
11:05:08.015   Training iter 450, batch loss 0.4629, batch acc 0.5968
11:05:08.539   Training iter 500, batch loss 0.4624, batch acc 0.5918
11:05:09.104   Training iter 550, batch loss 0.4626, batch acc 0.5874
11:05:09.656   Training iter 600, batch loss 0.4634, batch acc 0.5898
11:05:09.658 Training @ 420 epoch...
11:05:10.233   Training iter 50, batch loss 0.4628, batch acc 0.5916
11:05:10.806   Training iter 100, batch loss 0.4630, batch acc 0.5888
11:05:11.388   Training iter 150, batch loss 0.4625, batch acc 0.5938
11:05:11.972   Training iter 200, batch loss 0.4634, batch acc 0.5764
11:05:12.536   Training iter 250, batch loss 0.4627, batch acc 0.5910
11:05:13.069   Training iter 300, batch loss 0.4640, batch acc 0.5766
11:05:13.657   Training iter 350, batch loss 0.4624, batch acc 0.6016
11:05:14.268   Training iter 400, batch loss 0.4627, batch acc 0.5942
11:05:14.893   Training iter 450, batch loss 0.4626, batch acc 0.5974
11:05:15.515   Training iter 500, batch loss 0.4635, batch acc 0.5774
11:05:16.038   Training iter 550, batch loss 0.4628, batch acc 0.5922
11:05:16.563   Training iter 600, batch loss 0.4628, batch acc 0.5854
11:05:16.564 Testing @ 420 epoch...
11:05:16.607     Testing, total mean loss 0.46246, total acc 0.60310
11:05:16.607 Training @ 421 epoch...
11:05:17.123   Training iter 50, batch loss 0.4628, batch acc 0.5926
11:05:17.674   Training iter 100, batch loss 0.4624, batch acc 0.5946
11:05:18.235   Training iter 150, batch loss 0.4631, batch acc 0.5808
11:05:18.793   Training iter 200, batch loss 0.4633, batch acc 0.5824
11:05:19.316   Training iter 250, batch loss 0.4629, batch acc 0.5828
11:05:19.855   Training iter 300, batch loss 0.4629, batch acc 0.5908
11:05:20.398   Training iter 350, batch loss 0.4627, batch acc 0.6020
11:05:20.941   Training iter 400, batch loss 0.4630, batch acc 0.5944
11:05:21.489   Training iter 450, batch loss 0.4628, batch acc 0.5892
11:05:22.056   Training iter 500, batch loss 0.4632, batch acc 0.5776
11:05:22.626   Training iter 550, batch loss 0.4633, batch acc 0.5926
11:05:23.209   Training iter 600, batch loss 0.4627, batch acc 0.5860
11:05:23.211 Training @ 422 epoch...
11:05:23.757   Training iter 50, batch loss 0.4634, batch acc 0.5788
11:05:24.288   Training iter 100, batch loss 0.4630, batch acc 0.5812
11:05:24.827   Training iter 150, batch loss 0.4629, batch acc 0.5978
11:05:25.379   Training iter 200, batch loss 0.4631, batch acc 0.5902
11:05:25.915   Training iter 250, batch loss 0.4626, batch acc 0.5894
11:05:26.462   Training iter 300, batch loss 0.4635, batch acc 0.5780
11:05:26.977   Training iter 350, batch loss 0.4626, batch acc 0.5960
11:05:27.528   Training iter 400, batch loss 0.4630, batch acc 0.5820
11:05:28.095   Training iter 450, batch loss 0.4629, batch acc 0.5950
11:05:28.696   Training iter 500, batch loss 0.4627, batch acc 0.5938
11:05:29.263   Training iter 550, batch loss 0.4630, batch acc 0.5902
11:05:29.824   Training iter 600, batch loss 0.4624, batch acc 0.5948
11:05:29.826 Training @ 423 epoch...
11:05:30.396   Training iter 50, batch loss 0.4628, batch acc 0.5848
11:05:30.983   Training iter 100, batch loss 0.4632, batch acc 0.5896
11:05:31.567   Training iter 150, batch loss 0.4625, batch acc 0.5942
11:05:32.138   Training iter 200, batch loss 0.4632, batch acc 0.5850
11:05:32.683   Training iter 250, batch loss 0.4629, batch acc 0.5892
11:05:33.228   Training iter 300, batch loss 0.4631, batch acc 0.5810
11:05:33.793   Training iter 350, batch loss 0.4626, batch acc 0.5944
11:05:34.353   Training iter 400, batch loss 0.4626, batch acc 0.5966
11:05:34.912   Training iter 450, batch loss 0.4631, batch acc 0.5928
11:05:35.477   Training iter 500, batch loss 0.4632, batch acc 0.5910
11:05:36.047   Training iter 550, batch loss 0.4628, batch acc 0.5846
11:05:36.621   Training iter 600, batch loss 0.4630, batch acc 0.5850
11:05:36.623 Training @ 424 epoch...
11:05:37.192   Training iter 50, batch loss 0.4629, batch acc 0.5946
11:05:37.690   Training iter 100, batch loss 0.4629, batch acc 0.5860
11:05:38.191   Training iter 150, batch loss 0.4624, batch acc 0.5890
11:05:38.709   Training iter 200, batch loss 0.4627, batch acc 0.5888
11:05:39.212   Training iter 250, batch loss 0.4631, batch acc 0.5880
11:05:39.706   Training iter 300, batch loss 0.4630, batch acc 0.5948
11:05:40.217   Training iter 350, batch loss 0.4631, batch acc 0.5870
11:05:40.714   Training iter 400, batch loss 0.4630, batch acc 0.5882
11:05:41.228   Training iter 450, batch loss 0.4628, batch acc 0.5894
11:05:41.720   Training iter 500, batch loss 0.4632, batch acc 0.5834
11:05:42.218   Training iter 550, batch loss 0.4628, batch acc 0.5924
11:05:42.728   Training iter 600, batch loss 0.4628, batch acc 0.5858
11:05:42.730 Training @ 425 epoch...
11:05:43.251   Training iter 50, batch loss 0.4623, batch acc 0.5976
11:05:43.764   Training iter 100, batch loss 0.4632, batch acc 0.5842
11:05:44.280   Training iter 150, batch loss 0.4627, batch acc 0.5978
11:05:44.801   Training iter 200, batch loss 0.4633, batch acc 0.5942
11:05:45.309   Training iter 250, batch loss 0.4631, batch acc 0.5864
11:05:45.798   Training iter 300, batch loss 0.4628, batch acc 0.5852
11:05:46.304   Training iter 350, batch loss 0.4629, batch acc 0.5854
11:05:46.788   Training iter 400, batch loss 0.4626, batch acc 0.5980
11:05:47.279   Training iter 450, batch loss 0.4627, batch acc 0.5834
11:05:47.780   Training iter 500, batch loss 0.4631, batch acc 0.5866
11:05:48.292   Training iter 550, batch loss 0.4630, batch acc 0.5848
11:05:48.786   Training iter 600, batch loss 0.4629, batch acc 0.5838
11:05:48.788 Testing @ 425 epoch...
11:05:48.824     Testing, total mean loss 0.46241, total acc 0.60350
11:05:48.824 Training @ 426 epoch...
11:05:49.322   Training iter 50, batch loss 0.4628, batch acc 0.5866
11:05:49.814   Training iter 100, batch loss 0.4631, batch acc 0.5794
11:05:50.318   Training iter 150, batch loss 0.4627, batch acc 0.5978
11:05:50.833   Training iter 200, batch loss 0.4628, batch acc 0.6024
11:05:51.365   Training iter 250, batch loss 0.4629, batch acc 0.5800
11:05:51.832   Training iter 300, batch loss 0.4629, batch acc 0.5854
11:05:52.301   Training iter 350, batch loss 0.4626, batch acc 0.5978
11:05:52.774   Training iter 400, batch loss 0.4629, batch acc 0.5826
11:05:53.287   Training iter 450, batch loss 0.4632, batch acc 0.5840
11:05:53.801   Training iter 500, batch loss 0.4631, batch acc 0.5864
11:05:54.271   Training iter 550, batch loss 0.4629, batch acc 0.5906
11:05:54.734   Training iter 600, batch loss 0.4627, batch acc 0.5940
11:05:54.736 Training @ 427 epoch...
11:05:55.247   Training iter 50, batch loss 0.4631, batch acc 0.5848
11:05:55.746   Training iter 100, batch loss 0.4630, batch acc 0.5870
11:05:56.196   Training iter 150, batch loss 0.4634, batch acc 0.5830
11:05:56.643   Training iter 200, batch loss 0.4627, batch acc 0.6022
11:05:57.093   Training iter 250, batch loss 0.4626, batch acc 0.5938
11:05:57.556   Training iter 300, batch loss 0.4627, batch acc 0.5880
11:05:58.011   Training iter 350, batch loss 0.4628, batch acc 0.5886
11:05:58.479   Training iter 400, batch loss 0.4622, batch acc 0.6102
11:05:58.958   Training iter 450, batch loss 0.4631, batch acc 0.5862
11:05:59.433   Training iter 500, batch loss 0.4630, batch acc 0.5756
11:05:59.899   Training iter 550, batch loss 0.4630, batch acc 0.5800
11:06:00.386   Training iter 600, batch loss 0.4629, batch acc 0.5886
11:06:00.387 Training @ 428 epoch...
11:06:00.887   Training iter 50, batch loss 0.4628, batch acc 0.5826
11:06:01.426   Training iter 100, batch loss 0.4629, batch acc 0.5840
11:06:01.987   Training iter 150, batch loss 0.4635, batch acc 0.5750
11:06:02.507   Training iter 200, batch loss 0.4635, batch acc 0.5796
11:06:03.037   Training iter 250, batch loss 0.4621, batch acc 0.6024
11:06:03.590   Training iter 300, batch loss 0.4632, batch acc 0.5802
11:06:04.141   Training iter 350, batch loss 0.4628, batch acc 0.5940
11:06:04.689   Training iter 400, batch loss 0.4625, batch acc 0.6022
11:06:05.243   Training iter 450, batch loss 0.4626, batch acc 0.5938
11:06:05.795   Training iter 500, batch loss 0.4625, batch acc 0.6036
11:06:06.360   Training iter 550, batch loss 0.4629, batch acc 0.5926
11:06:06.922   Training iter 600, batch loss 0.4632, batch acc 0.5764
11:06:06.924 Training @ 429 epoch...
11:06:07.472   Training iter 50, batch loss 0.4627, batch acc 0.5890
11:06:07.997   Training iter 100, batch loss 0.4628, batch acc 0.5888
11:06:08.542   Training iter 150, batch loss 0.4629, batch acc 0.5802
11:06:09.081   Training iter 200, batch loss 0.4631, batch acc 0.5892
11:06:09.617   Training iter 250, batch loss 0.4633, batch acc 0.5864
11:06:10.161   Training iter 300, batch loss 0.4625, batch acc 0.5892
11:06:10.706   Training iter 350, batch loss 0.4630, batch acc 0.5908
11:06:11.232   Training iter 400, batch loss 0.4627, batch acc 0.5862
11:06:11.763   Training iter 450, batch loss 0.4629, batch acc 0.5890
11:06:12.307   Training iter 500, batch loss 0.4627, batch acc 0.5956
11:06:12.893   Training iter 550, batch loss 0.4631, batch acc 0.5896
11:06:13.480   Training iter 600, batch loss 0.4626, batch acc 0.5936
11:06:13.482 Training @ 430 epoch...
11:06:14.071   Training iter 50, batch loss 0.4628, batch acc 0.5788
11:06:14.576   Training iter 100, batch loss 0.4627, batch acc 0.5978
11:06:15.086   Training iter 150, batch loss 0.4637, batch acc 0.5732
11:06:15.604   Training iter 200, batch loss 0.4624, batch acc 0.6110
11:06:16.135   Training iter 250, batch loss 0.4630, batch acc 0.5860
11:06:16.668   Training iter 300, batch loss 0.4628, batch acc 0.5860
11:06:17.189   Training iter 350, batch loss 0.4622, batch acc 0.6066
11:06:17.722   Training iter 400, batch loss 0.4630, batch acc 0.5826
11:06:18.251   Training iter 450, batch loss 0.4631, batch acc 0.5830
11:06:18.800   Training iter 500, batch loss 0.4626, batch acc 0.5936
11:06:19.356   Training iter 550, batch loss 0.4633, batch acc 0.5824
11:06:19.902   Training iter 600, batch loss 0.4628, batch acc 0.5862
11:06:19.904 Testing @ 430 epoch...
11:06:19.941     Testing, total mean loss 0.46237, total acc 0.60310
11:06:19.941 Training @ 431 epoch...
11:06:20.496   Training iter 50, batch loss 0.4633, batch acc 0.5832
11:06:21.063   Training iter 100, batch loss 0.4629, batch acc 0.5870
11:06:21.622   Training iter 150, batch loss 0.4627, batch acc 0.5874
11:06:22.176   Training iter 200, batch loss 0.4630, batch acc 0.5886
11:06:22.735   Training iter 250, batch loss 0.4631, batch acc 0.5836
11:06:23.297   Training iter 300, batch loss 0.4624, batch acc 0.5974
11:06:23.844   Training iter 350, batch loss 0.4626, batch acc 0.5914
11:06:24.391   Training iter 400, batch loss 0.4628, batch acc 0.5952
11:06:24.910   Training iter 450, batch loss 0.4628, batch acc 0.5900
11:06:25.440   Training iter 500, batch loss 0.4626, batch acc 0.5880
11:06:25.965   Training iter 550, batch loss 0.4626, batch acc 0.5850
11:06:26.496   Training iter 600, batch loss 0.4632, batch acc 0.5910
11:06:26.498 Training @ 432 epoch...
11:06:27.032   Training iter 50, batch loss 0.4629, batch acc 0.5824
11:06:27.551   Training iter 100, batch loss 0.4632, batch acc 0.5854
11:06:28.065   Training iter 150, batch loss 0.4631, batch acc 0.5834
11:06:28.604   Training iter 200, batch loss 0.4630, batch acc 0.5894
11:06:29.142   Training iter 250, batch loss 0.4627, batch acc 0.5894
11:06:29.659   Training iter 300, batch loss 0.4628, batch acc 0.5860
11:06:30.179   Training iter 350, batch loss 0.4628, batch acc 0.5868
11:06:30.704   Training iter 400, batch loss 0.4623, batch acc 0.5956
11:06:31.195   Training iter 450, batch loss 0.4622, batch acc 0.6050
11:06:31.695   Training iter 500, batch loss 0.4626, batch acc 0.5980
11:06:32.163   Training iter 550, batch loss 0.4629, batch acc 0.5842
11:06:32.644   Training iter 600, batch loss 0.4633, batch acc 0.5812
11:06:32.645 Training @ 433 epoch...
11:06:33.149   Training iter 50, batch loss 0.4630, batch acc 0.5926
11:06:33.645   Training iter 100, batch loss 0.4632, batch acc 0.5790
11:06:34.143   Training iter 150, batch loss 0.4630, batch acc 0.5770
11:06:34.651   Training iter 200, batch loss 0.4624, batch acc 0.5980
11:06:35.182   Training iter 250, batch loss 0.4628, batch acc 0.5884
11:06:35.705   Training iter 300, batch loss 0.4630, batch acc 0.5926
11:06:36.231   Training iter 350, batch loss 0.4627, batch acc 0.5914
11:06:36.762   Training iter 400, batch loss 0.4624, batch acc 0.5928
11:06:37.310   Training iter 450, batch loss 0.4629, batch acc 0.5860
11:06:37.877   Training iter 500, batch loss 0.4626, batch acc 0.5860
11:06:38.429   Training iter 550, batch loss 0.4628, batch acc 0.5928
11:06:38.984   Training iter 600, batch loss 0.4631, batch acc 0.5914
11:06:38.986 Training @ 434 epoch...
11:06:39.550   Training iter 50, batch loss 0.4630, batch acc 0.5988
11:06:40.117   Training iter 100, batch loss 0.4630, batch acc 0.5820
11:06:40.656   Training iter 150, batch loss 0.4627, batch acc 0.5920
11:06:41.178   Training iter 200, batch loss 0.4628, batch acc 0.5816
11:06:41.679   Training iter 250, batch loss 0.4627, batch acc 0.5940
11:06:42.194   Training iter 300, batch loss 0.4629, batch acc 0.5950
11:06:42.705   Training iter 350, batch loss 0.4629, batch acc 0.5928
11:06:43.222   Training iter 400, batch loss 0.4626, batch acc 0.5848
11:06:43.733   Training iter 450, batch loss 0.4626, batch acc 0.5836
11:06:44.236   Training iter 500, batch loss 0.4630, batch acc 0.5872
11:06:44.745   Training iter 550, batch loss 0.4627, batch acc 0.5924
11:06:45.281   Training iter 600, batch loss 0.4629, batch acc 0.5826
11:06:45.282 Training @ 435 epoch...
11:06:45.826   Training iter 50, batch loss 0.4623, batch acc 0.5974
11:06:46.349   Training iter 100, batch loss 0.4628, batch acc 0.5820
11:06:46.875   Training iter 150, batch loss 0.4628, batch acc 0.5872
11:06:47.401   Training iter 200, batch loss 0.4629, batch acc 0.5874
11:06:47.931   Training iter 250, batch loss 0.4629, batch acc 0.5770
11:06:48.462   Training iter 300, batch loss 0.4627, batch acc 0.5874
11:06:48.997   Training iter 350, batch loss 0.4631, batch acc 0.5904
11:06:49.525   Training iter 400, batch loss 0.4627, batch acc 0.5878
11:06:50.039   Training iter 450, batch loss 0.4628, batch acc 0.5890
11:06:50.588   Training iter 500, batch loss 0.4628, batch acc 0.5998
11:06:51.151   Training iter 550, batch loss 0.4631, batch acc 0.5828
11:06:51.698   Training iter 600, batch loss 0.4626, batch acc 0.5990
11:06:51.700 Testing @ 435 epoch...
11:06:51.736     Testing, total mean loss 0.46232, total acc 0.60300
11:06:51.736 Training @ 436 epoch...
11:06:52.285   Training iter 50, batch loss 0.4629, batch acc 0.5864
11:06:52.834   Training iter 100, batch loss 0.4630, batch acc 0.5836
11:06:53.401   Training iter 150, batch loss 0.4633, batch acc 0.5792
11:06:53.963   Training iter 200, batch loss 0.4636, batch acc 0.5784
11:06:54.517   Training iter 250, batch loss 0.4630, batch acc 0.5850
11:06:55.080   Training iter 300, batch loss 0.4627, batch acc 0.5952
11:06:55.642   Training iter 350, batch loss 0.4625, batch acc 0.5938
11:06:56.197   Training iter 400, batch loss 0.4625, batch acc 0.5940
11:06:56.743   Training iter 450, batch loss 0.4629, batch acc 0.5804
11:06:57.286   Training iter 500, batch loss 0.4622, batch acc 0.6064
11:06:57.809   Training iter 550, batch loss 0.4625, batch acc 0.5960
11:06:58.342   Training iter 600, batch loss 0.4624, batch acc 0.5922
11:06:58.343 Training @ 437 epoch...
11:06:58.878   Training iter 50, batch loss 0.4626, batch acc 0.5896
11:06:59.413   Training iter 100, batch loss 0.4624, batch acc 0.5954
11:06:59.946   Training iter 150, batch loss 0.4628, batch acc 0.5904
11:07:00.479   Training iter 200, batch loss 0.4626, batch acc 0.5914
11:07:01.011   Training iter 250, batch loss 0.4630, batch acc 0.5976
11:07:01.564   Training iter 300, batch loss 0.4625, batch acc 0.5976
11:07:02.159   Training iter 350, batch loss 0.4628, batch acc 0.5806
11:07:02.723   Training iter 400, batch loss 0.4631, batch acc 0.5738
11:07:03.263   Training iter 450, batch loss 0.4628, batch acc 0.5898
11:07:03.819   Training iter 500, batch loss 0.4625, batch acc 0.5914
11:07:04.365   Training iter 550, batch loss 0.4631, batch acc 0.5904
11:07:04.922   Training iter 600, batch loss 0.4633, batch acc 0.5768
11:07:04.924 Training @ 438 epoch...
11:07:05.478   Training iter 50, batch loss 0.4627, batch acc 0.5880
11:07:06.045   Training iter 100, batch loss 0.4628, batch acc 0.5932
11:07:06.627   Training iter 150, batch loss 0.4629, batch acc 0.5928
11:07:07.225   Training iter 200, batch loss 0.4626, batch acc 0.5922
11:07:07.819   Training iter 250, batch loss 0.4627, batch acc 0.5920
11:07:08.398   Training iter 300, batch loss 0.4631, batch acc 0.5928
11:07:08.964   Training iter 350, batch loss 0.4629, batch acc 0.5836
11:07:09.541   Training iter 400, batch loss 0.4631, batch acc 0.5788
11:07:10.112   Training iter 450, batch loss 0.4629, batch acc 0.5838
11:07:10.688   Training iter 500, batch loss 0.4624, batch acc 0.5984
11:07:11.270   Training iter 550, batch loss 0.4627, batch acc 0.5872
11:07:11.847   Training iter 600, batch loss 0.4626, batch acc 0.5854
11:07:11.849 Training @ 439 epoch...
11:07:12.357   Training iter 50, batch loss 0.4624, batch acc 0.5972
11:07:12.834   Training iter 100, batch loss 0.4624, batch acc 0.5926
11:07:13.301   Training iter 150, batch loss 0.4629, batch acc 0.5914
11:07:13.773   Training iter 200, batch loss 0.4631, batch acc 0.5874
11:07:14.245   Training iter 250, batch loss 0.4624, batch acc 0.5968
11:07:14.737   Training iter 300, batch loss 0.4629, batch acc 0.5824
11:07:15.221   Training iter 350, batch loss 0.4629, batch acc 0.5920
11:07:15.696   Training iter 400, batch loss 0.4628, batch acc 0.5806
11:07:16.182   Training iter 450, batch loss 0.4629, batch acc 0.5922
11:07:16.657   Training iter 500, batch loss 0.4630, batch acc 0.5840
11:07:17.121   Training iter 550, batch loss 0.4629, batch acc 0.5856
11:07:17.596   Training iter 600, batch loss 0.4628, batch acc 0.5842
11:07:17.598 Training @ 440 epoch...
11:07:18.094   Training iter 50, batch loss 0.4626, batch acc 0.5912
11:07:18.587   Training iter 100, batch loss 0.4628, batch acc 0.5942
11:07:19.074   Training iter 150, batch loss 0.4627, batch acc 0.5916
11:07:19.550   Training iter 200, batch loss 0.4626, batch acc 0.5940
11:07:20.030   Training iter 250, batch loss 0.4627, batch acc 0.5918
11:07:20.513   Training iter 300, batch loss 0.4631, batch acc 0.5840
11:07:21.008   Training iter 350, batch loss 0.4626, batch acc 0.5770
11:07:21.484   Training iter 400, batch loss 0.4630, batch acc 0.5906
11:07:21.963   Training iter 450, batch loss 0.4625, batch acc 0.5988
11:07:22.447   Training iter 500, batch loss 0.4632, batch acc 0.5778
11:07:22.973   Training iter 550, batch loss 0.4628, batch acc 0.5900
11:07:23.509   Training iter 600, batch loss 0.4626, batch acc 0.5850
11:07:23.511 Testing @ 440 epoch...
11:07:23.547     Testing, total mean loss 0.46228, total acc 0.60310
11:07:23.547 Training @ 441 epoch...
11:07:24.073   Training iter 50, batch loss 0.4629, batch acc 0.5740
11:07:24.604   Training iter 100, batch loss 0.4627, batch acc 0.5930
11:07:25.146   Training iter 150, batch loss 0.4624, batch acc 0.5876
11:07:25.683   Training iter 200, batch loss 0.4633, batch acc 0.5826
11:07:26.201   Training iter 250, batch loss 0.4625, batch acc 0.5850
11:07:26.726   Training iter 300, batch loss 0.4628, batch acc 0.5986
11:07:27.240   Training iter 350, batch loss 0.4628, batch acc 0.5924
11:07:27.758   Training iter 400, batch loss 0.4624, batch acc 0.5870
11:07:28.276   Training iter 450, batch loss 0.4625, batch acc 0.6008
11:07:28.772   Training iter 500, batch loss 0.4628, batch acc 0.5884
11:07:29.288   Training iter 550, batch loss 0.4632, batch acc 0.5906
11:07:29.803   Training iter 600, batch loss 0.4627, batch acc 0.5882
11:07:29.805 Training @ 442 epoch...
11:07:30.308   Training iter 50, batch loss 0.4628, batch acc 0.5780
11:07:30.784   Training iter 100, batch loss 0.4627, batch acc 0.5866
11:07:31.249   Training iter 150, batch loss 0.4624, batch acc 0.5986
11:07:31.717   Training iter 200, batch loss 0.4627, batch acc 0.5956
11:07:32.181   Training iter 250, batch loss 0.4636, batch acc 0.5786
11:07:32.661   Training iter 300, batch loss 0.4623, batch acc 0.5984
11:07:33.164   Training iter 350, batch loss 0.4625, batch acc 0.5892
11:07:33.665   Training iter 400, batch loss 0.4628, batch acc 0.5864
11:07:34.173   Training iter 450, batch loss 0.4629, batch acc 0.5868
11:07:34.677   Training iter 500, batch loss 0.4627, batch acc 0.5906
11:07:35.195   Training iter 550, batch loss 0.4627, batch acc 0.5934
11:07:35.697   Training iter 600, batch loss 0.4628, batch acc 0.5884
11:07:35.699 Training @ 443 epoch...
11:07:36.220   Training iter 50, batch loss 0.4627, batch acc 0.5932
11:07:36.722   Training iter 100, batch loss 0.4627, batch acc 0.5910
11:07:37.239   Training iter 150, batch loss 0.4625, batch acc 0.6084
11:07:37.743   Training iter 200, batch loss 0.4626, batch acc 0.5838
11:07:38.262   Training iter 250, batch loss 0.4627, batch acc 0.5812
11:07:38.781   Training iter 300, batch loss 0.4627, batch acc 0.5950
11:07:39.325   Training iter 350, batch loss 0.4628, batch acc 0.5912
11:07:39.852   Training iter 400, batch loss 0.4633, batch acc 0.5722
11:07:40.395   Training iter 450, batch loss 0.4627, batch acc 0.5932
11:07:40.931   Training iter 500, batch loss 0.4628, batch acc 0.5898
11:07:41.446   Training iter 550, batch loss 0.4630, batch acc 0.5860
11:07:41.962   Training iter 600, batch loss 0.4623, batch acc 0.5842
11:07:41.964 Training @ 444 epoch...
11:07:42.493   Training iter 50, batch loss 0.4626, batch acc 0.5870
11:07:43.025   Training iter 100, batch loss 0.4629, batch acc 0.5992
11:07:43.571   Training iter 150, batch loss 0.4629, batch acc 0.5902
11:07:44.138   Training iter 200, batch loss 0.4622, batch acc 0.5902
11:07:44.694   Training iter 250, batch loss 0.4626, batch acc 0.5884
11:07:45.254   Training iter 300, batch loss 0.4633, batch acc 0.5734
11:07:45.800   Training iter 350, batch loss 0.4632, batch acc 0.5840
11:07:46.334   Training iter 400, batch loss 0.4623, batch acc 0.5960
11:07:46.861   Training iter 450, batch loss 0.4627, batch acc 0.5896
11:07:47.381   Training iter 500, batch loss 0.4625, batch acc 0.5906
11:07:47.927   Training iter 550, batch loss 0.4628, batch acc 0.5914
11:07:48.478   Training iter 600, batch loss 0.4627, batch acc 0.5906
11:07:48.479 Training @ 445 epoch...
11:07:49.002   Training iter 50, batch loss 0.4627, batch acc 0.5910
11:07:49.512   Training iter 100, batch loss 0.4630, batch acc 0.5774
11:07:50.020   Training iter 150, batch loss 0.4627, batch acc 0.6016
11:07:50.517   Training iter 200, batch loss 0.4631, batch acc 0.5888
11:07:51.012   Training iter 250, batch loss 0.4624, batch acc 0.5866
11:07:51.514   Training iter 300, batch loss 0.4622, batch acc 0.5914
11:07:52.028   Training iter 350, batch loss 0.4635, batch acc 0.5774
11:07:52.553   Training iter 400, batch loss 0.4622, batch acc 0.5966
11:07:53.062   Training iter 450, batch loss 0.4626, batch acc 0.5870
11:07:53.602   Training iter 500, batch loss 0.4626, batch acc 0.5976
11:07:54.153   Training iter 550, batch loss 0.4624, batch acc 0.5892
11:07:54.695   Training iter 600, batch loss 0.4630, batch acc 0.5862
11:07:54.697 Testing @ 445 epoch...
11:07:54.733     Testing, total mean loss 0.46224, total acc 0.60340
11:07:54.733 Training @ 446 epoch...
11:07:55.292   Training iter 50, batch loss 0.4631, batch acc 0.5846
11:07:55.854   Training iter 100, batch loss 0.4624, batch acc 0.5912
11:07:56.438   Training iter 150, batch loss 0.4628, batch acc 0.5882
11:07:56.999   Training iter 200, batch loss 0.4623, batch acc 0.5992
11:07:57.567   Training iter 250, batch loss 0.4624, batch acc 0.5946
11:07:58.131   Training iter 300, batch loss 0.4625, batch acc 0.5858
11:07:58.679   Training iter 350, batch loss 0.4631, batch acc 0.5848
11:07:59.206   Training iter 400, batch loss 0.4630, batch acc 0.5874
11:07:59.729   Training iter 450, batch loss 0.4627, batch acc 0.5968
11:08:00.279   Training iter 500, batch loss 0.4628, batch acc 0.5884
11:08:00.833   Training iter 550, batch loss 0.4626, batch acc 0.5784
11:08:01.410   Training iter 600, batch loss 0.4628, batch acc 0.5912
11:08:01.412 Training @ 447 epoch...
11:08:01.994   Training iter 50, batch loss 0.4628, batch acc 0.5822
11:08:02.579   Training iter 100, batch loss 0.4624, batch acc 0.5990
11:08:03.163   Training iter 150, batch loss 0.4626, batch acc 0.5934
11:08:03.722   Training iter 200, batch loss 0.4624, batch acc 0.5898
11:08:04.256   Training iter 250, batch loss 0.4624, batch acc 0.6022
11:08:04.784   Training iter 300, batch loss 0.4631, batch acc 0.5868
11:08:05.312   Training iter 350, batch loss 0.4626, batch acc 0.5830
11:08:05.817   Training iter 400, batch loss 0.4623, batch acc 0.5908
11:08:06.314   Training iter 450, batch loss 0.4628, batch acc 0.5874
11:08:06.836   Training iter 500, batch loss 0.4634, batch acc 0.5770
11:08:07.359   Training iter 550, batch loss 0.4630, batch acc 0.5790
11:08:07.878   Training iter 600, batch loss 0.4627, batch acc 0.5998
11:08:07.880 Training @ 448 epoch...
11:08:08.398   Training iter 50, batch loss 0.4632, batch acc 0.5806
11:08:08.905   Training iter 100, batch loss 0.4624, batch acc 0.5972
11:08:09.402   Training iter 150, batch loss 0.4625, batch acc 0.5918
11:08:09.922   Training iter 200, batch loss 0.4630, batch acc 0.5786
11:08:10.438   Training iter 250, batch loss 0.4627, batch acc 0.5940
11:08:10.961   Training iter 300, batch loss 0.4630, batch acc 0.5850
11:08:11.500   Training iter 350, batch loss 0.4628, batch acc 0.5818
11:08:12.034   Training iter 400, batch loss 0.4630, batch acc 0.5900
11:08:12.572   Training iter 450, batch loss 0.4628, batch acc 0.5850
11:08:13.228   Training iter 500, batch loss 0.4628, batch acc 0.5908
11:08:14.208   Training iter 550, batch loss 0.4621, batch acc 0.5968
11:08:14.773   Training iter 600, batch loss 0.4621, batch acc 0.6004
11:08:14.775 Training @ 449 epoch...
11:08:15.315   Training iter 50, batch loss 0.4629, batch acc 0.5870
11:08:15.855   Training iter 100, batch loss 0.4626, batch acc 0.5818
11:08:16.395   Training iter 150, batch loss 0.4629, batch acc 0.5914
11:08:16.931   Training iter 200, batch loss 0.4631, batch acc 0.5830
11:08:17.444   Training iter 250, batch loss 0.4624, batch acc 0.5954
11:08:17.939   Training iter 300, batch loss 0.4628, batch acc 0.5802
11:08:18.457   Training iter 350, batch loss 0.4629, batch acc 0.5876
11:08:18.955   Training iter 400, batch loss 0.4620, batch acc 0.6068
11:08:19.459   Training iter 450, batch loss 0.4627, batch acc 0.5902
11:08:19.946   Training iter 500, batch loss 0.4625, batch acc 0.5938
11:08:20.435   Training iter 550, batch loss 0.4628, batch acc 0.5902
11:08:20.946   Training iter 600, batch loss 0.4627, batch acc 0.5838
11:08:20.948 Training @ 450 epoch...
11:08:21.497   Training iter 50, batch loss 0.4625, batch acc 0.5958
11:08:22.023   Training iter 100, batch loss 0.4626, batch acc 0.5912
11:08:22.547   Training iter 150, batch loss 0.4634, batch acc 0.5866
11:08:23.081   Training iter 200, batch loss 0.4628, batch acc 0.5988
11:08:23.599   Training iter 250, batch loss 0.4629, batch acc 0.5902
11:08:24.141   Training iter 300, batch loss 0.4628, batch acc 0.5882
11:08:24.669   Training iter 350, batch loss 0.4623, batch acc 0.5886
11:08:25.203   Training iter 400, batch loss 0.4625, batch acc 0.5888
11:08:25.726   Training iter 450, batch loss 0.4625, batch acc 0.5814
11:08:26.252   Training iter 500, batch loss 0.4627, batch acc 0.5886
11:08:26.785   Training iter 550, batch loss 0.4627, batch acc 0.5862
11:08:27.319   Training iter 600, batch loss 0.4625, batch acc 0.5870
11:08:27.321 Testing @ 450 epoch...
11:08:27.357     Testing, total mean loss 0.46220, total acc 0.60370
11:08:27.357 Training @ 451 epoch...
11:08:27.920   Training iter 50, batch loss 0.4629, batch acc 0.5878
11:08:28.479   Training iter 100, batch loss 0.4625, batch acc 0.5940
11:08:29.038   Training iter 150, batch loss 0.4632, batch acc 0.5866
11:08:29.574   Training iter 200, batch loss 0.4624, batch acc 0.5944
11:08:30.109   Training iter 250, batch loss 0.4631, batch acc 0.5768
11:08:30.641   Training iter 300, batch loss 0.4625, batch acc 0.5904
11:08:31.202   Training iter 350, batch loss 0.4628, batch acc 0.5786
11:08:31.763   Training iter 400, batch loss 0.4624, batch acc 0.6006
11:08:32.318   Training iter 450, batch loss 0.4631, batch acc 0.5868
11:08:32.856   Training iter 500, batch loss 0.4627, batch acc 0.5874
11:08:33.408   Training iter 550, batch loss 0.4625, batch acc 0.5860
11:08:33.932   Training iter 600, batch loss 0.4621, batch acc 0.6038
11:08:33.934 Training @ 452 epoch...
11:08:34.466   Training iter 50, batch loss 0.4623, batch acc 0.6022
11:08:34.995   Training iter 100, batch loss 0.4629, batch acc 0.5832
11:08:35.518   Training iter 150, batch loss 0.4630, batch acc 0.5806
11:08:36.040   Training iter 200, batch loss 0.4632, batch acc 0.5858
11:08:36.552   Training iter 250, batch loss 0.4631, batch acc 0.5924
11:08:37.065   Training iter 300, batch loss 0.4623, batch acc 0.5866
11:08:37.574   Training iter 350, batch loss 0.4626, batch acc 0.5936
11:08:38.089   Training iter 400, batch loss 0.4622, batch acc 0.5962
11:08:38.598   Training iter 450, batch loss 0.4626, batch acc 0.5754
11:08:39.106   Training iter 500, batch loss 0.4623, batch acc 0.5894
11:08:39.608   Training iter 550, batch loss 0.4630, batch acc 0.5890
11:08:40.074   Training iter 600, batch loss 0.4625, batch acc 0.5986
11:08:40.076 Training @ 453 epoch...
11:08:40.564   Training iter 50, batch loss 0.4629, batch acc 0.5764
11:08:41.076   Training iter 100, batch loss 0.4629, batch acc 0.5882
11:08:41.613   Training iter 150, batch loss 0.4627, batch acc 0.5818
11:08:42.147   Training iter 200, batch loss 0.4625, batch acc 0.5942
11:08:42.634   Training iter 250, batch loss 0.4632, batch acc 0.5800
11:08:43.127   Training iter 300, batch loss 0.4624, batch acc 0.5932
11:08:43.641   Training iter 350, batch loss 0.4629, batch acc 0.5860
11:08:44.144   Training iter 400, batch loss 0.4623, batch acc 0.5974
11:08:44.644   Training iter 450, batch loss 0.4625, batch acc 0.5930
11:08:45.146   Training iter 500, batch loss 0.4630, batch acc 0.5808
11:08:45.657   Training iter 550, batch loss 0.4621, batch acc 0.6070
11:08:46.203   Training iter 600, batch loss 0.4626, batch acc 0.5956
11:08:46.205 Training @ 454 epoch...
11:08:46.702   Training iter 50, batch loss 0.4630, batch acc 0.5862
11:08:47.208   Training iter 100, batch loss 0.4625, batch acc 0.5846
11:08:47.723   Training iter 150, batch loss 0.4625, batch acc 0.5954
11:08:48.227   Training iter 200, batch loss 0.4629, batch acc 0.5902
11:08:48.765   Training iter 250, batch loss 0.4629, batch acc 0.5918
11:08:49.302   Training iter 300, batch loss 0.4629, batch acc 0.5932
11:08:49.825   Training iter 350, batch loss 0.4630, batch acc 0.5736
11:08:50.338   Training iter 400, batch loss 0.4620, batch acc 0.6014
11:08:50.855   Training iter 450, batch loss 0.4630, batch acc 0.5806
11:08:51.369   Training iter 500, batch loss 0.4623, batch acc 0.5964
11:08:51.880   Training iter 550, batch loss 0.4625, batch acc 0.5890
11:08:52.378   Training iter 600, batch loss 0.4623, batch acc 0.5914
11:08:52.379 Training @ 455 epoch...
11:08:52.887   Training iter 50, batch loss 0.4623, batch acc 0.5940
11:08:53.399   Training iter 100, batch loss 0.4626, batch acc 0.5974
11:08:53.908   Training iter 150, batch loss 0.4629, batch acc 0.5838
11:08:54.422   Training iter 200, batch loss 0.4630, batch acc 0.5700
11:08:54.950   Training iter 250, batch loss 0.4624, batch acc 0.5852
11:08:55.466   Training iter 300, batch loss 0.4632, batch acc 0.5776
11:08:55.963   Training iter 350, batch loss 0.4630, batch acc 0.5910
11:08:56.468   Training iter 400, batch loss 0.4623, batch acc 0.6078
11:08:56.970   Training iter 450, batch loss 0.4628, batch acc 0.5864
11:08:57.478   Training iter 500, batch loss 0.4622, batch acc 0.5948
11:08:57.988   Training iter 550, batch loss 0.4623, batch acc 0.5978
11:08:58.515   Training iter 600, batch loss 0.4627, batch acc 0.5882
11:08:58.517 Testing @ 455 epoch...
11:08:58.553     Testing, total mean loss 0.46216, total acc 0.60350
11:08:58.553 Training @ 456 epoch...
11:08:59.092   Training iter 50, batch loss 0.4628, batch acc 0.5784
11:08:59.618   Training iter 100, batch loss 0.4622, batch acc 0.5898
11:09:00.166   Training iter 150, batch loss 0.4627, batch acc 0.5860
11:09:00.718   Training iter 200, batch loss 0.4625, batch acc 0.5922
11:09:01.281   Training iter 250, batch loss 0.4623, batch acc 0.5984
11:09:01.857   Training iter 300, batch loss 0.4626, batch acc 0.5902
11:09:02.437   Training iter 350, batch loss 0.4627, batch acc 0.5882
11:09:03.006   Training iter 400, batch loss 0.4630, batch acc 0.5890
11:09:03.562   Training iter 450, batch loss 0.4622, batch acc 0.6028
11:09:04.118   Training iter 500, batch loss 0.4629, batch acc 0.5880
11:09:04.670   Training iter 550, batch loss 0.4632, batch acc 0.5800
11:09:05.218   Training iter 600, batch loss 0.4627, batch acc 0.5894
11:09:05.220 Training @ 457 epoch...
11:09:05.757   Training iter 50, batch loss 0.4630, batch acc 0.5840
11:09:06.279   Training iter 100, batch loss 0.4627, batch acc 0.5870
11:09:06.807   Training iter 150, batch loss 0.4626, batch acc 0.5942
11:09:07.339   Training iter 200, batch loss 0.4622, batch acc 0.5972
11:09:07.860   Training iter 250, batch loss 0.4629, batch acc 0.5884
11:09:08.375   Training iter 300, batch loss 0.4627, batch acc 0.5858
11:09:08.906   Training iter 350, batch loss 0.4626, batch acc 0.5934
11:09:09.437   Training iter 400, batch loss 0.4627, batch acc 0.5818
11:09:09.963   Training iter 450, batch loss 0.4628, batch acc 0.6000
11:09:10.482   Training iter 500, batch loss 0.4622, batch acc 0.5856
11:09:10.984   Training iter 550, batch loss 0.4625, batch acc 0.5880
11:09:11.491   Training iter 600, batch loss 0.4626, batch acc 0.5900
11:09:11.493 Training @ 458 epoch...
11:09:12.007   Training iter 50, batch loss 0.4623, batch acc 0.6006
11:09:12.533   Training iter 100, batch loss 0.4627, batch acc 0.5794
11:09:13.062   Training iter 150, batch loss 0.4623, batch acc 0.5944
11:09:13.593   Training iter 200, batch loss 0.4632, batch acc 0.5774
11:09:14.114   Training iter 250, batch loss 0.4629, batch acc 0.5846
11:09:14.632   Training iter 300, batch loss 0.4624, batch acc 0.6010
11:09:15.145   Training iter 350, batch loss 0.4627, batch acc 0.5904
11:09:15.675   Training iter 400, batch loss 0.4623, batch acc 0.5962
11:09:16.217   Training iter 450, batch loss 0.4622, batch acc 0.5990
11:09:16.751   Training iter 500, batch loss 0.4629, batch acc 0.5760
11:09:17.276   Training iter 550, batch loss 0.4628, batch acc 0.5862
11:09:17.822   Training iter 600, batch loss 0.4627, batch acc 0.5874
11:09:17.824 Training @ 459 epoch...
11:09:18.380   Training iter 50, batch loss 0.4624, batch acc 0.5796
11:09:18.922   Training iter 100, batch loss 0.4622, batch acc 0.5950
11:09:19.449   Training iter 150, batch loss 0.4627, batch acc 0.5928
11:09:19.979   Training iter 200, batch loss 0.4628, batch acc 0.5850
11:09:20.517   Training iter 250, batch loss 0.4623, batch acc 0.5922
11:09:21.073   Training iter 300, batch loss 0.4626, batch acc 0.5878
11:09:21.614   Training iter 350, batch loss 0.4625, batch acc 0.5882
11:09:22.137   Training iter 400, batch loss 0.4629, batch acc 0.5914
11:09:22.671   Training iter 450, batch loss 0.4630, batch acc 0.5844
11:09:23.202   Training iter 500, batch loss 0.4627, batch acc 0.5896
11:09:23.748   Training iter 550, batch loss 0.4629, batch acc 0.5894
11:09:24.293   Training iter 600, batch loss 0.4623, batch acc 0.5984
11:09:24.295 Training @ 460 epoch...
11:09:24.795   Training iter 50, batch loss 0.4623, batch acc 0.5978
11:09:25.314   Training iter 100, batch loss 0.4627, batch acc 0.5946
11:09:25.826   Training iter 150, batch loss 0.4627, batch acc 0.5882
11:09:26.320   Training iter 200, batch loss 0.4629, batch acc 0.5816
11:09:26.826   Training iter 250, batch loss 0.4629, batch acc 0.5868
11:09:27.347   Training iter 300, batch loss 0.4620, batch acc 0.5926
11:09:27.871   Training iter 350, batch loss 0.4627, batch acc 0.5902
11:09:28.406   Training iter 400, batch loss 0.4624, batch acc 0.5924
11:09:28.924   Training iter 450, batch loss 0.4632, batch acc 0.5864
11:09:29.459   Training iter 500, batch loss 0.4627, batch acc 0.5840
11:09:30.005   Training iter 550, batch loss 0.4623, batch acc 0.5950
11:09:30.552   Training iter 600, batch loss 0.4626, batch acc 0.5854
11:09:30.554 Testing @ 460 epoch...
11:09:30.594     Testing, total mean loss 0.46212, total acc 0.60370
11:09:30.594 Training @ 461 epoch...
11:09:31.157   Training iter 50, batch loss 0.4630, batch acc 0.5780
11:09:31.700   Training iter 100, batch loss 0.4626, batch acc 0.6000
11:09:32.246   Training iter 150, batch loss 0.4626, batch acc 0.5950
11:09:32.792   Training iter 200, batch loss 0.4625, batch acc 0.5952
11:09:33.320   Training iter 250, batch loss 0.4621, batch acc 0.5942
11:09:33.809   Training iter 300, batch loss 0.4622, batch acc 0.5984
11:09:34.286   Training iter 350, batch loss 0.4621, batch acc 0.5994
11:09:34.812   Training iter 400, batch loss 0.4625, batch acc 0.5834
11:09:35.319   Training iter 450, batch loss 0.4627, batch acc 0.5762
11:09:35.817   Training iter 500, batch loss 0.4624, batch acc 0.5922
11:09:36.316   Training iter 550, batch loss 0.4629, batch acc 0.5870
11:09:36.825   Training iter 600, batch loss 0.4636, batch acc 0.5758
11:09:36.827 Training @ 462 epoch...
11:09:37.367   Training iter 50, batch loss 0.4623, batch acc 0.5958
11:09:37.932   Training iter 100, batch loss 0.4628, batch acc 0.5824
11:09:38.522   Training iter 150, batch loss 0.4626, batch acc 0.5886
11:09:39.097   Training iter 200, batch loss 0.4629, batch acc 0.5816
11:09:39.670   Training iter 250, batch loss 0.4629, batch acc 0.5822
11:09:40.228   Training iter 300, batch loss 0.4626, batch acc 0.5982
11:09:40.764   Training iter 350, batch loss 0.4619, batch acc 0.6008
11:09:41.316   Training iter 400, batch loss 0.4627, batch acc 0.5910
11:09:41.871   Training iter 450, batch loss 0.4633, batch acc 0.5866
11:09:42.433   Training iter 500, batch loss 0.4624, batch acc 0.5844
11:09:42.973   Training iter 550, batch loss 0.4620, batch acc 0.6004
11:09:43.498   Training iter 600, batch loss 0.4628, batch acc 0.5838
11:09:43.500 Training @ 463 epoch...
11:09:44.053   Training iter 50, batch loss 0.4622, batch acc 0.5964
11:09:44.576   Training iter 100, batch loss 0.4629, batch acc 0.5778
11:09:45.099   Training iter 150, batch loss 0.4625, batch acc 0.5976
11:09:45.637   Training iter 200, batch loss 0.4628, batch acc 0.5924
11:09:46.133   Training iter 250, batch loss 0.4626, batch acc 0.5922
11:09:46.612   Training iter 300, batch loss 0.4623, batch acc 0.5910
11:09:47.093   Training iter 350, batch loss 0.4624, batch acc 0.5956
11:09:47.619   Training iter 400, batch loss 0.4624, batch acc 0.5902
11:09:48.147   Training iter 450, batch loss 0.4627, batch acc 0.5820
11:09:48.686   Training iter 500, batch loss 0.4628, batch acc 0.5864
11:09:49.208   Training iter 550, batch loss 0.4624, batch acc 0.5836
11:09:49.743   Training iter 600, batch loss 0.4629, batch acc 0.5884
11:09:49.745 Training @ 464 epoch...
11:09:50.289   Training iter 50, batch loss 0.4629, batch acc 0.5826
11:09:50.818   Training iter 100, batch loss 0.4629, batch acc 0.5936
11:09:51.318   Training iter 150, batch loss 0.4625, batch acc 0.5880
11:09:51.811   Training iter 200, batch loss 0.4624, batch acc 0.5928
11:09:52.333   Training iter 250, batch loss 0.4626, batch acc 0.5882
11:09:52.878   Training iter 300, batch loss 0.4623, batch acc 0.5972
11:09:53.408   Training iter 350, batch loss 0.4623, batch acc 0.5824
11:09:53.916   Training iter 400, batch loss 0.4625, batch acc 0.5828
11:09:54.446   Training iter 450, batch loss 0.4627, batch acc 0.5880
11:09:54.970   Training iter 500, batch loss 0.4624, batch acc 0.6068
11:09:55.490   Training iter 550, batch loss 0.4628, batch acc 0.5842
11:09:55.987   Training iter 600, batch loss 0.4626, batch acc 0.5898
11:09:55.989 Training @ 465 epoch...
11:09:56.484   Training iter 50, batch loss 0.4634, batch acc 0.5780
11:09:56.980   Training iter 100, batch loss 0.4624, batch acc 0.6012
11:09:57.474   Training iter 150, batch loss 0.4622, batch acc 0.5934
11:09:57.964   Training iter 200, batch loss 0.4627, batch acc 0.5936
11:09:58.453   Training iter 250, batch loss 0.4617, batch acc 0.6012
11:09:58.984   Training iter 300, batch loss 0.4627, batch acc 0.5946
11:09:59.508   Training iter 350, batch loss 0.4628, batch acc 0.5892
11:10:00.007   Training iter 400, batch loss 0.4625, batch acc 0.5890
11:10:00.520   Training iter 450, batch loss 0.4619, batch acc 0.5898
11:10:00.986   Training iter 500, batch loss 0.4635, batch acc 0.5696
11:10:01.518   Training iter 550, batch loss 0.4627, batch acc 0.5802
11:10:02.096   Training iter 600, batch loss 0.4623, batch acc 0.5954
11:10:02.098 Testing @ 465 epoch...
11:10:02.137     Testing, total mean loss 0.46208, total acc 0.60370
11:10:02.137 Training @ 466 epoch...
11:10:02.723   Training iter 50, batch loss 0.4621, batch acc 0.5940
11:10:03.284   Training iter 100, batch loss 0.4625, batch acc 0.6008
11:10:03.841   Training iter 150, batch loss 0.4625, batch acc 0.5914
11:10:04.406   Training iter 200, batch loss 0.4627, batch acc 0.5862
11:10:04.946   Training iter 250, batch loss 0.4626, batch acc 0.5904
11:10:05.502   Training iter 300, batch loss 0.4626, batch acc 0.5796
11:10:06.070   Training iter 350, batch loss 0.4628, batch acc 0.5904
11:10:06.625   Training iter 400, batch loss 0.4623, batch acc 0.6024
11:10:07.202   Training iter 450, batch loss 0.4629, batch acc 0.5846
11:10:07.794   Training iter 500, batch loss 0.4625, batch acc 0.5850
11:10:08.393   Training iter 550, batch loss 0.4627, batch acc 0.5790
11:10:08.991   Training iter 600, batch loss 0.4626, batch acc 0.5898
11:10:08.993 Training @ 467 epoch...
11:10:09.592   Training iter 50, batch loss 0.4626, batch acc 0.5870
11:10:10.153   Training iter 100, batch loss 0.4625, batch acc 0.5874
11:10:10.699   Training iter 150, batch loss 0.4631, batch acc 0.5796
11:10:11.250   Training iter 200, batch loss 0.4621, batch acc 0.5946
11:10:11.797   Training iter 250, batch loss 0.4626, batch acc 0.5882
11:10:12.322   Training iter 300, batch loss 0.4626, batch acc 0.5980
11:10:12.872   Training iter 350, batch loss 0.4628, batch acc 0.5914
11:10:13.419   Training iter 400, batch loss 0.4623, batch acc 0.5904
11:10:13.963   Training iter 450, batch loss 0.4627, batch acc 0.5872
11:10:14.500   Training iter 500, batch loss 0.4628, batch acc 0.5894
11:10:15.033   Training iter 550, batch loss 0.4625, batch acc 0.5846
11:10:15.559   Training iter 600, batch loss 0.4620, batch acc 0.5970
11:10:15.561 Training @ 468 epoch...
11:10:16.079   Training iter 50, batch loss 0.4627, batch acc 0.5924
11:10:16.585   Training iter 100, batch loss 0.4629, batch acc 0.5796
11:10:17.102   Training iter 150, batch loss 0.4625, batch acc 0.5886
11:10:17.621   Training iter 200, batch loss 0.4621, batch acc 0.6002
11:10:18.156   Training iter 250, batch loss 0.4625, batch acc 0.5850
11:10:18.716   Training iter 300, batch loss 0.4622, batch acc 0.5928
11:10:19.247   Training iter 350, batch loss 0.4621, batch acc 0.5926
11:10:19.764   Training iter 400, batch loss 0.4631, batch acc 0.5704
11:10:20.317   Training iter 450, batch loss 0.4625, batch acc 0.5934
11:10:20.873   Training iter 500, batch loss 0.4625, batch acc 0.6006
11:10:21.422   Training iter 550, batch loss 0.4625, batch acc 0.5984
11:10:21.966   Training iter 600, batch loss 0.4629, batch acc 0.5814
11:10:21.968 Training @ 469 epoch...
11:10:22.502   Training iter 50, batch loss 0.4621, batch acc 0.5982
11:10:23.025   Training iter 100, batch loss 0.4626, batch acc 0.5864
11:10:23.549   Training iter 150, batch loss 0.4626, batch acc 0.5894
11:10:24.075   Training iter 200, batch loss 0.4627, batch acc 0.5872
11:10:24.595   Training iter 250, batch loss 0.4629, batch acc 0.5866
11:10:25.152   Training iter 300, batch loss 0.4625, batch acc 0.5982
11:10:25.681   Training iter 350, batch loss 0.4623, batch acc 0.5922
11:10:26.172   Training iter 400, batch loss 0.4630, batch acc 0.5816
11:10:26.658   Training iter 450, batch loss 0.4627, batch acc 0.5916
11:10:27.167   Training iter 500, batch loss 0.4625, batch acc 0.5842
11:10:27.676   Training iter 550, batch loss 0.4622, batch acc 0.5948
11:10:28.187   Training iter 600, batch loss 0.4624, batch acc 0.5866
11:10:28.189 Training @ 470 epoch...
11:10:28.675   Training iter 50, batch loss 0.4630, batch acc 0.5756
11:10:29.167   Training iter 100, batch loss 0.4626, batch acc 0.5912
11:10:29.662   Training iter 150, batch loss 0.4626, batch acc 0.5800
11:10:30.163   Training iter 200, batch loss 0.4625, batch acc 0.6014
11:10:30.675   Training iter 250, batch loss 0.4620, batch acc 0.5990
11:10:31.232   Training iter 300, batch loss 0.4628, batch acc 0.5930
11:10:31.820   Training iter 350, batch loss 0.4626, batch acc 0.5818
11:10:32.409   Training iter 400, batch loss 0.4624, batch acc 0.5834
11:10:33.001   Training iter 450, batch loss 0.4624, batch acc 0.5918
11:10:33.581   Training iter 500, batch loss 0.4623, batch acc 0.6052
11:10:34.141   Training iter 550, batch loss 0.4624, batch acc 0.5958
11:10:34.689   Training iter 600, batch loss 0.4629, batch acc 0.5786
11:10:34.691 Testing @ 470 epoch...
11:10:34.731     Testing, total mean loss 0.46205, total acc 0.60430
11:10:34.731 Training @ 471 epoch...
11:10:35.291   Training iter 50, batch loss 0.4625, batch acc 0.5876
11:10:35.847   Training iter 100, batch loss 0.4625, batch acc 0.5968
11:10:36.442   Training iter 150, batch loss 0.4625, batch acc 0.5998
11:10:37.011   Training iter 200, batch loss 0.4625, batch acc 0.5846
11:10:37.571   Training iter 250, batch loss 0.4624, batch acc 0.5932
11:10:38.139   Training iter 300, batch loss 0.4623, batch acc 0.5910
11:10:38.692   Training iter 350, batch loss 0.4626, batch acc 0.5890
11:10:39.249   Training iter 400, batch loss 0.4626, batch acc 0.5914
11:10:39.813   Training iter 450, batch loss 0.4628, batch acc 0.5726
11:10:40.378   Training iter 500, batch loss 0.4626, batch acc 0.5876
11:10:40.925   Training iter 550, batch loss 0.4625, batch acc 0.5906
11:10:41.469   Training iter 600, batch loss 0.4625, batch acc 0.5940
11:10:41.471 Training @ 472 epoch...
11:10:42.002   Training iter 50, batch loss 0.4628, batch acc 0.5886
11:10:42.551   Training iter 100, batch loss 0.4622, batch acc 0.5852
11:10:43.099   Training iter 150, batch loss 0.4628, batch acc 0.5888
11:10:43.627   Training iter 200, batch loss 0.4626, batch acc 0.5826
11:10:44.163   Training iter 250, batch loss 0.4622, batch acc 0.5992
11:10:44.709   Training iter 300, batch loss 0.4625, batch acc 0.5932
11:10:45.258   Training iter 350, batch loss 0.4627, batch acc 0.5824
11:10:45.809   Training iter 400, batch loss 0.4620, batch acc 0.5972
11:10:46.358   Training iter 450, batch loss 0.4626, batch acc 0.5932
11:10:46.904   Training iter 500, batch loss 0.4628, batch acc 0.5864
11:10:47.444   Training iter 550, batch loss 0.4623, batch acc 0.5870
11:10:47.984   Training iter 600, batch loss 0.4628, batch acc 0.5942
11:10:47.985 Training @ 473 epoch...
11:10:48.531   Training iter 50, batch loss 0.4623, batch acc 0.5898
11:10:49.078   Training iter 100, batch loss 0.4626, batch acc 0.5970
11:10:49.621   Training iter 150, batch loss 0.4623, batch acc 0.5956
11:10:50.166   Training iter 200, batch loss 0.4623, batch acc 0.5870
11:10:50.720   Training iter 250, batch loss 0.4622, batch acc 0.5932
11:10:51.254   Training iter 300, batch loss 0.4629, batch acc 0.5844
11:10:51.781   Training iter 350, batch loss 0.4623, batch acc 0.5914
11:10:52.336   Training iter 400, batch loss 0.4627, batch acc 0.5804
11:10:52.896   Training iter 450, batch loss 0.4629, batch acc 0.5796
11:10:53.459   Training iter 500, batch loss 0.4628, batch acc 0.6032
11:10:54.027   Training iter 550, batch loss 0.4625, batch acc 0.5870
11:10:54.592   Training iter 600, batch loss 0.4624, batch acc 0.5886
11:10:54.594 Training @ 474 epoch...
11:10:55.170   Training iter 50, batch loss 0.4624, batch acc 0.5826
11:10:55.734   Training iter 100, batch loss 0.4628, batch acc 0.5812
11:10:56.307   Training iter 150, batch loss 0.4625, batch acc 0.5880
11:10:56.867   Training iter 200, batch loss 0.4629, batch acc 0.5916
11:10:57.426   Training iter 250, batch loss 0.4628, batch acc 0.5922
11:10:57.995   Training iter 300, batch loss 0.4619, batch acc 0.5994
11:10:58.548   Training iter 350, batch loss 0.4624, batch acc 0.5906
11:10:59.072   Training iter 400, batch loss 0.4629, batch acc 0.5816
11:10:59.590   Training iter 450, batch loss 0.4622, batch acc 0.5928
11:11:00.138   Training iter 500, batch loss 0.4624, batch acc 0.5990
11:11:00.674   Training iter 550, batch loss 0.4624, batch acc 0.5890
11:11:01.213   Training iter 600, batch loss 0.4625, batch acc 0.5902
11:11:01.215 Training @ 475 epoch...
11:11:01.787   Training iter 50, batch loss 0.4627, batch acc 0.5950
11:11:02.361   Training iter 100, batch loss 0.4623, batch acc 0.5928
11:11:02.920   Training iter 150, batch loss 0.4619, batch acc 0.5926
11:11:03.492   Training iter 200, batch loss 0.4626, batch acc 0.5834
11:11:04.066   Training iter 250, batch loss 0.4626, batch acc 0.5924
11:11:04.639   Training iter 300, batch loss 0.4628, batch acc 0.5780
11:11:05.200   Training iter 350, batch loss 0.4625, batch acc 0.5932
11:11:05.751   Training iter 400, batch loss 0.4628, batch acc 0.5824
11:11:06.300   Training iter 450, batch loss 0.4622, batch acc 0.5978
11:11:06.848   Training iter 500, batch loss 0.4626, batch acc 0.5916
11:11:07.394   Training iter 550, batch loss 0.4626, batch acc 0.5864
11:11:07.913   Training iter 600, batch loss 0.4625, batch acc 0.5916
11:11:07.915 Testing @ 475 epoch...
11:11:07.953     Testing, total mean loss 0.46201, total acc 0.60410
11:11:07.953 Training @ 476 epoch...
11:11:08.485   Training iter 50, batch loss 0.4621, batch acc 0.6030
11:11:09.024   Training iter 100, batch loss 0.4620, batch acc 0.5958
11:11:09.539   Training iter 150, batch loss 0.4627, batch acc 0.5872
11:11:10.079   Training iter 200, batch loss 0.4623, batch acc 0.5856
11:11:10.629   Training iter 250, batch loss 0.4623, batch acc 0.5948
11:11:11.136   Training iter 300, batch loss 0.4630, batch acc 0.5816
11:11:11.639   Training iter 350, batch loss 0.4624, batch acc 0.5964
11:11:12.196   Training iter 400, batch loss 0.4625, batch acc 0.5908
11:11:12.744   Training iter 450, batch loss 0.4626, batch acc 0.5824
11:11:13.298   Training iter 500, batch loss 0.4627, batch acc 0.5888
11:11:13.849   Training iter 550, batch loss 0.4626, batch acc 0.5860
11:11:14.389   Training iter 600, batch loss 0.4627, batch acc 0.5854
11:11:14.391 Training @ 477 epoch...
11:11:14.931   Training iter 50, batch loss 0.4625, batch acc 0.5878
11:11:15.471   Training iter 100, batch loss 0.4619, batch acc 0.6010
11:11:16.011   Training iter 150, batch loss 0.4620, batch acc 0.5992
11:11:16.497   Training iter 200, batch loss 0.4626, batch acc 0.5856
11:11:17.027   Training iter 250, batch loss 0.4629, batch acc 0.5830
11:11:17.559   Training iter 300, batch loss 0.4624, batch acc 0.5898
11:11:18.089   Training iter 350, batch loss 0.4625, batch acc 0.5910
11:11:18.629   Training iter 400, batch loss 0.4629, batch acc 0.5852
11:11:19.163   Training iter 450, batch loss 0.4627, batch acc 0.5812
11:11:19.701   Training iter 500, batch loss 0.4621, batch acc 0.5920
11:11:20.231   Training iter 550, batch loss 0.4629, batch acc 0.5890
11:11:20.754   Training iter 600, batch loss 0.4624, batch acc 0.5942
11:11:20.756 Training @ 478 epoch...
11:11:21.256   Training iter 50, batch loss 0.4626, batch acc 0.5898
11:11:21.757   Training iter 100, batch loss 0.4625, batch acc 0.5898
11:11:22.277   Training iter 150, batch loss 0.4631, batch acc 0.5838
11:11:22.753   Training iter 200, batch loss 0.4622, batch acc 0.5928
11:11:23.251   Training iter 250, batch loss 0.4622, batch acc 0.6012
11:11:23.776   Training iter 300, batch loss 0.4625, batch acc 0.5868
11:11:24.332   Training iter 350, batch loss 0.4624, batch acc 0.5832
11:11:24.880   Training iter 400, batch loss 0.4628, batch acc 0.5866
11:11:25.416   Training iter 450, batch loss 0.4619, batch acc 0.5958
11:11:25.946   Training iter 500, batch loss 0.4626, batch acc 0.5922
11:11:26.490   Training iter 550, batch loss 0.4627, batch acc 0.5848
11:11:27.038   Training iter 600, batch loss 0.4622, batch acc 0.5938
11:11:27.040 Training @ 479 epoch...
11:11:27.582   Training iter 50, batch loss 0.4624, batch acc 0.5922
11:11:28.129   Training iter 100, batch loss 0.4628, batch acc 0.5864
11:11:28.674   Training iter 150, batch loss 0.4624, batch acc 0.5932
11:11:29.225   Training iter 200, batch loss 0.4624, batch acc 0.5860
11:11:29.793   Training iter 250, batch loss 0.4625, batch acc 0.5978
11:11:30.370   Training iter 300, batch loss 0.4622, batch acc 0.5914
11:11:30.929   Training iter 350, batch loss 0.4624, batch acc 0.5924
11:11:31.447   Training iter 400, batch loss 0.4626, batch acc 0.5916
11:11:31.953   Training iter 450, batch loss 0.4628, batch acc 0.5912
11:11:32.432   Training iter 500, batch loss 0.4624, batch acc 0.5930
11:11:32.916   Training iter 550, batch loss 0.4621, batch acc 0.5902
11:11:33.438   Training iter 600, batch loss 0.4628, batch acc 0.5742
11:11:33.440 Training @ 480 epoch...
11:11:33.966   Training iter 50, batch loss 0.4630, batch acc 0.5814
11:11:34.484   Training iter 100, batch loss 0.4622, batch acc 0.5934
11:11:35.036   Training iter 150, batch loss 0.4621, batch acc 0.5894
11:11:35.579   Training iter 200, batch loss 0.4625, batch acc 0.6034
11:11:36.137   Training iter 250, batch loss 0.4628, batch acc 0.5878
11:11:36.688   Training iter 300, batch loss 0.4626, batch acc 0.6048
11:11:37.220   Training iter 350, batch loss 0.4625, batch acc 0.5802
11:11:37.753   Training iter 400, batch loss 0.4625, batch acc 0.5878
11:11:38.307   Training iter 450, batch loss 0.4629, batch acc 0.5716
11:11:38.854   Training iter 500, batch loss 0.4621, batch acc 0.5864
11:11:39.396   Training iter 550, batch loss 0.4626, batch acc 0.5914
11:11:39.950   Training iter 600, batch loss 0.4619, batch acc 0.6030
11:11:39.952 Testing @ 480 epoch...
11:11:39.990     Testing, total mean loss 0.46198, total acc 0.60420
11:11:39.990 Training @ 481 epoch...
11:11:40.563   Training iter 50, batch loss 0.4622, batch acc 0.5922
11:11:41.119   Training iter 100, batch loss 0.4627, batch acc 0.5870
11:11:41.656   Training iter 150, batch loss 0.4623, batch acc 0.5922
11:11:42.211   Training iter 200, batch loss 0.4623, batch acc 0.5930
11:11:42.754   Training iter 250, batch loss 0.4623, batch acc 0.5840
11:11:43.302   Training iter 300, batch loss 0.4627, batch acc 0.5914
11:11:43.873   Training iter 350, batch loss 0.4626, batch acc 0.5810
11:11:44.445   Training iter 400, batch loss 0.4625, batch acc 0.5834
11:11:45.003   Training iter 450, batch loss 0.4626, batch acc 0.5992
11:11:45.530   Training iter 500, batch loss 0.4622, batch acc 0.5912
11:11:46.080   Training iter 550, batch loss 0.4623, batch acc 0.5984
11:11:46.635   Training iter 600, batch loss 0.4628, batch acc 0.5864
11:11:46.637 Training @ 482 epoch...
11:11:47.169   Training iter 50, batch loss 0.4626, batch acc 0.5834
11:11:47.682   Training iter 100, batch loss 0.4624, batch acc 0.5834
11:11:48.216   Training iter 150, batch loss 0.4622, batch acc 0.5952
11:11:48.760   Training iter 200, batch loss 0.4624, batch acc 0.5896
11:11:49.288   Training iter 250, batch loss 0.4627, batch acc 0.5830
11:11:49.849   Training iter 300, batch loss 0.4625, batch acc 0.5780
11:11:50.460   Training iter 350, batch loss 0.4625, batch acc 0.5898
11:11:51.016   Training iter 400, batch loss 0.4624, batch acc 0.5900
11:11:51.548   Training iter 450, batch loss 0.4622, batch acc 0.5934
11:11:52.148   Training iter 500, batch loss 0.4625, batch acc 0.5938
11:11:52.686   Training iter 550, batch loss 0.4621, batch acc 0.6040
11:11:53.218   Training iter 600, batch loss 0.4630, batch acc 0.5968
11:11:53.219 Training @ 483 epoch...
11:11:53.760   Training iter 50, batch loss 0.4626, batch acc 0.5854
11:11:54.304   Training iter 100, batch loss 0.4622, batch acc 0.5916
11:11:54.864   Training iter 150, batch loss 0.4621, batch acc 0.5878
11:11:55.436   Training iter 200, batch loss 0.4628, batch acc 0.5894
11:11:55.978   Training iter 250, batch loss 0.4616, batch acc 0.6070
11:11:56.530   Training iter 300, batch loss 0.4630, batch acc 0.5866
11:11:57.073   Training iter 350, batch loss 0.4624, batch acc 0.5932
11:11:57.678   Training iter 400, batch loss 0.4625, batch acc 0.5820
11:11:58.229   Training iter 450, batch loss 0.4632, batch acc 0.5826
11:11:58.775   Training iter 500, batch loss 0.4620, batch acc 0.5872
11:11:59.336   Training iter 550, batch loss 0.4624, batch acc 0.5920
11:11:59.950   Training iter 600, batch loss 0.4624, batch acc 0.5950
11:11:59.952 Training @ 484 epoch...
11:12:00.506   Training iter 50, batch loss 0.4628, batch acc 0.5870
11:12:01.077   Training iter 100, batch loss 0.4621, batch acc 0.5940
11:12:01.791   Training iter 150, batch loss 0.4620, batch acc 0.5962
11:12:02.542   Training iter 200, batch loss 0.4627, batch acc 0.5956
11:12:03.266   Training iter 250, batch loss 0.4628, batch acc 0.5852
11:12:03.997   Training iter 300, batch loss 0.4626, batch acc 0.5900
11:12:04.721   Training iter 350, batch loss 0.4622, batch acc 0.5982
11:12:05.456   Training iter 400, batch loss 0.4626, batch acc 0.5868
11:12:06.077   Training iter 450, batch loss 0.4625, batch acc 0.5864
11:12:06.628   Training iter 500, batch loss 0.4626, batch acc 0.5898
11:12:07.193   Training iter 550, batch loss 0.4621, batch acc 0.5872
11:12:07.760   Training iter 600, batch loss 0.4622, batch acc 0.5858
11:12:07.762 Training @ 485 epoch...
11:12:08.309   Training iter 50, batch loss 0.4623, batch acc 0.5936
11:12:08.804   Training iter 100, batch loss 0.4628, batch acc 0.5824
11:12:09.314   Training iter 150, batch loss 0.4620, batch acc 0.5922
11:12:09.846   Training iter 200, batch loss 0.4628, batch acc 0.5906
11:12:10.370   Training iter 250, batch loss 0.4622, batch acc 0.5882
11:12:10.850   Training iter 300, batch loss 0.4622, batch acc 0.6008
11:12:11.355   Training iter 350, batch loss 0.4624, batch acc 0.5882
11:12:11.859   Training iter 400, batch loss 0.4624, batch acc 0.5884
11:12:12.380   Training iter 450, batch loss 0.4628, batch acc 0.5846
11:12:12.903   Training iter 500, batch loss 0.4620, batch acc 0.5964
11:12:13.458   Training iter 550, batch loss 0.4623, batch acc 0.5958
11:12:14.018   Training iter 600, batch loss 0.4630, batch acc 0.5786
11:12:14.020 Testing @ 485 epoch...
11:12:14.058     Testing, total mean loss 0.46194, total acc 0.60430
11:12:14.058 Training @ 486 epoch...
11:12:14.625   Training iter 50, batch loss 0.4630, batch acc 0.5820
11:12:15.199   Training iter 100, batch loss 0.4622, batch acc 0.5976
11:12:15.788   Training iter 150, batch loss 0.4629, batch acc 0.5822
11:12:16.371   Training iter 200, batch loss 0.4620, batch acc 0.5968
11:12:16.942   Training iter 250, batch loss 0.4626, batch acc 0.5924
11:12:17.523   Training iter 300, batch loss 0.4617, batch acc 0.5974
11:12:18.113   Training iter 350, batch loss 0.4622, batch acc 0.5890
11:12:18.710   Training iter 400, batch loss 0.4623, batch acc 0.5974
11:12:19.251   Training iter 450, batch loss 0.4629, batch acc 0.5872
11:12:19.736   Training iter 500, batch loss 0.4624, batch acc 0.5814
11:12:20.232   Training iter 550, batch loss 0.4627, batch acc 0.5862
11:12:20.707   Training iter 600, batch loss 0.4622, batch acc 0.5920
11:12:20.708 Training @ 487 epoch...
11:12:21.206   Training iter 50, batch loss 0.4621, batch acc 0.5934
11:12:21.733   Training iter 100, batch loss 0.4628, batch acc 0.5840
11:12:22.233   Training iter 150, batch loss 0.4621, batch acc 0.5992
11:12:22.734   Training iter 200, batch loss 0.4625, batch acc 0.5914
11:12:23.205   Training iter 250, batch loss 0.4620, batch acc 0.5988
11:12:23.687   Training iter 300, batch loss 0.4623, batch acc 0.5830
11:12:24.206   Training iter 350, batch loss 0.4622, batch acc 0.5970
11:12:24.750   Training iter 400, batch loss 0.4625, batch acc 0.5956
11:12:25.300   Training iter 450, batch loss 0.4633, batch acc 0.5786
11:12:25.801   Training iter 500, batch loss 0.4626, batch acc 0.5784
11:12:26.308   Training iter 550, batch loss 0.4619, batch acc 0.5960
11:12:26.814   Training iter 600, batch loss 0.4625, batch acc 0.5836
11:12:26.816 Training @ 488 epoch...
11:12:27.331   Training iter 50, batch loss 0.4621, batch acc 0.6076
11:12:27.866   Training iter 100, batch loss 0.4623, batch acc 0.5932
11:12:28.414   Training iter 150, batch loss 0.4625, batch acc 0.5822
11:12:28.960   Training iter 200, batch loss 0.4630, batch acc 0.5770
11:12:29.546   Training iter 250, batch loss 0.4624, batch acc 0.5834
11:12:30.132   Training iter 300, batch loss 0.4625, batch acc 0.5856
11:12:30.711   Training iter 350, batch loss 0.4626, batch acc 0.5850
11:12:31.275   Training iter 400, batch loss 0.4619, batch acc 0.6014
11:12:31.818   Training iter 450, batch loss 0.4623, batch acc 0.5914
11:12:32.366   Training iter 500, batch loss 0.4624, batch acc 0.5960
11:12:32.924   Training iter 550, batch loss 0.4623, batch acc 0.5912
11:12:33.488   Training iter 600, batch loss 0.4625, batch acc 0.5882
11:12:33.490 Training @ 489 epoch...
11:12:34.081   Training iter 50, batch loss 0.4625, batch acc 0.5870
11:12:34.678   Training iter 100, batch loss 0.4621, batch acc 0.5900
11:12:35.214   Training iter 150, batch loss 0.4622, batch acc 0.5912
11:12:35.736   Training iter 200, batch loss 0.4630, batch acc 0.5852
11:12:36.254   Training iter 250, batch loss 0.4625, batch acc 0.5854
11:12:36.765   Training iter 300, batch loss 0.4626, batch acc 0.5808
11:12:37.240   Training iter 350, batch loss 0.4628, batch acc 0.5938
11:12:37.675   Training iter 400, batch loss 0.4623, batch acc 0.5994
11:12:38.157   Training iter 450, batch loss 0.4627, batch acc 0.5858
11:12:38.620   Training iter 500, batch loss 0.4623, batch acc 0.5888
11:12:39.093   Training iter 550, batch loss 0.4621, batch acc 0.6026
11:12:39.579   Training iter 600, batch loss 0.4619, batch acc 0.5908
11:12:39.580 Training @ 490 epoch...
11:12:40.088   Training iter 50, batch loss 0.4623, batch acc 0.5924
11:12:40.568   Training iter 100, batch loss 0.4624, batch acc 0.5918
11:12:41.009   Training iter 150, batch loss 0.4618, batch acc 0.5914
11:12:41.485   Training iter 200, batch loss 0.4623, batch acc 0.5904
11:12:41.932   Training iter 250, batch loss 0.4626, batch acc 0.5876
11:12:42.400   Training iter 300, batch loss 0.4624, batch acc 0.5960
11:12:42.904   Training iter 350, batch loss 0.4626, batch acc 0.5892
11:12:43.429   Training iter 400, batch loss 0.4627, batch acc 0.5902
11:12:43.969   Training iter 450, batch loss 0.4626, batch acc 0.5790
11:12:44.481   Training iter 500, batch loss 0.4620, batch acc 0.5922
11:12:45.012   Training iter 550, batch loss 0.4629, batch acc 0.5792
11:12:45.553   Training iter 600, batch loss 0.4622, batch acc 0.6000
11:12:45.555 Testing @ 490 epoch...
11:12:45.590     Testing, total mean loss 0.46191, total acc 0.60430
11:12:45.590 Training @ 491 epoch...
11:12:46.064   Training iter 50, batch loss 0.4623, batch acc 0.5900
11:12:46.597   Training iter 100, batch loss 0.4626, batch acc 0.5882
11:12:47.109   Training iter 150, batch loss 0.4624, batch acc 0.5832
11:12:47.627   Training iter 200, batch loss 0.4622, batch acc 0.5978
11:12:48.136   Training iter 250, batch loss 0.4628, batch acc 0.5790
11:12:48.653   Training iter 300, batch loss 0.4628, batch acc 0.5832
11:12:49.157   Training iter 350, batch loss 0.4622, batch acc 0.5936
11:12:49.638   Training iter 400, batch loss 0.4621, batch acc 0.6024
11:12:50.137   Training iter 450, batch loss 0.4622, batch acc 0.5942
11:12:50.642   Training iter 500, batch loss 0.4623, batch acc 0.5958
11:12:51.145   Training iter 550, batch loss 0.4623, batch acc 0.5944
11:12:51.681   Training iter 600, batch loss 0.4625, batch acc 0.5798
11:12:51.683 Training @ 492 epoch...
11:12:52.257   Training iter 50, batch loss 0.4626, batch acc 0.5908
11:12:52.794   Training iter 100, batch loss 0.4627, batch acc 0.5836
11:12:53.338   Training iter 150, batch loss 0.4626, batch acc 0.5832
11:12:53.877   Training iter 200, batch loss 0.4623, batch acc 0.5852
11:12:54.410   Training iter 250, batch loss 0.4626, batch acc 0.5858
11:12:54.950   Training iter 300, batch loss 0.4623, batch acc 0.5980
11:12:55.500   Training iter 350, batch loss 0.4620, batch acc 0.6084
11:12:56.037   Training iter 400, batch loss 0.4624, batch acc 0.6020
11:12:56.568   Training iter 450, batch loss 0.4622, batch acc 0.5854
11:12:57.089   Training iter 500, batch loss 0.4627, batch acc 0.5828
11:12:57.580   Training iter 550, batch loss 0.4621, batch acc 0.5960
11:12:58.084   Training iter 600, batch loss 0.4625, batch acc 0.5806
11:12:58.086 Training @ 493 epoch...
11:12:58.603   Training iter 50, batch loss 0.4631, batch acc 0.5840
11:12:59.121   Training iter 100, batch loss 0.4621, batch acc 0.5930
11:12:59.607   Training iter 150, batch loss 0.4620, batch acc 0.6018
11:13:00.108   Training iter 200, batch loss 0.4616, batch acc 0.6088
11:13:00.612   Training iter 250, batch loss 0.4622, batch acc 0.5916
11:13:01.137   Training iter 300, batch loss 0.4623, batch acc 0.5880
11:13:01.671   Training iter 350, batch loss 0.4629, batch acc 0.5836
11:13:02.247   Training iter 400, batch loss 0.4620, batch acc 0.5964
11:13:02.808   Training iter 450, batch loss 0.4624, batch acc 0.5950
11:13:03.372   Training iter 500, batch loss 0.4624, batch acc 0.5866
11:13:03.915   Training iter 550, batch loss 0.4629, batch acc 0.5778
11:13:04.467   Training iter 600, batch loss 0.4627, batch acc 0.5732
11:13:04.469 Training @ 494 epoch...
11:13:05.021   Training iter 50, batch loss 0.4630, batch acc 0.5886
11:13:05.569   Training iter 100, batch loss 0.4619, batch acc 0.5974
11:13:06.120   Training iter 150, batch loss 0.4624, batch acc 0.5806
11:13:06.664   Training iter 200, batch loss 0.4627, batch acc 0.5846
11:13:07.216   Training iter 250, batch loss 0.4622, batch acc 0.5910
11:13:07.738   Training iter 300, batch loss 0.4629, batch acc 0.5924
11:13:08.263   Training iter 350, batch loss 0.4625, batch acc 0.5848
11:13:08.777   Training iter 400, batch loss 0.4629, batch acc 0.5834
11:13:09.296   Training iter 450, batch loss 0.4619, batch acc 0.5966
11:13:09.818   Training iter 500, batch loss 0.4619, batch acc 0.5984
11:13:10.343   Training iter 550, batch loss 0.4620, batch acc 0.5984
11:13:10.853   Training iter 600, batch loss 0.4624, batch acc 0.5886
11:13:10.855 Training @ 495 epoch...
11:13:11.355   Training iter 50, batch loss 0.4621, batch acc 0.5920
11:13:11.850   Training iter 100, batch loss 0.4620, batch acc 0.5884
11:13:12.337   Training iter 150, batch loss 0.4625, batch acc 0.5886
11:13:12.833   Training iter 200, batch loss 0.4620, batch acc 0.5984
11:13:13.355   Training iter 250, batch loss 0.4623, batch acc 0.5758
11:13:13.889   Training iter 300, batch loss 0.4622, batch acc 0.5994
11:13:14.412   Training iter 350, batch loss 0.4628, batch acc 0.5862
11:13:14.932   Training iter 400, batch loss 0.4626, batch acc 0.5892
11:13:15.450   Training iter 450, batch loss 0.4627, batch acc 0.5922
11:13:15.985   Training iter 500, batch loss 0.4627, batch acc 0.5860
11:13:16.510   Training iter 550, batch loss 0.4622, batch acc 0.5818
11:13:17.050   Training iter 600, batch loss 0.4624, batch acc 0.6042
11:13:17.051 Testing @ 495 epoch...
11:13:17.088     Testing, total mean loss 0.46188, total acc 0.60460
11:13:17.088 Training @ 496 epoch...
11:13:17.652   Training iter 50, batch loss 0.4621, batch acc 0.5904
11:13:18.220   Training iter 100, batch loss 0.4622, batch acc 0.6044
11:13:18.771   Training iter 150, batch loss 0.4626, batch acc 0.5830
11:13:19.321   Training iter 200, batch loss 0.4625, batch acc 0.5838
11:13:19.869   Training iter 250, batch loss 0.4624, batch acc 0.5834
11:13:20.435   Training iter 300, batch loss 0.4626, batch acc 0.5840
11:13:20.999   Training iter 350, batch loss 0.4626, batch acc 0.5808
11:13:21.552   Training iter 400, batch loss 0.4624, batch acc 0.5888
11:13:22.101   Training iter 450, batch loss 0.4627, batch acc 0.5876
11:13:22.653   Training iter 500, batch loss 0.4623, batch acc 0.5968
11:13:23.221   Training iter 550, batch loss 0.4626, batch acc 0.5964
11:13:23.760   Training iter 600, batch loss 0.4617, batch acc 0.6030
11:13:23.762 Training @ 497 epoch...
11:13:24.319   Training iter 50, batch loss 0.4625, batch acc 0.5852
11:13:24.906   Training iter 100, batch loss 0.4625, batch acc 0.5834
11:13:25.496   Training iter 150, batch loss 0.4625, batch acc 0.5862
11:13:26.088   Training iter 200, batch loss 0.4626, batch acc 0.5936
11:13:26.629   Training iter 250, batch loss 0.4627, batch acc 0.5916
11:13:27.154   Training iter 300, batch loss 0.4624, batch acc 0.5926
11:13:27.690   Training iter 350, batch loss 0.4625, batch acc 0.5908
11:13:28.246   Training iter 400, batch loss 0.4621, batch acc 0.5936
11:13:28.762   Training iter 450, batch loss 0.4619, batch acc 0.6008
11:13:29.293   Training iter 500, batch loss 0.4621, batch acc 0.5914
11:13:29.823   Training iter 550, batch loss 0.4622, batch acc 0.5920
11:13:30.347   Training iter 600, batch loss 0.4624, batch acc 0.5850
11:13:30.349 Training @ 498 epoch...
11:13:30.875   Training iter 50, batch loss 0.4620, batch acc 0.5972
11:13:31.389   Training iter 100, batch loss 0.4626, batch acc 0.5850
11:13:31.896   Training iter 150, batch loss 0.4623, batch acc 0.5978
11:13:32.403   Training iter 200, batch loss 0.4624, batch acc 0.5900
11:13:32.936   Training iter 250, batch loss 0.4625, batch acc 0.5944
11:13:33.472   Training iter 300, batch loss 0.4621, batch acc 0.5856
11:13:34.005   Training iter 350, batch loss 0.4627, batch acc 0.5822
11:13:34.498   Training iter 400, batch loss 0.4624, batch acc 0.5944
11:13:35.022   Training iter 450, batch loss 0.4628, batch acc 0.5778
11:13:35.556   Training iter 500, batch loss 0.4623, batch acc 0.5982
11:13:36.088   Training iter 550, batch loss 0.4619, batch acc 0.5976
11:13:36.616   Training iter 600, batch loss 0.4623, batch acc 0.5830
11:13:36.617 Training @ 499 epoch...
11:13:37.137   Training iter 50, batch loss 0.4624, batch acc 0.5864
11:13:37.664   Training iter 100, batch loss 0.4625, batch acc 0.5912
11:13:38.168   Training iter 150, batch loss 0.4623, batch acc 0.5902
11:13:38.699   Training iter 200, batch loss 0.4624, batch acc 0.5904
11:13:39.233   Training iter 250, batch loss 0.4620, batch acc 0.5948
11:13:39.755   Training iter 300, batch loss 0.4624, batch acc 0.5874
11:13:40.279   Training iter 350, batch loss 0.4622, batch acc 0.5916
11:13:40.799   Training iter 400, batch loss 0.4630, batch acc 0.5828
11:13:41.324   Training iter 450, batch loss 0.4626, batch acc 0.5862
11:13:41.853   Training iter 500, batch loss 0.4621, batch acc 0.5996
11:13:42.382   Training iter 550, batch loss 0.4621, batch acc 0.5930
11:13:42.920   Training iter 600, batch loss 0.4624, batch acc 0.5892
======================================================
11:13:42.922 Testing @ final epoch...
11:13:42.958     Testing, total mean loss 0.46186, total acc 0.60500
training time: 3112 seconds
