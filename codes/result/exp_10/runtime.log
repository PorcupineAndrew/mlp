======================================================
learning_rate: 1.0
weight_decay: 1e-05
momentum: 0.1
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Relu Lin-10-10 Relu Lin-10-10 Sigm
loss: Softmax
result dir: ./result/exp_10
======================================================
17:21:31.721 Training @ 0 epoch...
17:21:32.291   Training iter 50, batch loss 2.3022, batch acc 0.1062
17:21:32.810   Training iter 100, batch loss 2.3013, batch acc 0.1108
17:21:33.373   Training iter 150, batch loss 2.3017, batch acc 0.1098
17:21:33.898   Training iter 200, batch loss 2.3023, batch acc 0.1044
17:21:34.435   Training iter 250, batch loss 2.3015, batch acc 0.1156
17:21:34.954   Training iter 300, batch loss 2.2753, batch acc 0.1272
17:21:35.486   Training iter 350, batch loss 2.0830, batch acc 0.1888
17:21:36.012   Training iter 400, batch loss 2.0249, batch acc 0.1910
17:21:36.538   Training iter 450, batch loss 1.9939, batch acc 0.1866
17:21:37.070   Training iter 500, batch loss 1.9665, batch acc 0.2058
17:21:37.609   Training iter 550, batch loss 1.9614, batch acc 0.2060
17:21:38.150   Training iter 600, batch loss 1.9735, batch acc 0.2028
17:21:38.151 Testing @ 0 epoch...
17:21:38.200     Testing, total mean loss 1.95367, total acc 0.20320
17:21:38.200 Plot @ 0 epoch...
17:21:38.200 Training @ 1 epoch...
17:21:38.740   Training iter 50, batch loss 1.9540, batch acc 0.2018
17:21:39.296   Training iter 100, batch loss 1.9556, batch acc 0.2086
17:21:39.851   Training iter 150, batch loss 1.9560, batch acc 0.2084
17:21:40.403   Training iter 200, batch loss 1.9519, batch acc 0.2450
17:21:40.947   Training iter 250, batch loss 1.9564, batch acc 0.2214
17:21:41.505   Training iter 300, batch loss 1.9390, batch acc 0.2072
17:21:42.085   Training iter 350, batch loss 1.9403, batch acc 0.1854
17:21:42.626   Training iter 400, batch loss 1.9259, batch acc 0.1956
17:21:43.153   Training iter 450, batch loss 1.9220, batch acc 0.2118
17:21:43.671   Training iter 500, batch loss 1.9307, batch acc 0.2240
17:21:44.197   Training iter 550, batch loss 1.9299, batch acc 0.2100
17:21:44.719   Training iter 600, batch loss 1.9263, batch acc 0.1682
17:21:44.720 Training @ 2 epoch...
17:21:45.242   Training iter 50, batch loss 1.9242, batch acc 0.1920
17:21:45.762   Training iter 100, batch loss 1.9197, batch acc 0.1948
17:21:46.300   Training iter 150, batch loss 1.9253, batch acc 0.2028
17:21:46.890   Training iter 200, batch loss 1.9129, batch acc 0.1912
17:21:47.501   Training iter 250, batch loss 1.9104, batch acc 0.1950
17:21:48.090   Training iter 300, batch loss 1.9047, batch acc 0.2038
17:21:48.686   Training iter 350, batch loss 1.9108, batch acc 0.1940
17:21:49.268   Training iter 400, batch loss 1.9195, batch acc 0.1828
17:21:49.859   Training iter 450, batch loss 1.9051, batch acc 0.2010
17:21:50.463   Training iter 500, batch loss 1.9028, batch acc 0.2036
17:21:51.086   Training iter 550, batch loss 1.9067, batch acc 0.1788
17:21:51.689   Training iter 600, batch loss 1.8977, batch acc 0.2056
17:21:51.691 Training @ 3 epoch...
17:21:52.288   Training iter 50, batch loss 1.8996, batch acc 0.2076
17:21:52.853   Training iter 100, batch loss 1.9067, batch acc 0.1970
17:21:53.415   Training iter 150, batch loss 1.9147, batch acc 0.2220
17:21:53.947   Training iter 200, batch loss 1.8992, batch acc 0.1878
17:21:54.507   Training iter 250, batch loss 1.9025, batch acc 0.2060
17:21:55.053   Training iter 300, batch loss 1.8972, batch acc 0.2050
17:21:55.608   Training iter 350, batch loss 1.8991, batch acc 0.1850
17:21:56.180   Training iter 400, batch loss 1.8958, batch acc 0.1948
17:21:56.722   Training iter 450, batch loss 1.8961, batch acc 0.2062
17:21:57.275   Training iter 500, batch loss 1.8961, batch acc 0.1936
17:21:57.841   Training iter 550, batch loss 1.8987, batch acc 0.1968
17:21:58.397   Training iter 600, batch loss 1.8855, batch acc 0.2072
17:21:58.398 Training @ 4 epoch...
17:21:58.952   Training iter 50, batch loss 1.8906, batch acc 0.2204
17:21:59.517   Training iter 100, batch loss 1.8987, batch acc 0.1852
17:22:00.070   Training iter 150, batch loss 1.8852, batch acc 0.2072
17:22:00.636   Training iter 200, batch loss 1.8808, batch acc 0.2272
17:22:01.200   Training iter 250, batch loss 1.8862, batch acc 0.2244
17:22:01.801   Training iter 300, batch loss 1.8656, batch acc 0.2782
17:22:02.363   Training iter 350, batch loss 1.8442, batch acc 0.3276
17:22:02.889   Training iter 400, batch loss 1.8367, batch acc 0.3332
17:22:03.410   Training iter 450, batch loss 1.8205, batch acc 0.3362
17:22:03.963   Training iter 500, batch loss 1.8238, batch acc 0.3362
17:22:04.512   Training iter 550, batch loss 1.8020, batch acc 0.3124
17:22:05.070   Training iter 600, batch loss 1.7876, batch acc 0.3268
17:22:05.072 Training @ 5 epoch...
17:22:05.648   Training iter 50, batch loss 1.7691, batch acc 0.3172
17:22:06.192   Training iter 100, batch loss 1.7672, batch acc 0.3198
17:22:06.742   Training iter 150, batch loss 1.7608, batch acc 0.3276
17:22:07.306   Training iter 200, batch loss 1.7594, batch acc 0.3122
17:22:07.864   Training iter 250, batch loss 1.7538, batch acc 0.3250
17:22:08.413   Training iter 300, batch loss 1.7438, batch acc 0.3250
17:22:08.956   Training iter 350, batch loss 1.7333, batch acc 0.3444
17:22:09.509   Training iter 400, batch loss 1.7461, batch acc 0.3370
17:22:10.041   Training iter 450, batch loss 1.7292, batch acc 0.3398
17:22:10.576   Training iter 500, batch loss 1.7189, batch acc 0.3688
17:22:11.105   Training iter 550, batch loss 1.7216, batch acc 0.3766
17:22:11.625   Training iter 600, batch loss 1.7207, batch acc 0.3582
17:22:11.627 Testing @ 5 epoch...
17:22:11.676     Testing, total mean loss 1.71991, total acc 0.34660
17:22:11.676 Training @ 6 epoch...
17:22:12.208   Training iter 50, batch loss 1.7172, batch acc 0.3556
17:22:12.748   Training iter 100, batch loss 1.7201, batch acc 0.3690
17:22:13.280   Training iter 150, batch loss 1.7095, batch acc 0.3786
17:22:13.765   Training iter 200, batch loss 1.7095, batch acc 0.3802
17:22:14.261   Training iter 250, batch loss 1.7150, batch acc 0.3934
17:22:14.778   Training iter 300, batch loss 1.7039, batch acc 0.4030
17:22:15.290   Training iter 350, batch loss 1.7144, batch acc 0.3996
17:22:15.813   Training iter 400, batch loss 1.7139, batch acc 0.4052
17:22:16.332   Training iter 450, batch loss 1.7106, batch acc 0.4132
17:22:16.838   Training iter 500, batch loss 1.7043, batch acc 0.4376
17:22:17.333   Training iter 550, batch loss 1.7050, batch acc 0.4340
17:22:17.832   Training iter 600, batch loss 1.7015, batch acc 0.4526
17:22:17.834 Training @ 7 epoch...
17:22:18.348   Training iter 50, batch loss 1.6987, batch acc 0.4312
17:22:18.869   Training iter 100, batch loss 1.7019, batch acc 0.4310
17:22:19.388   Training iter 150, batch loss 1.6983, batch acc 0.4526
17:22:19.897   Training iter 200, batch loss 1.6938, batch acc 0.4394
17:22:20.405   Training iter 250, batch loss 1.6898, batch acc 0.4446
17:22:20.923   Training iter 300, batch loss 1.6971, batch acc 0.4258
17:22:21.432   Training iter 350, batch loss 1.6876, batch acc 0.4544
17:22:21.930   Training iter 400, batch loss 1.6903, batch acc 0.4634
17:22:22.430   Training iter 450, batch loss 1.6931, batch acc 0.4768
17:22:22.943   Training iter 500, batch loss 1.6932, batch acc 0.4604
17:22:23.446   Training iter 550, batch loss 1.6944, batch acc 0.4716
17:22:24.006   Training iter 600, batch loss 1.6857, batch acc 0.4920
17:22:24.008 Training @ 8 epoch...
17:22:24.559   Training iter 50, batch loss 1.6882, batch acc 0.4662
17:22:25.098   Training iter 100, batch loss 1.6817, batch acc 0.4620
17:22:25.643   Training iter 150, batch loss 1.6841, batch acc 0.4558
17:22:26.175   Training iter 200, batch loss 1.6750, batch acc 0.4632
17:22:26.706   Training iter 250, batch loss 1.6772, batch acc 0.4622
17:22:27.238   Training iter 300, batch loss 1.6800, batch acc 0.4858
17:22:27.793   Training iter 350, batch loss 1.6698, batch acc 0.4824
17:22:28.347   Training iter 400, batch loss 1.6660, batch acc 0.4788
17:22:28.897   Training iter 450, batch loss 1.6697, batch acc 0.4704
17:22:29.462   Training iter 500, batch loss 1.6594, batch acc 0.4932
17:22:30.011   Training iter 550, batch loss 1.6573, batch acc 0.5198
17:22:30.574   Training iter 600, batch loss 1.6548, batch acc 0.5448
17:22:30.576 Training @ 9 epoch...
17:22:31.150   Training iter 50, batch loss 1.6500, batch acc 0.5172
17:22:31.715   Training iter 100, batch loss 1.6461, batch acc 0.5304
17:22:32.257   Training iter 150, batch loss 1.6572, batch acc 0.5412
17:22:32.793   Training iter 200, batch loss 1.6282, batch acc 0.5976
17:22:33.347   Training iter 250, batch loss 1.6317, batch acc 0.6152
17:22:33.880   Training iter 300, batch loss 1.6162, batch acc 0.5972
17:22:34.457   Training iter 350, batch loss 1.6215, batch acc 0.5914
17:22:35.021   Training iter 400, batch loss 1.6225, batch acc 0.6118
17:22:35.580   Training iter 450, batch loss 1.6234, batch acc 0.6162
17:22:36.149   Training iter 500, batch loss 1.6149, batch acc 0.6126
17:22:36.698   Training iter 550, batch loss 1.6166, batch acc 0.6144
17:22:37.274   Training iter 600, batch loss 1.6163, batch acc 0.6112
17:22:37.276 Training @ 10 epoch...
17:22:37.867   Training iter 50, batch loss 1.6178, batch acc 0.6116
17:22:38.461   Training iter 100, batch loss 1.6143, batch acc 0.5966
17:22:39.055   Training iter 150, batch loss 1.6092, batch acc 0.6198
17:22:39.638   Training iter 200, batch loss 1.6065, batch acc 0.6278
17:22:40.215   Training iter 250, batch loss 1.6031, batch acc 0.6312
17:22:40.776   Training iter 300, batch loss 1.6043, batch acc 0.6222
17:22:41.335   Training iter 350, batch loss 1.6027, batch acc 0.6212
17:22:41.890   Training iter 400, batch loss 1.6107, batch acc 0.6824
17:22:42.441   Training iter 450, batch loss 1.6128, batch acc 0.6674
17:22:42.989   Training iter 500, batch loss 1.6080, batch acc 0.6606
17:22:43.523   Training iter 550, batch loss 1.6029, batch acc 0.6720
17:22:44.049   Training iter 600, batch loss 1.5983, batch acc 0.6452
17:22:44.051 Testing @ 10 epoch...
17:22:44.099     Testing, total mean loss 1.60654, total acc 0.62080
17:22:44.100 Training @ 11 epoch...
17:22:44.616   Training iter 50, batch loss 1.6114, batch acc 0.6762
17:22:45.137   Training iter 100, batch loss 1.6041, batch acc 0.6692
17:22:45.633   Training iter 150, batch loss 1.6011, batch acc 0.6804
17:22:46.109   Training iter 200, batch loss 1.5950, batch acc 0.7134
17:22:46.586   Training iter 250, batch loss 1.5942, batch acc 0.7132
17:22:47.057   Training iter 300, batch loss 1.5900, batch acc 0.7356
17:22:47.542   Training iter 350, batch loss 1.5911, batch acc 0.7342
17:22:48.035   Training iter 400, batch loss 1.5985, batch acc 0.7534
17:22:48.551   Training iter 450, batch loss 1.5879, batch acc 0.7608
17:22:49.048   Training iter 500, batch loss 1.5935, batch acc 0.7432
17:22:49.561   Training iter 550, batch loss 1.5976, batch acc 0.7346
17:22:50.081   Training iter 600, batch loss 1.5919, batch acc 0.7564
17:22:50.083 Training @ 12 epoch...
17:22:50.603   Training iter 50, batch loss 1.5896, batch acc 0.7578
17:22:51.116   Training iter 100, batch loss 1.5944, batch acc 0.7530
17:22:51.644   Training iter 150, batch loss 1.5826, batch acc 0.7710
17:22:52.161   Training iter 200, batch loss 1.5915, batch acc 0.7516
17:22:52.670   Training iter 250, batch loss 1.5880, batch acc 0.7594
17:22:53.176   Training iter 300, batch loss 1.5874, batch acc 0.7524
17:22:53.692   Training iter 350, batch loss 1.5745, batch acc 0.7912
17:22:54.205   Training iter 400, batch loss 1.5833, batch acc 0.7766
17:22:54.725   Training iter 450, batch loss 1.5863, batch acc 0.7816
17:22:55.229   Training iter 500, batch loss 1.5770, batch acc 0.7912
17:22:55.742   Training iter 550, batch loss 1.5785, batch acc 0.7804
17:22:56.247   Training iter 600, batch loss 1.5813, batch acc 0.7650
17:22:56.248 Training @ 13 epoch...
17:22:56.774   Training iter 50, batch loss 1.5777, batch acc 0.7790
17:22:57.288   Training iter 100, batch loss 1.5804, batch acc 0.7758
17:22:57.789   Training iter 150, batch loss 1.5797, batch acc 0.7772
17:22:58.301   Training iter 200, batch loss 1.5743, batch acc 0.7804
17:22:58.804   Training iter 250, batch loss 1.5823, batch acc 0.7706
17:22:59.320   Training iter 300, batch loss 1.5785, batch acc 0.7808
17:22:59.850   Training iter 350, batch loss 1.5819, batch acc 0.7870
17:23:00.382   Training iter 400, batch loss 1.5780, batch acc 0.7770
17:23:00.943   Training iter 450, batch loss 1.5676, batch acc 0.8014
17:23:01.503   Training iter 500, batch loss 1.5747, batch acc 0.7850
17:23:02.079   Training iter 550, batch loss 1.5765, batch acc 0.7830
17:23:02.634   Training iter 600, batch loss 1.5727, batch acc 0.7802
17:23:02.636 Training @ 14 epoch...
17:23:03.192   Training iter 50, batch loss 1.5786, batch acc 0.7792
17:23:03.735   Training iter 100, batch loss 1.5662, batch acc 0.8046
17:23:04.282   Training iter 150, batch loss 1.5680, batch acc 0.7960
17:23:04.805   Training iter 200, batch loss 1.5629, batch acc 0.7870
17:23:05.329   Training iter 250, batch loss 1.5767, batch acc 0.7704
17:23:05.885   Training iter 300, batch loss 1.5799, batch acc 0.7768
17:23:06.476   Training iter 350, batch loss 1.5779, batch acc 0.7880
17:23:07.038   Training iter 400, batch loss 1.5756, batch acc 0.7830
17:23:07.595   Training iter 450, batch loss 1.5748, batch acc 0.7912
17:23:08.167   Training iter 500, batch loss 1.5734, batch acc 0.7868
17:23:08.747   Training iter 550, batch loss 1.5736, batch acc 0.7832
17:23:09.308   Training iter 600, batch loss 1.5650, batch acc 0.7966
17:23:09.309 Training @ 15 epoch...
17:23:09.871   Training iter 50, batch loss 1.5656, batch acc 0.7928
17:23:10.440   Training iter 100, batch loss 1.5695, batch acc 0.7880
17:23:11.011   Training iter 150, batch loss 1.5661, batch acc 0.7794
17:23:11.601   Training iter 200, batch loss 1.5641, batch acc 0.7992
17:23:12.177   Training iter 250, batch loss 1.5733, batch acc 0.7974
17:23:12.740   Training iter 300, batch loss 1.5704, batch acc 0.7934
17:23:13.317   Training iter 350, batch loss 1.5738, batch acc 0.7902
17:23:13.933   Training iter 400, batch loss 1.5715, batch acc 0.8036
17:23:14.551   Training iter 450, batch loss 1.5788, batch acc 0.7918
17:23:15.134   Training iter 500, batch loss 1.5715, batch acc 0.7936
17:23:15.702   Training iter 550, batch loss 1.5715, batch acc 0.7848
17:23:16.251   Training iter 600, batch loss 1.5633, batch acc 0.8060
17:23:16.253 Testing @ 15 epoch...
17:23:16.303     Testing, total mean loss 1.58582, total acc 0.78030
17:23:16.303 Training @ 16 epoch...
17:23:16.866   Training iter 50, batch loss 1.5660, batch acc 0.8024
17:23:17.409   Training iter 100, batch loss 1.5795, batch acc 0.7782
17:23:17.965   Training iter 150, batch loss 1.5651, batch acc 0.7986
17:23:18.525   Training iter 200, batch loss 1.5682, batch acc 0.8120
17:23:19.087   Training iter 250, batch loss 1.5646, batch acc 0.8044
17:23:19.643   Training iter 300, batch loss 1.5637, batch acc 0.7952
17:23:20.204   Training iter 350, batch loss 1.5598, batch acc 0.8012
17:23:20.760   Training iter 400, batch loss 1.5649, batch acc 0.7980
17:23:21.313   Training iter 450, batch loss 1.5630, batch acc 0.7980
17:23:21.893   Training iter 500, batch loss 1.5757, batch acc 0.7784
17:23:22.483   Training iter 550, batch loss 1.5639, batch acc 0.8176
17:23:23.065   Training iter 600, batch loss 1.5647, batch acc 0.8014
17:23:23.067 Training @ 17 epoch...
17:23:23.656   Training iter 50, batch loss 1.5628, batch acc 0.7986
17:23:24.247   Training iter 100, batch loss 1.5676, batch acc 0.7966
17:23:24.816   Training iter 150, batch loss 1.5612, batch acc 0.8040
17:23:25.381   Training iter 200, batch loss 1.5760, batch acc 0.7846
17:23:25.956   Training iter 250, batch loss 1.5688, batch acc 0.8002
17:23:26.529   Training iter 300, batch loss 1.5606, batch acc 0.8038
17:23:27.100   Training iter 350, batch loss 1.5613, batch acc 0.8158
17:23:27.673   Training iter 400, batch loss 1.5700, batch acc 0.8056
17:23:28.239   Training iter 450, batch loss 1.5619, batch acc 0.8106
17:23:28.771   Training iter 500, batch loss 1.5679, batch acc 0.8066
17:23:29.319   Training iter 550, batch loss 1.5577, batch acc 0.8102
17:23:29.870   Training iter 600, batch loss 1.5662, batch acc 0.8152
17:23:29.872 Training @ 18 epoch...
17:23:30.481   Training iter 50, batch loss 1.5640, batch acc 0.8076
17:23:31.045   Training iter 100, batch loss 1.5624, batch acc 0.8150
17:23:31.618   Training iter 150, batch loss 1.5579, batch acc 0.8280
17:23:32.205   Training iter 200, batch loss 1.5610, batch acc 0.8028
17:23:32.884   Training iter 250, batch loss 1.5690, batch acc 0.8110
17:23:33.541   Training iter 300, batch loss 1.5676, batch acc 0.7984
17:23:34.213   Training iter 350, batch loss 1.5615, batch acc 0.8164
17:23:34.868   Training iter 400, batch loss 1.5626, batch acc 0.8076
17:23:35.552   Training iter 450, batch loss 1.5611, batch acc 0.8088
17:23:36.110   Training iter 500, batch loss 1.5598, batch acc 0.8088
17:23:36.624   Training iter 550, batch loss 1.5603, batch acc 0.8068
17:23:37.118   Training iter 600, batch loss 1.5609, batch acc 0.8328
17:23:37.119 Training @ 19 epoch...
17:23:37.663   Training iter 50, batch loss 1.5539, batch acc 0.8382
17:23:38.217   Training iter 100, batch loss 1.5593, batch acc 0.8062
17:23:38.791   Training iter 150, batch loss 1.5594, batch acc 0.8148
17:23:39.379   Training iter 200, batch loss 1.5519, batch acc 0.8414
17:23:39.948   Training iter 250, batch loss 1.5610, batch acc 0.8082
17:23:40.485   Training iter 300, batch loss 1.5626, batch acc 0.8274
17:23:41.020   Training iter 350, batch loss 1.5603, batch acc 0.8344
17:23:41.542   Training iter 400, batch loss 1.5558, batch acc 0.8184
17:23:42.062   Training iter 450, batch loss 1.5593, batch acc 0.8142
17:23:42.603   Training iter 500, batch loss 1.5582, batch acc 0.8290
17:23:43.160   Training iter 550, batch loss 1.5670, batch acc 0.8072
17:23:43.709   Training iter 600, batch loss 1.5654, batch acc 0.8114
17:23:43.711 Training @ 20 epoch...
17:23:44.238   Training iter 50, batch loss 1.5553, batch acc 0.8104
17:23:44.764   Training iter 100, batch loss 1.5642, batch acc 0.8208
17:23:45.290   Training iter 150, batch loss 1.5561, batch acc 0.8300
17:23:45.842   Training iter 200, batch loss 1.5583, batch acc 0.8056
17:23:46.374   Training iter 250, batch loss 1.5626, batch acc 0.8068
17:23:46.907   Training iter 300, batch loss 1.5599, batch acc 0.8146
17:23:47.436   Training iter 350, batch loss 1.5542, batch acc 0.8522
17:23:47.967   Training iter 400, batch loss 1.5557, batch acc 0.8408
17:23:48.514   Training iter 450, batch loss 1.5601, batch acc 0.8132
17:23:49.030   Training iter 500, batch loss 1.5583, batch acc 0.8272
17:23:49.580   Training iter 550, batch loss 1.5589, batch acc 0.8104
17:23:50.122   Training iter 600, batch loss 1.5583, batch acc 0.8260
17:23:50.124 Testing @ 20 epoch...
17:23:50.173     Testing, total mean loss 1.54952, total acc 0.84490
17:23:50.173 Training @ 21 epoch...
17:23:50.731   Training iter 50, batch loss 1.5523, batch acc 0.8196
17:23:51.252   Training iter 100, batch loss 1.5537, batch acc 0.8136
17:23:51.783   Training iter 150, batch loss 1.5520, batch acc 0.8120
17:23:52.312   Training iter 200, batch loss 1.5566, batch acc 0.8164
17:23:52.870   Training iter 250, batch loss 1.5577, batch acc 0.8160
17:23:53.433   Training iter 300, batch loss 1.5538, batch acc 0.8192
17:23:54.006   Training iter 350, batch loss 1.5527, batch acc 0.8232
17:23:54.615   Training iter 400, batch loss 1.5520, batch acc 0.8334
17:23:55.209   Training iter 450, batch loss 1.5589, batch acc 0.8310
17:23:55.781   Training iter 500, batch loss 1.5643, batch acc 0.8322
17:23:56.349   Training iter 550, batch loss 1.5643, batch acc 0.8226
17:23:56.909   Training iter 600, batch loss 1.5604, batch acc 0.8582
17:23:56.911 Training @ 22 epoch...
17:23:57.459   Training iter 50, batch loss 1.5541, batch acc 0.8246
17:23:57.977   Training iter 100, batch loss 1.5545, batch acc 0.8330
17:23:58.502   Training iter 150, batch loss 1.5512, batch acc 0.8168
17:23:59.028   Training iter 200, batch loss 1.5521, batch acc 0.8354
17:23:59.594   Training iter 250, batch loss 1.5634, batch acc 0.8420
17:24:00.203   Training iter 300, batch loss 1.5585, batch acc 0.8418
17:24:00.799   Training iter 350, batch loss 1.5512, batch acc 0.8646
17:24:01.378   Training iter 400, batch loss 1.5552, batch acc 0.8528
17:24:01.984   Training iter 450, batch loss 1.5567, batch acc 0.8338
17:24:02.586   Training iter 500, batch loss 1.5518, batch acc 0.8292
17:24:03.164   Training iter 550, batch loss 1.5591, batch acc 0.8268
17:24:03.754   Training iter 600, batch loss 1.5554, batch acc 0.8436
17:24:03.755 Training @ 23 epoch...
17:24:04.349   Training iter 50, batch loss 1.5559, batch acc 0.8296
17:24:04.944   Training iter 100, batch loss 1.5459, batch acc 0.8270
17:24:05.516   Training iter 150, batch loss 1.5557, batch acc 0.8248
17:24:06.108   Training iter 200, batch loss 1.5494, batch acc 0.8420
17:24:06.695   Training iter 250, batch loss 1.5547, batch acc 0.8332
17:24:07.276   Training iter 300, batch loss 1.5447, batch acc 0.8438
17:24:07.824   Training iter 350, batch loss 1.5499, batch acc 0.8568
17:24:08.382   Training iter 400, batch loss 1.5593, batch acc 0.8152
17:24:08.920   Training iter 450, batch loss 1.5613, batch acc 0.8222
17:24:09.455   Training iter 500, batch loss 1.5569, batch acc 0.8266
17:24:09.968   Training iter 550, batch loss 1.5557, batch acc 0.8180
17:24:10.513   Training iter 600, batch loss 1.5589, batch acc 0.8316
17:24:10.515 Training @ 24 epoch...
17:24:11.089   Training iter 50, batch loss 1.5525, batch acc 0.8254
17:24:11.672   Training iter 100, batch loss 1.5609, batch acc 0.8100
17:24:12.251   Training iter 150, batch loss 1.5574, batch acc 0.8028
17:24:12.812   Training iter 200, batch loss 1.5510, batch acc 0.8426
17:24:13.371   Training iter 250, batch loss 1.5501, batch acc 0.8296
17:24:13.932   Training iter 300, batch loss 1.5620, batch acc 0.8120
17:24:14.494   Training iter 350, batch loss 1.5439, batch acc 0.8610
17:24:15.043   Training iter 400, batch loss 1.5536, batch acc 0.8526
17:24:15.602   Training iter 450, batch loss 1.5462, batch acc 0.8364
17:24:16.153   Training iter 500, batch loss 1.5575, batch acc 0.8336
17:24:16.669   Training iter 550, batch loss 1.5493, batch acc 0.8302
17:24:17.196   Training iter 600, batch loss 1.5519, batch acc 0.8364
17:24:17.198 Training @ 25 epoch...
17:24:17.710   Training iter 50, batch loss 1.5527, batch acc 0.8656
17:24:18.200   Training iter 100, batch loss 1.5554, batch acc 0.8546
17:24:18.705   Training iter 150, batch loss 1.5616, batch acc 0.8234
17:24:19.209   Training iter 200, batch loss 1.5455, batch acc 0.8312
17:24:19.720   Training iter 250, batch loss 1.5479, batch acc 0.8518
17:24:20.238   Training iter 300, batch loss 1.5570, batch acc 0.8440
17:24:20.758   Training iter 350, batch loss 1.5452, batch acc 0.8480
17:24:21.286   Training iter 400, batch loss 1.5462, batch acc 0.8402
17:24:21.835   Training iter 450, batch loss 1.5541, batch acc 0.8538
17:24:22.367   Training iter 500, batch loss 1.5497, batch acc 0.8786
17:24:22.919   Training iter 550, batch loss 1.5576, batch acc 0.8618
17:24:23.455   Training iter 600, batch loss 1.5480, batch acc 0.8806
17:24:23.457 Testing @ 25 epoch...
17:24:23.506     Testing, total mean loss 1.54550, total acc 0.88970
17:24:23.506 Training @ 26 epoch...
17:24:24.024   Training iter 50, batch loss 1.5470, batch acc 0.8772
17:24:24.550   Training iter 100, batch loss 1.5511, batch acc 0.8778
17:24:25.071   Training iter 150, batch loss 1.5431, batch acc 0.8416
17:24:25.592   Training iter 200, batch loss 1.5539, batch acc 0.8382
17:24:26.110   Training iter 250, batch loss 1.5548, batch acc 0.8492
17:24:26.627   Training iter 300, batch loss 1.5528, batch acc 0.8430
17:24:27.143   Training iter 350, batch loss 1.5569, batch acc 0.8434
17:24:27.667   Training iter 400, batch loss 1.5493, batch acc 0.8620
17:24:28.205   Training iter 450, batch loss 1.5533, batch acc 0.8362
17:24:28.729   Training iter 500, batch loss 1.5550, batch acc 0.8464
17:24:29.308   Training iter 550, batch loss 1.5476, batch acc 0.8444
17:24:29.875   Training iter 600, batch loss 1.5512, batch acc 0.8266
17:24:29.876 Training @ 27 epoch...
17:24:30.456   Training iter 50, batch loss 1.5500, batch acc 0.8398
17:24:31.037   Training iter 100, batch loss 1.5530, batch acc 0.8214
17:24:31.621   Training iter 150, batch loss 1.5481, batch acc 0.8262
17:24:32.185   Training iter 200, batch loss 1.5493, batch acc 0.8346
17:24:32.728   Training iter 250, batch loss 1.5423, batch acc 0.8380
17:24:33.274   Training iter 300, batch loss 1.5524, batch acc 0.8330
17:24:33.792   Training iter 350, batch loss 1.5521, batch acc 0.8204
17:24:34.320   Training iter 400, batch loss 1.5513, batch acc 0.8336
17:24:34.828   Training iter 450, batch loss 1.5486, batch acc 0.8466
17:24:35.350   Training iter 500, batch loss 1.5611, batch acc 0.8716
17:24:35.849   Training iter 550, batch loss 1.5502, batch acc 0.8584
17:24:36.355   Training iter 600, batch loss 1.5501, batch acc 0.8422
17:24:36.357 Training @ 28 epoch...
17:24:36.850   Training iter 50, batch loss 1.5465, batch acc 0.8570
17:24:37.358   Training iter 100, batch loss 1.5581, batch acc 0.8166
17:24:37.886   Training iter 150, batch loss 1.5476, batch acc 0.8378
17:24:38.417   Training iter 200, batch loss 1.5464, batch acc 0.8448
17:24:38.947   Training iter 250, batch loss 1.5519, batch acc 0.8420
17:24:39.486   Training iter 300, batch loss 1.5503, batch acc 0.8322
17:24:40.002   Training iter 350, batch loss 1.5516, batch acc 0.8282
17:24:40.498   Training iter 400, batch loss 1.5547, batch acc 0.8258
17:24:40.995   Training iter 450, batch loss 1.5504, batch acc 0.8318
17:24:41.502   Training iter 500, batch loss 1.5544, batch acc 0.8336
17:24:42.008   Training iter 550, batch loss 1.5500, batch acc 0.8550
17:24:42.549   Training iter 600, batch loss 1.5445, batch acc 0.8538
17:24:42.551 Training @ 29 epoch...
17:24:43.091   Training iter 50, batch loss 1.5459, batch acc 0.8650
17:24:43.637   Training iter 100, batch loss 1.5531, batch acc 0.8544
17:24:44.187   Training iter 150, batch loss 1.5513, batch acc 0.8552
17:24:44.757   Training iter 200, batch loss 1.5485, batch acc 0.8412
17:24:45.326   Training iter 250, batch loss 1.5475, batch acc 0.8526
17:24:45.877   Training iter 300, batch loss 1.5428, batch acc 0.8714
17:24:46.429   Training iter 350, batch loss 1.5575, batch acc 0.8320
17:24:46.996   Training iter 400, batch loss 1.5530, batch acc 0.8318
17:24:47.567   Training iter 450, batch loss 1.5419, batch acc 0.8296
17:24:48.142   Training iter 500, batch loss 1.5540, batch acc 0.8188
17:24:48.708   Training iter 550, batch loss 1.5518, batch acc 0.8380
17:24:49.249   Training iter 600, batch loss 1.5498, batch acc 0.8530
17:24:49.251 Training @ 30 epoch...
17:24:49.771   Training iter 50, batch loss 1.5500, batch acc 0.8382
17:24:50.281   Training iter 100, batch loss 1.5459, batch acc 0.8266
17:24:50.797   Training iter 150, batch loss 1.5467, batch acc 0.8350
17:24:51.299   Training iter 200, batch loss 1.5425, batch acc 0.8380
17:24:51.806   Training iter 250, batch loss 1.5561, batch acc 0.8394
17:24:52.323   Training iter 300, batch loss 1.5462, batch acc 0.8558
17:24:52.864   Training iter 350, batch loss 1.5504, batch acc 0.8548
17:24:53.422   Training iter 400, batch loss 1.5531, batch acc 0.8370
17:24:54.032   Training iter 450, batch loss 1.5467, batch acc 0.8630
17:24:54.624   Training iter 500, batch loss 1.5494, batch acc 0.8660
17:24:55.179   Training iter 550, batch loss 1.5488, batch acc 0.8534
17:24:55.738   Training iter 600, batch loss 1.5453, batch acc 0.8850
17:24:55.739 Testing @ 30 epoch...
17:24:55.788     Testing, total mean loss 1.56139, total acc 0.85780
17:24:55.788 Training @ 31 epoch...
17:24:56.360   Training iter 50, batch loss 1.5414, batch acc 0.8466
17:24:56.903   Training iter 100, batch loss 1.5508, batch acc 0.8648
17:24:57.459   Training iter 150, batch loss 1.5489, batch acc 0.8660
17:24:58.016   Training iter 200, batch loss 1.5463, batch acc 0.8558
17:24:58.571   Training iter 250, batch loss 1.5475, batch acc 0.8424
17:24:59.145   Training iter 300, batch loss 1.5540, batch acc 0.8336
17:24:59.728   Training iter 350, batch loss 1.5478, batch acc 0.8608
17:25:00.316   Training iter 400, batch loss 1.5472, batch acc 0.8488
17:25:00.900   Training iter 450, batch loss 1.5518, batch acc 0.8814
17:25:01.496   Training iter 500, batch loss 1.5477, batch acc 0.8808
17:25:02.109   Training iter 550, batch loss 1.5446, batch acc 0.8512
17:25:02.706   Training iter 600, batch loss 1.5414, batch acc 0.8706
17:25:02.708 Training @ 32 epoch...
17:25:03.289   Training iter 50, batch loss 1.5560, batch acc 0.8594
17:25:03.869   Training iter 100, batch loss 1.5433, batch acc 0.8630
17:25:04.458   Training iter 150, batch loss 1.5395, batch acc 0.8414
17:25:05.010   Training iter 200, batch loss 1.5511, batch acc 0.8392
17:25:05.539   Training iter 250, batch loss 1.5435, batch acc 0.8748
17:25:06.060   Training iter 300, batch loss 1.5405, batch acc 0.8682
17:25:06.593   Training iter 350, batch loss 1.5441, batch acc 0.8722
17:25:07.123   Training iter 400, batch loss 1.5431, batch acc 0.8558
17:25:07.608   Training iter 450, batch loss 1.5575, batch acc 0.8302
17:25:08.101   Training iter 500, batch loss 1.5460, batch acc 0.8382
17:25:08.586   Training iter 550, batch loss 1.5475, batch acc 0.8304
17:25:09.070   Training iter 600, batch loss 1.5518, batch acc 0.8564
17:25:09.072 Training @ 33 epoch...
17:25:09.574   Training iter 50, batch loss 1.5469, batch acc 0.8658
17:25:10.050   Training iter 100, batch loss 1.5488, batch acc 0.8778
17:25:10.525   Training iter 150, batch loss 1.5437, batch acc 0.8712
17:25:11.006   Training iter 200, batch loss 1.5510, batch acc 0.8542
17:25:11.499   Training iter 250, batch loss 1.5410, batch acc 0.8606
17:25:11.984   Training iter 300, batch loss 1.5514, batch acc 0.8196
17:25:12.478   Training iter 350, batch loss 1.5492, batch acc 0.8510
17:25:12.957   Training iter 400, batch loss 1.5545, batch acc 0.8724
17:25:13.463   Training iter 450, batch loss 1.5451, batch acc 0.8666
17:25:13.994   Training iter 500, batch loss 1.5413, batch acc 0.8556
17:25:14.525   Training iter 550, batch loss 1.5518, batch acc 0.8230
17:25:15.049   Training iter 600, batch loss 1.5501, batch acc 0.8504
17:25:15.051 Training @ 34 epoch...
17:25:15.587   Training iter 50, batch loss 1.5449, batch acc 0.8784
17:25:16.130   Training iter 100, batch loss 1.5425, batch acc 0.8686
17:25:16.711   Training iter 150, batch loss 1.5459, batch acc 0.8690
17:25:17.275   Training iter 200, batch loss 1.5555, batch acc 0.8604
17:25:17.854   Training iter 250, batch loss 1.5488, batch acc 0.8480
17:25:18.431   Training iter 300, batch loss 1.5449, batch acc 0.8610
17:25:18.996   Training iter 350, batch loss 1.5486, batch acc 0.8464
17:25:19.559   Training iter 400, batch loss 1.5471, batch acc 0.8526
17:25:20.098   Training iter 450, batch loss 1.5493, batch acc 0.8454
17:25:20.644   Training iter 500, batch loss 1.5392, batch acc 0.8478
17:25:21.184   Training iter 550, batch loss 1.5461, batch acc 0.8386
17:25:21.719   Training iter 600, batch loss 1.5383, batch acc 0.8728
17:25:21.721 Training @ 35 epoch...
17:25:22.257   Training iter 50, batch loss 1.5395, batch acc 0.8532
17:25:22.785   Training iter 100, batch loss 1.5433, batch acc 0.8442
17:25:23.314   Training iter 150, batch loss 1.5404, batch acc 0.8682
17:25:23.804   Training iter 200, batch loss 1.5520, batch acc 0.8350
17:25:24.299   Training iter 250, batch loss 1.5605, batch acc 0.8438
17:25:24.806   Training iter 300, batch loss 1.5541, batch acc 0.8460
17:25:25.320   Training iter 350, batch loss 1.5423, batch acc 0.8830
17:25:25.842   Training iter 400, batch loss 1.5457, batch acc 0.8670
17:25:26.349   Training iter 450, batch loss 1.5504, batch acc 0.8742
17:25:26.877   Training iter 500, batch loss 1.5508, batch acc 0.8438
17:25:27.388   Training iter 550, batch loss 1.5445, batch acc 0.8894
17:25:27.897   Training iter 600, batch loss 1.5427, batch acc 0.8738
17:25:27.899 Testing @ 35 epoch...
17:25:27.947     Testing, total mean loss 1.53988, total acc 0.84740
17:25:27.947 Training @ 36 epoch...
17:25:28.468   Training iter 50, batch loss 1.5519, batch acc 0.8540
17:25:28.982   Training iter 100, batch loss 1.5458, batch acc 0.8590
17:25:29.473   Training iter 150, batch loss 1.5533, batch acc 0.8512
17:25:29.985   Training iter 200, batch loss 1.5444, batch acc 0.8570
17:25:30.488   Training iter 250, batch loss 1.5466, batch acc 0.8718
17:25:30.977   Training iter 300, batch loss 1.5471, batch acc 0.8458
17:25:31.479   Training iter 350, batch loss 1.5636, batch acc 0.7944
17:25:31.974   Training iter 400, batch loss 1.5402, batch acc 0.8686
17:25:32.475   Training iter 450, batch loss 1.5441, batch acc 0.8862
17:25:32.967   Training iter 500, batch loss 1.5431, batch acc 0.8710
17:25:33.475   Training iter 550, batch loss 1.5441, batch acc 0.8530
17:25:34.021   Training iter 600, batch loss 1.5472, batch acc 0.8648
17:25:34.023 Training @ 37 epoch...
17:25:34.588   Training iter 50, batch loss 1.5488, batch acc 0.8632
17:25:35.142   Training iter 100, batch loss 1.5528, batch acc 0.8920
17:25:35.652   Training iter 150, batch loss 1.5405, batch acc 0.8834
17:25:36.168   Training iter 200, batch loss 1.5370, batch acc 0.8966
17:25:36.683   Training iter 250, batch loss 1.5427, batch acc 0.8676
17:25:37.189   Training iter 300, batch loss 1.5458, batch acc 0.8592
17:25:37.693   Training iter 350, batch loss 1.5502, batch acc 0.8576
17:25:38.191   Training iter 400, batch loss 1.5441, batch acc 0.8828
17:25:38.685   Training iter 450, batch loss 1.5444, batch acc 0.8650
17:25:39.208   Training iter 500, batch loss 1.5462, batch acc 0.8544
17:25:39.743   Training iter 550, batch loss 1.5429, batch acc 0.8764
17:25:40.272   Training iter 600, batch loss 1.5520, batch acc 0.8472
17:25:40.273 Training @ 38 epoch...
17:25:40.790   Training iter 50, batch loss 1.5490, batch acc 0.8660
17:25:41.325   Training iter 100, batch loss 1.5542, batch acc 0.8802
17:25:41.836   Training iter 150, batch loss 1.5473, batch acc 0.8614
17:25:42.341   Training iter 200, batch loss 1.5471, batch acc 0.8662
17:25:42.871   Training iter 250, batch loss 1.5463, batch acc 0.8636
17:25:43.404   Training iter 300, batch loss 1.5493, batch acc 0.8620
17:25:43.929   Training iter 350, batch loss 1.5453, batch acc 0.8826
17:25:44.436   Training iter 400, batch loss 1.5425, batch acc 0.8800
17:25:44.928   Training iter 450, batch loss 1.5500, batch acc 0.8698
17:25:45.415   Training iter 500, batch loss 1.5474, batch acc 0.8606
17:25:45.917   Training iter 550, batch loss 1.5510, batch acc 0.8542
17:25:46.427   Training iter 600, batch loss 1.5483, batch acc 0.8280
17:25:46.429 Training @ 39 epoch...
17:25:46.986   Training iter 50, batch loss 1.5464, batch acc 0.8296
17:25:47.553   Training iter 100, batch loss 1.5411, batch acc 0.8618
17:25:48.123   Training iter 150, batch loss 1.5513, batch acc 0.8682
17:25:48.671   Training iter 200, batch loss 1.5492, batch acc 0.8714
17:25:49.201   Training iter 250, batch loss 1.5393, batch acc 0.8854
17:25:49.743   Training iter 300, batch loss 1.5447, batch acc 0.8528
17:25:50.293   Training iter 350, batch loss 1.5493, batch acc 0.8488
17:25:50.832   Training iter 400, batch loss 1.5456, batch acc 0.8456
17:25:51.361   Training iter 450, batch loss 1.5461, batch acc 0.8388
17:25:51.886   Training iter 500, batch loss 1.5400, batch acc 0.8540
17:25:52.421   Training iter 550, batch loss 1.5441, batch acc 0.8272
17:25:52.921   Training iter 600, batch loss 1.5411, batch acc 0.8354
17:25:52.922 Training @ 40 epoch...
17:25:53.431   Training iter 50, batch loss 1.5393, batch acc 0.8574
17:25:53.929   Training iter 100, batch loss 1.5439, batch acc 0.8514
17:25:54.447   Training iter 150, batch loss 1.5446, batch acc 0.8542
17:25:54.983   Training iter 200, batch loss 1.5450, batch acc 0.8766
17:25:55.526   Training iter 250, batch loss 1.5448, batch acc 0.8788
17:25:56.055   Training iter 300, batch loss 1.5432, batch acc 0.8492
17:25:56.595   Training iter 350, batch loss 1.5461, batch acc 0.8888
17:25:57.132   Training iter 400, batch loss 1.5484, batch acc 0.8752
17:25:57.680   Training iter 450, batch loss 1.5510, batch acc 0.8884
17:25:58.219   Training iter 500, batch loss 1.5493, batch acc 0.8874
17:25:58.756   Training iter 550, batch loss 1.5480, batch acc 0.8804
17:25:59.301   Training iter 600, batch loss 1.5429, batch acc 0.8774
17:25:59.303 Testing @ 40 epoch...
17:25:59.351     Testing, total mean loss 1.54858, total acc 0.85990
17:25:59.351 Training @ 41 epoch...
17:25:59.894   Training iter 50, batch loss 1.5429, batch acc 0.8644
17:26:00.437   Training iter 100, batch loss 1.5380, batch acc 0.8756
17:26:00.964   Training iter 150, batch loss 1.5514, batch acc 0.8764
17:26:01.524   Training iter 200, batch loss 1.5515, batch acc 0.8384
17:26:02.077   Training iter 250, batch loss 1.5390, batch acc 0.8622
17:26:02.627   Training iter 300, batch loss 1.5449, batch acc 0.8650
17:26:03.203   Training iter 350, batch loss 1.5452, batch acc 0.8654
17:26:03.811   Training iter 400, batch loss 1.5578, batch acc 0.8842
17:26:04.433   Training iter 450, batch loss 1.5467, batch acc 0.8736
17:26:05.038   Training iter 500, batch loss 1.5423, batch acc 0.8618
17:26:05.631   Training iter 550, batch loss 1.5559, batch acc 0.8424
17:26:06.212   Training iter 600, batch loss 1.5484, batch acc 0.8370
17:26:06.214 Training @ 42 epoch...
17:26:06.805   Training iter 50, batch loss 1.5408, batch acc 0.8492
17:26:07.386   Training iter 100, batch loss 1.5465, batch acc 0.8654
17:26:07.960   Training iter 150, batch loss 1.5413, batch acc 0.8766
17:26:08.530   Training iter 200, batch loss 1.5563, batch acc 0.8704
17:26:09.069   Training iter 250, batch loss 1.5486, batch acc 0.8518
17:26:09.621   Training iter 300, batch loss 1.5413, batch acc 0.8780
17:26:10.168   Training iter 350, batch loss 1.5363, batch acc 0.8558
17:26:10.713   Training iter 400, batch loss 1.5388, batch acc 0.8616
17:26:11.272   Training iter 450, batch loss 1.5466, batch acc 0.8902
17:26:11.837   Training iter 500, batch loss 1.5433, batch acc 0.8804
17:26:12.433   Training iter 550, batch loss 1.5448, batch acc 0.8772
17:26:13.024   Training iter 600, batch loss 1.5372, batch acc 0.8762
17:26:13.026 Training @ 43 epoch...
17:26:13.602   Training iter 50, batch loss 1.5479, batch acc 0.8636
17:26:14.187   Training iter 100, batch loss 1.5447, batch acc 0.8718
17:26:14.766   Training iter 150, batch loss 1.5572, batch acc 0.8530
17:26:15.335   Training iter 200, batch loss 1.5379, batch acc 0.8688
17:26:15.913   Training iter 250, batch loss 1.5432, batch acc 0.8964
17:26:16.501   Training iter 300, batch loss 1.5486, batch acc 0.8912
17:26:17.077   Training iter 350, batch loss 1.5366, batch acc 0.8840
17:26:17.656   Training iter 400, batch loss 1.5354, batch acc 0.8852
17:26:18.239   Training iter 450, batch loss 1.5404, batch acc 0.8904
17:26:18.882   Training iter 500, batch loss 1.5386, batch acc 0.8864
17:26:19.491   Training iter 550, batch loss 1.5524, batch acc 0.8458
17:26:20.083   Training iter 600, batch loss 1.5439, batch acc 0.8532
17:26:20.085 Training @ 44 epoch...
17:26:20.664   Training iter 50, batch loss 1.5464, batch acc 0.8578
17:26:21.268   Training iter 100, batch loss 1.5423, batch acc 0.8524
17:26:21.952   Training iter 150, batch loss 1.5496, batch acc 0.8676
17:26:22.545   Training iter 200, batch loss 1.5474, batch acc 0.8698
17:26:23.095   Training iter 250, batch loss 1.5501, batch acc 0.8576
17:26:23.648   Training iter 300, batch loss 1.5416, batch acc 0.8486
17:26:24.186   Training iter 350, batch loss 1.5432, batch acc 0.8578
17:26:24.696   Training iter 400, batch loss 1.5436, batch acc 0.8574
17:26:25.197   Training iter 450, batch loss 1.5436, batch acc 0.8528
17:26:25.725   Training iter 500, batch loss 1.5502, batch acc 0.8356
17:26:26.277   Training iter 550, batch loss 1.5460, batch acc 0.8520
17:26:26.836   Training iter 600, batch loss 1.5443, batch acc 0.8836
17:26:26.838 Training @ 45 epoch...
17:26:27.392   Training iter 50, batch loss 1.5463, batch acc 0.8622
17:26:27.987   Training iter 100, batch loss 1.5465, batch acc 0.8830
17:26:28.581   Training iter 150, batch loss 1.5484, batch acc 0.8700
17:26:29.184   Training iter 200, batch loss 1.5420, batch acc 0.8618
17:26:29.733   Training iter 250, batch loss 1.5360, batch acc 0.8768
17:26:30.271   Training iter 300, batch loss 1.5425, batch acc 0.8918
17:26:30.810   Training iter 350, batch loss 1.5481, batch acc 0.8822
17:26:31.381   Training iter 400, batch loss 1.5563, batch acc 0.8664
17:26:31.953   Training iter 450, batch loss 1.5493, batch acc 0.8598
17:26:32.495   Training iter 500, batch loss 1.5465, batch acc 0.8724
17:26:33.032   Training iter 550, batch loss 1.5475, batch acc 0.8554
17:26:33.561   Training iter 600, batch loss 1.5385, batch acc 0.8584
17:26:33.563 Testing @ 45 epoch...
17:26:33.612     Testing, total mean loss 1.53925, total acc 0.84410
17:26:33.612 Training @ 46 epoch...
17:26:34.169   Training iter 50, batch loss 1.5458, batch acc 0.8900
17:26:34.743   Training iter 100, batch loss 1.5417, batch acc 0.8958
17:26:35.315   Training iter 150, batch loss 1.5519, batch acc 0.8820
17:26:35.890   Training iter 200, batch loss 1.5416, batch acc 0.8832
17:26:36.469   Training iter 250, batch loss 1.5435, batch acc 0.8820
17:26:37.043   Training iter 300, batch loss 1.5507, batch acc 0.8710
17:26:37.616   Training iter 350, batch loss 1.5459, batch acc 0.8838
17:26:38.186   Training iter 400, batch loss 1.5426, batch acc 0.8776
17:26:38.755   Training iter 450, batch loss 1.5425, batch acc 0.8416
17:26:39.334   Training iter 500, batch loss 1.5416, batch acc 0.8578
17:26:39.909   Training iter 550, batch loss 1.5445, batch acc 0.8592
17:26:40.468   Training iter 600, batch loss 1.5467, batch acc 0.8652
17:26:40.470 Training @ 47 epoch...
17:26:41.036   Training iter 50, batch loss 1.5462, batch acc 0.8752
17:26:41.642   Training iter 100, batch loss 1.5368, batch acc 0.8700
17:26:42.212   Training iter 150, batch loss 1.5431, batch acc 0.8528
17:26:42.779   Training iter 200, batch loss 1.5406, batch acc 0.8472
17:26:43.342   Training iter 250, batch loss 1.5391, batch acc 0.8832
17:26:43.911   Training iter 300, batch loss 1.5488, batch acc 0.8648
17:26:44.482   Training iter 350, batch loss 1.5479, batch acc 0.8398
17:26:45.038   Training iter 400, batch loss 1.5471, batch acc 0.8558
17:26:45.598   Training iter 450, batch loss 1.5538, batch acc 0.8700
17:26:46.178   Training iter 500, batch loss 1.5431, batch acc 0.8818
17:26:46.766   Training iter 550, batch loss 1.5456, batch acc 0.8896
17:26:47.323   Training iter 600, batch loss 1.5599, batch acc 0.8512
17:26:47.325 Training @ 48 epoch...
17:26:47.910   Training iter 50, batch loss 1.5498, batch acc 0.8262
17:26:48.486   Training iter 100, batch loss 1.5438, batch acc 0.8758
17:26:49.061   Training iter 150, batch loss 1.5448, batch acc 0.8742
17:26:49.620   Training iter 200, batch loss 1.5420, batch acc 0.8738
17:26:50.149   Training iter 250, batch loss 1.5449, batch acc 0.8636
17:26:50.709   Training iter 300, batch loss 1.5421, batch acc 0.8670
17:26:51.283   Training iter 350, batch loss 1.5424, batch acc 0.8844
17:26:51.853   Training iter 400, batch loss 1.5494, batch acc 0.8548
17:26:52.425   Training iter 450, batch loss 1.5522, batch acc 0.8798
17:26:53.001   Training iter 500, batch loss 1.5497, batch acc 0.8832
17:26:53.557   Training iter 550, batch loss 1.5540, batch acc 0.8782
17:26:54.140   Training iter 600, batch loss 1.5472, batch acc 0.8876
17:26:54.142 Training @ 49 epoch...
17:26:54.738   Training iter 50, batch loss 1.5516, batch acc 0.8708
17:26:55.341   Training iter 100, batch loss 1.5406, batch acc 0.8676
17:26:55.929   Training iter 150, batch loss 1.5359, batch acc 0.8918
17:26:56.503   Training iter 200, batch loss 1.5446, batch acc 0.8836
17:26:57.016   Training iter 250, batch loss 1.5408, batch acc 0.8672
17:26:57.541   Training iter 300, batch loss 1.5401, batch acc 0.8736
17:26:58.067   Training iter 350, batch loss 1.5425, batch acc 0.8670
17:26:58.585   Training iter 400, batch loss 1.5463, batch acc 0.8740
17:26:59.121   Training iter 450, batch loss 1.5418, batch acc 0.8956
17:26:59.639   Training iter 500, batch loss 1.5359, batch acc 0.8588
17:27:00.138   Training iter 550, batch loss 1.5463, batch acc 0.8832
17:27:00.682   Training iter 600, batch loss 1.5442, batch acc 0.8708
17:27:00.684 Training @ 50 epoch...
17:27:01.218   Training iter 50, batch loss 1.5411, batch acc 0.8764
17:27:01.770   Training iter 100, batch loss 1.5395, batch acc 0.8790
17:27:02.350   Training iter 150, batch loss 1.5434, batch acc 0.8698
17:27:02.935   Training iter 200, batch loss 1.5457, batch acc 0.8874
17:27:03.498   Training iter 250, batch loss 1.5429, batch acc 0.8886
17:27:04.074   Training iter 300, batch loss 1.5527, batch acc 0.8900
17:27:04.649   Training iter 350, batch loss 1.5395, batch acc 0.9002
17:27:05.218   Training iter 400, batch loss 1.5376, batch acc 0.9016
17:27:05.772   Training iter 450, batch loss 1.5424, batch acc 0.8744
17:27:06.329   Training iter 500, batch loss 1.5556, batch acc 0.8544
17:27:06.895   Training iter 550, batch loss 1.5438, batch acc 0.8768
17:27:07.473   Training iter 600, batch loss 1.5546, batch acc 0.8564
17:27:07.475 Testing @ 50 epoch...
17:27:07.524     Testing, total mean loss 1.53958, total acc 0.86320
17:27:07.524 Training @ 51 epoch...
17:27:08.096   Training iter 50, batch loss 1.5346, batch acc 0.8874
17:27:08.640   Training iter 100, batch loss 1.5419, batch acc 0.8762
17:27:09.179   Training iter 150, batch loss 1.5388, batch acc 0.8944
17:27:09.726   Training iter 200, batch loss 1.5371, batch acc 0.8788
17:27:10.278   Training iter 250, batch loss 1.5353, batch acc 0.8818
17:27:10.823   Training iter 300, batch loss 1.5363, batch acc 0.8624
17:27:11.385   Training iter 350, batch loss 1.5549, batch acc 0.8648
17:27:11.939   Training iter 400, batch loss 1.5412, batch acc 0.8798
17:27:12.512   Training iter 450, batch loss 1.5505, batch acc 0.8610
17:27:13.065   Training iter 500, batch loss 1.5412, batch acc 0.8682
17:27:13.622   Training iter 550, batch loss 1.5440, batch acc 0.8792
17:27:14.182   Training iter 600, batch loss 1.5393, batch acc 0.8968
17:27:14.184 Training @ 52 epoch...
17:27:14.750   Training iter 50, batch loss 1.5423, batch acc 0.8938
17:27:15.307   Training iter 100, batch loss 1.5519, batch acc 0.8830
17:27:15.872   Training iter 150, batch loss 1.5447, batch acc 0.8586
17:27:16.460   Training iter 200, batch loss 1.5377, batch acc 0.8608
17:27:17.016   Training iter 250, batch loss 1.5511, batch acc 0.8628
17:27:17.624   Training iter 300, batch loss 1.5428, batch acc 0.8574
17:27:18.211   Training iter 350, batch loss 1.5432, batch acc 0.8776
17:27:18.800   Training iter 400, batch loss 1.5382, batch acc 0.8720
17:27:19.396   Training iter 450, batch loss 1.5388, batch acc 0.8604
17:27:19.992   Training iter 500, batch loss 1.5507, batch acc 0.8826
17:27:20.563   Training iter 550, batch loss 1.5324, batch acc 0.8684
17:27:21.139   Training iter 600, batch loss 1.5420, batch acc 0.8996
17:27:21.140 Training @ 53 epoch...
17:27:21.730   Training iter 50, batch loss 1.5448, batch acc 0.8910
17:27:22.307   Training iter 100, batch loss 1.5410, batch acc 0.8866
17:27:22.892   Training iter 150, batch loss 1.5445, batch acc 0.8754
17:27:23.478   Training iter 200, batch loss 1.5470, batch acc 0.8886
17:27:24.108   Training iter 250, batch loss 1.5459, batch acc 0.8694
17:27:24.745   Training iter 300, batch loss 1.5424, batch acc 0.8650
17:27:25.371   Training iter 350, batch loss 1.5352, batch acc 0.8842
17:27:25.971   Training iter 400, batch loss 1.5401, batch acc 0.8816
17:27:26.589   Training iter 450, batch loss 1.5429, batch acc 0.8720
17:27:27.197   Training iter 500, batch loss 1.5594, batch acc 0.8612
17:27:27.796   Training iter 550, batch loss 1.5415, batch acc 0.8794
17:27:28.391   Training iter 600, batch loss 1.5451, batch acc 0.8696
17:27:28.393 Training @ 54 epoch...
17:27:28.990   Training iter 50, batch loss 1.5387, batch acc 0.8824
17:27:29.581   Training iter 100, batch loss 1.5426, batch acc 0.8746
17:27:30.172   Training iter 150, batch loss 1.5418, batch acc 0.8702
17:27:30.764   Training iter 200, batch loss 1.5449, batch acc 0.9016
17:27:31.339   Training iter 250, batch loss 1.5440, batch acc 0.9068
17:27:31.924   Training iter 300, batch loss 1.5551, batch acc 0.8688
17:27:32.521   Training iter 350, batch loss 1.5424, batch acc 0.8884
17:27:33.118   Training iter 400, batch loss 1.5411, batch acc 0.9058
17:27:33.723   Training iter 450, batch loss 1.5412, batch acc 0.8914
17:27:34.324   Training iter 500, batch loss 1.5484, batch acc 0.8608
17:27:34.927   Training iter 550, batch loss 1.5570, batch acc 0.8632
17:27:35.548   Training iter 600, batch loss 1.5423, batch acc 0.8666
17:27:35.549 Training @ 55 epoch...
17:27:36.106   Training iter 50, batch loss 1.5439, batch acc 0.8884
17:27:36.647   Training iter 100, batch loss 1.5392, batch acc 0.8942
17:27:37.180   Training iter 150, batch loss 1.5404, batch acc 0.8956
17:27:37.707   Training iter 200, batch loss 1.5425, batch acc 0.8964
17:27:38.236   Training iter 250, batch loss 1.5486, batch acc 0.8860
17:27:38.777   Training iter 300, batch loss 1.5442, batch acc 0.8902
17:27:39.326   Training iter 350, batch loss 1.5423, batch acc 0.8876
17:27:39.881   Training iter 400, batch loss 1.5434, batch acc 0.8840
17:27:40.444   Training iter 450, batch loss 1.5746, batch acc 0.8316
17:27:41.007   Training iter 500, batch loss 1.5547, batch acc 0.8812
17:27:41.574   Training iter 550, batch loss 1.5408, batch acc 0.8888
17:27:42.125   Training iter 600, batch loss 1.5419, batch acc 0.9018
17:27:42.127 Testing @ 55 epoch...
17:27:42.176     Testing, total mean loss 1.54681, total acc 0.89970
17:27:42.176 Training @ 56 epoch...
17:27:42.731   Training iter 50, batch loss 1.5378, batch acc 0.9056
17:27:43.278   Training iter 100, batch loss 1.5454, batch acc 0.8922
17:27:43.820   Training iter 150, batch loss 1.5416, batch acc 0.8882
17:27:44.350   Training iter 200, batch loss 1.5400, batch acc 0.8838
17:27:44.855   Training iter 250, batch loss 1.5429, batch acc 0.8740
17:27:45.363   Training iter 300, batch loss 1.5414, batch acc 0.8586
17:27:45.871   Training iter 350, batch loss 1.5460, batch acc 0.8580
17:27:46.393   Training iter 400, batch loss 1.5366, batch acc 0.8740
17:27:46.915   Training iter 450, batch loss 1.5383, batch acc 0.8804
17:27:47.398   Training iter 500, batch loss 1.5419, batch acc 0.8866
17:27:47.885   Training iter 550, batch loss 1.5479, batch acc 0.8644
17:27:48.400   Training iter 600, batch loss 1.5403, batch acc 0.8766
17:27:48.402 Training @ 57 epoch...
17:27:48.910   Training iter 50, batch loss 1.5449, batch acc 0.8786
17:27:49.417   Training iter 100, batch loss 1.5429, batch acc 0.8640
17:27:49.922   Training iter 150, batch loss 1.5407, batch acc 0.8910
17:27:50.419   Training iter 200, batch loss 1.5434, batch acc 0.9022
17:27:50.924   Training iter 250, batch loss 1.5543, batch acc 0.8518
17:27:51.445   Training iter 300, batch loss 1.5444, batch acc 0.8664
17:27:51.952   Training iter 350, batch loss 1.5329, batch acc 0.8880
17:27:52.474   Training iter 400, batch loss 1.5401, batch acc 0.8698
17:27:52.975   Training iter 450, batch loss 1.5430, batch acc 0.8702
17:27:53.486   Training iter 500, batch loss 1.5446, batch acc 0.8694
17:27:53.987   Training iter 550, batch loss 1.5471, batch acc 0.8568
17:27:54.536   Training iter 600, batch loss 1.5375, batch acc 0.8866
17:27:54.538 Training @ 58 epoch...
17:27:55.105   Training iter 50, batch loss 1.5448, batch acc 0.8496
17:27:55.660   Training iter 100, batch loss 1.5433, batch acc 0.8846
17:27:56.228   Training iter 150, batch loss 1.5458, batch acc 0.8910
17:27:56.804   Training iter 200, batch loss 1.5451, batch acc 0.8744
17:27:57.387   Training iter 250, batch loss 1.5460, batch acc 0.8758
17:27:57.951   Training iter 300, batch loss 1.5392, batch acc 0.8898
17:27:58.565   Training iter 350, batch loss 1.5382, batch acc 0.8826
17:27:59.192   Training iter 400, batch loss 1.5425, batch acc 0.8904
17:27:59.774   Training iter 450, batch loss 1.5493, batch acc 0.8866
17:28:00.367   Training iter 500, batch loss 1.5492, batch acc 0.8926
17:28:00.930   Training iter 550, batch loss 1.5445, batch acc 0.8892
17:28:01.509   Training iter 600, batch loss 1.5433, batch acc 0.9038
17:28:01.511 Training @ 59 epoch...
17:28:02.105   Training iter 50, batch loss 1.5509, batch acc 0.8690
17:28:02.705   Training iter 100, batch loss 1.5406, batch acc 0.9030
17:28:03.439   Training iter 150, batch loss 1.5441, batch acc 0.8912
17:28:04.166   Training iter 200, batch loss 1.5454, batch acc 0.8826
17:28:04.742   Training iter 250, batch loss 1.5488, batch acc 0.8854
17:28:05.312   Training iter 300, batch loss 1.5515, batch acc 0.8776
17:28:05.879   Training iter 350, batch loss 1.5468, batch acc 0.8832
17:28:06.464   Training iter 400, batch loss 1.5362, batch acc 0.8858
17:28:07.041   Training iter 450, batch loss 1.5447, batch acc 0.8870
17:28:07.599   Training iter 500, batch loss 1.5381, batch acc 0.8878
17:28:08.147   Training iter 550, batch loss 1.5446, batch acc 0.8794
17:28:08.688   Training iter 600, batch loss 1.5392, batch acc 0.8996
17:28:08.690 Training @ 60 epoch...
17:28:09.237   Training iter 50, batch loss 1.5466, batch acc 0.9102
17:28:09.790   Training iter 100, batch loss 1.5359, batch acc 0.9052
17:28:10.312   Training iter 150, batch loss 1.5385, batch acc 0.8856
17:28:10.866   Training iter 200, batch loss 1.5477, batch acc 0.8972
17:28:11.461   Training iter 250, batch loss 1.5489, batch acc 0.8952
17:28:12.028   Training iter 300, batch loss 1.5430, batch acc 0.9014
17:28:12.592   Training iter 350, batch loss 1.5357, batch acc 0.9008
17:28:13.189   Training iter 400, batch loss 1.5374, batch acc 0.9008
17:28:13.803   Training iter 450, batch loss 1.5401, batch acc 0.8892
17:28:14.427   Training iter 500, batch loss 1.5362, batch acc 0.9064
17:28:15.039   Training iter 550, batch loss 1.5571, batch acc 0.8868
17:28:15.639   Training iter 600, batch loss 1.5476, batch acc 0.9002
17:28:15.641 Testing @ 60 epoch...
17:28:15.694     Testing, total mean loss 1.54472, total acc 0.90800
17:28:15.694 Training @ 61 epoch...
17:28:16.307   Training iter 50, batch loss 1.5422, batch acc 0.9018
17:28:16.873   Training iter 100, batch loss 1.5435, batch acc 0.9008
17:28:17.413   Training iter 150, batch loss 1.5455, batch acc 0.8956
17:28:17.961   Training iter 200, batch loss 1.5439, batch acc 0.9012
17:28:18.507   Training iter 250, batch loss 1.5441, batch acc 0.8936
17:28:19.046   Training iter 300, batch loss 1.5484, batch acc 0.8738
17:28:19.580   Training iter 350, batch loss 1.5411, batch acc 0.8854
17:28:20.106   Training iter 400, batch loss 1.5396, batch acc 0.8938
17:28:20.642   Training iter 450, batch loss 1.5525, batch acc 0.8852
17:28:21.180   Training iter 500, batch loss 1.5431, batch acc 0.8882
17:28:21.725   Training iter 550, batch loss 1.5437, batch acc 0.8946
17:28:22.281   Training iter 600, batch loss 1.5432, batch acc 0.8926
17:28:22.283 Training @ 62 epoch...
17:28:22.852   Training iter 50, batch loss 1.5425, batch acc 0.9012
17:28:23.401   Training iter 100, batch loss 1.5401, batch acc 0.9030
17:28:23.933   Training iter 150, batch loss 1.5368, batch acc 0.9022
17:28:24.474   Training iter 200, batch loss 1.5430, batch acc 0.8970
17:28:25.010   Training iter 250, batch loss 1.5406, batch acc 0.8920
17:28:25.560   Training iter 300, batch loss 1.5426, batch acc 0.8890
17:28:26.106   Training iter 350, batch loss 1.5422, batch acc 0.8596
17:28:26.649   Training iter 400, batch loss 1.5437, batch acc 0.8968
17:28:27.222   Training iter 450, batch loss 1.5394, batch acc 0.8962
17:28:27.793   Training iter 500, batch loss 1.5401, batch acc 0.8878
17:28:28.370   Training iter 550, batch loss 1.5430, batch acc 0.8826
17:28:28.940   Training iter 600, batch loss 1.5393, batch acc 0.8754
17:28:28.942 Training @ 63 epoch...
17:28:29.521   Training iter 50, batch loss 1.5475, batch acc 0.8710
17:28:30.084   Training iter 100, batch loss 1.5382, batch acc 0.8884
17:28:30.651   Training iter 150, batch loss 1.5390, batch acc 0.8936
17:28:31.213   Training iter 200, batch loss 1.5489, batch acc 0.8900
17:28:31.767   Training iter 250, batch loss 1.5351, batch acc 0.9036
17:28:32.327   Training iter 300, batch loss 1.5486, batch acc 0.8586
17:28:32.869   Training iter 350, batch loss 1.5384, batch acc 0.8980
17:28:33.386   Training iter 400, batch loss 1.5366, batch acc 0.8994
17:28:33.913   Training iter 450, batch loss 1.5362, batch acc 0.9080
17:28:34.460   Training iter 500, batch loss 1.5406, batch acc 0.9034
17:28:34.974   Training iter 550, batch loss 1.5492, batch acc 0.8904
17:28:35.520   Training iter 600, batch loss 1.5425, batch acc 0.8930
17:28:35.522 Training @ 64 epoch...
17:28:36.079   Training iter 50, batch loss 1.5370, batch acc 0.9016
17:28:36.624   Training iter 100, batch loss 1.5391, batch acc 0.9000
17:28:37.157   Training iter 150, batch loss 1.5417, batch acc 0.8990
17:28:37.693   Training iter 200, batch loss 1.5391, batch acc 0.8984
17:28:38.230   Training iter 250, batch loss 1.5383, batch acc 0.9084
17:28:38.763   Training iter 300, batch loss 1.5328, batch acc 0.9134
17:28:39.302   Training iter 350, batch loss 1.5313, batch acc 0.9110
17:28:39.834   Training iter 400, batch loss 1.5357, batch acc 0.9092
17:28:40.379   Training iter 450, batch loss 1.5320, batch acc 0.9038
17:28:40.908   Training iter 500, batch loss 1.5394, batch acc 0.8950
17:28:41.449   Training iter 550, batch loss 1.5430, batch acc 0.8946
17:28:41.975   Training iter 600, batch loss 1.5324, batch acc 0.9058
17:28:41.977 Training @ 65 epoch...
17:28:42.525   Training iter 50, batch loss 1.5357, batch acc 0.9060
17:28:43.067   Training iter 100, batch loss 1.5428, batch acc 0.8960
17:28:43.614   Training iter 150, batch loss 1.5286, batch acc 0.9098
17:28:44.154   Training iter 200, batch loss 1.5399, batch acc 0.9014
17:28:44.671   Training iter 250, batch loss 1.5433, batch acc 0.8966
17:28:45.199   Training iter 300, batch loss 1.5368, batch acc 0.9012
17:28:45.732   Training iter 350, batch loss 1.5270, batch acc 0.9136
17:28:46.263   Training iter 400, batch loss 1.5373, batch acc 0.9038
17:28:46.737   Training iter 450, batch loss 1.5413, batch acc 0.8986
17:28:47.230   Training iter 500, batch loss 1.5297, batch acc 0.9054
17:28:47.731   Training iter 550, batch loss 1.5418, batch acc 0.9030
17:28:48.240   Training iter 600, batch loss 1.5389, batch acc 0.9002
17:28:48.242 Testing @ 65 epoch...
17:28:48.290     Testing, total mean loss 1.54818, total acc 0.89250
17:28:48.290 Training @ 66 epoch...
17:28:48.793   Training iter 50, batch loss 1.5305, batch acc 0.9072
17:28:49.304   Training iter 100, batch loss 1.5286, batch acc 0.9112
17:28:49.823   Training iter 150, batch loss 1.5553, batch acc 0.8650
17:28:50.354   Training iter 200, batch loss 1.5394, batch acc 0.8958
17:28:50.897   Training iter 250, batch loss 1.5435, batch acc 0.8974
17:28:51.438   Training iter 300, batch loss 1.5344, batch acc 0.8990
17:28:51.967   Training iter 350, batch loss 1.5322, batch acc 0.9024
17:28:52.504   Training iter 400, batch loss 1.5340, batch acc 0.9074
17:28:53.032   Training iter 450, batch loss 1.5282, batch acc 0.9188
17:28:53.545   Training iter 500, batch loss 1.5424, batch acc 0.8928
17:28:54.056   Training iter 550, batch loss 1.5353, batch acc 0.9100
17:28:54.537   Training iter 600, batch loss 1.5277, batch acc 0.9166
17:28:54.539 Training @ 67 epoch...
17:28:55.015   Training iter 50, batch loss 1.5370, batch acc 0.9098
17:28:55.491   Training iter 100, batch loss 1.5303, batch acc 0.9168
17:28:56.142   Training iter 150, batch loss 1.5325, batch acc 0.9152
17:28:56.699   Training iter 200, batch loss 1.5359, batch acc 0.9070
17:28:57.194   Training iter 250, batch loss 1.5300, batch acc 0.9108
17:28:57.698   Training iter 300, batch loss 1.5280, batch acc 0.9080
17:28:58.220   Training iter 350, batch loss 1.5383, batch acc 0.8964
17:28:58.810   Training iter 400, batch loss 1.5339, batch acc 0.9120
17:28:59.401   Training iter 450, batch loss 1.5360, batch acc 0.8976
17:28:59.990   Training iter 500, batch loss 1.5339, batch acc 0.8952
17:29:00.513   Training iter 550, batch loss 1.5293, batch acc 0.9088
17:29:01.025   Training iter 600, batch loss 1.5451, batch acc 0.8968
17:29:01.027 Training @ 68 epoch...
17:29:01.578   Training iter 50, batch loss 1.5304, batch acc 0.9112
17:29:02.165   Training iter 100, batch loss 1.5389, batch acc 0.8980
17:29:02.711   Training iter 150, batch loss 1.5298, batch acc 0.9052
17:29:03.237   Training iter 200, batch loss 1.5327, batch acc 0.8988
17:29:03.769   Training iter 250, batch loss 1.5364, batch acc 0.8814
17:29:04.307   Training iter 300, batch loss 1.5323, batch acc 0.9020
17:29:04.898   Training iter 350, batch loss 1.5276, batch acc 0.9142
17:29:05.494   Training iter 400, batch loss 1.5277, batch acc 0.9214
17:29:06.106   Training iter 450, batch loss 1.5326, batch acc 0.9176
17:29:06.718   Training iter 500, batch loss 1.5363, batch acc 0.9012
17:29:07.308   Training iter 550, batch loss 1.5311, batch acc 0.9082
17:29:07.864   Training iter 600, batch loss 1.5272, batch acc 0.9180
17:29:07.866 Training @ 69 epoch...
17:29:08.403   Training iter 50, batch loss 1.5305, batch acc 0.9116
17:29:08.930   Training iter 100, batch loss 1.5278, batch acc 0.9178
17:29:09.474   Training iter 150, batch loss 1.5317, batch acc 0.9152
17:29:09.989   Training iter 200, batch loss 1.5380, batch acc 0.8942
17:29:10.525   Training iter 250, batch loss 1.5313, batch acc 0.8992
17:29:11.173   Training iter 300, batch loss 1.5308, batch acc 0.9104
17:29:11.712   Training iter 350, batch loss 1.5278, batch acc 0.9162
17:29:12.249   Training iter 400, batch loss 1.5364, batch acc 0.9074
17:29:12.795   Training iter 450, batch loss 1.5395, batch acc 0.8992
17:29:13.328   Training iter 500, batch loss 1.5270, batch acc 0.9172
17:29:13.870   Training iter 550, batch loss 1.5446, batch acc 0.8998
17:29:14.450   Training iter 600, batch loss 1.5341, batch acc 0.9004
17:29:14.452 Training @ 70 epoch...
17:29:14.982   Training iter 50, batch loss 1.5320, batch acc 0.9086
17:29:15.522   Training iter 100, batch loss 1.5278, batch acc 0.9150
17:29:16.085   Training iter 150, batch loss 1.5340, batch acc 0.9112
17:29:16.703   Training iter 200, batch loss 1.5289, batch acc 0.9148
17:29:17.227   Training iter 250, batch loss 1.5281, batch acc 0.9168
17:29:17.754   Training iter 300, batch loss 1.5324, batch acc 0.9090
17:29:18.281   Training iter 350, batch loss 1.5290, batch acc 0.9128
17:29:18.860   Training iter 400, batch loss 1.5345, batch acc 0.9046
17:29:19.437   Training iter 450, batch loss 1.5330, batch acc 0.9146
17:29:20.019   Training iter 500, batch loss 1.5330, batch acc 0.9136
17:29:20.610   Training iter 550, batch loss 1.5309, batch acc 0.9078
17:29:21.177   Training iter 600, batch loss 1.5258, batch acc 0.9166
17:29:21.179 Testing @ 70 epoch...
17:29:21.236     Testing, total mean loss 1.52904, total acc 0.91070
17:29:21.236 Training @ 71 epoch...
17:29:21.804   Training iter 50, batch loss 1.5306, batch acc 0.9054
17:29:22.358   Training iter 100, batch loss 1.5395, batch acc 0.9006
17:29:22.911   Training iter 150, batch loss 1.5310, batch acc 0.9126
17:29:23.409   Training iter 200, batch loss 1.5351, batch acc 0.9120
17:29:23.908   Training iter 250, batch loss 1.5338, batch acc 0.9068
17:29:24.433   Training iter 300, batch loss 1.5306, batch acc 0.9120
17:29:24.942   Training iter 350, batch loss 1.5238, batch acc 0.9216
17:29:25.433   Training iter 400, batch loss 1.5305, batch acc 0.9080
17:29:25.947   Training iter 450, batch loss 1.5367, batch acc 0.9010
17:29:26.463   Training iter 500, batch loss 1.5311, batch acc 0.8986
17:29:26.978   Training iter 550, batch loss 1.5332, batch acc 0.9028
17:29:27.498   Training iter 600, batch loss 1.5357, batch acc 0.9078
17:29:27.500 Training @ 72 epoch...
17:29:28.028   Training iter 50, batch loss 1.5258, batch acc 0.9216
17:29:28.544   Training iter 100, batch loss 1.5304, batch acc 0.9092
17:29:29.066   Training iter 150, batch loss 1.5276, batch acc 0.9112
17:29:29.592   Training iter 200, batch loss 1.5380, batch acc 0.9046
17:29:30.100   Training iter 250, batch loss 1.5359, batch acc 0.9020
17:29:30.642   Training iter 300, batch loss 1.5294, batch acc 0.9132
17:29:31.188   Training iter 350, batch loss 1.5205, batch acc 0.9284
17:29:31.707   Training iter 400, batch loss 1.5322, batch acc 0.9060
17:29:32.213   Training iter 450, batch loss 1.5275, batch acc 0.9170
17:29:32.705   Training iter 500, batch loss 1.5273, batch acc 0.9178
17:29:33.195   Training iter 550, batch loss 1.5358, batch acc 0.9034
17:29:33.699   Training iter 600, batch loss 1.5312, batch acc 0.9010
17:29:33.702 Training @ 73 epoch...
17:29:34.218   Training iter 50, batch loss 1.5285, batch acc 0.9158
17:29:34.785   Training iter 100, batch loss 1.5310, batch acc 0.9050
17:29:35.355   Training iter 150, batch loss 1.5320, batch acc 0.9066
17:29:35.948   Training iter 200, batch loss 1.5377, batch acc 0.8986
17:29:36.560   Training iter 250, batch loss 1.5391, batch acc 0.8926
17:29:37.160   Training iter 300, batch loss 1.5255, batch acc 0.9156
17:29:37.688   Training iter 350, batch loss 1.5304, batch acc 0.9072
17:29:38.236   Training iter 400, batch loss 1.5372, batch acc 0.9114
17:29:38.796   Training iter 450, batch loss 1.5230, batch acc 0.9190
17:29:39.339   Training iter 500, batch loss 1.5307, batch acc 0.9058
17:29:39.869   Training iter 550, batch loss 1.5246, batch acc 0.9182
17:29:40.413   Training iter 600, batch loss 1.5241, batch acc 0.9104
17:29:40.414 Training @ 74 epoch...
17:29:40.936   Training iter 50, batch loss 1.5325, batch acc 0.9062
17:29:41.462   Training iter 100, batch loss 1.5301, batch acc 0.9082
17:29:41.988   Training iter 150, batch loss 1.5279, batch acc 0.9168
17:29:42.509   Training iter 200, batch loss 1.5328, batch acc 0.9126
17:29:43.049   Training iter 250, batch loss 1.5243, batch acc 0.9202
17:29:43.631   Training iter 300, batch loss 1.5341, batch acc 0.9082
17:29:44.170   Training iter 350, batch loss 1.5265, batch acc 0.9116
17:29:44.688   Training iter 400, batch loss 1.5280, batch acc 0.9180
17:29:45.190   Training iter 450, batch loss 1.5328, batch acc 0.9096
17:29:45.706   Training iter 500, batch loss 1.5298, batch acc 0.9170
17:29:46.219   Training iter 550, batch loss 1.5315, batch acc 0.9076
17:29:46.771   Training iter 600, batch loss 1.5293, batch acc 0.9118
17:29:46.773 Training @ 75 epoch...
17:29:47.326   Training iter 50, batch loss 1.5257, batch acc 0.9166
17:29:47.882   Training iter 100, batch loss 1.5364, batch acc 0.9002
17:29:48.430   Training iter 150, batch loss 1.5339, batch acc 0.9064
17:29:48.961   Training iter 200, batch loss 1.5407, batch acc 0.8918
17:29:49.490   Training iter 250, batch loss 1.5244, batch acc 0.9156
17:29:50.035   Training iter 300, batch loss 1.5323, batch acc 0.8958
17:29:50.577   Training iter 350, batch loss 1.5386, batch acc 0.8972
17:29:51.102   Training iter 400, batch loss 1.5184, batch acc 0.9310
17:29:51.644   Training iter 450, batch loss 1.5348, batch acc 0.9074
17:29:52.179   Training iter 500, batch loss 1.5294, batch acc 0.9092
17:29:52.711   Training iter 550, batch loss 1.5275, batch acc 0.9076
17:29:53.244   Training iter 600, batch loss 1.5405, batch acc 0.8964
17:29:53.246 Testing @ 75 epoch...
17:29:53.295     Testing, total mean loss 1.55052, total acc 0.88770
17:29:53.295 Training @ 76 epoch...
17:29:53.843   Training iter 50, batch loss 1.5273, batch acc 0.9146
17:29:54.373   Training iter 100, batch loss 1.5300, batch acc 0.9158
17:29:54.898   Training iter 150, batch loss 1.5332, batch acc 0.9090
17:29:55.415   Training iter 200, batch loss 1.5289, batch acc 0.9060
17:29:55.933   Training iter 250, batch loss 1.5259, batch acc 0.9138
17:29:56.469   Training iter 300, batch loss 1.5319, batch acc 0.9070
17:29:56.962   Training iter 350, batch loss 1.5329, batch acc 0.8924
17:29:57.463   Training iter 400, batch loss 1.5420, batch acc 0.8918
17:29:57.964   Training iter 450, batch loss 1.5399, batch acc 0.8840
17:29:58.452   Training iter 500, batch loss 1.5323, batch acc 0.9022
17:29:58.959   Training iter 550, batch loss 1.5367, batch acc 0.9014
17:29:59.481   Training iter 600, batch loss 1.5282, batch acc 0.9182
17:29:59.483 Training @ 77 epoch...
17:30:00.028   Training iter 50, batch loss 1.5361, batch acc 0.9096
17:30:00.590   Training iter 100, batch loss 1.5303, batch acc 0.9112
17:30:01.142   Training iter 150, batch loss 1.5329, batch acc 0.9102
17:30:01.751   Training iter 200, batch loss 1.5282, batch acc 0.9134
17:30:02.403   Training iter 250, batch loss 1.5385, batch acc 0.9020
17:30:02.963   Training iter 300, batch loss 1.5380, batch acc 0.9018
17:30:03.547   Training iter 350, batch loss 1.5320, batch acc 0.9144
17:30:04.095   Training iter 400, batch loss 1.5358, batch acc 0.9076
17:30:04.626   Training iter 450, batch loss 1.5335, batch acc 0.9052
17:30:05.149   Training iter 500, batch loss 1.5283, batch acc 0.9106
17:30:05.680   Training iter 550, batch loss 1.5303, batch acc 0.9134
17:30:06.213   Training iter 600, batch loss 1.5274, batch acc 0.9078
17:30:06.215 Training @ 78 epoch...
17:30:06.740   Training iter 50, batch loss 1.5295, batch acc 0.9050
17:30:07.266   Training iter 100, batch loss 1.5334, batch acc 0.9040
17:30:07.777   Training iter 150, batch loss 1.5373, batch acc 0.8964
17:30:08.290   Training iter 200, batch loss 1.5324, batch acc 0.9128
17:30:08.809   Training iter 250, batch loss 1.5388, batch acc 0.8984
17:30:09.350   Training iter 300, batch loss 1.5425, batch acc 0.8856
17:30:09.880   Training iter 350, batch loss 1.5275, batch acc 0.9068
17:30:10.394   Training iter 400, batch loss 1.5349, batch acc 0.9094
17:30:10.883   Training iter 450, batch loss 1.5347, batch acc 0.8954
17:30:11.403   Training iter 500, batch loss 1.5267, batch acc 0.9080
17:30:11.923   Training iter 550, batch loss 1.5273, batch acc 0.9106
17:30:12.453   Training iter 600, batch loss 1.5357, batch acc 0.9054
17:30:12.454 Training @ 79 epoch...
17:30:12.972   Training iter 50, batch loss 1.5282, batch acc 0.9182
17:30:13.495   Training iter 100, batch loss 1.5339, batch acc 0.9014
17:30:14.052   Training iter 150, batch loss 1.5290, batch acc 0.9168
17:30:14.590   Training iter 200, batch loss 1.5313, batch acc 0.9104
17:30:15.112   Training iter 250, batch loss 1.5270, batch acc 0.9082
17:30:15.619   Training iter 300, batch loss 1.5270, batch acc 0.9126
17:30:16.152   Training iter 350, batch loss 1.5312, batch acc 0.9180
17:30:16.686   Training iter 400, batch loss 1.5289, batch acc 0.9100
17:30:17.217   Training iter 450, batch loss 1.5446, batch acc 0.8758
17:30:17.715   Training iter 500, batch loss 1.5389, batch acc 0.8904
17:30:18.244   Training iter 550, batch loss 1.5281, batch acc 0.9082
17:30:18.776   Training iter 600, batch loss 1.5224, batch acc 0.9242
17:30:18.778 Training @ 80 epoch...
17:30:19.320   Training iter 50, batch loss 1.5261, batch acc 0.9172
17:30:19.866   Training iter 100, batch loss 1.5323, batch acc 0.9148
17:30:20.414   Training iter 150, batch loss 1.5306, batch acc 0.9158
17:30:20.962   Training iter 200, batch loss 1.5294, batch acc 0.9130
17:30:21.513   Training iter 250, batch loss 1.5286, batch acc 0.9180
17:30:22.059   Training iter 300, batch loss 1.5327, batch acc 0.9166
17:30:22.593   Training iter 350, batch loss 1.5294, batch acc 0.9098
17:30:23.157   Training iter 400, batch loss 1.5323, batch acc 0.9072
17:30:23.713   Training iter 450, batch loss 1.5341, batch acc 0.9084
17:30:24.249   Training iter 500, batch loss 1.5355, batch acc 0.9034
17:30:24.808   Training iter 550, batch loss 1.5241, batch acc 0.9094
17:30:25.375   Training iter 600, batch loss 1.5330, batch acc 0.8966
17:30:25.377 Testing @ 80 epoch...
17:30:25.430     Testing, total mean loss 1.53462, total acc 0.90270
17:30:25.430 Training @ 81 epoch...
17:30:26.033   Training iter 50, batch loss 1.5280, batch acc 0.9176
17:30:26.588   Training iter 100, batch loss 1.5307, batch acc 0.9058
17:30:27.131   Training iter 150, batch loss 1.5326, batch acc 0.9130
17:30:27.657   Training iter 200, batch loss 1.5323, batch acc 0.9138
17:30:28.222   Training iter 250, batch loss 1.5375, batch acc 0.9034
17:30:28.821   Training iter 300, batch loss 1.5192, batch acc 0.9230
17:30:29.444   Training iter 350, batch loss 1.5259, batch acc 0.9228
17:30:30.012   Training iter 400, batch loss 1.5264, batch acc 0.9070
17:30:30.587   Training iter 450, batch loss 1.5359, batch acc 0.9000
17:30:31.158   Training iter 500, batch loss 1.5264, batch acc 0.9144
17:30:31.748   Training iter 550, batch loss 1.5267, batch acc 0.9096
17:30:32.299   Training iter 600, batch loss 1.5347, batch acc 0.8996
17:30:32.301 Training @ 82 epoch...
17:30:32.850   Training iter 50, batch loss 1.5310, batch acc 0.9088
17:30:33.362   Training iter 100, batch loss 1.5311, batch acc 0.9054
17:30:33.866   Training iter 150, batch loss 1.5287, batch acc 0.9160
17:30:34.401   Training iter 200, batch loss 1.5307, batch acc 0.9124
17:30:35.024   Training iter 250, batch loss 1.5320, batch acc 0.9086
17:30:35.511   Training iter 300, batch loss 1.5248, batch acc 0.9186
17:30:36.044   Training iter 350, batch loss 1.5309, batch acc 0.9004
17:30:36.571   Training iter 400, batch loss 1.5311, batch acc 0.9130
17:30:37.090   Training iter 450, batch loss 1.5333, batch acc 0.9060
17:30:37.617   Training iter 500, batch loss 1.5305, batch acc 0.9132
17:30:38.162   Training iter 550, batch loss 1.5436, batch acc 0.8966
17:30:38.688   Training iter 600, batch loss 1.5334, batch acc 0.9056
17:30:38.690 Training @ 83 epoch...
17:30:39.232   Training iter 50, batch loss 1.5321, batch acc 0.9102
17:30:39.750   Training iter 100, batch loss 1.5260, batch acc 0.9066
17:30:40.278   Training iter 150, batch loss 1.5376, batch acc 0.8984
17:30:40.778   Training iter 200, batch loss 1.5261, batch acc 0.9128
17:30:41.309   Training iter 250, batch loss 1.5377, batch acc 0.9042
17:30:41.870   Training iter 300, batch loss 1.5307, batch acc 0.9010
17:30:42.423   Training iter 350, batch loss 1.5387, batch acc 0.8950
17:30:42.978   Training iter 400, batch loss 1.5324, batch acc 0.8990
17:30:43.531   Training iter 450, batch loss 1.5346, batch acc 0.9000
17:30:44.080   Training iter 500, batch loss 1.5328, batch acc 0.9088
17:30:44.632   Training iter 550, batch loss 1.5270, batch acc 0.9016
17:30:45.153   Training iter 600, batch loss 1.5284, batch acc 0.9048
17:30:45.155 Training @ 84 epoch...
17:30:45.678   Training iter 50, batch loss 1.5298, batch acc 0.9102
17:30:46.167   Training iter 100, batch loss 1.5254, batch acc 0.9164
17:30:46.668   Training iter 150, batch loss 1.5293, batch acc 0.9098
17:30:47.136   Training iter 200, batch loss 1.5243, batch acc 0.9098
17:30:47.616   Training iter 250, batch loss 1.5381, batch acc 0.8990
17:30:48.111   Training iter 300, batch loss 1.5248, batch acc 0.9150
17:30:48.616   Training iter 350, batch loss 1.5279, batch acc 0.9146
17:30:49.142   Training iter 400, batch loss 1.5352, batch acc 0.8922
17:30:49.665   Training iter 450, batch loss 1.5381, batch acc 0.8920
17:30:50.214   Training iter 500, batch loss 1.5288, batch acc 0.9054
17:30:50.759   Training iter 550, batch loss 1.5289, batch acc 0.9126
17:30:51.311   Training iter 600, batch loss 1.5295, batch acc 0.9058
17:30:51.313 Training @ 85 epoch...
17:30:51.895   Training iter 50, batch loss 1.5298, batch acc 0.9114
17:30:52.526   Training iter 100, batch loss 1.5348, batch acc 0.9062
17:30:53.125   Training iter 150, batch loss 1.5288, batch acc 0.9126
17:30:53.745   Training iter 200, batch loss 1.5288, batch acc 0.9094
17:30:54.367   Training iter 250, batch loss 1.5254, batch acc 0.9104
17:30:54.944   Training iter 300, batch loss 1.5287, batch acc 0.9122
17:30:55.501   Training iter 350, batch loss 1.5454, batch acc 0.8738
17:30:56.080   Training iter 400, batch loss 1.5263, batch acc 0.9144
17:30:56.649   Training iter 450, batch loss 1.5277, batch acc 0.9070
17:30:57.217   Training iter 500, batch loss 1.5271, batch acc 0.9174
17:30:57.772   Training iter 550, batch loss 1.5256, batch acc 0.9154
17:30:58.323   Training iter 600, batch loss 1.5218, batch acc 0.9180
17:30:58.325 Testing @ 85 epoch...
17:30:58.373     Testing, total mean loss 1.52553, total acc 0.91660
17:30:58.373 Training @ 86 epoch...
17:30:58.927   Training iter 50, batch loss 1.5293, batch acc 0.9112
17:30:59.474   Training iter 100, batch loss 1.5257, batch acc 0.9156
17:31:00.010   Training iter 150, batch loss 1.5382, batch acc 0.8974
17:31:00.548   Training iter 200, batch loss 1.5229, batch acc 0.9214
17:31:01.089   Training iter 250, batch loss 1.5307, batch acc 0.9080
17:31:01.659   Training iter 300, batch loss 1.5369, batch acc 0.8988
17:31:02.237   Training iter 350, batch loss 1.5266, batch acc 0.9110
17:31:02.801   Training iter 400, batch loss 1.5377, batch acc 0.9032
17:31:03.361   Training iter 450, batch loss 1.5289, batch acc 0.8972
17:31:03.918   Training iter 500, batch loss 1.5289, batch acc 0.9108
17:31:04.490   Training iter 550, batch loss 1.5252, batch acc 0.9158
17:31:05.054   Training iter 600, batch loss 1.5310, batch acc 0.9112
17:31:05.055 Training @ 87 epoch...
17:31:05.625   Training iter 50, batch loss 1.5317, batch acc 0.8974
17:31:06.206   Training iter 100, batch loss 1.5256, batch acc 0.9140
17:31:06.792   Training iter 150, batch loss 1.5254, batch acc 0.9192
17:31:07.380   Training iter 200, batch loss 1.5183, batch acc 0.9270
17:31:07.954   Training iter 250, batch loss 1.5300, batch acc 0.9160
17:31:08.533   Training iter 300, batch loss 1.5341, batch acc 0.9148
17:31:09.109   Training iter 350, batch loss 1.5262, batch acc 0.9162
17:31:09.675   Training iter 400, batch loss 1.5280, batch acc 0.9140
17:31:10.246   Training iter 450, batch loss 1.5278, batch acc 0.9084
17:31:10.821   Training iter 500, batch loss 1.5224, batch acc 0.9206
17:31:11.423   Training iter 550, batch loss 1.5256, batch acc 0.9202
17:31:12.009   Training iter 600, batch loss 1.5368, batch acc 0.9114
17:31:12.011 Training @ 88 epoch...
17:31:12.585   Training iter 50, batch loss 1.5406, batch acc 0.9026
17:31:13.175   Training iter 100, batch loss 1.5352, batch acc 0.9030
17:31:13.769   Training iter 150, batch loss 1.5315, batch acc 0.9112
17:31:14.379   Training iter 200, batch loss 1.5298, batch acc 0.9126
17:31:14.956   Training iter 250, batch loss 1.5267, batch acc 0.9158
17:31:15.468   Training iter 300, batch loss 1.5243, batch acc 0.9172
17:31:16.012   Training iter 350, batch loss 1.5302, batch acc 0.9194
17:31:16.561   Training iter 400, batch loss 1.5297, batch acc 0.9114
17:31:17.110   Training iter 450, batch loss 1.5250, batch acc 0.9172
17:31:17.647   Training iter 500, batch loss 1.5471, batch acc 0.8856
17:31:18.171   Training iter 550, batch loss 1.5278, batch acc 0.9156
17:31:18.667   Training iter 600, batch loss 1.5347, batch acc 0.9078
17:31:18.669 Training @ 89 epoch...
17:31:19.174   Training iter 50, batch loss 1.5356, batch acc 0.8886
17:31:19.686   Training iter 100, batch loss 1.5320, batch acc 0.9000
17:31:20.194   Training iter 150, batch loss 1.5377, batch acc 0.8926
17:31:20.706   Training iter 200, batch loss 1.5321, batch acc 0.9046
17:31:21.280   Training iter 250, batch loss 1.5331, batch acc 0.9080
17:31:21.823   Training iter 300, batch loss 1.5265, batch acc 0.9160
17:31:22.354   Training iter 350, batch loss 1.5341, batch acc 0.9092
17:31:22.881   Training iter 400, batch loss 1.5224, batch acc 0.9200
17:31:23.413   Training iter 450, batch loss 1.5228, batch acc 0.9246
17:31:23.936   Training iter 500, batch loss 1.5277, batch acc 0.9218
17:31:24.498   Training iter 550, batch loss 1.5377, batch acc 0.9010
17:31:25.226   Training iter 600, batch loss 1.5348, batch acc 0.9070
17:31:25.228 Training @ 90 epoch...
17:31:25.749   Training iter 50, batch loss 1.5288, batch acc 0.9126
17:31:26.246   Training iter 100, batch loss 1.5422, batch acc 0.8972
17:31:26.756   Training iter 150, batch loss 1.5288, batch acc 0.9144
17:31:27.293   Training iter 200, batch loss 1.5358, batch acc 0.9088
17:31:27.816   Training iter 250, batch loss 1.5264, batch acc 0.9090
17:31:28.342   Training iter 300, batch loss 1.5301, batch acc 0.9080
17:31:28.887   Training iter 350, batch loss 1.5394, batch acc 0.8992
17:31:29.445   Training iter 400, batch loss 1.5202, batch acc 0.9182
17:31:29.989   Training iter 450, batch loss 1.5330, batch acc 0.8872
17:31:30.536   Training iter 500, batch loss 1.5202, batch acc 0.9154
17:31:31.100   Training iter 550, batch loss 1.5316, batch acc 0.9004
17:31:31.671   Training iter 600, batch loss 1.5262, batch acc 0.9134
17:31:31.673 Testing @ 90 epoch...
17:31:31.725     Testing, total mean loss 1.53710, total acc 0.90550
17:31:31.725 Training @ 91 epoch...
17:31:32.303   Training iter 50, batch loss 1.5319, batch acc 0.9122
17:31:32.866   Training iter 100, batch loss 1.5448, batch acc 0.8764
17:31:33.431   Training iter 150, batch loss 1.5370, batch acc 0.8938
17:31:33.981   Training iter 200, batch loss 1.5377, batch acc 0.8852
17:31:34.521   Training iter 250, batch loss 1.5293, batch acc 0.8950
17:31:35.072   Training iter 300, batch loss 1.5286, batch acc 0.9040
17:31:35.568   Training iter 350, batch loss 1.5457, batch acc 0.8838
17:31:36.075   Training iter 400, batch loss 1.5313, batch acc 0.9082
17:31:36.618   Training iter 450, batch loss 1.5273, batch acc 0.9070
17:31:37.118   Training iter 500, batch loss 1.5285, batch acc 0.8992
17:31:37.617   Training iter 550, batch loss 1.5302, batch acc 0.8904
17:31:38.166   Training iter 600, batch loss 1.5326, batch acc 0.8866
17:31:38.168 Training @ 92 epoch...
17:31:38.707   Training iter 50, batch loss 1.5368, batch acc 0.8764
17:31:39.244   Training iter 100, batch loss 1.5202, batch acc 0.9148
17:31:39.780   Training iter 150, batch loss 1.5248, batch acc 0.9128
17:31:40.324   Training iter 200, batch loss 1.5321, batch acc 0.9034
17:31:40.854   Training iter 250, batch loss 1.5302, batch acc 0.9036
17:31:41.398   Training iter 300, batch loss 1.5253, batch acc 0.9176
17:31:41.987   Training iter 350, batch loss 1.5317, batch acc 0.9102
17:31:42.575   Training iter 400, batch loss 1.5449, batch acc 0.8888
17:31:43.121   Training iter 450, batch loss 1.5240, batch acc 0.9106
17:31:43.649   Training iter 500, batch loss 1.5345, batch acc 0.8998
17:31:44.157   Training iter 550, batch loss 1.5309, batch acc 0.9076
17:31:44.642   Training iter 600, batch loss 1.5253, batch acc 0.9096
17:31:44.644 Training @ 93 epoch...
17:31:45.123   Training iter 50, batch loss 1.5323, batch acc 0.8982
17:31:45.616   Training iter 100, batch loss 1.5245, batch acc 0.9194
17:31:46.162   Training iter 150, batch loss 1.5263, batch acc 0.9150
17:31:46.705   Training iter 200, batch loss 1.5276, batch acc 0.9100
17:31:47.221   Training iter 250, batch loss 1.5405, batch acc 0.8812
17:31:47.741   Training iter 300, batch loss 1.5297, batch acc 0.9050
17:31:48.270   Training iter 350, batch loss 1.5336, batch acc 0.9110
17:31:48.804   Training iter 400, batch loss 1.5248, batch acc 0.9152
17:31:49.333   Training iter 450, batch loss 1.5360, batch acc 0.8936
17:31:49.871   Training iter 500, batch loss 1.5321, batch acc 0.9088
17:31:50.401   Training iter 550, batch loss 1.5237, batch acc 0.9144
17:31:50.944   Training iter 600, batch loss 1.5285, batch acc 0.9110
17:31:50.945 Training @ 94 epoch...
17:31:51.508   Training iter 50, batch loss 1.5245, batch acc 0.9154
17:31:52.076   Training iter 100, batch loss 1.5335, batch acc 0.8992
17:31:52.629   Training iter 150, batch loss 1.5231, batch acc 0.9154
17:31:53.171   Training iter 200, batch loss 1.5219, batch acc 0.9210
17:31:53.738   Training iter 250, batch loss 1.5274, batch acc 0.9114
17:31:54.341   Training iter 300, batch loss 1.5315, batch acc 0.9084
17:31:54.934   Training iter 350, batch loss 1.5350, batch acc 0.8960
17:31:55.501   Training iter 400, batch loss 1.5192, batch acc 0.9156
17:31:56.118   Training iter 450, batch loss 1.5317, batch acc 0.8936
17:31:56.693   Training iter 500, batch loss 1.5304, batch acc 0.9066
17:31:57.286   Training iter 550, batch loss 1.5328, batch acc 0.9022
17:31:57.883   Training iter 600, batch loss 1.5329, batch acc 0.9010
17:31:57.885 Training @ 95 epoch...
17:31:58.494   Training iter 50, batch loss 1.5349, batch acc 0.8986
17:31:59.094   Training iter 100, batch loss 1.5276, batch acc 0.9136
17:31:59.696   Training iter 150, batch loss 1.5299, batch acc 0.9088
17:32:00.247   Training iter 200, batch loss 1.5244, batch acc 0.9148
17:32:00.796   Training iter 250, batch loss 1.5321, batch acc 0.9042
17:32:01.370   Training iter 300, batch loss 1.5240, batch acc 0.9166
17:32:01.958   Training iter 350, batch loss 1.5257, batch acc 0.9122
17:32:02.547   Training iter 400, batch loss 1.5254, batch acc 0.9118
17:32:03.136   Training iter 450, batch loss 1.5294, batch acc 0.9152
17:32:03.717   Training iter 500, batch loss 1.5259, batch acc 0.9058
17:32:04.296   Training iter 550, batch loss 1.5417, batch acc 0.8944
17:32:04.870   Training iter 600, batch loss 1.5403, batch acc 0.9006
17:32:04.872 Testing @ 95 epoch...
17:32:04.920     Testing, total mean loss 1.52787, total acc 0.90900
17:32:04.921 Training @ 96 epoch...
17:32:05.507   Training iter 50, batch loss 1.5204, batch acc 0.9244
17:32:06.098   Training iter 100, batch loss 1.5352, batch acc 0.8974
17:32:06.695   Training iter 150, batch loss 1.5179, batch acc 0.9252
17:32:07.305   Training iter 200, batch loss 1.5284, batch acc 0.9126
17:32:07.940   Training iter 250, batch loss 1.5357, batch acc 0.8994
17:32:08.549   Training iter 300, batch loss 1.5252, batch acc 0.9196
17:32:09.124   Training iter 350, batch loss 1.5292, batch acc 0.9042
17:32:09.690   Training iter 400, batch loss 1.5276, batch acc 0.9160
17:32:10.263   Training iter 450, batch loss 1.5299, batch acc 0.9188
17:32:10.833   Training iter 500, batch loss 1.5281, batch acc 0.9180
17:32:11.417   Training iter 550, batch loss 1.5312, batch acc 0.9088
17:32:11.982   Training iter 600, batch loss 1.5324, batch acc 0.9016
17:32:11.984 Training @ 97 epoch...
17:32:12.577   Training iter 50, batch loss 1.5278, batch acc 0.9162
17:32:13.153   Training iter 100, batch loss 1.5360, batch acc 0.9042
17:32:13.730   Training iter 150, batch loss 1.5207, batch acc 0.9254
17:32:14.301   Training iter 200, batch loss 1.5286, batch acc 0.9080
17:32:14.876   Training iter 250, batch loss 1.5167, batch acc 0.9232
17:32:15.426   Training iter 300, batch loss 1.5269, batch acc 0.9194
17:32:15.983   Training iter 350, batch loss 1.5253, batch acc 0.9188
17:32:16.535   Training iter 400, batch loss 1.5241, batch acc 0.9212
17:32:17.098   Training iter 450, batch loss 1.5292, batch acc 0.8998
17:32:17.648   Training iter 500, batch loss 1.5262, batch acc 0.9048
17:32:18.213   Training iter 550, batch loss 1.5307, batch acc 0.8978
17:32:18.764   Training iter 600, batch loss 1.5274, batch acc 0.9122
17:32:18.766 Training @ 98 epoch...
17:32:19.325   Training iter 50, batch loss 1.5249, batch acc 0.9128
17:32:19.890   Training iter 100, batch loss 1.5247, batch acc 0.9168
17:32:20.478   Training iter 150, batch loss 1.5286, batch acc 0.9030
17:32:21.035   Training iter 200, batch loss 1.5247, batch acc 0.9142
17:32:21.584   Training iter 250, batch loss 1.5282, batch acc 0.9014
17:32:22.257   Training iter 300, batch loss 1.5274, batch acc 0.9082
17:32:22.814   Training iter 350, batch loss 1.5258, batch acc 0.9174
17:32:23.373   Training iter 400, batch loss 1.5274, batch acc 0.9108
17:32:23.919   Training iter 450, batch loss 1.5265, batch acc 0.9156
17:32:24.467   Training iter 500, batch loss 1.5233, batch acc 0.9148
17:32:25.022   Training iter 550, batch loss 1.5345, batch acc 0.8946
17:32:25.589   Training iter 600, batch loss 1.5292, batch acc 0.9076
17:32:25.591 Training @ 99 epoch...
17:32:26.143   Training iter 50, batch loss 1.5249, batch acc 0.9190
17:32:26.706   Training iter 100, batch loss 1.5310, batch acc 0.9094
17:32:27.280   Training iter 150, batch loss 1.5233, batch acc 0.9202
17:32:27.867   Training iter 200, batch loss 1.5181, batch acc 0.9244
17:32:28.458   Training iter 250, batch loss 1.5441, batch acc 0.8892
17:32:29.071   Training iter 300, batch loss 1.5313, batch acc 0.9074
17:32:29.665   Training iter 350, batch loss 1.5291, batch acc 0.9090
17:32:30.280   Training iter 400, batch loss 1.5325, batch acc 0.9056
17:32:30.907   Training iter 450, batch loss 1.5393, batch acc 0.8824
17:32:31.550   Training iter 500, batch loss 1.5284, batch acc 0.8882
17:32:32.181   Training iter 550, batch loss 1.5344, batch acc 0.8980
17:32:32.797   Training iter 600, batch loss 1.5357, batch acc 0.9022
17:32:32.799 Training @ 100 epoch...
17:32:33.421   Training iter 50, batch loss 1.5301, batch acc 0.9002
17:32:34.037   Training iter 100, batch loss 1.5284, batch acc 0.9068
17:32:34.652   Training iter 150, batch loss 1.5320, batch acc 0.9022
17:32:35.251   Training iter 200, batch loss 1.5501, batch acc 0.8740
17:32:35.867   Training iter 250, batch loss 1.5233, batch acc 0.9088
17:32:36.503   Training iter 300, batch loss 1.5322, batch acc 0.9056
17:32:37.124   Training iter 350, batch loss 1.5300, batch acc 0.9086
17:32:37.719   Training iter 400, batch loss 1.5305, batch acc 0.9048
17:32:38.324   Training iter 450, batch loss 1.5301, batch acc 0.9056
17:32:38.925   Training iter 500, batch loss 1.5311, batch acc 0.9098
17:32:39.526   Training iter 550, batch loss 1.5240, batch acc 0.9140
17:32:40.116   Training iter 600, batch loss 1.5328, batch acc 0.8866
17:32:40.118 Testing @ 100 epoch...
17:32:40.174     Testing, total mean loss 1.52334, total acc 0.90980
17:32:40.174 Plot @ 100 epoch...
17:32:40.175 Training @ 101 epoch...
17:32:40.769   Training iter 50, batch loss 1.5216, batch acc 0.9204
17:32:41.327   Training iter 100, batch loss 1.5261, batch acc 0.9162
17:32:41.858   Training iter 150, batch loss 1.5260, batch acc 0.9168
17:32:42.390   Training iter 200, batch loss 1.5249, batch acc 0.9244
17:32:42.945   Training iter 250, batch loss 1.5299, batch acc 0.9046
17:32:43.496   Training iter 300, batch loss 1.5284, batch acc 0.9032
17:32:44.058   Training iter 350, batch loss 1.5267, batch acc 0.9176
17:32:44.617   Training iter 400, batch loss 1.5265, batch acc 0.9106
17:32:45.177   Training iter 450, batch loss 1.5422, batch acc 0.8928
17:32:45.735   Training iter 500, batch loss 1.5325, batch acc 0.9040
17:32:46.297   Training iter 550, batch loss 1.5308, batch acc 0.9058
17:32:46.866   Training iter 600, batch loss 1.5333, batch acc 0.9176
17:32:46.868 Training @ 102 epoch...
17:32:47.448   Training iter 50, batch loss 1.5251, batch acc 0.9166
17:32:48.015   Training iter 100, batch loss 1.5230, batch acc 0.9196
17:32:48.564   Training iter 150, batch loss 1.5361, batch acc 0.9006
17:32:49.086   Training iter 200, batch loss 1.5302, batch acc 0.9004
17:32:49.616   Training iter 250, batch loss 1.5213, batch acc 0.9096
17:32:50.163   Training iter 300, batch loss 1.5303, batch acc 0.9106
17:32:50.705   Training iter 350, batch loss 1.5283, batch acc 0.9130
17:32:51.250   Training iter 400, batch loss 1.5278, batch acc 0.9156
17:32:51.802   Training iter 450, batch loss 1.5328, batch acc 0.9090
17:32:52.350   Training iter 500, batch loss 1.5278, batch acc 0.9206
17:32:52.886   Training iter 550, batch loss 1.5281, batch acc 0.9048
17:32:53.421   Training iter 600, batch loss 1.5313, batch acc 0.8930
17:32:53.423 Training @ 103 epoch...
17:32:53.962   Training iter 50, batch loss 1.5361, batch acc 0.8966
17:32:54.494   Training iter 100, batch loss 1.5336, batch acc 0.8872
17:32:55.016   Training iter 150, batch loss 1.5309, batch acc 0.9004
17:32:55.531   Training iter 200, batch loss 1.5327, batch acc 0.9030
17:32:56.071   Training iter 250, batch loss 1.5297, batch acc 0.9048
17:32:56.604   Training iter 300, batch loss 1.5257, batch acc 0.9098
17:32:57.121   Training iter 350, batch loss 1.5308, batch acc 0.8952
17:32:57.647   Training iter 400, batch loss 1.5287, batch acc 0.9044
17:32:58.140   Training iter 450, batch loss 1.5338, batch acc 0.8930
17:32:58.674   Training iter 500, batch loss 1.5273, batch acc 0.9132
17:32:59.313   Training iter 550, batch loss 1.5313, batch acc 0.9096
17:32:59.900   Training iter 600, batch loss 1.5487, batch acc 0.8916
17:32:59.902 Training @ 104 epoch...
17:33:00.481   Training iter 50, batch loss 1.5354, batch acc 0.8994
17:33:01.049   Training iter 100, batch loss 1.5291, batch acc 0.8958
17:33:01.619   Training iter 150, batch loss 1.5319, batch acc 0.8952
17:33:02.205   Training iter 200, batch loss 1.5210, batch acc 0.9160
17:33:02.761   Training iter 250, batch loss 1.5292, batch acc 0.9140
17:33:03.308   Training iter 300, batch loss 1.5368, batch acc 0.8982
17:33:03.852   Training iter 350, batch loss 1.5277, batch acc 0.9122
17:33:04.407   Training iter 400, batch loss 1.5316, batch acc 0.9022
17:33:04.982   Training iter 450, batch loss 1.5248, batch acc 0.9100
17:33:05.539   Training iter 500, batch loss 1.5358, batch acc 0.9076
17:33:06.122   Training iter 550, batch loss 1.5327, batch acc 0.9038
17:33:06.715   Training iter 600, batch loss 1.5453, batch acc 0.8840
17:33:06.716 Training @ 105 epoch...
17:33:07.302   Training iter 50, batch loss 1.5186, batch acc 0.9258
17:33:07.824   Training iter 100, batch loss 1.5239, batch acc 0.9194
17:33:08.290   Training iter 150, batch loss 1.5244, batch acc 0.9086
17:33:08.756   Training iter 200, batch loss 1.5354, batch acc 0.9092
17:33:09.220   Training iter 250, batch loss 1.5316, batch acc 0.9012
17:33:09.695   Training iter 300, batch loss 1.5285, batch acc 0.9088
17:33:10.188   Training iter 350, batch loss 1.5378, batch acc 0.8970
17:33:10.671   Training iter 400, batch loss 1.5367, batch acc 0.8926
17:33:11.172   Training iter 450, batch loss 1.5290, batch acc 0.9102
17:33:11.662   Training iter 500, batch loss 1.5415, batch acc 0.8816
17:33:12.147   Training iter 550, batch loss 1.5191, batch acc 0.9176
17:33:12.621   Training iter 600, batch loss 1.5357, batch acc 0.9006
17:33:12.622 Testing @ 105 epoch...
17:33:12.674     Testing, total mean loss 1.52811, total acc 0.90770
17:33:12.674 Training @ 106 epoch...
17:33:13.213   Training iter 50, batch loss 1.5225, batch acc 0.9110
17:33:13.737   Training iter 100, batch loss 1.5202, batch acc 0.9144
17:33:14.265   Training iter 150, batch loss 1.5371, batch acc 0.9012
17:33:14.805   Training iter 200, batch loss 1.5281, batch acc 0.9026
17:33:15.369   Training iter 250, batch loss 1.5250, batch acc 0.9040
17:33:15.931   Training iter 300, batch loss 1.5360, batch acc 0.8946
17:33:16.490   Training iter 350, batch loss 1.5239, batch acc 0.9074
17:33:17.065   Training iter 400, batch loss 1.5335, batch acc 0.9096
17:33:17.625   Training iter 450, batch loss 1.5356, batch acc 0.9080
17:33:18.207   Training iter 500, batch loss 1.5332, batch acc 0.8972
17:33:18.772   Training iter 550, batch loss 1.5331, batch acc 0.9096
17:33:19.353   Training iter 600, batch loss 1.5268, batch acc 0.9058
17:33:19.355 Training @ 107 epoch...
17:33:19.915   Training iter 50, batch loss 1.5269, batch acc 0.9136
17:33:20.464   Training iter 100, batch loss 1.5311, batch acc 0.9118
17:33:20.966   Training iter 150, batch loss 1.5224, batch acc 0.9176
17:33:21.469   Training iter 200, batch loss 1.5369, batch acc 0.8946
17:33:21.963   Training iter 250, batch loss 1.5253, batch acc 0.9132
17:33:22.469   Training iter 300, batch loss 1.5359, batch acc 0.8964
17:33:23.010   Training iter 350, batch loss 1.5614, batch acc 0.8536
17:33:23.546   Training iter 400, batch loss 1.5278, batch acc 0.9044
17:33:24.098   Training iter 450, batch loss 1.5284, batch acc 0.9176
17:33:24.637   Training iter 500, batch loss 1.5283, batch acc 0.9166
17:33:25.196   Training iter 550, batch loss 1.5275, batch acc 0.9020
17:33:25.722   Training iter 600, batch loss 1.5423, batch acc 0.8788
17:33:25.724 Training @ 108 epoch...
17:33:26.264   Training iter 50, batch loss 1.5295, batch acc 0.9096
17:33:26.772   Training iter 100, batch loss 1.5323, batch acc 0.9010
17:33:27.291   Training iter 150, batch loss 1.5306, batch acc 0.9042
17:33:27.810   Training iter 200, batch loss 1.5311, batch acc 0.9094
17:33:28.327   Training iter 250, batch loss 1.5411, batch acc 0.8800
17:33:28.844   Training iter 300, batch loss 1.5305, batch acc 0.9030
17:33:29.349   Training iter 350, batch loss 1.5341, batch acc 0.8940
17:33:29.864   Training iter 400, batch loss 1.5256, batch acc 0.9234
17:33:30.399   Training iter 450, batch loss 1.5320, batch acc 0.9064
17:33:30.936   Training iter 500, batch loss 1.5363, batch acc 0.8920
17:33:31.464   Training iter 550, batch loss 1.5328, batch acc 0.9014
17:33:32.001   Training iter 600, batch loss 1.5251, batch acc 0.9162
17:33:32.003 Training @ 109 epoch...
17:33:32.537   Training iter 50, batch loss 1.5237, batch acc 0.9168
17:33:33.065   Training iter 100, batch loss 1.5336, batch acc 0.9032
17:33:33.578   Training iter 150, batch loss 1.5384, batch acc 0.8906
17:33:34.098   Training iter 200, batch loss 1.5307, batch acc 0.9010
17:33:34.629   Training iter 250, batch loss 1.5258, batch acc 0.9058
17:33:35.145   Training iter 300, batch loss 1.5194, batch acc 0.9142
17:33:35.680   Training iter 350, batch loss 1.5318, batch acc 0.9038
17:33:36.220   Training iter 400, batch loss 1.5411, batch acc 0.8900
17:33:36.748   Training iter 450, batch loss 1.5580, batch acc 0.8534
17:33:37.276   Training iter 500, batch loss 1.5480, batch acc 0.8602
17:33:37.809   Training iter 550, batch loss 1.5434, batch acc 0.8608
17:33:38.347   Training iter 600, batch loss 1.5417, batch acc 0.8720
17:33:38.349 Training @ 110 epoch...
17:33:38.876   Training iter 50, batch loss 1.5289, batch acc 0.8972
17:33:39.399   Training iter 100, batch loss 1.5347, batch acc 0.8936
17:33:39.925   Training iter 150, batch loss 1.5287, batch acc 0.9124
17:33:40.432   Training iter 200, batch loss 1.5299, batch acc 0.8980
17:33:40.925   Training iter 250, batch loss 1.5290, batch acc 0.9060
17:33:41.420   Training iter 300, batch loss 1.5291, batch acc 0.9160
17:33:41.916   Training iter 350, batch loss 1.5208, batch acc 0.9236
17:33:42.410   Training iter 400, batch loss 1.5271, batch acc 0.9050
17:33:42.914   Training iter 450, batch loss 1.5326, batch acc 0.9056
17:33:43.431   Training iter 500, batch loss 1.5322, batch acc 0.9080
17:33:43.968   Training iter 550, batch loss 1.5204, batch acc 0.9214
17:33:44.498   Training iter 600, batch loss 1.5286, batch acc 0.9128
17:33:44.500 Testing @ 110 epoch...
17:33:44.550     Testing, total mean loss 1.52802, total acc 0.91510
17:33:44.551 Training @ 111 epoch...
17:33:45.094   Training iter 50, batch loss 1.5275, batch acc 0.9188
17:33:45.596   Training iter 100, batch loss 1.5326, batch acc 0.9060
17:33:46.109   Training iter 150, batch loss 1.5343, batch acc 0.9112
17:33:46.618   Training iter 200, batch loss 1.5309, batch acc 0.9178
17:33:47.140   Training iter 250, batch loss 1.5274, batch acc 0.9044
17:33:47.652   Training iter 300, batch loss 1.5277, batch acc 0.9134
17:33:48.158   Training iter 350, batch loss 1.5354, batch acc 0.9028
17:33:48.678   Training iter 400, batch loss 1.5246, batch acc 0.9144
17:33:49.225   Training iter 450, batch loss 1.5254, batch acc 0.9160
17:33:49.770   Training iter 500, batch loss 1.5416, batch acc 0.8950
17:33:50.328   Training iter 550, batch loss 1.5368, batch acc 0.8968
17:33:50.889   Training iter 600, batch loss 1.5289, batch acc 0.9046
17:33:50.891 Training @ 112 epoch...
17:33:51.433   Training iter 50, batch loss 1.5323, batch acc 0.8952
17:33:51.959   Training iter 100, batch loss 1.5327, batch acc 0.8994
17:33:52.491   Training iter 150, batch loss 1.5234, batch acc 0.9146
17:33:53.025   Training iter 200, batch loss 1.5304, batch acc 0.9018
17:33:53.523   Training iter 250, batch loss 1.5293, batch acc 0.8964
17:33:54.013   Training iter 300, batch loss 1.5283, batch acc 0.9018
17:33:54.505   Training iter 350, batch loss 1.5205, batch acc 0.9130
17:33:54.996   Training iter 400, batch loss 1.5240, batch acc 0.9160
17:33:55.491   Training iter 450, batch loss 1.5280, batch acc 0.9038
17:33:55.987   Training iter 500, batch loss 1.5286, batch acc 0.9036
17:33:56.508   Training iter 550, batch loss 1.5386, batch acc 0.8840
17:33:57.004   Training iter 600, batch loss 1.5289, batch acc 0.8976
17:33:57.006 Training @ 113 epoch...
17:33:57.513   Training iter 50, batch loss 1.5307, batch acc 0.8908
17:33:58.021   Training iter 100, batch loss 1.5200, batch acc 0.9136
17:33:58.514   Training iter 150, batch loss 1.5281, batch acc 0.8906
17:33:59.007   Training iter 200, batch loss 1.5143, batch acc 0.9180
17:33:59.499   Training iter 250, batch loss 1.5272, batch acc 0.9116
17:33:59.999   Training iter 300, batch loss 1.5571, batch acc 0.8448
17:34:00.490   Training iter 350, batch loss 1.5370, batch acc 0.8880
17:34:00.982   Training iter 400, batch loss 1.5278, batch acc 0.9048
17:34:01.494   Training iter 450, batch loss 1.5382, batch acc 0.8744
17:34:02.083   Training iter 500, batch loss 1.5334, batch acc 0.8934
17:34:02.644   Training iter 550, batch loss 1.5267, batch acc 0.9108
17:34:03.195   Training iter 600, batch loss 1.5317, batch acc 0.9040
17:34:03.197 Training @ 114 epoch...
17:34:03.745   Training iter 50, batch loss 1.5271, batch acc 0.9068
17:34:04.305   Training iter 100, batch loss 1.5287, batch acc 0.9116
17:34:04.876   Training iter 150, batch loss 1.5430, batch acc 0.8794
17:34:05.441   Training iter 200, batch loss 1.5264, batch acc 0.9058
17:34:06.003   Training iter 250, batch loss 1.5250, batch acc 0.9144
17:34:06.590   Training iter 300, batch loss 1.5231, batch acc 0.9218
17:34:07.173   Training iter 350, batch loss 1.5308, batch acc 0.9164
17:34:07.765   Training iter 400, batch loss 1.5268, batch acc 0.9146
17:34:08.350   Training iter 450, batch loss 1.5185, batch acc 0.9216
17:34:08.901   Training iter 500, batch loss 1.5218, batch acc 0.9124
17:34:09.458   Training iter 550, batch loss 1.5257, batch acc 0.9064
17:34:10.009   Training iter 600, batch loss 1.5352, batch acc 0.8840
17:34:10.011 Training @ 115 epoch...
17:34:10.574   Training iter 50, batch loss 1.5299, batch acc 0.9024
17:34:11.102   Training iter 100, batch loss 1.5654, batch acc 0.8496
17:34:11.624   Training iter 150, batch loss 1.5412, batch acc 0.8876
17:34:12.150   Training iter 200, batch loss 1.5438, batch acc 0.8830
17:34:12.674   Training iter 250, batch loss 1.5464, batch acc 0.8884
17:34:13.197   Training iter 300, batch loss 1.5335, batch acc 0.8946
17:34:13.722   Training iter 350, batch loss 1.5214, batch acc 0.9096
17:34:14.262   Training iter 400, batch loss 1.5271, batch acc 0.9116
17:34:14.789   Training iter 450, batch loss 1.5365, batch acc 0.8818
17:34:15.310   Training iter 500, batch loss 1.5247, batch acc 0.9094
17:34:15.839   Training iter 550, batch loss 1.5372, batch acc 0.8754
17:34:16.357   Training iter 600, batch loss 1.5265, batch acc 0.9062
17:34:16.359 Testing @ 115 epoch...
17:34:16.407     Testing, total mean loss 1.53793, total acc 0.88290
17:34:16.407 Training @ 116 epoch...
17:34:16.953   Training iter 50, batch loss 1.5304, batch acc 0.8974
17:34:17.478   Training iter 100, batch loss 1.5280, batch acc 0.9036
17:34:18.079   Training iter 150, batch loss 1.5403, batch acc 0.8986
17:34:18.766   Training iter 200, batch loss 1.5411, batch acc 0.8960
17:34:19.362   Training iter 250, batch loss 1.5270, batch acc 0.9132
17:34:19.908   Training iter 300, batch loss 1.5233, batch acc 0.9220
17:34:20.469   Training iter 350, batch loss 1.5318, batch acc 0.9034
17:34:21.023   Training iter 400, batch loss 1.5453, batch acc 0.8760
17:34:21.564   Training iter 450, batch loss 1.5361, batch acc 0.8924
17:34:22.104   Training iter 500, batch loss 1.5232, batch acc 0.9128
17:34:22.664   Training iter 550, batch loss 1.5253, batch acc 0.9068
17:34:23.224   Training iter 600, batch loss 1.5267, batch acc 0.9104
17:34:23.226 Training @ 117 epoch...
17:34:23.766   Training iter 50, batch loss 1.5228, batch acc 0.9144
17:34:24.310   Training iter 100, batch loss 1.5224, batch acc 0.9136
17:34:24.859   Training iter 150, batch loss 1.5245, batch acc 0.9196
17:34:25.440   Training iter 200, batch loss 1.5292, batch acc 0.9052
17:34:26.031   Training iter 250, batch loss 1.5407, batch acc 0.9074
17:34:26.617   Training iter 300, batch loss 1.5452, batch acc 0.8890
17:34:27.167   Training iter 350, batch loss 1.5509, batch acc 0.8724
17:34:27.699   Training iter 400, batch loss 1.5256, batch acc 0.9128
17:34:28.237   Training iter 450, batch loss 1.5284, batch acc 0.9112
17:34:28.761   Training iter 500, batch loss 1.5322, batch acc 0.9034
17:34:29.293   Training iter 550, batch loss 1.5285, batch acc 0.9072
17:34:29.817   Training iter 600, batch loss 1.5417, batch acc 0.8844
17:34:29.819 Training @ 118 epoch...
17:34:30.381   Training iter 50, batch loss 1.5409, batch acc 0.8942
17:34:31.044   Training iter 100, batch loss 1.5317, batch acc 0.8860
17:34:31.692   Training iter 150, batch loss 1.5357, batch acc 0.8864
17:34:32.321   Training iter 200, batch loss 1.5193, batch acc 0.9166
17:34:32.975   Training iter 250, batch loss 1.5300, batch acc 0.9118
17:34:33.600   Training iter 300, batch loss 1.5274, batch acc 0.9050
17:34:34.146   Training iter 350, batch loss 1.5341, batch acc 0.8958
17:34:34.704   Training iter 400, batch loss 1.5261, batch acc 0.9084
17:34:35.249   Training iter 450, batch loss 1.5310, batch acc 0.8994
17:34:35.786   Training iter 500, batch loss 1.5352, batch acc 0.8848
17:34:36.334   Training iter 550, batch loss 1.5282, batch acc 0.8866
17:34:36.879   Training iter 600, batch loss 1.5245, batch acc 0.9178
17:34:36.881 Training @ 119 epoch...
17:34:37.428   Training iter 50, batch loss 1.5260, batch acc 0.9138
17:34:37.984   Training iter 100, batch loss 1.5228, batch acc 0.9202
17:34:38.570   Training iter 150, batch loss 1.5311, batch acc 0.9096
17:34:39.122   Training iter 200, batch loss 1.5189, batch acc 0.9206
17:34:39.672   Training iter 250, batch loss 1.5420, batch acc 0.8904
17:34:40.234   Training iter 300, batch loss 1.5273, batch acc 0.9106
17:34:40.758   Training iter 350, batch loss 1.5327, batch acc 0.9034
17:34:41.280   Training iter 400, batch loss 1.5361, batch acc 0.8840
17:34:41.800   Training iter 450, batch loss 1.5320, batch acc 0.9028
17:34:42.321   Training iter 500, batch loss 1.5466, batch acc 0.8698
17:34:42.838   Training iter 550, batch loss 1.5223, batch acc 0.9144
17:34:43.356   Training iter 600, batch loss 1.5292, batch acc 0.9026
17:34:43.358 Training @ 120 epoch...
17:34:43.898   Training iter 50, batch loss 1.5339, batch acc 0.8846
17:34:44.439   Training iter 100, batch loss 1.5216, batch acc 0.9070
17:34:44.965   Training iter 150, batch loss 1.5310, batch acc 0.9080
17:34:45.501   Training iter 200, batch loss 1.5250, batch acc 0.9144
17:34:46.017   Training iter 250, batch loss 1.5288, batch acc 0.9118
17:34:46.513   Training iter 300, batch loss 1.5350, batch acc 0.9136
17:34:47.024   Training iter 350, batch loss 1.5272, batch acc 0.9172
17:34:47.520   Training iter 400, batch loss 1.5210, batch acc 0.9214
17:34:48.018   Training iter 450, batch loss 1.5298, batch acc 0.9116
17:34:48.505   Training iter 500, batch loss 1.5349, batch acc 0.8940
17:34:48.986   Training iter 550, batch loss 1.5303, batch acc 0.9080
17:34:49.489   Training iter 600, batch loss 1.5289, batch acc 0.9052
17:34:49.491 Testing @ 120 epoch...
17:34:49.539     Testing, total mean loss 1.53368, total acc 0.90250
17:34:49.539 Training @ 121 epoch...
17:34:50.044   Training iter 50, batch loss 1.5346, batch acc 0.8912
17:34:50.567   Training iter 100, batch loss 1.5442, batch acc 0.8904
17:34:51.106   Training iter 150, batch loss 1.5366, batch acc 0.9004
17:34:51.654   Training iter 200, batch loss 1.5273, batch acc 0.9102
17:34:52.202   Training iter 250, batch loss 1.5387, batch acc 0.8808
17:34:52.747   Training iter 300, batch loss 1.5354, batch acc 0.9128
17:34:53.297   Training iter 350, batch loss 1.5196, batch acc 0.9200
17:34:53.866   Training iter 400, batch loss 1.5316, batch acc 0.9128
17:34:54.435   Training iter 450, batch loss 1.5329, batch acc 0.9068
17:34:54.993   Training iter 500, batch loss 1.5280, batch acc 0.9188
17:34:55.552   Training iter 550, batch loss 1.5319, batch acc 0.9142
17:34:56.107   Training iter 600, batch loss 1.5400, batch acc 0.9082
17:34:56.108 Training @ 122 epoch...
17:34:56.654   Training iter 50, batch loss 1.5294, batch acc 0.9116
17:34:57.184   Training iter 100, batch loss 1.5277, batch acc 0.9156
17:34:57.696   Training iter 150, batch loss 1.5399, batch acc 0.8932
17:34:58.212   Training iter 200, batch loss 1.5305, batch acc 0.9142
17:34:58.740   Training iter 250, batch loss 1.5346, batch acc 0.8884
17:34:59.256   Training iter 300, batch loss 1.5257, batch acc 0.9108
17:34:59.784   Training iter 350, batch loss 1.5180, batch acc 0.9196
17:35:00.326   Training iter 400, batch loss 1.5292, batch acc 0.9184
17:35:00.881   Training iter 450, batch loss 1.5295, batch acc 0.9000
17:35:01.513   Training iter 500, batch loss 1.5303, batch acc 0.8922
17:35:02.123   Training iter 550, batch loss 1.5337, batch acc 0.9004
17:35:02.658   Training iter 600, batch loss 1.5292, batch acc 0.9064
17:35:02.660 Training @ 123 epoch...
17:35:03.200   Training iter 50, batch loss 1.5167, batch acc 0.9304
17:35:03.713   Training iter 100, batch loss 1.5205, batch acc 0.9172
17:35:04.251   Training iter 150, batch loss 1.5250, batch acc 0.9072
17:35:04.802   Training iter 200, batch loss 1.5246, batch acc 0.9190
17:35:05.332   Training iter 250, batch loss 1.5232, batch acc 0.9170
17:35:05.926   Training iter 300, batch loss 1.5274, batch acc 0.9096
17:35:06.530   Training iter 350, batch loss 1.5314, batch acc 0.9034
17:35:07.130   Training iter 400, batch loss 1.5428, batch acc 0.8684
17:35:07.650   Training iter 450, batch loss 1.5238, batch acc 0.9124
17:35:08.174   Training iter 500, batch loss 1.5319, batch acc 0.8986
17:35:08.706   Training iter 550, batch loss 1.5251, batch acc 0.9080
17:35:09.252   Training iter 600, batch loss 1.5292, batch acc 0.9114
17:35:09.254 Training @ 124 epoch...
17:35:09.795   Training iter 50, batch loss 1.5295, batch acc 0.8972
17:35:10.339   Training iter 100, batch loss 1.5362, batch acc 0.8868
17:35:10.871   Training iter 150, batch loss 1.5237, batch acc 0.9082
17:35:11.392   Training iter 200, batch loss 1.5290, batch acc 0.9044
17:35:11.925   Training iter 250, batch loss 1.5417, batch acc 0.8964
17:35:12.456   Training iter 300, batch loss 1.5321, batch acc 0.8870
17:35:13.006   Training iter 350, batch loss 1.5283, batch acc 0.9064
17:35:13.542   Training iter 400, batch loss 1.5243, batch acc 0.9134
17:35:14.083   Training iter 450, batch loss 1.5174, batch acc 0.9286
17:35:14.626   Training iter 500, batch loss 1.5300, batch acc 0.9188
17:35:15.147   Training iter 550, batch loss 1.5255, batch acc 0.9136
17:35:15.681   Training iter 600, batch loss 1.5295, batch acc 0.9036
17:35:15.683 Training @ 125 epoch...
17:35:16.216   Training iter 50, batch loss 1.5405, batch acc 0.8954
17:35:16.753   Training iter 100, batch loss 1.5345, batch acc 0.8972
17:35:17.279   Training iter 150, batch loss 1.5390, batch acc 0.8826
17:35:17.813   Training iter 200, batch loss 1.5349, batch acc 0.8976
17:35:18.347   Training iter 250, batch loss 1.5401, batch acc 0.9054
17:35:18.886   Training iter 300, batch loss 1.5324, batch acc 0.9024
17:35:19.434   Training iter 350, batch loss 1.5262, batch acc 0.9094
17:35:19.992   Training iter 400, batch loss 1.5301, batch acc 0.9030
17:35:20.560   Training iter 450, batch loss 1.5225, batch acc 0.9102
17:35:21.131   Training iter 500, batch loss 1.5242, batch acc 0.9072
17:35:21.701   Training iter 550, batch loss 1.5285, batch acc 0.9060
17:35:22.267   Training iter 600, batch loss 1.5412, batch acc 0.8796
17:35:22.269 Testing @ 125 epoch...
17:35:22.319     Testing, total mean loss 1.56091, total acc 0.84130
17:35:22.319 Training @ 126 epoch...
17:35:22.919   Training iter 50, batch loss 1.5348, batch acc 0.8874
17:35:23.511   Training iter 100, batch loss 1.5212, batch acc 0.9150
17:35:24.103   Training iter 150, batch loss 1.5291, batch acc 0.9110
17:35:24.695   Training iter 200, batch loss 1.5307, batch acc 0.9110
17:35:25.284   Training iter 250, batch loss 1.5250, batch acc 0.9026
17:35:25.885   Training iter 300, batch loss 1.5300, batch acc 0.9078
17:35:26.516   Training iter 350, batch loss 1.5392, batch acc 0.8960
17:35:27.094   Training iter 400, batch loss 1.5420, batch acc 0.8506
17:35:27.663   Training iter 450, batch loss 1.5275, batch acc 0.9010
17:35:28.222   Training iter 500, batch loss 1.5346, batch acc 0.8808
17:35:28.719   Training iter 550, batch loss 1.5318, batch acc 0.8644
17:35:29.234   Training iter 600, batch loss 1.5354, batch acc 0.8860
17:35:29.236 Training @ 127 epoch...
17:35:29.752   Training iter 50, batch loss 1.5301, batch acc 0.9098
17:35:30.244   Training iter 100, batch loss 1.5220, batch acc 0.9180
17:35:30.744   Training iter 150, batch loss 1.5347, batch acc 0.8942
17:35:31.256   Training iter 200, batch loss 1.5342, batch acc 0.8844
17:35:31.769   Training iter 250, batch loss 1.5247, batch acc 0.9082
17:35:32.280   Training iter 300, batch loss 1.5416, batch acc 0.8688
17:35:32.779   Training iter 350, batch loss 1.5309, batch acc 0.9064
17:35:33.289   Training iter 400, batch loss 1.5314, batch acc 0.8956
17:35:33.789   Training iter 450, batch loss 1.5290, batch acc 0.9030
17:35:34.298   Training iter 500, batch loss 1.5281, batch acc 0.9126
17:35:34.800   Training iter 550, batch loss 1.5258, batch acc 0.9148
17:35:35.297   Training iter 600, batch loss 1.5240, batch acc 0.9084
17:35:35.298 Training @ 128 epoch...
17:35:35.795   Training iter 50, batch loss 1.5475, batch acc 0.8972
17:35:36.300   Training iter 100, batch loss 1.5289, batch acc 0.9046
17:35:36.816   Training iter 150, batch loss 1.5245, batch acc 0.9152
17:35:37.322   Training iter 200, batch loss 1.5337, batch acc 0.8950
17:35:37.819   Training iter 250, batch loss 1.5359, batch acc 0.8834
17:35:38.326   Training iter 300, batch loss 1.5286, batch acc 0.9088
17:35:38.866   Training iter 350, batch loss 1.5293, batch acc 0.9010
17:35:39.434   Training iter 400, batch loss 1.5320, batch acc 0.9096
17:35:39.995   Training iter 450, batch loss 1.5228, batch acc 0.9196
17:35:40.554   Training iter 500, batch loss 1.5218, batch acc 0.9204
17:35:41.123   Training iter 550, batch loss 1.5300, batch acc 0.9114
17:35:41.698   Training iter 600, batch loss 1.5309, batch acc 0.9056
17:35:41.700 Training @ 129 epoch...
17:35:42.277   Training iter 50, batch loss 1.5317, batch acc 0.8914
17:35:42.842   Training iter 100, batch loss 1.5424, batch acc 0.8828
17:35:43.409   Training iter 150, batch loss 1.5353, batch acc 0.8746
17:35:43.980   Training iter 200, batch loss 1.5312, batch acc 0.8976
17:35:44.544   Training iter 250, batch loss 1.5258, batch acc 0.9120
17:35:45.066   Training iter 300, batch loss 1.5222, batch acc 0.9134
17:35:45.583   Training iter 350, batch loss 1.5332, batch acc 0.9056
17:35:46.114   Training iter 400, batch loss 1.5381, batch acc 0.8960
17:35:46.631   Training iter 450, batch loss 1.5430, batch acc 0.8802
17:35:47.149   Training iter 500, batch loss 1.5263, batch acc 0.8954
17:35:47.652   Training iter 550, batch loss 1.5310, batch acc 0.8930
17:35:48.153   Training iter 600, batch loss 1.5369, batch acc 0.8896
17:35:48.155 Training @ 130 epoch...
17:35:48.680   Training iter 50, batch loss 1.5326, batch acc 0.8996
17:35:49.190   Training iter 100, batch loss 1.5345, batch acc 0.9058
17:35:49.703   Training iter 150, batch loss 1.5282, batch acc 0.9108
17:35:50.218   Training iter 200, batch loss 1.5278, batch acc 0.9106
17:35:50.735   Training iter 250, batch loss 1.5350, batch acc 0.9036
17:35:51.237   Training iter 300, batch loss 1.5351, batch acc 0.9062
17:35:51.757   Training iter 350, batch loss 1.5231, batch acc 0.9246
17:35:52.272   Training iter 400, batch loss 1.5318, batch acc 0.8930
17:35:52.782   Training iter 450, batch loss 1.5393, batch acc 0.8822
17:35:53.297   Training iter 500, batch loss 1.5345, batch acc 0.8972
17:35:53.807   Training iter 550, batch loss 1.5295, batch acc 0.9104
17:35:54.333   Training iter 600, batch loss 1.5302, batch acc 0.8972
17:35:54.335 Testing @ 130 epoch...
17:35:54.383     Testing, total mean loss 1.53018, total acc 0.89380
17:35:54.383 Training @ 131 epoch...
17:35:54.932   Training iter 50, batch loss 1.5283, batch acc 0.8976
17:35:55.485   Training iter 100, batch loss 1.5289, batch acc 0.9126
17:35:56.032   Training iter 150, batch loss 1.5256, batch acc 0.9092
17:35:56.586   Training iter 200, batch loss 1.5345, batch acc 0.8962
17:35:57.145   Training iter 250, batch loss 1.5276, batch acc 0.9130
17:35:57.689   Training iter 300, batch loss 1.5226, batch acc 0.9154
17:35:58.235   Training iter 350, batch loss 1.5413, batch acc 0.9004
17:35:58.827   Training iter 400, batch loss 1.5246, batch acc 0.9132
17:35:59.421   Training iter 450, batch loss 1.5276, batch acc 0.9116
17:36:00.022   Training iter 500, batch loss 1.5322, batch acc 0.9048
17:36:00.568   Training iter 550, batch loss 1.5326, batch acc 0.8998
17:36:01.104   Training iter 600, batch loss 1.5283, batch acc 0.9116
17:36:01.106 Training @ 132 epoch...
17:36:01.673   Training iter 50, batch loss 1.5271, batch acc 0.9138
17:36:02.246   Training iter 100, batch loss 1.5294, batch acc 0.9100
17:36:02.750   Training iter 150, batch loss 1.5279, batch acc 0.8998
17:36:03.282   Training iter 200, batch loss 1.5293, batch acc 0.9006
17:36:03.815   Training iter 250, batch loss 1.5238, batch acc 0.9132
17:36:04.376   Training iter 300, batch loss 1.5463, batch acc 0.8642
17:36:04.937   Training iter 350, batch loss 1.5233, batch acc 0.9064
17:36:05.477   Training iter 400, batch loss 1.5261, batch acc 0.9126
17:36:06.021   Training iter 450, batch loss 1.5228, batch acc 0.9154
17:36:06.540   Training iter 500, batch loss 1.5251, batch acc 0.9150
17:36:07.045   Training iter 550, batch loss 1.5290, batch acc 0.9092
17:36:07.550   Training iter 600, batch loss 1.5422, batch acc 0.8788
17:36:07.552 Training @ 133 epoch...
17:36:08.095   Training iter 50, batch loss 1.5252, batch acc 0.9088
17:36:08.626   Training iter 100, batch loss 1.5327, batch acc 0.9088
17:36:09.156   Training iter 150, batch loss 1.5338, batch acc 0.8872
17:36:09.698   Training iter 200, batch loss 1.5253, batch acc 0.9070
17:36:10.234   Training iter 250, batch loss 1.5306, batch acc 0.8968
17:36:10.759   Training iter 300, batch loss 1.5248, batch acc 0.9084
17:36:11.253   Training iter 350, batch loss 1.5322, batch acc 0.9070
17:36:11.757   Training iter 400, batch loss 1.5292, batch acc 0.8976
17:36:12.255   Training iter 450, batch loss 1.5359, batch acc 0.8894
17:36:12.762   Training iter 500, batch loss 1.5273, batch acc 0.9070
17:36:13.273   Training iter 550, batch loss 1.5460, batch acc 0.8750
17:36:13.773   Training iter 600, batch loss 1.5402, batch acc 0.8700
17:36:13.774 Training @ 134 epoch...
17:36:14.305   Training iter 50, batch loss 1.5255, batch acc 0.8908
17:36:14.823   Training iter 100, batch loss 1.5303, batch acc 0.8932
17:36:15.354   Training iter 150, batch loss 1.5300, batch acc 0.8942
17:36:15.871   Training iter 200, batch loss 1.5287, batch acc 0.9122
17:36:16.392   Training iter 250, batch loss 1.5380, batch acc 0.8926
17:36:16.915   Training iter 300, batch loss 1.5324, batch acc 0.8852
17:36:17.424   Training iter 350, batch loss 1.5268, batch acc 0.8978
17:36:17.935   Training iter 400, batch loss 1.5237, batch acc 0.9128
17:36:18.425   Training iter 450, batch loss 1.5281, batch acc 0.8954
17:36:18.928   Training iter 500, batch loss 1.5225, batch acc 0.9122
17:36:19.418   Training iter 550, batch loss 1.5262, batch acc 0.9092
17:36:19.924   Training iter 600, batch loss 1.5464, batch acc 0.8962
17:36:19.926 Training @ 135 epoch...
17:36:20.445   Training iter 50, batch loss 1.5287, batch acc 0.9080
17:36:20.964   Training iter 100, batch loss 1.5279, batch acc 0.9136
17:36:21.473   Training iter 150, batch loss 1.5289, batch acc 0.9070
17:36:21.977   Training iter 200, batch loss 1.5252, batch acc 0.9140
17:36:22.488   Training iter 250, batch loss 1.5251, batch acc 0.9132
17:36:22.994   Training iter 300, batch loss 1.5222, batch acc 0.9216
17:36:23.496   Training iter 350, batch loss 1.5322, batch acc 0.9026
17:36:23.978   Training iter 400, batch loss 1.5325, batch acc 0.9116
17:36:24.491   Training iter 450, batch loss 1.5369, batch acc 0.8914
17:36:24.991   Training iter 500, batch loss 1.5288, batch acc 0.9100
17:36:25.501   Training iter 550, batch loss 1.5274, batch acc 0.9110
17:36:25.984   Training iter 600, batch loss 1.5359, batch acc 0.9018
17:36:25.985 Testing @ 135 epoch...
17:36:26.036     Testing, total mean loss 1.53173, total acc 0.91110
17:36:26.036 Training @ 136 epoch...
17:36:26.520   Training iter 50, batch loss 1.5328, batch acc 0.9046
17:36:27.019   Training iter 100, batch loss 1.5299, batch acc 0.9210
17:36:27.530   Training iter 150, batch loss 1.5332, batch acc 0.8916
17:36:28.047   Training iter 200, batch loss 1.5237, batch acc 0.9114
17:36:28.558   Training iter 250, batch loss 1.5231, batch acc 0.9192
17:36:29.076   Training iter 300, batch loss 1.5221, batch acc 0.9266
17:36:29.628   Training iter 350, batch loss 1.5225, batch acc 0.9220
17:36:30.177   Training iter 400, batch loss 1.5204, batch acc 0.9214
17:36:30.742   Training iter 450, batch loss 1.5400, batch acc 0.8862
17:36:31.320   Training iter 500, batch loss 1.5323, batch acc 0.8996
17:36:31.899   Training iter 550, batch loss 1.5274, batch acc 0.9126
17:36:32.462   Training iter 600, batch loss 1.5237, batch acc 0.9128
17:36:32.464 Training @ 137 epoch...
17:36:33.015   Training iter 50, batch loss 1.5177, batch acc 0.9178
17:36:33.547   Training iter 100, batch loss 1.5384, batch acc 0.9004
17:36:34.084   Training iter 150, batch loss 1.5362, batch acc 0.8872
17:36:34.609   Training iter 200, batch loss 1.5300, batch acc 0.9008
17:36:35.135   Training iter 250, batch loss 1.5164, batch acc 0.9192
17:36:35.673   Training iter 300, batch loss 1.5308, batch acc 0.9042
17:36:36.214   Training iter 350, batch loss 1.5349, batch acc 0.8948
17:36:36.736   Training iter 400, batch loss 1.5322, batch acc 0.9016
17:36:37.263   Training iter 450, batch loss 1.5286, batch acc 0.9056
17:36:37.795   Training iter 500, batch loss 1.5234, batch acc 0.9158
17:36:38.327   Training iter 550, batch loss 1.5333, batch acc 0.8914
17:36:38.856   Training iter 600, batch loss 1.5275, batch acc 0.9116
17:36:38.858 Training @ 138 epoch...
17:36:39.403   Training iter 50, batch loss 1.5172, batch acc 0.9220
17:36:39.942   Training iter 100, batch loss 1.5228, batch acc 0.9158
17:36:40.494   Training iter 150, batch loss 1.5196, batch acc 0.9238
17:36:41.034   Training iter 200, batch loss 1.5399, batch acc 0.8888
17:36:41.568   Training iter 250, batch loss 1.5394, batch acc 0.8926
17:36:42.113   Training iter 300, batch loss 1.5336, batch acc 0.8870
17:36:42.679   Training iter 350, batch loss 1.5383, batch acc 0.8980
17:36:43.266   Training iter 400, batch loss 1.5365, batch acc 0.8844
17:36:43.843   Training iter 450, batch loss 1.5281, batch acc 0.8996
17:36:44.423   Training iter 500, batch loss 1.5386, batch acc 0.8984
17:36:44.991   Training iter 550, batch loss 1.5352, batch acc 0.8942
17:36:45.566   Training iter 600, batch loss 1.5367, batch acc 0.8842
17:36:45.567 Training @ 139 epoch...
17:36:46.147   Training iter 50, batch loss 1.5274, batch acc 0.9052
17:36:46.729   Training iter 100, batch loss 1.5281, batch acc 0.9124
17:36:47.312   Training iter 150, batch loss 1.5274, batch acc 0.9094
17:36:47.898   Training iter 200, batch loss 1.5299, batch acc 0.9166
17:36:48.487   Training iter 250, batch loss 1.5315, batch acc 0.9022
17:36:49.059   Training iter 300, batch loss 1.5237, batch acc 0.9226
17:36:49.652   Training iter 350, batch loss 1.5299, batch acc 0.9124
17:36:50.222   Training iter 400, batch loss 1.5321, batch acc 0.8958
17:36:50.798   Training iter 450, batch loss 1.5394, batch acc 0.8714
17:36:51.510   Training iter 500, batch loss 1.5278, batch acc 0.9052
17:36:52.162   Training iter 550, batch loss 1.5275, batch acc 0.8938
17:36:52.737   Training iter 600, batch loss 1.5306, batch acc 0.8960
17:36:52.739 Training @ 140 epoch...
17:36:53.248   Training iter 50, batch loss 1.5303, batch acc 0.9014
17:36:53.756   Training iter 100, batch loss 1.5222, batch acc 0.9114
17:36:54.290   Training iter 150, batch loss 1.5341, batch acc 0.9018
17:36:54.834   Training iter 200, batch loss 1.5281, batch acc 0.9010
17:36:55.378   Training iter 250, batch loss 1.5264, batch acc 0.8972
17:36:55.890   Training iter 300, batch loss 1.5401, batch acc 0.8846
17:36:56.408   Training iter 350, batch loss 1.5355, batch acc 0.8954
17:36:56.911   Training iter 400, batch loss 1.5452, batch acc 0.8822
17:36:57.427   Training iter 450, batch loss 1.5261, batch acc 0.9136
17:36:57.969   Training iter 500, batch loss 1.5295, batch acc 0.9018
17:36:58.543   Training iter 550, batch loss 1.5351, batch acc 0.9052
17:36:59.110   Training iter 600, batch loss 1.5379, batch acc 0.8920
17:36:59.111 Testing @ 140 epoch...
17:36:59.163     Testing, total mean loss 1.53401, total acc 0.89580
17:36:59.163 Training @ 141 epoch...
17:36:59.774   Training iter 50, batch loss 1.5316, batch acc 0.8990
17:37:00.489   Training iter 100, batch loss 1.5319, batch acc 0.8964
17:37:01.048   Training iter 150, batch loss 1.5295, batch acc 0.8986
17:37:01.621   Training iter 200, batch loss 1.5290, batch acc 0.8896
17:37:02.222   Training iter 250, batch loss 1.5336, batch acc 0.8928
17:37:02.793   Training iter 300, batch loss 1.5297, batch acc 0.9076
17:37:03.351   Training iter 350, batch loss 1.5315, batch acc 0.8972
17:37:03.902   Training iter 400, batch loss 1.5588, batch acc 0.8518
17:37:04.477   Training iter 450, batch loss 1.5377, batch acc 0.8948
17:37:05.042   Training iter 500, batch loss 1.5303, batch acc 0.9054
17:37:05.594   Training iter 550, batch loss 1.5337, batch acc 0.8976
17:37:06.158   Training iter 600, batch loss 1.5243, batch acc 0.9152
17:37:06.160 Training @ 142 epoch...
17:37:06.719   Training iter 50, batch loss 1.5266, batch acc 0.9060
17:37:07.261   Training iter 100, batch loss 1.5266, batch acc 0.9112
17:37:07.775   Training iter 150, batch loss 1.5338, batch acc 0.9028
17:37:08.292   Training iter 200, batch loss 1.5303, batch acc 0.9036
17:37:08.795   Training iter 250, batch loss 1.5281, batch acc 0.9064
17:37:09.313   Training iter 300, batch loss 1.5330, batch acc 0.8968
17:37:09.825   Training iter 350, batch loss 1.5307, batch acc 0.8942
17:37:10.361   Training iter 400, batch loss 1.5263, batch acc 0.9090
17:37:10.906   Training iter 450, batch loss 1.5281, batch acc 0.9128
17:37:11.460   Training iter 500, batch loss 1.5312, batch acc 0.9052
17:37:12.007   Training iter 550, batch loss 1.5228, batch acc 0.9114
17:37:12.557   Training iter 600, batch loss 1.5415, batch acc 0.8888
17:37:12.558 Training @ 143 epoch...
17:37:13.114   Training iter 50, batch loss 1.5386, batch acc 0.8976
17:37:13.667   Training iter 100, batch loss 1.5512, batch acc 0.8652
17:37:14.210   Training iter 150, batch loss 1.5206, batch acc 0.9066
17:37:14.758   Training iter 200, batch loss 1.5252, batch acc 0.9000
17:37:15.297   Training iter 250, batch loss 1.5331, batch acc 0.8938
17:37:15.837   Training iter 300, batch loss 1.5272, batch acc 0.9032
17:37:16.392   Training iter 350, batch loss 1.5393, batch acc 0.8820
17:37:16.937   Training iter 400, batch loss 1.5336, batch acc 0.8726
17:37:17.488   Training iter 450, batch loss 1.5382, batch acc 0.8796
17:37:18.026   Training iter 500, batch loss 1.5335, batch acc 0.8870
17:37:18.554   Training iter 550, batch loss 1.5312, batch acc 0.8830
17:37:19.078   Training iter 600, batch loss 1.5242, batch acc 0.8932
17:37:19.080 Training @ 144 epoch...
17:37:19.614   Training iter 50, batch loss 1.5320, batch acc 0.8886
17:37:20.123   Training iter 100, batch loss 1.5334, batch acc 0.8782
17:37:20.636   Training iter 150, batch loss 1.5401, batch acc 0.8602
17:37:21.157   Training iter 200, batch loss 1.5389, batch acc 0.8804
17:37:21.716   Training iter 250, batch loss 1.5273, batch acc 0.9064
17:37:22.255   Training iter 300, batch loss 1.5304, batch acc 0.9078
17:37:22.782   Training iter 350, batch loss 1.5313, batch acc 0.8986
17:37:23.323   Training iter 400, batch loss 1.5626, batch acc 0.8708
17:37:23.851   Training iter 450, batch loss 1.5208, batch acc 0.9166
17:37:24.409   Training iter 500, batch loss 1.5391, batch acc 0.8890
17:37:24.974   Training iter 550, batch loss 1.5249, batch acc 0.9058
17:37:25.529   Training iter 600, batch loss 1.5290, batch acc 0.9094
17:37:25.531 Training @ 145 epoch...
17:37:26.075   Training iter 50, batch loss 1.5195, batch acc 0.9198
17:37:26.631   Training iter 100, batch loss 1.5267, batch acc 0.9068
17:37:27.178   Training iter 150, batch loss 1.5260, batch acc 0.9058
17:37:27.712   Training iter 200, batch loss 1.5384, batch acc 0.8928
17:37:28.267   Training iter 250, batch loss 1.5313, batch acc 0.9084
17:37:28.826   Training iter 300, batch loss 1.5404, batch acc 0.8938
17:37:29.378   Training iter 350, batch loss 1.5273, batch acc 0.9120
17:37:29.930   Training iter 400, batch loss 1.5224, batch acc 0.9012
17:37:30.479   Training iter 450, batch loss 1.5234, batch acc 0.9088
17:37:31.051   Training iter 500, batch loss 1.5340, batch acc 0.9014
17:37:31.620   Training iter 550, batch loss 1.5397, batch acc 0.8820
17:37:32.182   Training iter 600, batch loss 1.5328, batch acc 0.8692
17:37:32.184 Testing @ 145 epoch...
17:37:32.232     Testing, total mean loss 1.52928, total acc 0.90130
17:37:32.232 Training @ 146 epoch...
17:37:32.804   Training iter 50, batch loss 1.5356, batch acc 0.8872
17:37:33.368   Training iter 100, batch loss 1.5568, batch acc 0.8470
17:37:33.926   Training iter 150, batch loss 1.5349, batch acc 0.8848
17:37:34.479   Training iter 200, batch loss 1.5358, batch acc 0.8796
17:37:35.038   Training iter 250, batch loss 1.5388, batch acc 0.8844
17:37:35.562   Training iter 300, batch loss 1.5288, batch acc 0.9078
17:37:36.098   Training iter 350, batch loss 1.5247, batch acc 0.9086
17:37:36.625   Training iter 400, batch loss 1.5239, batch acc 0.9084
17:37:37.144   Training iter 450, batch loss 1.5273, batch acc 0.9196
17:37:37.655   Training iter 500, batch loss 1.5301, batch acc 0.9174
17:37:38.170   Training iter 550, batch loss 1.5359, batch acc 0.9086
17:37:38.687   Training iter 600, batch loss 1.5279, batch acc 0.9060
17:37:38.688 Training @ 147 epoch...
17:37:39.212   Training iter 50, batch loss 1.5323, batch acc 0.8874
17:37:39.742   Training iter 100, batch loss 1.5409, batch acc 0.8926
17:37:40.269   Training iter 150, batch loss 1.5209, batch acc 0.9078
17:37:40.763   Training iter 200, batch loss 1.5314, batch acc 0.9012
17:37:41.253   Training iter 250, batch loss 1.5330, batch acc 0.8890
17:37:41.750   Training iter 300, batch loss 1.5236, batch acc 0.9164
17:37:42.249   Training iter 350, batch loss 1.5250, batch acc 0.9080
17:37:42.735   Training iter 400, batch loss 1.5311, batch acc 0.9018
17:37:43.332   Training iter 450, batch loss 1.5275, batch acc 0.8974
17:37:43.952   Training iter 500, batch loss 1.5237, batch acc 0.9130
17:37:44.561   Training iter 550, batch loss 1.5241, batch acc 0.9196
17:37:45.112   Training iter 600, batch loss 1.5255, batch acc 0.9120
17:37:45.114 Training @ 148 epoch...
17:37:45.659   Training iter 50, batch loss 1.5294, batch acc 0.9070
17:37:46.246   Training iter 100, batch loss 1.5339, batch acc 0.9152
17:37:46.822   Training iter 150, batch loss 1.5341, batch acc 0.8790
17:37:47.386   Training iter 200, batch loss 1.5377, batch acc 0.8638
17:37:47.948   Training iter 250, batch loss 1.5344, batch acc 0.8808
17:37:48.492   Training iter 300, batch loss 1.5246, batch acc 0.8912
17:37:49.039   Training iter 350, batch loss 1.5301, batch acc 0.9052
17:37:49.586   Training iter 400, batch loss 1.5257, batch acc 0.9072
17:37:50.132   Training iter 450, batch loss 1.5339, batch acc 0.8998
17:37:50.654   Training iter 500, batch loss 1.5267, batch acc 0.9154
17:37:51.163   Training iter 550, batch loss 1.5255, batch acc 0.9052
17:37:51.685   Training iter 600, batch loss 1.5306, batch acc 0.8914
17:37:51.686 Training @ 149 epoch...
17:37:52.224   Training iter 50, batch loss 1.5225, batch acc 0.9192
17:37:52.733   Training iter 100, batch loss 1.5242, batch acc 0.9084
17:37:53.249   Training iter 150, batch loss 1.5229, batch acc 0.9080
17:37:53.748   Training iter 200, batch loss 1.5399, batch acc 0.8918
17:37:54.260   Training iter 250, batch loss 1.5348, batch acc 0.9008
17:37:54.776   Training iter 300, batch loss 1.5329, batch acc 0.8884
17:37:55.291   Training iter 350, batch loss 1.5275, batch acc 0.8990
17:37:55.831   Training iter 400, batch loss 1.5231, batch acc 0.9208
17:37:56.377   Training iter 450, batch loss 1.5351, batch acc 0.8940
17:37:56.904   Training iter 500, batch loss 1.5277, batch acc 0.9064
17:37:57.448   Training iter 550, batch loss 1.5252, batch acc 0.9100
17:37:57.987   Training iter 600, batch loss 1.5346, batch acc 0.8984
17:37:57.989 Training @ 150 epoch...
17:37:58.526   Training iter 50, batch loss 1.5291, batch acc 0.8976
17:37:59.073   Training iter 100, batch loss 1.5202, batch acc 0.9118
17:37:59.596   Training iter 150, batch loss 1.5328, batch acc 0.8836
17:38:00.129   Training iter 200, batch loss 1.5320, batch acc 0.9008
17:38:00.689   Training iter 250, batch loss 1.5368, batch acc 0.8718
17:38:01.254   Training iter 300, batch loss 1.5278, batch acc 0.8828
17:38:01.815   Training iter 350, batch loss 1.5348, batch acc 0.8870
17:38:02.376   Training iter 400, batch loss 1.5643, batch acc 0.8708
17:38:02.942   Training iter 450, batch loss 1.5395, batch acc 0.8892
17:38:03.503   Training iter 500, batch loss 1.5264, batch acc 0.9016
17:38:04.068   Training iter 550, batch loss 1.5277, batch acc 0.9018
17:38:04.658   Training iter 600, batch loss 1.5238, batch acc 0.9034
17:38:04.660 Testing @ 150 epoch...
17:38:04.708     Testing, total mean loss 1.53949, total acc 0.89400
17:38:04.708 Training @ 151 epoch...
17:38:05.303   Training iter 50, batch loss 1.5377, batch acc 0.8892
17:38:05.883   Training iter 100, batch loss 1.5360, batch acc 0.8754
17:38:06.460   Training iter 150, batch loss 1.5281, batch acc 0.9070
17:38:07.017   Training iter 200, batch loss 1.5218, batch acc 0.9156
17:38:07.560   Training iter 250, batch loss 1.5431, batch acc 0.8830
17:38:08.094   Training iter 300, batch loss 1.5306, batch acc 0.8906
17:38:08.613   Training iter 350, batch loss 1.5401, batch acc 0.8976
17:38:09.107   Training iter 400, batch loss 1.5317, batch acc 0.8898
17:38:09.606   Training iter 450, batch loss 1.5351, batch acc 0.8726
17:38:10.098   Training iter 500, batch loss 1.5242, batch acc 0.8938
17:38:10.602   Training iter 550, batch loss 1.5316, batch acc 0.9084
17:38:11.090   Training iter 600, batch loss 1.5236, batch acc 0.9078
17:38:11.092 Training @ 152 epoch...
17:38:11.568   Training iter 50, batch loss 1.5290, batch acc 0.8996
17:38:12.092   Training iter 100, batch loss 1.5306, batch acc 0.9032
17:38:12.607   Training iter 150, batch loss 1.5345, batch acc 0.8930
17:38:13.113   Training iter 200, batch loss 1.5245, batch acc 0.8886
17:38:13.618   Training iter 250, batch loss 1.5279, batch acc 0.8982
17:38:14.129   Training iter 300, batch loss 1.5419, batch acc 0.8828
17:38:14.637   Training iter 350, batch loss 1.5436, batch acc 0.8678
17:38:15.132   Training iter 400, batch loss 1.5312, batch acc 0.9096
17:38:15.635   Training iter 450, batch loss 1.5405, batch acc 0.9016
17:38:16.147   Training iter 500, batch loss 1.5322, batch acc 0.8960
17:38:16.657   Training iter 550, batch loss 1.5234, batch acc 0.8976
17:38:17.168   Training iter 600, batch loss 1.5453, batch acc 0.8804
17:38:17.169 Training @ 153 epoch...
17:38:17.675   Training iter 50, batch loss 1.5324, batch acc 0.9020
17:38:18.187   Training iter 100, batch loss 1.5289, batch acc 0.8908
17:38:18.713   Training iter 150, batch loss 1.5400, batch acc 0.8898
17:38:19.271   Training iter 200, batch loss 1.5360, batch acc 0.8862
17:38:19.829   Training iter 250, batch loss 1.5305, batch acc 0.8996
17:38:20.385   Training iter 300, batch loss 1.5364, batch acc 0.8940
17:38:20.939   Training iter 350, batch loss 1.5279, batch acc 0.9054
17:38:21.490   Training iter 400, batch loss 1.5304, batch acc 0.8910
17:38:22.046   Training iter 450, batch loss 1.5301, batch acc 0.8998
17:38:22.606   Training iter 500, batch loss 1.5305, batch acc 0.9036
17:38:23.198   Training iter 550, batch loss 1.5228, batch acc 0.8996
17:38:23.785   Training iter 600, batch loss 1.5351, batch acc 0.9034
17:38:23.787 Training @ 154 epoch...
17:38:24.373   Training iter 50, batch loss 1.5328, batch acc 0.9000
17:38:24.920   Training iter 100, batch loss 1.5320, batch acc 0.9016
17:38:25.481   Training iter 150, batch loss 1.5333, batch acc 0.9024
17:38:26.047   Training iter 200, batch loss 1.5250, batch acc 0.9120
17:38:26.618   Training iter 250, batch loss 1.5330, batch acc 0.8958
17:38:27.196   Training iter 300, batch loss 1.5297, batch acc 0.8898
17:38:27.766   Training iter 350, batch loss 1.5303, batch acc 0.8960
17:38:28.336   Training iter 400, batch loss 1.5268, batch acc 0.9078
17:38:28.912   Training iter 450, batch loss 1.5331, batch acc 0.9066
17:38:29.486   Training iter 500, batch loss 1.5293, batch acc 0.8910
17:38:30.042   Training iter 550, batch loss 1.5392, batch acc 0.8968
17:38:30.586   Training iter 600, batch loss 1.5345, batch acc 0.9080
17:38:30.588 Training @ 155 epoch...
17:38:31.131   Training iter 50, batch loss 1.5308, batch acc 0.8984
17:38:31.661   Training iter 100, batch loss 1.5260, batch acc 0.9094
17:38:32.202   Training iter 150, batch loss 1.5239, batch acc 0.9056
17:38:32.733   Training iter 200, batch loss 1.5244, batch acc 0.9204
17:38:33.278   Training iter 250, batch loss 1.5270, batch acc 0.9048
17:38:33.823   Training iter 300, batch loss 1.5372, batch acc 0.8972
17:38:34.383   Training iter 350, batch loss 1.5346, batch acc 0.9000
17:38:34.981   Training iter 400, batch loss 1.5391, batch acc 0.8868
17:38:35.585   Training iter 450, batch loss 1.5222, batch acc 0.9126
17:38:36.193   Training iter 500, batch loss 1.5254, batch acc 0.9072
17:38:36.747   Training iter 550, batch loss 1.5219, batch acc 0.9182
17:38:37.289   Training iter 600, batch loss 1.5419, batch acc 0.8864
17:38:37.291 Testing @ 155 epoch...
17:38:37.340     Testing, total mean loss 1.54680, total acc 0.86900
17:38:37.340 Training @ 156 epoch...
17:38:37.888   Training iter 50, batch loss 1.5357, batch acc 0.8954
17:38:38.447   Training iter 100, batch loss 1.5422, batch acc 0.8962
17:38:39.023   Training iter 150, batch loss 1.5299, batch acc 0.9016
17:38:39.602   Training iter 200, batch loss 1.5365, batch acc 0.8876
17:38:40.176   Training iter 250, batch loss 1.5314, batch acc 0.9090
17:38:40.736   Training iter 300, batch loss 1.5334, batch acc 0.9054
17:38:41.282   Training iter 350, batch loss 1.5245, batch acc 0.9132
17:38:41.831   Training iter 400, batch loss 1.5355, batch acc 0.8968
17:38:42.372   Training iter 450, batch loss 1.5305, batch acc 0.9192
17:38:42.918   Training iter 500, batch loss 1.5382, batch acc 0.8820
17:38:43.462   Training iter 550, batch loss 1.5402, batch acc 0.8946
17:38:44.022   Training iter 600, batch loss 1.5318, batch acc 0.8898
17:38:44.024 Training @ 157 epoch...
17:38:44.581   Training iter 50, batch loss 1.5321, batch acc 0.9076
17:38:45.111   Training iter 100, batch loss 1.5285, batch acc 0.9140
17:38:45.623   Training iter 150, batch loss 1.5285, batch acc 0.9020
17:38:46.147   Training iter 200, batch loss 1.5357, batch acc 0.8894
17:38:46.654   Training iter 250, batch loss 1.5296, batch acc 0.9050
17:38:47.179   Training iter 300, batch loss 1.5234, batch acc 0.9152
17:38:47.680   Training iter 350, batch loss 1.5399, batch acc 0.8736
17:38:48.186   Training iter 400, batch loss 1.5287, batch acc 0.8812
17:38:48.687   Training iter 450, batch loss 1.5232, batch acc 0.8952
17:38:49.203   Training iter 500, batch loss 1.5319, batch acc 0.8866
17:38:49.733   Training iter 550, batch loss 1.5474, batch acc 0.8450
17:38:50.258   Training iter 600, batch loss 1.5322, batch acc 0.8826
17:38:50.260 Training @ 158 epoch...
17:38:50.775   Training iter 50, batch loss 1.5373, batch acc 0.8740
17:38:51.270   Training iter 100, batch loss 1.5335, batch acc 0.8796
17:38:51.761   Training iter 150, batch loss 1.5224, batch acc 0.9090
17:38:52.269   Training iter 200, batch loss 1.5257, batch acc 0.9078
17:38:52.761   Training iter 250, batch loss 1.5376, batch acc 0.8938
17:38:53.261   Training iter 300, batch loss 1.5233, batch acc 0.9100
17:38:53.774   Training iter 350, batch loss 1.5243, batch acc 0.9052
17:38:54.307   Training iter 400, batch loss 1.5301, batch acc 0.8846
17:38:54.858   Training iter 450, batch loss 1.5269, batch acc 0.8898
17:38:55.417   Training iter 500, batch loss 1.5340, batch acc 0.8886
17:38:56.009   Training iter 550, batch loss 1.5239, batch acc 0.9064
17:38:56.572   Training iter 600, batch loss 1.5258, batch acc 0.9048
17:38:56.573 Training @ 159 epoch...
17:38:57.133   Training iter 50, batch loss 1.5177, batch acc 0.9274
17:38:57.692   Training iter 100, batch loss 1.5302, batch acc 0.9100
17:38:58.251   Training iter 150, batch loss 1.5472, batch acc 0.8924
17:38:58.789   Training iter 200, batch loss 1.5283, batch acc 0.9066
17:38:59.340   Training iter 250, batch loss 1.5393, batch acc 0.8842
17:38:59.892   Training iter 300, batch loss 1.5339, batch acc 0.9076
17:39:00.418   Training iter 350, batch loss 1.5274, batch acc 0.9112
17:39:00.925   Training iter 400, batch loss 1.5210, batch acc 0.9118
17:39:01.485   Training iter 450, batch loss 1.5405, batch acc 0.8934
17:39:02.065   Training iter 500, batch loss 1.5372, batch acc 0.9048
17:39:02.631   Training iter 550, batch loss 1.5328, batch acc 0.8968
17:39:03.202   Training iter 600, batch loss 1.5262, batch acc 0.8926
17:39:03.204 Training @ 160 epoch...
17:39:03.779   Training iter 50, batch loss 1.5326, batch acc 0.8898
17:39:04.335   Training iter 100, batch loss 1.5335, batch acc 0.8846
17:39:04.923   Training iter 150, batch loss 1.5288, batch acc 0.8954
17:39:05.509   Training iter 200, batch loss 1.5234, batch acc 0.9052
17:39:06.096   Training iter 250, batch loss 1.5288, batch acc 0.9010
17:39:06.707   Training iter 300, batch loss 1.5360, batch acc 0.9066
17:39:07.261   Training iter 350, batch loss 1.5361, batch acc 0.8980
17:39:07.805   Training iter 400, batch loss 1.5362, batch acc 0.8912
17:39:08.338   Training iter 450, batch loss 1.5518, batch acc 0.8792
17:39:08.879   Training iter 500, batch loss 1.5279, batch acc 0.8998
17:39:09.457   Training iter 550, batch loss 1.5278, batch acc 0.8982
17:39:10.042   Training iter 600, batch loss 1.5326, batch acc 0.8956
17:39:10.043 Testing @ 160 epoch...
17:39:10.092     Testing, total mean loss 1.52981, total acc 0.89520
17:39:10.092 Training @ 161 epoch...
17:39:10.674   Training iter 50, batch loss 1.5317, batch acc 0.8992
17:39:11.251   Training iter 100, batch loss 1.5267, batch acc 0.9052
17:39:11.834   Training iter 150, batch loss 1.5443, batch acc 0.8562
17:39:12.473   Training iter 200, batch loss 1.5353, batch acc 0.8626
17:39:13.061   Training iter 250, batch loss 1.5378, batch acc 0.8638
17:39:13.650   Training iter 300, batch loss 1.5332, batch acc 0.8924
17:39:14.236   Training iter 350, batch loss 1.5313, batch acc 0.8934
17:39:14.811   Training iter 400, batch loss 1.5317, batch acc 0.8926
17:39:15.395   Training iter 450, batch loss 1.5293, batch acc 0.9076
17:39:15.973   Training iter 500, batch loss 1.5286, batch acc 0.8916
17:39:16.540   Training iter 550, batch loss 1.5215, batch acc 0.9088
17:39:17.111   Training iter 600, batch loss 1.5299, batch acc 0.9040
17:39:17.113 Training @ 162 epoch...
17:39:17.662   Training iter 50, batch loss 1.5334, batch acc 0.8944
17:39:18.225   Training iter 100, batch loss 1.5269, batch acc 0.9142
17:39:18.779   Training iter 150, batch loss 1.5255, batch acc 0.9152
17:39:19.360   Training iter 200, batch loss 1.5384, batch acc 0.8984
17:39:19.942   Training iter 250, batch loss 1.5212, batch acc 0.9200
17:39:20.516   Training iter 300, batch loss 1.5263, batch acc 0.9124
17:39:21.084   Training iter 350, batch loss 1.5281, batch acc 0.9006
17:39:21.648   Training iter 400, batch loss 1.5325, batch acc 0.8838
17:39:22.237   Training iter 450, batch loss 1.5459, batch acc 0.8764
17:39:22.803   Training iter 500, batch loss 1.5373, batch acc 0.9028
17:39:23.373   Training iter 550, batch loss 1.5353, batch acc 0.8864
17:39:23.959   Training iter 600, batch loss 1.5309, batch acc 0.9014
17:39:23.960 Training @ 163 epoch...
17:39:24.564   Training iter 50, batch loss 1.5278, batch acc 0.9058
17:39:25.216   Training iter 100, batch loss 1.5340, batch acc 0.8990
17:39:25.775   Training iter 150, batch loss 1.5283, batch acc 0.9002
17:39:26.376   Training iter 200, batch loss 1.5277, batch acc 0.9012
17:39:27.026   Training iter 250, batch loss 1.5323, batch acc 0.9022
17:39:27.578   Training iter 300, batch loss 1.5215, batch acc 0.9146
17:39:28.146   Training iter 350, batch loss 1.5276, batch acc 0.9048
17:39:28.704   Training iter 400, batch loss 1.5297, batch acc 0.8946
17:39:29.248   Training iter 450, batch loss 1.5349, batch acc 0.8950
17:39:29.822   Training iter 500, batch loss 1.5316, batch acc 0.9072
17:39:30.373   Training iter 550, batch loss 1.5356, batch acc 0.8908
17:39:30.925   Training iter 600, batch loss 1.5284, batch acc 0.9010
17:39:30.927 Training @ 164 epoch...
17:39:31.466   Training iter 50, batch loss 1.5456, batch acc 0.8912
17:39:32.004   Training iter 100, batch loss 1.5353, batch acc 0.9048
17:39:32.542   Training iter 150, batch loss 1.5288, batch acc 0.9056
17:39:33.073   Training iter 200, batch loss 1.5255, batch acc 0.9142
17:39:33.595   Training iter 250, batch loss 1.5242, batch acc 0.9058
17:39:34.107   Training iter 300, batch loss 1.5244, batch acc 0.9124
17:39:34.648   Training iter 350, batch loss 1.5456, batch acc 0.8694
17:39:35.178   Training iter 400, batch loss 1.5263, batch acc 0.9078
17:39:35.701   Training iter 450, batch loss 1.5182, batch acc 0.9274
17:39:36.233   Training iter 500, batch loss 1.5297, batch acc 0.9126
17:39:36.765   Training iter 550, batch loss 1.5425, batch acc 0.8616
17:39:37.299   Training iter 600, batch loss 1.5332, batch acc 0.8832
17:39:37.301 Training @ 165 epoch...
17:39:37.874   Training iter 50, batch loss 1.5248, batch acc 0.9132
17:39:38.558   Training iter 100, batch loss 1.5351, batch acc 0.9044
17:39:39.252   Training iter 150, batch loss 1.5340, batch acc 0.9090
17:39:39.882   Training iter 200, batch loss 1.5354, batch acc 0.9084
17:39:40.510   Training iter 250, batch loss 1.5304, batch acc 0.8998
17:39:41.147   Training iter 300, batch loss 1.5239, batch acc 0.9096
17:39:41.789   Training iter 350, batch loss 1.5239, batch acc 0.9108
17:39:42.426   Training iter 400, batch loss 1.5275, batch acc 0.8990
17:39:43.050   Training iter 450, batch loss 1.5331, batch acc 0.8954
17:39:43.670   Training iter 500, batch loss 1.5291, batch acc 0.8968
17:39:44.290   Training iter 550, batch loss 1.5327, batch acc 0.9018
17:39:44.907   Training iter 600, batch loss 1.5202, batch acc 0.9220
17:39:44.908 Testing @ 165 epoch...
17:39:44.966     Testing, total mean loss 1.53588, total acc 0.91570
17:39:44.966 Training @ 166 epoch...
17:39:45.584   Training iter 50, batch loss 1.5452, batch acc 0.8918
17:39:46.145   Training iter 100, batch loss 1.5284, batch acc 0.9070
17:39:46.702   Training iter 150, batch loss 1.5213, batch acc 0.9188
17:39:47.261   Training iter 200, batch loss 1.5306, batch acc 0.9148
17:39:47.803   Training iter 250, batch loss 1.5223, batch acc 0.9134
17:39:48.310   Training iter 300, batch loss 1.5377, batch acc 0.8936
17:39:48.837   Training iter 350, batch loss 1.5421, batch acc 0.8880
17:39:49.358   Training iter 400, batch loss 1.5262, batch acc 0.9108
17:39:49.881   Training iter 450, batch loss 1.5336, batch acc 0.8952
17:39:50.413   Training iter 500, batch loss 1.5375, batch acc 0.8938
17:39:50.957   Training iter 550, batch loss 1.5290, batch acc 0.9240
17:39:51.494   Training iter 600, batch loss 1.5377, batch acc 0.8982
17:39:51.496 Training @ 167 epoch...
17:39:52.053   Training iter 50, batch loss 1.5340, batch acc 0.8914
17:39:52.588   Training iter 100, batch loss 1.5271, batch acc 0.9060
17:39:53.119   Training iter 150, batch loss 1.5357, batch acc 0.8904
17:39:53.635   Training iter 200, batch loss 1.5245, batch acc 0.9128
17:39:54.167   Training iter 250, batch loss 1.5330, batch acc 0.9010
17:39:54.702   Training iter 300, batch loss 1.5358, batch acc 0.9010
17:39:55.238   Training iter 350, batch loss 1.5262, batch acc 0.9048
17:39:55.774   Training iter 400, batch loss 1.5312, batch acc 0.8996
17:39:56.315   Training iter 450, batch loss 1.5230, batch acc 0.9164
17:39:56.888   Training iter 500, batch loss 1.5282, batch acc 0.9024
17:39:57.446   Training iter 550, batch loss 1.5312, batch acc 0.9072
17:39:57.994   Training iter 600, batch loss 1.5390, batch acc 0.8940
17:39:57.996 Training @ 168 epoch...
17:39:58.556   Training iter 50, batch loss 1.5406, batch acc 0.8850
17:39:59.120   Training iter 100, batch loss 1.5439, batch acc 0.8918
17:39:59.692   Training iter 150, batch loss 1.5240, batch acc 0.9142
17:40:00.253   Training iter 200, batch loss 1.5220, batch acc 0.9192
17:40:00.814   Training iter 250, batch loss 1.5390, batch acc 0.9014
17:40:01.375   Training iter 300, batch loss 1.5313, batch acc 0.8904
17:40:01.959   Training iter 350, batch loss 1.5276, batch acc 0.9010
17:40:02.538   Training iter 400, batch loss 1.5311, batch acc 0.9026
17:40:03.072   Training iter 450, batch loss 1.5312, batch acc 0.9032
17:40:03.612   Training iter 500, batch loss 1.5331, batch acc 0.9080
17:40:04.193   Training iter 550, batch loss 1.5350, batch acc 0.8960
17:40:04.751   Training iter 600, batch loss 1.5299, batch acc 0.9016
17:40:04.753 Training @ 169 epoch...
17:40:05.329   Training iter 50, batch loss 1.5377, batch acc 0.8840
17:40:05.892   Training iter 100, batch loss 1.5339, batch acc 0.8692
17:40:06.457   Training iter 150, batch loss 1.5369, batch acc 0.8726
17:40:07.016   Training iter 200, batch loss 1.5264, batch acc 0.9028
17:40:07.579   Training iter 250, batch loss 1.5313, batch acc 0.9074
17:40:08.147   Training iter 300, batch loss 1.5196, batch acc 0.9126
17:40:08.705   Training iter 350, batch loss 1.5330, batch acc 0.8826
17:40:09.265   Training iter 400, batch loss 1.5261, batch acc 0.8970
17:40:09.845   Training iter 450, batch loss 1.5337, batch acc 0.8810
17:40:10.434   Training iter 500, batch loss 1.5254, batch acc 0.9032
17:40:11.020   Training iter 550, batch loss 1.5314, batch acc 0.8814
17:40:11.622   Training iter 600, batch loss 1.5245, batch acc 0.8990
17:40:11.624 Training @ 170 epoch...
17:40:12.227   Training iter 50, batch loss 1.5247, batch acc 0.9130
17:40:12.823   Training iter 100, batch loss 1.5354, batch acc 0.8968
17:40:13.418   Training iter 150, batch loss 1.5263, batch acc 0.9064
17:40:14.015   Training iter 200, batch loss 1.5276, batch acc 0.9032
17:40:14.611   Training iter 250, batch loss 1.5354, batch acc 0.8948
17:40:15.208   Training iter 300, batch loss 1.5242, batch acc 0.9072
17:40:15.801   Training iter 350, batch loss 1.5218, batch acc 0.9050
17:40:16.405   Training iter 400, batch loss 1.5239, batch acc 0.9144
17:40:16.997   Training iter 450, batch loss 1.5288, batch acc 0.9142
17:40:17.557   Training iter 500, batch loss 1.5286, batch acc 0.9020
17:40:18.102   Training iter 550, batch loss 1.5249, batch acc 0.9084
17:40:18.647   Training iter 600, batch loss 1.5344, batch acc 0.8978
17:40:18.649 Testing @ 170 epoch...
17:40:18.698     Testing, total mean loss 1.57055, total acc 0.84510
17:40:18.698 Training @ 171 epoch...
17:40:19.256   Training iter 50, batch loss 1.5358, batch acc 0.8938
17:40:19.819   Training iter 100, batch loss 1.5384, batch acc 0.8986
17:40:20.412   Training iter 150, batch loss 1.5208, batch acc 0.9204
17:40:21.003   Training iter 200, batch loss 1.5253, batch acc 0.9130
17:40:21.596   Training iter 250, batch loss 1.5547, batch acc 0.8600
17:40:22.206   Training iter 300, batch loss 1.5403, batch acc 0.8962
17:40:22.805   Training iter 350, batch loss 1.5366, batch acc 0.8926
17:40:23.408   Training iter 400, batch loss 1.5249, batch acc 0.9024
17:40:23.992   Training iter 450, batch loss 1.5484, batch acc 0.8732
17:40:24.585   Training iter 500, batch loss 1.5208, batch acc 0.9076
17:40:25.180   Training iter 550, batch loss 1.5244, batch acc 0.9170
17:40:25.733   Training iter 600, batch loss 1.5281, batch acc 0.9020
17:40:25.735 Training @ 172 epoch...
17:40:26.309   Training iter 50, batch loss 1.5311, batch acc 0.8872
17:40:26.865   Training iter 100, batch loss 1.5374, batch acc 0.8740
17:40:27.428   Training iter 150, batch loss 1.5325, batch acc 0.8876
17:40:27.988   Training iter 200, batch loss 1.5284, batch acc 0.8958
17:40:28.538   Training iter 250, batch loss 1.5256, batch acc 0.8900
17:40:29.099   Training iter 300, batch loss 1.5304, batch acc 0.8982
17:40:29.638   Training iter 350, batch loss 1.5227, batch acc 0.9178
17:40:30.193   Training iter 400, batch loss 1.5179, batch acc 0.9184
17:40:30.743   Training iter 450, batch loss 1.5255, batch acc 0.9002
17:40:31.287   Training iter 500, batch loss 1.5450, batch acc 0.8674
17:40:31.830   Training iter 550, batch loss 1.5333, batch acc 0.8920
17:40:32.377   Training iter 600, batch loss 1.5271, batch acc 0.9118
17:40:32.379 Training @ 173 epoch...
17:40:32.932   Training iter 50, batch loss 1.5270, batch acc 0.9120
17:40:33.455   Training iter 100, batch loss 1.5398, batch acc 0.8910
17:40:33.988   Training iter 150, batch loss 1.5268, batch acc 0.9042
17:40:34.518   Training iter 200, batch loss 1.5263, batch acc 0.9078
17:40:35.023   Training iter 250, batch loss 1.5308, batch acc 0.8928
17:40:35.510   Training iter 300, batch loss 1.5256, batch acc 0.9170
17:40:36.004   Training iter 350, batch loss 1.5377, batch acc 0.8940
17:40:36.507   Training iter 400, batch loss 1.5330, batch acc 0.8992
17:40:37.017   Training iter 450, batch loss 1.5622, batch acc 0.8466
17:40:37.529   Training iter 500, batch loss 1.5320, batch acc 0.8926
17:40:38.037   Training iter 550, batch loss 1.5300, batch acc 0.8990
17:40:38.546   Training iter 600, batch loss 1.5375, batch acc 0.8802
17:40:38.547 Training @ 174 epoch...
17:40:39.062   Training iter 50, batch loss 1.5298, batch acc 0.8932
17:40:39.578   Training iter 100, batch loss 1.5195, batch acc 0.9072
17:40:40.104   Training iter 150, batch loss 1.5234, batch acc 0.9014
17:40:40.633   Training iter 200, batch loss 1.5233, batch acc 0.9094
17:40:41.186   Training iter 250, batch loss 1.5311, batch acc 0.8992
17:40:41.744   Training iter 300, batch loss 1.5404, batch acc 0.8880
17:40:42.305   Training iter 350, batch loss 1.5261, batch acc 0.9028
17:40:42.862   Training iter 400, batch loss 1.5300, batch acc 0.9018
17:40:43.427   Training iter 450, batch loss 1.5404, batch acc 0.9080
17:40:43.998   Training iter 500, batch loss 1.5331, batch acc 0.9018
17:40:44.574   Training iter 550, batch loss 1.5330, batch acc 0.9060
17:40:45.132   Training iter 600, batch loss 1.5363, batch acc 0.8920
17:40:45.134 Training @ 175 epoch...
17:40:45.703   Training iter 50, batch loss 1.5261, batch acc 0.9076
17:40:46.327   Training iter 100, batch loss 1.5423, batch acc 0.8724
17:40:46.995   Training iter 150, batch loss 1.5312, batch acc 0.8930
17:40:47.649   Training iter 200, batch loss 1.5274, batch acc 0.9060
17:40:48.280   Training iter 250, batch loss 1.5278, batch acc 0.8970
17:40:48.888   Training iter 300, batch loss 1.5309, batch acc 0.9066
17:40:49.508   Training iter 350, batch loss 1.5284, batch acc 0.9050
17:40:50.124   Training iter 400, batch loss 1.5367, batch acc 0.9034
17:40:50.734   Training iter 450, batch loss 1.5508, batch acc 0.8710
17:40:51.346   Training iter 500, batch loss 1.5209, batch acc 0.9126
17:40:51.963   Training iter 550, batch loss 1.5286, batch acc 0.8986
17:40:52.589   Training iter 600, batch loss 1.5308, batch acc 0.8872
17:40:52.591 Testing @ 175 epoch...
17:40:52.648     Testing, total mean loss 1.52832, total acc 0.89360
17:40:52.648 Training @ 176 epoch...
17:40:53.292   Training iter 50, batch loss 1.5325, batch acc 0.8670
17:40:53.902   Training iter 100, batch loss 1.5346, batch acc 0.8980
17:40:54.513   Training iter 150, batch loss 1.5508, batch acc 0.8818
17:40:55.143   Training iter 200, batch loss 1.5266, batch acc 0.8974
17:40:55.767   Training iter 250, batch loss 1.5406, batch acc 0.8908
17:40:56.402   Training iter 300, batch loss 1.5448, batch acc 0.8906
17:40:57.024   Training iter 350, batch loss 1.5312, batch acc 0.8978
17:40:57.580   Training iter 400, batch loss 1.5256, batch acc 0.9096
17:40:58.127   Training iter 450, batch loss 1.5358, batch acc 0.8940
17:40:58.667   Training iter 500, batch loss 1.5183, batch acc 0.9242
17:40:59.242   Training iter 550, batch loss 1.5312, batch acc 0.9062
17:40:59.826   Training iter 600, batch loss 1.5314, batch acc 0.9138
17:40:59.828 Training @ 177 epoch...
17:41:00.396   Training iter 50, batch loss 1.5177, batch acc 0.9214
17:41:00.969   Training iter 100, batch loss 1.5180, batch acc 0.9264
17:41:01.539   Training iter 150, batch loss 1.5227, batch acc 0.9130
17:41:02.141   Training iter 200, batch loss 1.5378, batch acc 0.8800
17:41:02.741   Training iter 250, batch loss 1.5244, batch acc 0.9050
17:41:03.331   Training iter 300, batch loss 1.5416, batch acc 0.9054
17:41:03.909   Training iter 350, batch loss 1.5276, batch acc 0.8936
17:41:04.504   Training iter 400, batch loss 1.5597, batch acc 0.8632
17:41:05.072   Training iter 450, batch loss 1.5351, batch acc 0.8844
17:41:05.614   Training iter 500, batch loss 1.5282, batch acc 0.9090
17:41:06.185   Training iter 550, batch loss 1.5350, batch acc 0.9086
17:41:06.748   Training iter 600, batch loss 1.5287, batch acc 0.9116
17:41:06.749 Training @ 178 epoch...
17:41:07.314   Training iter 50, batch loss 1.5326, batch acc 0.9052
17:41:07.861   Training iter 100, batch loss 1.5387, batch acc 0.8868
17:41:08.413   Training iter 150, batch loss 1.5234, batch acc 0.9208
17:41:08.961   Training iter 200, batch loss 1.5293, batch acc 0.9098
17:41:09.510   Training iter 250, batch loss 1.5310, batch acc 0.9058
17:41:10.061   Training iter 300, batch loss 1.5465, batch acc 0.8698
17:41:10.618   Training iter 350, batch loss 1.5276, batch acc 0.9040
17:41:11.175   Training iter 400, batch loss 1.5416, batch acc 0.8910
17:41:11.734   Training iter 450, batch loss 1.5344, batch acc 0.8926
17:41:12.310   Training iter 500, batch loss 1.5253, batch acc 0.9064
17:41:12.864   Training iter 550, batch loss 1.5381, batch acc 0.9098
17:41:13.406   Training iter 600, batch loss 1.5261, batch acc 0.9046
17:41:13.408 Training @ 179 epoch...
17:41:13.955   Training iter 50, batch loss 1.5306, batch acc 0.9122
17:41:14.509   Training iter 100, batch loss 1.5264, batch acc 0.9162
17:41:15.067   Training iter 150, batch loss 1.5185, batch acc 0.9200
17:41:15.616   Training iter 200, batch loss 1.5306, batch acc 0.8972
17:41:16.179   Training iter 250, batch loss 1.5291, batch acc 0.9030
17:41:16.748   Training iter 300, batch loss 1.5312, batch acc 0.9050
17:41:17.302   Training iter 350, batch loss 1.5288, batch acc 0.8976
17:41:17.805   Training iter 400, batch loss 1.5294, batch acc 0.8932
17:41:18.321   Training iter 450, batch loss 1.5246, batch acc 0.9046
17:41:18.849   Training iter 500, batch loss 1.5247, batch acc 0.9102
17:41:19.377   Training iter 550, batch loss 1.5379, batch acc 0.8934
17:41:19.920   Training iter 600, batch loss 1.5389, batch acc 0.8832
17:41:19.921 Training @ 180 epoch...
17:41:20.468   Training iter 50, batch loss 1.5374, batch acc 0.8760
17:41:21.022   Training iter 100, batch loss 1.5528, batch acc 0.8570
17:41:21.586   Training iter 150, batch loss 1.5420, batch acc 0.8670
17:41:22.162   Training iter 200, batch loss 1.5359, batch acc 0.8852
17:41:22.720   Training iter 250, batch loss 1.5347, batch acc 0.8862
17:41:23.278   Training iter 300, batch loss 1.5339, batch acc 0.8840
17:41:23.819   Training iter 350, batch loss 1.5312, batch acc 0.8882
17:41:24.374   Training iter 400, batch loss 1.5380, batch acc 0.8730
17:41:24.935   Training iter 450, batch loss 1.5232, batch acc 0.9014
17:41:25.478   Training iter 500, batch loss 1.5271, batch acc 0.8992
17:41:26.028   Training iter 550, batch loss 1.5289, batch acc 0.8986
17:41:26.586   Training iter 600, batch loss 1.5228, batch acc 0.9008
17:41:26.588 Testing @ 180 epoch...
17:41:26.636     Testing, total mean loss 1.53346, total acc 0.88610
17:41:26.636 Training @ 181 epoch...
17:41:27.204   Training iter 50, batch loss 1.5344, batch acc 0.8852
17:41:27.750   Training iter 100, batch loss 1.5366, batch acc 0.8824
17:41:28.298   Training iter 150, batch loss 1.5306, batch acc 0.9080
17:41:28.845   Training iter 200, batch loss 1.5235, batch acc 0.9136
17:41:29.411   Training iter 250, batch loss 1.5449, batch acc 0.8954
17:41:29.979   Training iter 300, batch loss 1.5397, batch acc 0.8974
17:41:30.529   Training iter 350, batch loss 1.5403, batch acc 0.8742
17:41:31.081   Training iter 400, batch loss 1.5312, batch acc 0.8998
17:41:31.636   Training iter 450, batch loss 1.5354, batch acc 0.8912
17:41:32.187   Training iter 500, batch loss 1.5252, batch acc 0.9104
17:41:32.728   Training iter 550, batch loss 1.5237, batch acc 0.9072
17:41:33.253   Training iter 600, batch loss 1.5284, batch acc 0.9064
17:41:33.255 Training @ 182 epoch...
17:41:33.800   Training iter 50, batch loss 1.5428, batch acc 0.8722
17:41:34.352   Training iter 100, batch loss 1.5283, batch acc 0.9066
17:41:34.910   Training iter 150, batch loss 1.5327, batch acc 0.8866
17:41:35.403   Training iter 200, batch loss 1.5265, batch acc 0.8950
17:41:35.915   Training iter 250, batch loss 1.5255, batch acc 0.8858
17:41:36.462   Training iter 300, batch loss 1.5260, batch acc 0.8942
17:41:36.987   Training iter 350, batch loss 1.5369, batch acc 0.8920
17:41:37.477   Training iter 400, batch loss 1.5311, batch acc 0.8858
17:41:37.935   Training iter 450, batch loss 1.5301, batch acc 0.9016
17:41:38.423   Training iter 500, batch loss 1.5282, batch acc 0.9028
17:41:38.877   Training iter 550, batch loss 1.5205, batch acc 0.9178
17:41:39.355   Training iter 600, batch loss 1.5355, batch acc 0.8934
17:41:39.356 Training @ 183 epoch...
17:41:39.815   Training iter 50, batch loss 1.5299, batch acc 0.9096
17:41:40.259   Training iter 100, batch loss 1.5244, batch acc 0.9194
17:41:40.718   Training iter 150, batch loss 1.5199, batch acc 0.9232
17:41:41.185   Training iter 200, batch loss 1.5237, batch acc 0.9216
17:41:41.645   Training iter 250, batch loss 1.5505, batch acc 0.8416
17:41:42.137   Training iter 300, batch loss 1.5269, batch acc 0.8870
17:41:42.636   Training iter 350, batch loss 1.5273, batch acc 0.9038
17:41:43.171   Training iter 400, batch loss 1.5501, batch acc 0.8534
17:41:43.730   Training iter 450, batch loss 1.5432, batch acc 0.8720
17:41:44.253   Training iter 500, batch loss 1.5687, batch acc 0.8792
17:41:44.758   Training iter 550, batch loss 1.5379, batch acc 0.8968
17:41:45.235   Training iter 600, batch loss 1.5254, batch acc 0.9130
17:41:45.236 Training @ 184 epoch...
17:41:45.718   Training iter 50, batch loss 1.5205, batch acc 0.9088
17:41:46.215   Training iter 100, batch loss 1.5259, batch acc 0.9100
17:41:46.794   Training iter 150, batch loss 1.5312, batch acc 0.9072
17:41:47.325   Training iter 200, batch loss 1.5336, batch acc 0.8850
17:41:47.848   Training iter 250, batch loss 1.5235, batch acc 0.8980
17:41:48.362   Training iter 300, batch loss 1.5262, batch acc 0.9068
17:41:48.867   Training iter 350, batch loss 1.5261, batch acc 0.9104
17:41:49.396   Training iter 400, batch loss 1.5286, batch acc 0.9154
17:41:49.920   Training iter 450, batch loss 1.5249, batch acc 0.9034
17:41:50.453   Training iter 500, batch loss 1.5354, batch acc 0.8952
17:41:50.992   Training iter 550, batch loss 1.5311, batch acc 0.9052
17:41:51.529   Training iter 600, batch loss 1.5380, batch acc 0.8824
17:41:51.530 Training @ 185 epoch...
17:41:52.076   Training iter 50, batch loss 1.5258, batch acc 0.9012
17:41:52.634   Training iter 100, batch loss 1.5276, batch acc 0.9072
17:41:53.184   Training iter 150, batch loss 1.5257, batch acc 0.9102
17:41:53.695   Training iter 200, batch loss 1.5368, batch acc 0.9006
17:41:54.231   Training iter 250, batch loss 1.5312, batch acc 0.8928
17:41:54.728   Training iter 300, batch loss 1.5317, batch acc 0.9010
17:41:55.215   Training iter 350, batch loss 1.5305, batch acc 0.8934
17:41:55.699   Training iter 400, batch loss 1.5287, batch acc 0.8874
17:41:56.195   Training iter 450, batch loss 1.5422, batch acc 0.8776
17:41:56.693   Training iter 500, batch loss 1.5365, batch acc 0.8804
17:41:57.189   Training iter 550, batch loss 1.5461, batch acc 0.8532
17:41:57.681   Training iter 600, batch loss 1.5227, batch acc 0.9012
17:41:57.682 Testing @ 185 epoch...
17:41:57.730     Testing, total mean loss 1.52662, total acc 0.90750
17:41:57.730 Training @ 186 epoch...
17:41:58.231   Training iter 50, batch loss 1.5412, batch acc 0.9006
17:41:58.722   Training iter 100, batch loss 1.5238, batch acc 0.9188
17:41:59.247   Training iter 150, batch loss 1.5353, batch acc 0.8918
17:41:59.756   Training iter 200, batch loss 1.5320, batch acc 0.9048
17:42:00.267   Training iter 250, batch loss 1.5399, batch acc 0.8966
17:42:00.735   Training iter 300, batch loss 1.5268, batch acc 0.9084
17:42:01.214   Training iter 350, batch loss 1.5229, batch acc 0.9030
17:42:01.727   Training iter 400, batch loss 1.5248, batch acc 0.9060
17:42:02.273   Training iter 450, batch loss 1.5357, batch acc 0.8844
17:42:02.781   Training iter 500, batch loss 1.5416, batch acc 0.8852
17:42:03.332   Training iter 550, batch loss 1.5320, batch acc 0.9054
17:42:03.873   Training iter 600, batch loss 1.5236, batch acc 0.9046
17:42:03.874 Training @ 187 epoch...
17:42:04.446   Training iter 50, batch loss 1.5420, batch acc 0.8512
17:42:04.980   Training iter 100, batch loss 1.5510, batch acc 0.8432
17:42:05.507   Training iter 150, batch loss 1.5423, batch acc 0.8882
17:42:06.074   Training iter 200, batch loss 1.5291, batch acc 0.9004
17:42:06.634   Training iter 250, batch loss 1.5308, batch acc 0.9084
17:42:07.153   Training iter 300, batch loss 1.5303, batch acc 0.8946
17:42:07.651   Training iter 350, batch loss 1.5344, batch acc 0.8912
17:42:08.155   Training iter 400, batch loss 1.5341, batch acc 0.8806
17:42:08.660   Training iter 450, batch loss 1.5211, batch acc 0.9056
17:42:09.191   Training iter 500, batch loss 1.5526, batch acc 0.8220
17:42:09.731   Training iter 550, batch loss 1.5325, batch acc 0.8912
17:42:10.251   Training iter 600, batch loss 1.5276, batch acc 0.9022
17:42:10.253 Training @ 188 epoch...
17:42:10.769   Training iter 50, batch loss 1.5377, batch acc 0.8758
17:42:11.269   Training iter 100, batch loss 1.5377, batch acc 0.8840
17:42:11.766   Training iter 150, batch loss 1.5254, batch acc 0.9002
17:42:12.278   Training iter 200, batch loss 1.5339, batch acc 0.8892
17:42:12.791   Training iter 250, batch loss 1.5413, batch acc 0.8766
17:42:13.313   Training iter 300, batch loss 1.5308, batch acc 0.9022
17:42:13.825   Training iter 350, batch loss 1.5304, batch acc 0.9086
17:42:14.343   Training iter 400, batch loss 1.5337, batch acc 0.8892
17:42:14.820   Training iter 450, batch loss 1.5235, batch acc 0.9034
17:42:15.300   Training iter 500, batch loss 1.5223, batch acc 0.9184
17:42:15.768   Training iter 550, batch loss 1.5256, batch acc 0.9082
17:42:16.230   Training iter 600, batch loss 1.5227, batch acc 0.9106
17:42:16.232 Training @ 189 epoch...
17:42:16.688   Training iter 50, batch loss 1.5402, batch acc 0.8958
17:42:17.413   Training iter 100, batch loss 1.5216, batch acc 0.9072
17:42:18.118   Training iter 150, batch loss 1.5361, batch acc 0.8946
17:42:18.649   Training iter 200, batch loss 1.5196, batch acc 0.9150
17:42:19.187   Training iter 250, batch loss 1.5249, batch acc 0.9186
17:42:19.723   Training iter 300, batch loss 1.5347, batch acc 0.9006
17:42:20.239   Training iter 350, batch loss 1.5379, batch acc 0.8868
17:42:20.763   Training iter 400, batch loss 1.5314, batch acc 0.9050
17:42:21.296   Training iter 450, batch loss 1.5278, batch acc 0.9024
17:42:21.837   Training iter 500, batch loss 1.5295, batch acc 0.8988
17:42:22.374   Training iter 550, batch loss 1.5309, batch acc 0.8982
17:42:22.914   Training iter 600, batch loss 1.5285, batch acc 0.8984
17:42:22.916 Training @ 190 epoch...
17:42:23.467   Training iter 50, batch loss 1.5210, batch acc 0.9076
17:42:24.016   Training iter 100, batch loss 1.5286, batch acc 0.9072
17:42:24.575   Training iter 150, batch loss 1.5449, batch acc 0.8518
17:42:25.121   Training iter 200, batch loss 1.5347, batch acc 0.8788
17:42:25.666   Training iter 250, batch loss 1.5408, batch acc 0.8670
17:42:26.213   Training iter 300, batch loss 1.5265, batch acc 0.9104
17:42:26.760   Training iter 350, batch loss 1.5318, batch acc 0.9102
17:42:27.308   Training iter 400, batch loss 1.5381, batch acc 0.8754
17:42:27.857   Training iter 450, batch loss 1.5325, batch acc 0.8834
17:42:28.407   Training iter 500, batch loss 1.5333, batch acc 0.8816
17:42:28.970   Training iter 550, batch loss 1.5540, batch acc 0.8386
17:42:29.554   Training iter 600, batch loss 1.5353, batch acc 0.8578
17:42:29.556 Testing @ 190 epoch...
17:42:29.611     Testing, total mean loss 1.54013, total acc 0.86720
17:42:29.611 Training @ 191 epoch...
17:42:30.176   Training iter 50, batch loss 1.5452, batch acc 0.8636
17:42:30.729   Training iter 100, batch loss 1.5393, batch acc 0.8646
17:42:31.332   Training iter 150, batch loss 1.5209, batch acc 0.9098
17:42:31.930   Training iter 200, batch loss 1.5268, batch acc 0.9088
17:42:32.527   Training iter 250, batch loss 1.5332, batch acc 0.8872
17:42:33.054   Training iter 300, batch loss 1.5490, batch acc 0.8586
17:42:33.580   Training iter 350, batch loss 1.5418, batch acc 0.8632
17:42:34.131   Training iter 400, batch loss 1.5321, batch acc 0.9096
17:42:34.686   Training iter 450, batch loss 1.5430, batch acc 0.8864
17:42:35.226   Training iter 500, batch loss 1.5317, batch acc 0.9022
17:42:35.779   Training iter 550, batch loss 1.5221, batch acc 0.9142
17:42:36.348   Training iter 600, batch loss 1.5253, batch acc 0.9174
17:42:36.349 Training @ 192 epoch...
17:42:36.931   Training iter 50, batch loss 1.5297, batch acc 0.9118
17:42:37.498   Training iter 100, batch loss 1.5306, batch acc 0.9118
17:42:38.074   Training iter 150, batch loss 1.5377, batch acc 0.9064
17:42:38.626   Training iter 200, batch loss 1.5427, batch acc 0.8876
17:42:39.185   Training iter 250, batch loss 1.5238, batch acc 0.9190
17:42:39.730   Training iter 300, batch loss 1.5307, batch acc 0.9004
17:42:40.279   Training iter 350, batch loss 1.5349, batch acc 0.9018
17:42:40.850   Training iter 400, batch loss 1.5211, batch acc 0.9050
17:42:41.416   Training iter 450, batch loss 1.5264, batch acc 0.9012
17:42:41.979   Training iter 500, batch loss 1.5285, batch acc 0.9074
17:42:42.531   Training iter 550, batch loss 1.5287, batch acc 0.9058
17:42:43.085   Training iter 600, batch loss 1.5288, batch acc 0.9134
17:42:43.087 Training @ 193 epoch...
17:42:43.646   Training iter 50, batch loss 1.5368, batch acc 0.8844
17:42:44.207   Training iter 100, batch loss 1.5332, batch acc 0.9048
17:42:44.760   Training iter 150, batch loss 1.5305, batch acc 0.9050
17:42:45.332   Training iter 200, batch loss 1.5335, batch acc 0.9092
17:42:45.902   Training iter 250, batch loss 1.5280, batch acc 0.8970
17:42:46.466   Training iter 300, batch loss 1.5279, batch acc 0.8992
17:42:47.014   Training iter 350, batch loss 1.5336, batch acc 0.8976
17:42:47.567   Training iter 400, batch loss 1.5244, batch acc 0.9136
17:42:48.118   Training iter 450, batch loss 1.5206, batch acc 0.9188
17:42:48.674   Training iter 500, batch loss 1.5220, batch acc 0.9144
17:42:49.239   Training iter 550, batch loss 1.5272, batch acc 0.9090
17:42:49.806   Training iter 600, batch loss 1.5231, batch acc 0.9186
17:42:49.808 Training @ 194 epoch...
17:42:50.378   Training iter 50, batch loss 1.5291, batch acc 0.8862
17:42:50.940   Training iter 100, batch loss 1.5376, batch acc 0.8866
17:42:51.525   Training iter 150, batch loss 1.5305, batch acc 0.9018
17:42:52.114   Training iter 200, batch loss 1.5457, batch acc 0.8768
17:42:52.705   Training iter 250, batch loss 1.5368, batch acc 0.8758
17:42:53.291   Training iter 300, batch loss 1.5259, batch acc 0.8980
17:42:53.871   Training iter 350, batch loss 1.5324, batch acc 0.9034
17:42:54.452   Training iter 400, batch loss 1.5276, batch acc 0.9038
17:42:55.034   Training iter 450, batch loss 1.5333, batch acc 0.8834
17:42:55.599   Training iter 500, batch loss 1.5325, batch acc 0.8848
17:42:56.162   Training iter 550, batch loss 1.5254, batch acc 0.9164
17:42:56.739   Training iter 600, batch loss 1.5483, batch acc 0.8764
17:42:56.741 Training @ 195 epoch...
17:42:57.288   Training iter 50, batch loss 1.5461, batch acc 0.8784
17:42:57.817   Training iter 100, batch loss 1.5269, batch acc 0.9076
17:42:58.351   Training iter 150, batch loss 1.5278, batch acc 0.9014
17:42:58.866   Training iter 200, batch loss 1.5263, batch acc 0.8904
17:42:59.411   Training iter 250, batch loss 1.5231, batch acc 0.8926
17:42:59.940   Training iter 300, batch loss 1.5419, batch acc 0.8762
17:43:00.477   Training iter 350, batch loss 1.5353, batch acc 0.8926
17:43:01.002   Training iter 400, batch loss 1.5353, batch acc 0.8762
17:43:01.528   Training iter 450, batch loss 1.5284, batch acc 0.9090
17:43:02.164   Training iter 500, batch loss 1.5430, batch acc 0.8816
17:43:02.793   Training iter 550, batch loss 1.5433, batch acc 0.8694
17:43:03.399   Training iter 600, batch loss 1.5319, batch acc 0.8906
17:43:03.401 Testing @ 195 epoch...
17:43:03.456     Testing, total mean loss 1.52974, total acc 0.88540
17:43:03.456 Training @ 196 epoch...
17:43:04.062   Training iter 50, batch loss 1.5428, batch acc 0.8526
17:43:04.677   Training iter 100, batch loss 1.5299, batch acc 0.8800
17:43:05.279   Training iter 150, batch loss 1.5400, batch acc 0.8686
17:43:05.869   Training iter 200, batch loss 1.5335, batch acc 0.8956
17:43:06.449   Training iter 250, batch loss 1.5296, batch acc 0.9018
17:43:06.989   Training iter 300, batch loss 1.5265, batch acc 0.8976
17:43:07.513   Training iter 350, batch loss 1.5341, batch acc 0.8962
17:43:08.085   Training iter 400, batch loss 1.5204, batch acc 0.9182
17:43:08.642   Training iter 450, batch loss 1.5353, batch acc 0.8904
17:43:09.222   Training iter 500, batch loss 1.5338, batch acc 0.8892
17:43:09.771   Training iter 550, batch loss 1.5232, batch acc 0.8962
17:43:10.296   Training iter 600, batch loss 1.5337, batch acc 0.8838
17:43:10.298 Training @ 197 epoch...
17:43:10.790   Training iter 50, batch loss 1.5283, batch acc 0.9014
17:43:11.302   Training iter 100, batch loss 1.5290, batch acc 0.8912
17:43:11.843   Training iter 150, batch loss 1.5348, batch acc 0.8876
17:43:12.386   Training iter 200, batch loss 1.5313, batch acc 0.8834
17:43:13.046   Training iter 250, batch loss 1.5273, batch acc 0.9056
17:43:13.574   Training iter 300, batch loss 1.5238, batch acc 0.8916
17:43:14.131   Training iter 350, batch loss 1.5351, batch acc 0.8990
17:43:14.695   Training iter 400, batch loss 1.5297, batch acc 0.9024
17:43:15.261   Training iter 450, batch loss 1.5281, batch acc 0.8948
17:43:15.813   Training iter 500, batch loss 1.5216, batch acc 0.9148
17:43:16.357   Training iter 550, batch loss 1.5285, batch acc 0.9058
17:43:16.894   Training iter 600, batch loss 1.5254, batch acc 0.9140
17:43:16.896 Training @ 198 epoch...
17:43:17.434   Training iter 50, batch loss 1.5248, batch acc 0.9080
17:43:17.973   Training iter 100, batch loss 1.5280, batch acc 0.9118
17:43:18.528   Training iter 150, batch loss 1.5241, batch acc 0.9074
17:43:19.071   Training iter 200, batch loss 1.5459, batch acc 0.8586
17:43:19.596   Training iter 250, batch loss 1.5215, batch acc 0.9142
17:43:20.133   Training iter 300, batch loss 1.5339, batch acc 0.8840
17:43:20.707   Training iter 350, batch loss 1.5362, batch acc 0.8646
17:43:21.267   Training iter 400, batch loss 1.5279, batch acc 0.9010
17:43:21.809   Training iter 450, batch loss 1.5350, batch acc 0.9048
17:43:22.384   Training iter 500, batch loss 1.5344, batch acc 0.8992
17:43:22.964   Training iter 550, batch loss 1.5346, batch acc 0.8978
17:43:23.554   Training iter 600, batch loss 1.5337, batch acc 0.9108
17:43:23.556 Training @ 199 epoch...
17:43:24.125   Training iter 50, batch loss 1.5414, batch acc 0.8796
17:43:24.695   Training iter 100, batch loss 1.5294, batch acc 0.8986
17:43:25.278   Training iter 150, batch loss 1.5355, batch acc 0.8734
17:43:25.842   Training iter 200, batch loss 1.5283, batch acc 0.8904
17:43:26.412   Training iter 250, batch loss 1.5387, batch acc 0.8984
17:43:26.957   Training iter 300, batch loss 1.5432, batch acc 0.8740
17:43:27.576   Training iter 350, batch loss 1.5225, batch acc 0.9110
17:43:28.202   Training iter 400, batch loss 1.5285, batch acc 0.9070
17:43:28.823   Training iter 450, batch loss 1.5403, batch acc 0.8522
17:43:29.418   Training iter 500, batch loss 1.5303, batch acc 0.8790
17:43:29.972   Training iter 550, batch loss 1.5216, batch acc 0.9098
17:43:30.539   Training iter 600, batch loss 1.5345, batch acc 0.8824
17:43:30.541 Training @ 200 epoch...
17:43:31.109   Training iter 50, batch loss 1.5368, batch acc 0.8678
17:43:31.681   Training iter 100, batch loss 1.5418, batch acc 0.8464
17:43:32.258   Training iter 150, batch loss 1.5413, batch acc 0.8686
17:43:32.811   Training iter 200, batch loss 1.5272, batch acc 0.9140
17:43:33.378   Training iter 250, batch loss 1.5341, batch acc 0.8934
17:43:33.931   Training iter 300, batch loss 1.5271, batch acc 0.9174
17:43:34.488   Training iter 350, batch loss 1.5322, batch acc 0.9118
17:43:35.043   Training iter 400, batch loss 1.5312, batch acc 0.9154
17:43:35.602   Training iter 450, batch loss 1.5250, batch acc 0.9070
17:43:36.164   Training iter 500, batch loss 1.5270, batch acc 0.9020
17:43:36.735   Training iter 550, batch loss 1.5299, batch acc 0.8966
17:43:37.310   Training iter 600, batch loss 1.5290, batch acc 0.9054
17:43:37.312 Testing @ 200 epoch...
17:43:37.364     Testing, total mean loss 1.52705, total acc 0.90990
17:43:37.364 Plot @ 200 epoch...
17:43:37.364 Training @ 201 epoch...
17:43:37.952   Training iter 50, batch loss 1.5279, batch acc 0.9012
17:43:38.544   Training iter 100, batch loss 1.5272, batch acc 0.8964
17:43:39.107   Training iter 150, batch loss 1.5308, batch acc 0.8742
17:43:39.677   Training iter 200, batch loss 1.5264, batch acc 0.8954
17:43:40.257   Training iter 250, batch loss 1.5252, batch acc 0.9060
17:43:40.818   Training iter 300, batch loss 1.5321, batch acc 0.8852
17:43:41.394   Training iter 350, batch loss 1.5294, batch acc 0.8966
17:43:41.986   Training iter 400, batch loss 1.5199, batch acc 0.9100
17:43:42.584   Training iter 450, batch loss 1.5230, batch acc 0.9066
17:43:43.188   Training iter 500, batch loss 1.5442, batch acc 0.8840
17:43:43.773   Training iter 550, batch loss 1.5215, batch acc 0.9076
17:43:44.378   Training iter 600, batch loss 1.5345, batch acc 0.8868
17:43:44.379 Training @ 202 epoch...
17:43:44.981   Training iter 50, batch loss 1.5202, batch acc 0.9168
17:43:45.578   Training iter 100, batch loss 1.5399, batch acc 0.9012
17:43:46.153   Training iter 150, batch loss 1.5411, batch acc 0.8942
17:43:46.678   Training iter 200, batch loss 1.5246, batch acc 0.9182
17:43:47.195   Training iter 250, batch loss 1.5193, batch acc 0.9206
17:43:47.708   Training iter 300, batch loss 1.5274, batch acc 0.9098
17:43:48.242   Training iter 350, batch loss 1.5254, batch acc 0.9084
17:43:48.788   Training iter 400, batch loss 1.5386, batch acc 0.8650
17:43:49.325   Training iter 450, batch loss 1.5218, batch acc 0.8890
17:43:49.872   Training iter 500, batch loss 1.5256, batch acc 0.9016
17:43:50.377   Training iter 550, batch loss 1.5309, batch acc 0.9044
17:43:50.887   Training iter 600, batch loss 1.5454, batch acc 0.8754
17:43:50.888 Training @ 203 epoch...
17:43:51.406   Training iter 50, batch loss 1.5270, batch acc 0.9112
17:43:51.901   Training iter 100, batch loss 1.5277, batch acc 0.8992
17:43:52.408   Training iter 150, batch loss 1.5292, batch acc 0.9012
17:43:52.912   Training iter 200, batch loss 1.5530, batch acc 0.8580
17:43:53.429   Training iter 250, batch loss 1.5330, batch acc 0.8740
17:43:53.930   Training iter 300, batch loss 1.5349, batch acc 0.8914
17:43:54.444   Training iter 350, batch loss 1.5348, batch acc 0.8922
17:43:54.958   Training iter 400, batch loss 1.5286, batch acc 0.9070
17:43:55.486   Training iter 450, batch loss 1.5222, batch acc 0.9202
17:43:56.025   Training iter 500, batch loss 1.5302, batch acc 0.9166
17:43:56.557   Training iter 550, batch loss 1.5375, batch acc 0.9044
17:43:57.081   Training iter 600, batch loss 1.5267, batch acc 0.8906
17:43:57.083 Training @ 204 epoch...
17:43:57.610   Training iter 50, batch loss 1.5556, batch acc 0.8492
17:43:58.134   Training iter 100, batch loss 1.5304, batch acc 0.8976
17:43:58.656   Training iter 150, batch loss 1.5221, batch acc 0.9130
17:43:59.210   Training iter 200, batch loss 1.5277, batch acc 0.9116
17:43:59.761   Training iter 250, batch loss 1.5357, batch acc 0.8842
17:44:00.334   Training iter 300, batch loss 1.5467, batch acc 0.8828
17:44:00.891   Training iter 350, batch loss 1.5350, batch acc 0.9084
17:44:01.451   Training iter 400, batch loss 1.5484, batch acc 0.8212
17:44:02.067   Training iter 450, batch loss 1.5372, batch acc 0.8606
17:44:02.609   Training iter 500, batch loss 1.5454, batch acc 0.8850
17:44:03.148   Training iter 550, batch loss 1.5364, batch acc 0.9006
17:44:03.681   Training iter 600, batch loss 1.5343, batch acc 0.8906
17:44:03.683 Training @ 205 epoch...
17:44:04.255   Training iter 50, batch loss 1.5282, batch acc 0.9004
17:44:04.801   Training iter 100, batch loss 1.5262, batch acc 0.8976
17:44:05.337   Training iter 150, batch loss 1.5556, batch acc 0.8658
17:44:05.875   Training iter 200, batch loss 1.5331, batch acc 0.8796
17:44:06.437   Training iter 250, batch loss 1.5329, batch acc 0.8882
17:44:06.964   Training iter 300, batch loss 1.5397, batch acc 0.8728
17:44:07.494   Training iter 350, batch loss 1.5325, batch acc 0.8916
17:44:08.288   Training iter 400, batch loss 1.5297, batch acc 0.8856
17:44:09.243   Training iter 450, batch loss 1.5261, batch acc 0.8990
17:44:09.742   Training iter 500, batch loss 1.5350, batch acc 0.8984
17:44:10.315   Training iter 550, batch loss 1.5216, batch acc 0.9112
17:44:10.818   Training iter 600, batch loss 1.5375, batch acc 0.8790
17:44:10.820 Testing @ 205 epoch...
17:44:10.871     Testing, total mean loss 1.53437, total acc 0.89220
17:44:10.871 Training @ 206 epoch...
17:44:11.412   Training iter 50, batch loss 1.5343, batch acc 0.9024
17:44:11.921   Training iter 100, batch loss 1.5272, batch acc 0.9140
17:44:12.463   Training iter 150, batch loss 1.5273, batch acc 0.9064
17:44:12.997   Training iter 200, batch loss 1.5269, batch acc 0.8970
17:44:13.542   Training iter 250, batch loss 1.5229, batch acc 0.9114
17:44:14.089   Training iter 300, batch loss 1.5254, batch acc 0.9082
17:44:14.620   Training iter 350, batch loss 1.5301, batch acc 0.9088
17:44:15.149   Training iter 400, batch loss 1.5287, batch acc 0.8922
17:44:15.681   Training iter 450, batch loss 1.5537, batch acc 0.8572
17:44:16.227   Training iter 500, batch loss 1.5291, batch acc 0.9012
17:44:16.776   Training iter 550, batch loss 1.5281, batch acc 0.9072
17:44:17.312   Training iter 600, batch loss 1.5311, batch acc 0.9044
17:44:17.314 Training @ 207 epoch...
17:44:17.856   Training iter 50, batch loss 1.5276, batch acc 0.9090
17:44:18.411   Training iter 100, batch loss 1.5386, batch acc 0.8962
17:44:18.955   Training iter 150, batch loss 1.5414, batch acc 0.8726
17:44:19.507   Training iter 200, batch loss 1.5353, batch acc 0.9012
17:44:20.056   Training iter 250, batch loss 1.5432, batch acc 0.8728
17:44:20.608   Training iter 300, batch loss 1.5302, batch acc 0.8930
17:44:21.153   Training iter 350, batch loss 1.5319, batch acc 0.9028
17:44:21.707   Training iter 400, batch loss 1.5265, batch acc 0.9028
17:44:22.271   Training iter 450, batch loss 1.5245, batch acc 0.9104
17:44:22.822   Training iter 500, batch loss 1.5285, batch acc 0.9044
17:44:23.360   Training iter 550, batch loss 1.5230, batch acc 0.9032
17:44:23.932   Training iter 600, batch loss 1.5278, batch acc 0.9110
17:44:23.934 Training @ 208 epoch...
17:44:24.533   Training iter 50, batch loss 1.5272, batch acc 0.9022
17:44:25.117   Training iter 100, batch loss 1.5481, batch acc 0.8880
17:44:25.672   Training iter 150, batch loss 1.5242, batch acc 0.9102
17:44:26.224   Training iter 200, batch loss 1.5245, batch acc 0.9058
17:44:26.760   Training iter 250, batch loss 1.5287, batch acc 0.9096
17:44:27.283   Training iter 300, batch loss 1.5286, batch acc 0.9094
17:44:27.771   Training iter 350, batch loss 1.5305, batch acc 0.9164
17:44:28.267   Training iter 400, batch loss 1.5249, batch acc 0.9076
17:44:28.767   Training iter 450, batch loss 1.5286, batch acc 0.8784
17:44:29.298   Training iter 500, batch loss 1.5240, batch acc 0.9104
17:44:29.835   Training iter 550, batch loss 1.5358, batch acc 0.8910
17:44:30.376   Training iter 600, batch loss 1.5282, batch acc 0.9108
17:44:30.378 Training @ 209 epoch...
17:44:30.931   Training iter 50, batch loss 1.5220, batch acc 0.9034
17:44:31.461   Training iter 100, batch loss 1.5334, batch acc 0.8924
17:44:31.995   Training iter 150, batch loss 1.5350, batch acc 0.8856
17:44:32.530   Training iter 200, batch loss 1.5227, batch acc 0.9082
17:44:33.052   Training iter 250, batch loss 1.5407, batch acc 0.8832
17:44:33.586   Training iter 300, batch loss 1.5357, batch acc 0.8830
17:44:34.113   Training iter 350, batch loss 1.5403, batch acc 0.8548
17:44:34.659   Training iter 400, batch loss 1.5326, batch acc 0.8848
17:44:35.210   Training iter 450, batch loss 1.5426, batch acc 0.8708
17:44:35.758   Training iter 500, batch loss 1.5570, batch acc 0.8860
17:44:36.298   Training iter 550, batch loss 1.5331, batch acc 0.8900
17:44:36.843   Training iter 600, batch loss 1.5309, batch acc 0.8830
17:44:36.845 Training @ 210 epoch...
17:44:37.393   Training iter 50, batch loss 1.5388, batch acc 0.8760
17:44:37.938   Training iter 100, batch loss 1.5422, batch acc 0.8934
17:44:38.488   Training iter 150, batch loss 1.5339, batch acc 0.8994
17:44:39.066   Training iter 200, batch loss 1.5523, batch acc 0.8550
17:44:39.606   Training iter 250, batch loss 1.5359, batch acc 0.8814
17:44:40.170   Training iter 300, batch loss 1.5201, batch acc 0.9060
17:44:40.741   Training iter 350, batch loss 1.5293, batch acc 0.9068
17:44:41.285   Training iter 400, batch loss 1.5451, batch acc 0.8594
17:44:41.847   Training iter 450, batch loss 1.5262, batch acc 0.8928
17:44:42.399   Training iter 500, batch loss 1.5541, batch acc 0.8504
17:44:42.951   Training iter 550, batch loss 1.5328, batch acc 0.9046
17:44:43.499   Training iter 600, batch loss 1.5228, batch acc 0.9258
17:44:43.500 Testing @ 210 epoch...
17:44:43.548     Testing, total mean loss 1.52826, total acc 0.90590
17:44:43.548 Training @ 211 epoch...
17:44:44.134   Training iter 50, batch loss 1.5409, batch acc 0.8964
17:44:44.707   Training iter 100, batch loss 1.5538, batch acc 0.8800
17:44:45.274   Training iter 150, batch loss 1.5346, batch acc 0.8910
17:44:45.863   Training iter 200, batch loss 1.5404, batch acc 0.8922
17:44:46.442   Training iter 250, batch loss 1.5219, batch acc 0.9150
17:44:47.009   Training iter 300, batch loss 1.5303, batch acc 0.9080
17:44:47.580   Training iter 350, batch loss 1.5309, batch acc 0.8976
17:44:48.151   Training iter 400, batch loss 1.5333, batch acc 0.8946
17:44:48.723   Training iter 450, batch loss 1.5368, batch acc 0.8910
17:44:49.289   Training iter 500, batch loss 1.5228, batch acc 0.9072
17:44:49.848   Training iter 550, batch loss 1.5331, batch acc 0.8952
17:44:50.391   Training iter 600, batch loss 1.5276, batch acc 0.9184
17:44:50.392 Training @ 212 epoch...
17:44:50.952   Training iter 50, batch loss 1.5443, batch acc 0.9020
17:44:51.488   Training iter 100, batch loss 1.5403, batch acc 0.9122
17:44:52.025   Training iter 150, batch loss 1.5480, batch acc 0.8840
17:44:52.561   Training iter 200, batch loss 1.5380, batch acc 0.8740
17:44:53.105   Training iter 250, batch loss 1.5217, batch acc 0.9000
17:44:53.636   Training iter 300, batch loss 1.5366, batch acc 0.8834
17:44:54.139   Training iter 350, batch loss 1.5302, batch acc 0.9000
17:44:54.648   Training iter 400, batch loss 1.5316, batch acc 0.8866
17:44:55.155   Training iter 450, batch loss 1.5375, batch acc 0.8664
17:44:55.665   Training iter 500, batch loss 1.5349, batch acc 0.9018
17:44:56.163   Training iter 550, batch loss 1.5365, batch acc 0.8834
17:44:56.685   Training iter 600, batch loss 1.5329, batch acc 0.8842
17:44:56.687 Training @ 213 epoch...
17:44:57.217   Training iter 50, batch loss 1.5283, batch acc 0.9030
17:44:57.714   Training iter 100, batch loss 1.5304, batch acc 0.8940
17:44:58.246   Training iter 150, batch loss 1.5395, batch acc 0.8588
17:44:58.758   Training iter 200, batch loss 1.5351, batch acc 0.8888
17:44:59.286   Training iter 250, batch loss 1.5272, batch acc 0.9144
17:44:59.811   Training iter 300, batch loss 1.5341, batch acc 0.9086
17:45:00.374   Training iter 350, batch loss 1.5226, batch acc 0.9164
17:45:00.929   Training iter 400, batch loss 1.5369, batch acc 0.8984
17:45:01.492   Training iter 450, batch loss 1.5319, batch acc 0.9002
17:45:02.123   Training iter 500, batch loss 1.5219, batch acc 0.9264
17:45:02.737   Training iter 550, batch loss 1.5245, batch acc 0.9146
17:45:03.360   Training iter 600, batch loss 1.5439, batch acc 0.8782
17:45:03.362 Training @ 214 epoch...
17:45:03.981   Training iter 50, batch loss 1.5264, batch acc 0.8866
17:45:04.610   Training iter 100, batch loss 1.5247, batch acc 0.8962
17:45:05.232   Training iter 150, batch loss 1.5297, batch acc 0.8960
17:45:05.840   Training iter 200, batch loss 1.5415, batch acc 0.8756
17:45:06.438   Training iter 250, batch loss 1.5535, batch acc 0.8518
17:45:07.005   Training iter 300, batch loss 1.5312, batch acc 0.8904
17:45:07.582   Training iter 350, batch loss 1.5332, batch acc 0.9028
17:45:08.158   Training iter 400, batch loss 1.5346, batch acc 0.8884
17:45:08.734   Training iter 450, batch loss 1.5261, batch acc 0.9148
17:45:09.348   Training iter 500, batch loss 1.5223, batch acc 0.9122
17:45:09.957   Training iter 550, batch loss 1.5290, batch acc 0.9008
17:45:10.584   Training iter 600, batch loss 1.5313, batch acc 0.8992
17:45:10.586 Training @ 215 epoch...
17:45:11.142   Training iter 50, batch loss 1.5431, batch acc 0.8690
17:45:11.683   Training iter 100, batch loss 1.5314, batch acc 0.9030
17:45:12.240   Training iter 150, batch loss 1.5322, batch acc 0.8898
17:45:12.792   Training iter 200, batch loss 1.5366, batch acc 0.8908
17:45:13.328   Training iter 250, batch loss 1.5300, batch acc 0.9022
17:45:13.855   Training iter 300, batch loss 1.5416, batch acc 0.8982
17:45:14.405   Training iter 350, batch loss 1.5498, batch acc 0.8826
17:45:14.933   Training iter 400, batch loss 1.5376, batch acc 0.8950
17:45:15.456   Training iter 450, batch loss 1.5229, batch acc 0.9056
17:45:15.986   Training iter 500, batch loss 1.5290, batch acc 0.8940
17:45:16.521   Training iter 550, batch loss 1.5400, batch acc 0.8766
17:45:17.062   Training iter 600, batch loss 1.5302, batch acc 0.8926
17:45:17.064 Testing @ 215 epoch...
17:45:17.112     Testing, total mean loss 1.52596, total acc 0.90660
17:45:17.112 Training @ 216 epoch...
17:45:17.651   Training iter 50, batch loss 1.5194, batch acc 0.9196
17:45:18.184   Training iter 100, batch loss 1.5206, batch acc 0.9220
17:45:18.685   Training iter 150, batch loss 1.5186, batch acc 0.9246
17:45:19.212   Training iter 200, batch loss 1.5221, batch acc 0.9192
17:45:19.730   Training iter 250, batch loss 1.5288, batch acc 0.9064
17:45:20.317   Training iter 300, batch loss 1.5313, batch acc 0.9006
17:45:20.903   Training iter 350, batch loss 1.5304, batch acc 0.8916
17:45:21.403   Training iter 400, batch loss 1.5296, batch acc 0.8920
17:45:21.910   Training iter 450, batch loss 1.5297, batch acc 0.9006
17:45:22.408   Training iter 500, batch loss 1.5267, batch acc 0.8930
17:45:22.913   Training iter 550, batch loss 1.5215, batch acc 0.9092
17:45:23.412   Training iter 600, batch loss 1.5290, batch acc 0.8932
17:45:23.413 Training @ 217 epoch...
17:45:23.923   Training iter 50, batch loss 1.5299, batch acc 0.8864
17:45:24.478   Training iter 100, batch loss 1.5374, batch acc 0.8814
17:45:25.042   Training iter 150, batch loss 1.5364, batch acc 0.8954
17:45:25.602   Training iter 200, batch loss 1.5270, batch acc 0.9116
17:45:26.163   Training iter 250, batch loss 1.5294, batch acc 0.9118
17:45:26.715   Training iter 300, batch loss 1.5310, batch acc 0.9144
17:45:27.286   Training iter 350, batch loss 1.5371, batch acc 0.9046
17:45:27.872   Training iter 400, batch loss 1.5243, batch acc 0.9048
17:45:28.417   Training iter 450, batch loss 1.5308, batch acc 0.8932
17:45:28.962   Training iter 500, batch loss 1.5349, batch acc 0.8934
17:45:29.523   Training iter 550, batch loss 1.5345, batch acc 0.8854
17:45:30.074   Training iter 600, batch loss 1.5296, batch acc 0.9014
17:45:30.076 Training @ 218 epoch...
17:45:30.619   Training iter 50, batch loss 1.5239, batch acc 0.9084
17:45:31.130   Training iter 100, batch loss 1.5342, batch acc 0.8876
17:45:31.626   Training iter 150, batch loss 1.5377, batch acc 0.8876
17:45:32.206   Training iter 200, batch loss 1.5338, batch acc 0.8964
17:45:32.725   Training iter 250, batch loss 1.5291, batch acc 0.9126
17:45:33.256   Training iter 300, batch loss 1.5291, batch acc 0.9028
17:45:33.772   Training iter 350, batch loss 1.5431, batch acc 0.8808
17:45:34.324   Training iter 400, batch loss 1.5394, batch acc 0.8768
17:45:34.890   Training iter 450, batch loss 1.5381, batch acc 0.8750
17:45:35.448   Training iter 500, batch loss 1.5200, batch acc 0.9090
17:45:36.007   Training iter 550, batch loss 1.5266, batch acc 0.8808
17:45:36.587   Training iter 600, batch loss 1.5188, batch acc 0.9124
17:45:36.589 Training @ 219 epoch...
17:45:37.161   Training iter 50, batch loss 1.5285, batch acc 0.9012
17:45:37.710   Training iter 100, batch loss 1.5556, batch acc 0.8536
17:45:38.245   Training iter 150, batch loss 1.5372, batch acc 0.8966
17:45:38.771   Training iter 200, batch loss 1.5315, batch acc 0.9018
17:45:39.312   Training iter 250, batch loss 1.5280, batch acc 0.8986
17:45:39.843   Training iter 300, batch loss 1.5171, batch acc 0.9160
17:45:40.398   Training iter 350, batch loss 1.5259, batch acc 0.9078
17:45:40.936   Training iter 400, batch loss 1.5339, batch acc 0.9098
17:45:41.477   Training iter 450, batch loss 1.5253, batch acc 0.9078
17:45:42.020   Training iter 500, batch loss 1.5214, batch acc 0.9124
17:45:42.578   Training iter 550, batch loss 1.5240, batch acc 0.9040
17:45:43.130   Training iter 600, batch loss 1.5285, batch acc 0.9064
17:45:43.132 Training @ 220 epoch...
17:45:43.682   Training iter 50, batch loss 1.5341, batch acc 0.8828
17:45:44.224   Training iter 100, batch loss 1.5358, batch acc 0.9020
17:45:44.736   Training iter 150, batch loss 1.5215, batch acc 0.9206
17:45:45.228   Training iter 200, batch loss 1.5479, batch acc 0.8870
17:45:45.716   Training iter 250, batch loss 1.5414, batch acc 0.9024
17:45:46.302   Training iter 300, batch loss 1.5370, batch acc 0.8906
17:45:46.866   Training iter 350, batch loss 1.5318, batch acc 0.8932
17:45:47.430   Training iter 400, batch loss 1.5254, batch acc 0.8984
17:45:47.942   Training iter 450, batch loss 1.5246, batch acc 0.9144
17:45:48.462   Training iter 500, batch loss 1.5330, batch acc 0.8948
17:45:48.976   Training iter 550, batch loss 1.5232, batch acc 0.9166
17:45:49.494   Training iter 600, batch loss 1.5214, batch acc 0.9232
17:45:49.496 Testing @ 220 epoch...
17:45:49.545     Testing, total mean loss 1.52305, total acc 0.92180
17:45:49.545 Training @ 221 epoch...
17:45:50.109   Training iter 50, batch loss 1.5328, batch acc 0.9166
17:45:50.685   Training iter 100, batch loss 1.5339, batch acc 0.9046
17:45:51.281   Training iter 150, batch loss 1.5208, batch acc 0.9112
17:45:51.857   Training iter 200, batch loss 1.5235, batch acc 0.9144
17:45:52.428   Training iter 250, batch loss 1.5222, batch acc 0.9202
17:45:53.003   Training iter 300, batch loss 1.5182, batch acc 0.9292
17:45:53.570   Training iter 350, batch loss 1.5271, batch acc 0.9138
17:45:54.125   Training iter 400, batch loss 1.5295, batch acc 0.9014
17:45:54.688   Training iter 450, batch loss 1.5425, batch acc 0.8726
17:45:55.250   Training iter 500, batch loss 1.5355, batch acc 0.8832
17:45:55.896   Training iter 550, batch loss 1.5313, batch acc 0.8878
17:45:56.456   Training iter 600, batch loss 1.5371, batch acc 0.8838
17:45:56.457 Training @ 222 epoch...
17:45:56.999   Training iter 50, batch loss 1.5338, batch acc 0.8750
17:45:57.569   Training iter 100, batch loss 1.5224, batch acc 0.9034
17:45:58.151   Training iter 150, batch loss 1.5247, batch acc 0.9102
17:45:58.723   Training iter 200, batch loss 1.5525, batch acc 0.8758
17:45:59.292   Training iter 250, batch loss 1.5432, batch acc 0.8826
17:45:59.862   Training iter 300, batch loss 1.5235, batch acc 0.9170
17:46:00.458   Training iter 350, batch loss 1.5305, batch acc 0.8882
17:46:01.039   Training iter 400, batch loss 1.5444, batch acc 0.8930
17:46:01.753   Training iter 450, batch loss 1.5239, batch acc 0.9008
17:46:02.411   Training iter 500, batch loss 1.5223, batch acc 0.9048
17:46:03.004   Training iter 550, batch loss 1.5494, batch acc 0.8554
17:46:03.632   Training iter 600, batch loss 1.5378, batch acc 0.8782
17:46:03.633 Training @ 223 epoch...
17:46:04.242   Training iter 50, batch loss 1.5328, batch acc 0.8966
17:46:04.818   Training iter 100, batch loss 1.5243, batch acc 0.9128
17:46:05.325   Training iter 150, batch loss 1.5268, batch acc 0.9022
17:46:05.870   Training iter 200, batch loss 1.5314, batch acc 0.8922
17:46:06.436   Training iter 250, batch loss 1.5270, batch acc 0.9144
17:46:06.988   Training iter 300, batch loss 1.5376, batch acc 0.8868
17:46:07.539   Training iter 350, batch loss 1.5308, batch acc 0.8862
17:46:08.083   Training iter 400, batch loss 1.5327, batch acc 0.8840
17:46:08.625   Training iter 450, batch loss 1.5262, batch acc 0.9020
17:46:09.153   Training iter 500, batch loss 1.5246, batch acc 0.9082
17:46:09.690   Training iter 550, batch loss 1.5368, batch acc 0.9046
17:46:10.234   Training iter 600, batch loss 1.5298, batch acc 0.9014
17:46:10.236 Training @ 224 epoch...
17:46:10.789   Training iter 50, batch loss 1.5297, batch acc 0.9028
17:46:11.328   Training iter 100, batch loss 1.5228, batch acc 0.9190
17:46:11.875   Training iter 150, batch loss 1.5287, batch acc 0.8954
17:46:12.423   Training iter 200, batch loss 1.5150, batch acc 0.9174
17:46:12.955   Training iter 250, batch loss 1.5304, batch acc 0.9108
17:46:13.471   Training iter 300, batch loss 1.5270, batch acc 0.9124
17:46:13.976   Training iter 350, batch loss 1.5279, batch acc 0.9032
17:46:14.503   Training iter 400, batch loss 1.5275, batch acc 0.9046
17:46:15.012   Training iter 450, batch loss 1.5418, batch acc 0.8708
17:46:15.523   Training iter 500, batch loss 1.5327, batch acc 0.8902
17:46:16.037   Training iter 550, batch loss 1.5306, batch acc 0.8890
17:46:16.561   Training iter 600, batch loss 1.5305, batch acc 0.8920
17:46:16.563 Training @ 225 epoch...
17:46:17.070   Training iter 50, batch loss 1.5374, batch acc 0.8956
17:46:17.575   Training iter 100, batch loss 1.5403, batch acc 0.8464
17:46:18.069   Training iter 150, batch loss 1.5281, batch acc 0.8960
17:46:18.571   Training iter 200, batch loss 1.5392, batch acc 0.8906
17:46:19.081   Training iter 250, batch loss 1.5377, batch acc 0.8924
17:46:19.588   Training iter 300, batch loss 1.5360, batch acc 0.8870
17:46:20.101   Training iter 350, batch loss 1.5340, batch acc 0.8746
17:46:20.636   Training iter 400, batch loss 1.5464, batch acc 0.8804
17:46:21.162   Training iter 450, batch loss 1.5274, batch acc 0.8956
17:46:21.689   Training iter 500, batch loss 1.5431, batch acc 0.8474
17:46:22.225   Training iter 550, batch loss 1.5373, batch acc 0.8800
17:46:22.781   Training iter 600, batch loss 1.5275, batch acc 0.8996
17:46:22.783 Testing @ 225 epoch...
17:46:22.833     Testing, total mean loss 1.53176, total acc 0.87810
17:46:22.834 Training @ 226 epoch...
17:46:23.405   Training iter 50, batch loss 1.5279, batch acc 0.8962
17:46:23.971   Training iter 100, batch loss 1.5220, batch acc 0.9212
17:46:24.526   Training iter 150, batch loss 1.5342, batch acc 0.8784
17:46:25.081   Training iter 200, batch loss 1.5323, batch acc 0.8816
17:46:25.624   Training iter 250, batch loss 1.5262, batch acc 0.8882
17:46:26.174   Training iter 300, batch loss 1.5318, batch acc 0.8814
17:46:26.717   Training iter 350, batch loss 1.5314, batch acc 0.9096
17:46:27.266   Training iter 400, batch loss 1.5309, batch acc 0.9038
17:46:27.819   Training iter 450, batch loss 1.5265, batch acc 0.9054
17:46:28.357   Training iter 500, batch loss 1.5331, batch acc 0.8854
17:46:28.874   Training iter 550, batch loss 1.5490, batch acc 0.8672
17:46:29.386   Training iter 600, batch loss 1.5295, batch acc 0.9124
17:46:29.388 Training @ 227 epoch...
17:46:29.876   Training iter 50, batch loss 1.5252, batch acc 0.9136
17:46:30.367   Training iter 100, batch loss 1.5304, batch acc 0.9034
17:46:30.852   Training iter 150, batch loss 1.5274, batch acc 0.9154
17:46:31.380   Training iter 200, batch loss 1.5247, batch acc 0.9168
17:46:31.906   Training iter 250, batch loss 1.5235, batch acc 0.9104
17:46:32.448   Training iter 300, batch loss 1.5219, batch acc 0.9096
17:46:32.983   Training iter 350, batch loss 1.5199, batch acc 0.9234
17:46:33.517   Training iter 400, batch loss 1.5284, batch acc 0.9142
17:46:34.057   Training iter 450, batch loss 1.5373, batch acc 0.8890
17:46:34.600   Training iter 500, batch loss 1.5371, batch acc 0.8964
17:46:35.154   Training iter 550, batch loss 1.5364, batch acc 0.8998
17:46:35.696   Training iter 600, batch loss 1.5288, batch acc 0.9050
17:46:35.698 Training @ 228 epoch...
17:46:36.242   Training iter 50, batch loss 1.5317, batch acc 0.9006
17:46:36.784   Training iter 100, batch loss 1.5311, batch acc 0.8934
17:46:37.327   Training iter 150, batch loss 1.5401, batch acc 0.8976
17:46:37.862   Training iter 200, batch loss 1.5473, batch acc 0.8814
17:46:38.407   Training iter 250, batch loss 1.5279, batch acc 0.9116
17:46:38.958   Training iter 300, batch loss 1.5258, batch acc 0.9090
17:46:39.513   Training iter 350, batch loss 1.5343, batch acc 0.8948
17:46:40.046   Training iter 400, batch loss 1.5321, batch acc 0.8934
17:46:40.578   Training iter 450, batch loss 1.5525, batch acc 0.8852
17:46:41.117   Training iter 500, batch loss 1.5301, batch acc 0.8986
17:46:41.647   Training iter 550, batch loss 1.5274, batch acc 0.9136
17:46:42.176   Training iter 600, batch loss 1.5394, batch acc 0.8614
17:46:42.178 Training @ 229 epoch...
17:46:42.711   Training iter 50, batch loss 1.5258, batch acc 0.8994
17:46:43.249   Training iter 100, batch loss 1.5541, batch acc 0.8480
17:46:43.811   Training iter 150, batch loss 1.5470, batch acc 0.8804
17:46:44.395   Training iter 200, batch loss 1.5372, batch acc 0.9018
17:46:44.967   Training iter 250, batch loss 1.5326, batch acc 0.9060
17:46:45.485   Training iter 300, batch loss 1.5318, batch acc 0.8906
17:46:46.007   Training iter 350, batch loss 1.5450, batch acc 0.8574
17:46:46.526   Training iter 400, batch loss 1.5256, batch acc 0.8924
17:46:47.053   Training iter 450, batch loss 1.5311, batch acc 0.8944
17:46:47.579   Training iter 500, batch loss 1.5270, batch acc 0.9004
17:46:48.134   Training iter 550, batch loss 1.5458, batch acc 0.9016
17:46:48.644   Training iter 600, batch loss 1.5292, batch acc 0.9156
17:46:48.646 Training @ 230 epoch...
17:46:49.159   Training iter 50, batch loss 1.5414, batch acc 0.9066
17:46:49.648   Training iter 100, batch loss 1.5251, batch acc 0.9188
17:46:50.166   Training iter 150, batch loss 1.5377, batch acc 0.9072
17:46:50.696   Training iter 200, batch loss 1.5405, batch acc 0.8994
17:46:51.228   Training iter 250, batch loss 1.5360, batch acc 0.8624
17:46:51.763   Training iter 300, batch loss 1.5223, batch acc 0.9164
17:46:52.304   Training iter 350, batch loss 1.5427, batch acc 0.8776
17:46:52.858   Training iter 400, batch loss 1.5546, batch acc 0.8330
17:46:53.385   Training iter 450, batch loss 1.5303, batch acc 0.8954
17:46:53.919   Training iter 500, batch loss 1.5347, batch acc 0.8940
17:46:54.451   Training iter 550, batch loss 1.5250, batch acc 0.9110
17:46:54.989   Training iter 600, batch loss 1.5211, batch acc 0.9134
17:46:54.991 Testing @ 230 epoch...
17:46:55.042     Testing, total mean loss 1.53677, total acc 0.86430
17:46:55.042 Training @ 231 epoch...
17:46:55.599   Training iter 50, batch loss 1.5394, batch acc 0.8736
17:46:56.144   Training iter 100, batch loss 1.5304, batch acc 0.9020
17:46:56.690   Training iter 150, batch loss 1.5369, batch acc 0.9062
17:46:57.223   Training iter 200, batch loss 1.5253, batch acc 0.8922
17:46:57.763   Training iter 250, batch loss 1.5359, batch acc 0.8936
17:46:58.299   Training iter 300, batch loss 1.5323, batch acc 0.9024
17:46:58.832   Training iter 350, batch loss 1.5420, batch acc 0.8858
17:46:59.377   Training iter 400, batch loss 1.5371, batch acc 0.8780
17:46:59.913   Training iter 450, batch loss 1.5420, batch acc 0.8488
17:47:00.456   Training iter 500, batch loss 1.5315, batch acc 0.8824
17:47:00.993   Training iter 550, batch loss 1.5279, batch acc 0.9032
17:47:01.519   Training iter 600, batch loss 1.5228, batch acc 0.9090
17:47:01.521 Training @ 232 epoch...
17:47:02.080   Training iter 50, batch loss 1.5223, batch acc 0.9104
17:47:02.635   Training iter 100, batch loss 1.5289, batch acc 0.9078
17:47:03.207   Training iter 150, batch loss 1.5282, batch acc 0.9014
17:47:03.780   Training iter 200, batch loss 1.5399, batch acc 0.8874
17:47:04.335   Training iter 250, batch loss 1.5257, batch acc 0.9040
17:47:04.930   Training iter 300, batch loss 1.5373, batch acc 0.8990
17:47:05.503   Training iter 350, batch loss 1.5325, batch acc 0.8928
17:47:06.082   Training iter 400, batch loss 1.5294, batch acc 0.8994
17:47:06.650   Training iter 450, batch loss 1.5234, batch acc 0.9144
17:47:07.196   Training iter 500, batch loss 1.5310, batch acc 0.9018
17:47:07.724   Training iter 550, batch loss 1.5357, batch acc 0.8916
17:47:08.282   Training iter 600, batch loss 1.5255, batch acc 0.8952
17:47:08.284 Training @ 233 epoch...
17:47:08.849   Training iter 50, batch loss 1.5300, batch acc 0.9110
17:47:09.409   Training iter 100, batch loss 1.5270, batch acc 0.9022
17:47:09.969   Training iter 150, batch loss 1.5306, batch acc 0.8900
17:47:10.521   Training iter 200, batch loss 1.5364, batch acc 0.8772
17:47:11.063   Training iter 250, batch loss 1.5387, batch acc 0.8888
17:47:11.643   Training iter 300, batch loss 1.5372, batch acc 0.8840
17:47:12.218   Training iter 350, batch loss 1.5317, batch acc 0.8984
17:47:12.892   Training iter 400, batch loss 1.5219, batch acc 0.9188
17:47:13.617   Training iter 450, batch loss 1.5329, batch acc 0.9030
17:47:14.161   Training iter 500, batch loss 1.5229, batch acc 0.9158
17:47:14.702   Training iter 550, batch loss 1.5292, batch acc 0.9102
17:47:15.234   Training iter 600, batch loss 1.5460, batch acc 0.8720
17:47:15.236 Training @ 234 epoch...
17:47:15.766   Training iter 50, batch loss 1.5501, batch acc 0.8822
17:47:16.298   Training iter 100, batch loss 1.5287, batch acc 0.9160
17:47:16.817   Training iter 150, batch loss 1.5282, batch acc 0.9114
17:47:17.319   Training iter 200, batch loss 1.5210, batch acc 0.9192
17:47:17.802   Training iter 250, batch loss 1.5307, batch acc 0.9088
17:47:18.290   Training iter 300, batch loss 1.5223, batch acc 0.9188
17:47:18.794   Training iter 350, batch loss 1.5358, batch acc 0.9102
17:47:19.302   Training iter 400, batch loss 1.5380, batch acc 0.9056
17:47:19.791   Training iter 450, batch loss 1.5179, batch acc 0.9248
17:47:20.312   Training iter 500, batch loss 1.5301, batch acc 0.9088
17:47:20.827   Training iter 550, batch loss 1.5282, batch acc 0.9080
17:47:21.348   Training iter 600, batch loss 1.5270, batch acc 0.9054
17:47:21.350 Training @ 235 epoch...
17:47:21.907   Training iter 50, batch loss 1.5266, batch acc 0.9054
17:47:22.465   Training iter 100, batch loss 1.5256, batch acc 0.9188
17:47:23.025   Training iter 150, batch loss 1.5261, batch acc 0.9148
17:47:23.586   Training iter 200, batch loss 1.5345, batch acc 0.8992
17:47:24.146   Training iter 250, batch loss 1.5215, batch acc 0.9116
17:47:24.711   Training iter 300, batch loss 1.5377, batch acc 0.8892
17:47:25.283   Training iter 350, batch loss 1.5289, batch acc 0.9020
17:47:25.849   Training iter 400, batch loss 1.5284, batch acc 0.8958
17:47:26.614   Training iter 450, batch loss 1.5279, batch acc 0.9044
17:47:27.402   Training iter 500, batch loss 1.5337, batch acc 0.8868
17:47:28.201   Training iter 550, batch loss 1.5394, batch acc 0.8916
17:47:28.997   Training iter 600, batch loss 1.5243, batch acc 0.8884
17:47:28.999 Testing @ 235 epoch...
17:47:29.078     Testing, total mean loss 1.53875, total acc 0.87430
17:47:29.078 Training @ 236 epoch...
17:47:29.867   Training iter 50, batch loss 1.5455, batch acc 0.8500
17:47:30.463   Training iter 100, batch loss 1.5245, batch acc 0.9032
17:47:31.061   Training iter 150, batch loss 1.5339, batch acc 0.8880
17:47:31.657   Training iter 200, batch loss 1.5357, batch acc 0.9088
17:47:32.247   Training iter 250, batch loss 1.5265, batch acc 0.9076
17:47:32.830   Training iter 300, batch loss 1.5200, batch acc 0.9174
17:47:33.391   Training iter 350, batch loss 1.5343, batch acc 0.9068
17:47:33.946   Training iter 400, batch loss 1.5315, batch acc 0.9140
17:47:34.499   Training iter 450, batch loss 1.5248, batch acc 0.9140
17:47:35.041   Training iter 500, batch loss 1.5269, batch acc 0.9024
17:47:35.594   Training iter 550, batch loss 1.5333, batch acc 0.9002
17:47:36.146   Training iter 600, batch loss 1.5437, batch acc 0.8918
17:47:36.148 Training @ 237 epoch...
17:47:36.715   Training iter 50, batch loss 1.5290, batch acc 0.9128
17:47:37.268   Training iter 100, batch loss 1.5249, batch acc 0.9202
17:47:37.820   Training iter 150, batch loss 1.5324, batch acc 0.8900
17:47:38.396   Training iter 200, batch loss 1.5370, batch acc 0.8982
17:47:38.966   Training iter 250, batch loss 1.5427, batch acc 0.8716
17:47:39.664   Training iter 300, batch loss 1.5327, batch acc 0.8874
17:47:40.456   Training iter 350, batch loss 1.5294, batch acc 0.8932
17:47:41.222   Training iter 400, batch loss 1.5366, batch acc 0.8818
17:47:41.816   Training iter 450, batch loss 1.5364, batch acc 0.8900
17:47:42.413   Training iter 500, batch loss 1.5354, batch acc 0.8876
17:47:42.988   Training iter 550, batch loss 1.5337, batch acc 0.8916
17:47:43.562   Training iter 600, batch loss 1.5197, batch acc 0.9134
17:47:43.565 Training @ 238 epoch...
17:47:44.181   Training iter 50, batch loss 1.5392, batch acc 0.8912
17:47:44.742   Training iter 100, batch loss 1.5299, batch acc 0.8940
17:47:45.299   Training iter 150, batch loss 1.5219, batch acc 0.9188
17:47:45.843   Training iter 200, batch loss 1.5259, batch acc 0.9012
17:47:46.411   Training iter 250, batch loss 1.5267, batch acc 0.9098
17:47:46.967   Training iter 300, batch loss 1.5305, batch acc 0.9076
17:47:47.539   Training iter 350, batch loss 1.5437, batch acc 0.8754
17:47:48.093   Training iter 400, batch loss 1.5311, batch acc 0.8916
17:47:48.657   Training iter 450, batch loss 1.5387, batch acc 0.8814
17:47:49.202   Training iter 500, batch loss 1.5322, batch acc 0.8866
17:47:49.732   Training iter 550, batch loss 1.5269, batch acc 0.8878
17:47:50.253   Training iter 600, batch loss 1.5248, batch acc 0.8982
17:47:50.255 Training @ 239 epoch...
17:47:50.775   Training iter 50, batch loss 1.5183, batch acc 0.9156
17:47:51.332   Training iter 100, batch loss 1.5324, batch acc 0.8942
17:47:51.870   Training iter 150, batch loss 1.5478, batch acc 0.8680
17:47:52.397   Training iter 200, batch loss 1.5399, batch acc 0.8948
17:47:52.923   Training iter 250, batch loss 1.5527, batch acc 0.8468
17:47:53.460   Training iter 300, batch loss 1.5469, batch acc 0.8840
17:47:53.980   Training iter 350, batch loss 1.5233, batch acc 0.9064
17:47:54.508   Training iter 400, batch loss 1.5401, batch acc 0.8686
17:47:55.031   Training iter 450, batch loss 1.5443, batch acc 0.8720
17:47:55.567   Training iter 500, batch loss 1.5315, batch acc 0.8830
17:47:56.094   Training iter 550, batch loss 1.5422, batch acc 0.8930
17:47:56.634   Training iter 600, batch loss 1.5406, batch acc 0.8852
17:47:56.635 Training @ 240 epoch...
17:47:57.167   Training iter 50, batch loss 1.5315, batch acc 0.9042
17:47:57.694   Training iter 100, batch loss 1.5308, batch acc 0.9096
17:47:58.238   Training iter 150, batch loss 1.5220, batch acc 0.9156
17:47:58.788   Training iter 200, batch loss 1.5245, batch acc 0.9200
17:47:59.359   Training iter 250, batch loss 1.5469, batch acc 0.8972
17:47:59.926   Training iter 300, batch loss 1.5410, batch acc 0.8936
17:48:00.492   Training iter 350, batch loss 1.5259, batch acc 0.8944
17:48:01.074   Training iter 400, batch loss 1.5323, batch acc 0.8970
17:48:01.682   Training iter 450, batch loss 1.5334, batch acc 0.9058
17:48:02.297   Training iter 500, batch loss 1.5364, batch acc 0.8944
17:48:02.876   Training iter 550, batch loss 1.5410, batch acc 0.8850
17:48:03.455   Training iter 600, batch loss 1.5281, batch acc 0.8940
17:48:03.457 Testing @ 240 epoch...
17:48:03.506     Testing, total mean loss 1.52276, total acc 0.90790
17:48:03.507 Training @ 241 epoch...
17:48:04.091   Training iter 50, batch loss 1.5411, batch acc 0.8874
17:48:04.675   Training iter 100, batch loss 1.5387, batch acc 0.8702
17:48:05.243   Training iter 150, batch loss 1.5311, batch acc 0.8782
17:48:05.811   Training iter 200, batch loss 1.5278, batch acc 0.9036
17:48:06.374   Training iter 250, batch loss 1.5231, batch acc 0.9072
17:48:06.945   Training iter 300, batch loss 1.5303, batch acc 0.8906
17:48:07.470   Training iter 350, batch loss 1.5304, batch acc 0.8946
17:48:07.998   Training iter 400, batch loss 1.5446, batch acc 0.8710
17:48:08.541   Training iter 450, batch loss 1.5258, batch acc 0.9064
17:48:09.077   Training iter 500, batch loss 1.5378, batch acc 0.8780
17:48:09.602   Training iter 550, batch loss 1.5344, batch acc 0.8950
17:48:10.145   Training iter 600, batch loss 1.5233, batch acc 0.9096
17:48:10.146 Training @ 242 epoch...
17:48:10.696   Training iter 50, batch loss 1.5363, batch acc 0.8972
17:48:11.249   Training iter 100, batch loss 1.5535, batch acc 0.8670
17:48:11.785   Training iter 150, batch loss 1.5541, batch acc 0.8554
17:48:12.323   Training iter 200, batch loss 1.5439, batch acc 0.8530
17:48:12.860   Training iter 250, batch loss 1.5348, batch acc 0.8740
17:48:13.397   Training iter 300, batch loss 1.5295, batch acc 0.8844
17:48:13.916   Training iter 350, batch loss 1.5467, batch acc 0.8408
17:48:14.424   Training iter 400, batch loss 1.5364, batch acc 0.8734
17:48:14.941   Training iter 450, batch loss 1.5341, batch acc 0.8914
17:48:15.465   Training iter 500, batch loss 1.5206, batch acc 0.9120
17:48:16.024   Training iter 550, batch loss 1.5317, batch acc 0.8994
17:48:16.593   Training iter 600, batch loss 1.5287, batch acc 0.8846
17:48:16.594 Training @ 243 epoch...
17:48:17.152   Training iter 50, batch loss 1.5297, batch acc 0.8848
17:48:17.683   Training iter 100, batch loss 1.5387, batch acc 0.8652
17:48:18.231   Training iter 150, batch loss 1.5400, batch acc 0.8750
17:48:18.786   Training iter 200, batch loss 1.5242, batch acc 0.9182
17:48:19.352   Training iter 250, batch loss 1.5271, batch acc 0.9086
17:48:19.922   Training iter 300, batch loss 1.5244, batch acc 0.8904
17:48:20.486   Training iter 350, batch loss 1.5286, batch acc 0.8914
17:48:21.044   Training iter 400, batch loss 1.5731, batch acc 0.7872
17:48:21.570   Training iter 450, batch loss 1.5385, batch acc 0.8568
17:48:22.097   Training iter 500, batch loss 1.5254, batch acc 0.8958
17:48:22.609   Training iter 550, batch loss 1.5290, batch acc 0.8844
17:48:23.161   Training iter 600, batch loss 1.5263, batch acc 0.8550
17:48:23.163 Training @ 244 epoch...
17:48:23.675   Training iter 50, batch loss 1.5357, batch acc 0.8764
17:48:24.224   Training iter 100, batch loss 1.5265, batch acc 0.8926
17:48:24.789   Training iter 150, batch loss 1.5340, batch acc 0.8714
17:48:25.337   Training iter 200, batch loss 1.5306, batch acc 0.8856
17:48:25.878   Training iter 250, batch loss 1.5298, batch acc 0.8876
17:48:26.448   Training iter 300, batch loss 1.5312, batch acc 0.9022
17:48:26.991   Training iter 350, batch loss 1.5287, batch acc 0.8988
17:48:27.556   Training iter 400, batch loss 1.5289, batch acc 0.9022
17:48:28.134   Training iter 450, batch loss 1.5384, batch acc 0.8866
17:48:28.705   Training iter 500, batch loss 1.5242, batch acc 0.9092
17:48:29.309   Training iter 550, batch loss 1.5293, batch acc 0.9028
17:48:29.919   Training iter 600, batch loss 1.5349, batch acc 0.8806
17:48:29.920 Training @ 245 epoch...
17:48:30.535   Training iter 50, batch loss 1.5351, batch acc 0.8796
17:48:31.106   Training iter 100, batch loss 1.5368, batch acc 0.8998
17:48:31.683   Training iter 150, batch loss 1.5211, batch acc 0.9198
17:48:32.266   Training iter 200, batch loss 1.5310, batch acc 0.9010
17:48:32.842   Training iter 250, batch loss 1.5315, batch acc 0.8940
17:48:33.419   Training iter 300, batch loss 1.5284, batch acc 0.8990
17:48:33.988   Training iter 350, batch loss 1.5273, batch acc 0.9134
17:48:34.567   Training iter 400, batch loss 1.5345, batch acc 0.8926
17:48:35.147   Training iter 450, batch loss 1.5214, batch acc 0.9084
17:48:35.728   Training iter 500, batch loss 1.5371, batch acc 0.8974
17:48:36.303   Training iter 550, batch loss 1.5262, batch acc 0.9124
17:48:36.859   Training iter 600, batch loss 1.5409, batch acc 0.8882
17:48:36.861 Testing @ 245 epoch...
17:48:36.910     Testing, total mean loss 1.52564, total acc 0.90210
17:48:36.910 Training @ 246 epoch...
17:48:37.461   Training iter 50, batch loss 1.5218, batch acc 0.9076
17:48:37.969   Training iter 100, batch loss 1.5378, batch acc 0.8972
17:48:38.442   Training iter 150, batch loss 1.5284, batch acc 0.9066
17:48:38.913   Training iter 200, batch loss 1.5316, batch acc 0.8994
17:48:39.406   Training iter 250, batch loss 1.5284, batch acc 0.9038
17:48:39.887   Training iter 300, batch loss 1.5336, batch acc 0.8954
17:48:40.382   Training iter 350, batch loss 1.5328, batch acc 0.8824
17:48:40.881   Training iter 400, batch loss 1.5291, batch acc 0.8972
17:48:41.397   Training iter 450, batch loss 1.5400, batch acc 0.8894
17:48:41.910   Training iter 500, batch loss 1.5292, batch acc 0.9134
17:48:42.424   Training iter 550, batch loss 1.5431, batch acc 0.8734
17:48:42.933   Training iter 600, batch loss 1.5444, batch acc 0.8710
17:48:42.935 Training @ 247 epoch...
17:48:43.447   Training iter 50, batch loss 1.5316, batch acc 0.8940
17:48:43.950   Training iter 100, batch loss 1.5296, batch acc 0.9058
17:48:44.488   Training iter 150, batch loss 1.5293, batch acc 0.9174
17:48:44.982   Training iter 200, batch loss 1.5263, batch acc 0.9136
17:48:45.452   Training iter 250, batch loss 1.5291, batch acc 0.9152
17:48:45.923   Training iter 300, batch loss 1.5240, batch acc 0.9082
17:48:46.450   Training iter 350, batch loss 1.5398, batch acc 0.8866
17:48:46.947   Training iter 400, batch loss 1.5406, batch acc 0.8704
17:48:47.449   Training iter 450, batch loss 1.5356, batch acc 0.8842
17:48:47.987   Training iter 500, batch loss 1.5338, batch acc 0.8732
17:48:48.534   Training iter 550, batch loss 1.5255, batch acc 0.8992
17:48:49.088   Training iter 600, batch loss 1.5306, batch acc 0.8936
17:48:49.089 Training @ 248 epoch...
17:48:49.650   Training iter 50, batch loss 1.5191, batch acc 0.9106
17:48:50.195   Training iter 100, batch loss 1.5198, batch acc 0.9252
17:48:50.706   Training iter 150, batch loss 1.5258, batch acc 0.9154
17:48:51.237   Training iter 200, batch loss 1.5320, batch acc 0.9062
17:48:51.770   Training iter 250, batch loss 1.5401, batch acc 0.8916
17:48:52.310   Training iter 300, batch loss 1.5357, batch acc 0.8850
17:48:52.835   Training iter 350, batch loss 1.5386, batch acc 0.9056
17:48:53.364   Training iter 400, batch loss 1.5397, batch acc 0.8888
17:48:53.849   Training iter 450, batch loss 1.5306, batch acc 0.8836
17:48:54.336   Training iter 500, batch loss 1.5391, batch acc 0.8902
17:48:54.825   Training iter 550, batch loss 1.5261, batch acc 0.9084
17:48:55.315   Training iter 600, batch loss 1.5423, batch acc 0.8574
17:48:55.317 Training @ 249 epoch...
17:48:55.802   Training iter 50, batch loss 1.5281, batch acc 0.8948
17:48:56.285   Training iter 100, batch loss 1.5251, batch acc 0.8952
17:48:56.766   Training iter 150, batch loss 1.5420, batch acc 0.8906
17:48:57.271   Training iter 200, batch loss 1.5362, batch acc 0.8994
17:48:57.764   Training iter 250, batch loss 1.5315, batch acc 0.8882
17:48:58.253   Training iter 300, batch loss 1.5281, batch acc 0.8910
17:48:58.725   Training iter 350, batch loss 1.5319, batch acc 0.8820
17:48:59.221   Training iter 400, batch loss 1.5207, batch acc 0.9120
17:48:59.715   Training iter 450, batch loss 1.5240, batch acc 0.9054
17:49:00.216   Training iter 500, batch loss 1.5242, batch acc 0.9096
17:49:00.718   Training iter 550, batch loss 1.5274, batch acc 0.9140
17:49:01.219   Training iter 600, batch loss 1.5339, batch acc 0.9122
17:49:01.220 Training @ 250 epoch...
17:49:01.739   Training iter 50, batch loss 1.5294, batch acc 0.9054
17:49:02.286   Training iter 100, batch loss 1.5232, batch acc 0.9140
17:49:02.828   Training iter 150, batch loss 1.5290, batch acc 0.9154
17:49:03.370   Training iter 200, batch loss 1.5300, batch acc 0.9020
17:49:03.940   Training iter 250, batch loss 1.5379, batch acc 0.8806
17:49:04.533   Training iter 300, batch loss 1.5493, batch acc 0.8882
17:49:05.149   Training iter 350, batch loss 1.5319, batch acc 0.8854
17:49:05.736   Training iter 400, batch loss 1.5239, batch acc 0.8862
17:49:06.339   Training iter 450, batch loss 1.5237, batch acc 0.9018
17:49:06.947   Training iter 500, batch loss 1.5241, batch acc 0.9136
17:49:07.533   Training iter 550, batch loss 1.5421, batch acc 0.8786
17:49:08.113   Training iter 600, batch loss 1.5293, batch acc 0.9062
17:49:08.115 Testing @ 250 epoch...
17:49:08.164     Testing, total mean loss 1.52595, total acc 0.90980
17:49:08.164 Training @ 251 epoch...
17:49:08.760   Training iter 50, batch loss 1.5275, batch acc 0.9106
17:49:09.317   Training iter 100, batch loss 1.5285, batch acc 0.9066
17:49:09.874   Training iter 150, batch loss 1.5329, batch acc 0.8846
17:49:10.430   Training iter 200, batch loss 1.5300, batch acc 0.8948
17:49:10.984   Training iter 250, batch loss 1.5194, batch acc 0.9192
17:49:11.546   Training iter 300, batch loss 1.5272, batch acc 0.9004
17:49:12.114   Training iter 350, batch loss 1.5327, batch acc 0.9010
17:49:12.679   Training iter 400, batch loss 1.5272, batch acc 0.9080
17:49:13.226   Training iter 450, batch loss 1.5387, batch acc 0.9020
17:49:13.780   Training iter 500, batch loss 1.5279, batch acc 0.8986
17:49:14.353   Training iter 550, batch loss 1.5369, batch acc 0.8960
17:49:14.942   Training iter 600, batch loss 1.5517, batch acc 0.8638
17:49:14.944 Training @ 252 epoch...
17:49:15.531   Training iter 50, batch loss 1.5279, batch acc 0.8968
17:49:16.101   Training iter 100, batch loss 1.5202, batch acc 0.9074
17:49:16.702   Training iter 150, batch loss 1.5235, batch acc 0.9038
17:49:17.277   Training iter 200, batch loss 1.5265, batch acc 0.9016
17:49:17.838   Training iter 250, batch loss 1.5221, batch acc 0.9172
17:49:18.408   Training iter 300, batch loss 1.5221, batch acc 0.9104
17:49:18.977   Training iter 350, batch loss 1.5302, batch acc 0.9086
17:49:19.568   Training iter 400, batch loss 1.5307, batch acc 0.9052
17:49:20.166   Training iter 450, batch loss 1.5321, batch acc 0.9104
17:49:20.752   Training iter 500, batch loss 1.5306, batch acc 0.8944
17:49:21.331   Training iter 550, batch loss 1.5339, batch acc 0.8862
17:49:21.910   Training iter 600, batch loss 1.5405, batch acc 0.8770
17:49:21.911 Training @ 253 epoch...
17:49:22.519   Training iter 50, batch loss 1.5214, batch acc 0.9128
17:49:23.115   Training iter 100, batch loss 1.5395, batch acc 0.8958
17:49:23.695   Training iter 150, batch loss 1.5283, batch acc 0.9054
17:49:24.293   Training iter 200, batch loss 1.5268, batch acc 0.9154
17:49:24.904   Training iter 250, batch loss 1.5254, batch acc 0.9160
17:49:25.529   Training iter 300, batch loss 1.5344, batch acc 0.9018
17:49:26.159   Training iter 350, batch loss 1.5510, batch acc 0.8778
17:49:26.744   Training iter 400, batch loss 1.5427, batch acc 0.8800
17:49:27.315   Training iter 450, batch loss 1.5380, batch acc 0.8802
17:49:27.858   Training iter 500, batch loss 1.5336, batch acc 0.8968
17:49:28.411   Training iter 550, batch loss 1.5161, batch acc 0.9148
17:49:28.968   Training iter 600, batch loss 1.5297, batch acc 0.8816
17:49:28.970 Training @ 254 epoch...
17:49:29.542   Training iter 50, batch loss 1.5388, batch acc 0.8616
17:49:30.101   Training iter 100, batch loss 1.5615, batch acc 0.8500
17:49:30.655   Training iter 150, batch loss 1.5336, batch acc 0.8952
17:49:31.208   Training iter 200, batch loss 1.5293, batch acc 0.9172
17:49:31.767   Training iter 250, batch loss 1.5272, batch acc 0.8924
17:49:32.323   Training iter 300, batch loss 1.5460, batch acc 0.8932
17:49:32.867   Training iter 350, batch loss 1.5271, batch acc 0.8974
17:49:33.407   Training iter 400, batch loss 1.5361, batch acc 0.9114
17:49:33.966   Training iter 450, batch loss 1.5306, batch acc 0.9034
17:49:34.536   Training iter 500, batch loss 1.5315, batch acc 0.8808
17:49:35.092   Training iter 550, batch loss 1.5362, batch acc 0.8878
17:49:35.642   Training iter 600, batch loss 1.5292, batch acc 0.9014
17:49:35.644 Training @ 255 epoch...
17:49:36.199   Training iter 50, batch loss 1.5301, batch acc 0.8950
17:49:36.764   Training iter 100, batch loss 1.5339, batch acc 0.9032
17:49:37.341   Training iter 150, batch loss 1.5202, batch acc 0.9168
17:49:37.917   Training iter 200, batch loss 1.5275, batch acc 0.9052
17:49:38.495   Training iter 250, batch loss 1.5209, batch acc 0.9110
17:49:39.075   Training iter 300, batch loss 1.5294, batch acc 0.9080
17:49:39.653   Training iter 350, batch loss 1.5477, batch acc 0.8320
17:49:40.218   Training iter 400, batch loss 1.5351, batch acc 0.8702
17:49:40.788   Training iter 450, batch loss 1.5329, batch acc 0.8840
17:49:41.380   Training iter 500, batch loss 1.5274, batch acc 0.8940
17:49:41.962   Training iter 550, batch loss 1.5471, batch acc 0.8460
17:49:42.527   Training iter 600, batch loss 1.5284, batch acc 0.9010
17:49:42.529 Testing @ 255 epoch...
17:49:42.578     Testing, total mean loss 1.54373, total acc 0.89570
17:49:42.578 Training @ 256 epoch...
17:49:43.134   Training iter 50, batch loss 1.5337, batch acc 0.8946
17:49:43.697   Training iter 100, batch loss 1.5446, batch acc 0.8932
17:49:44.254   Training iter 150, batch loss 1.5213, batch acc 0.9020
17:49:44.800   Training iter 200, batch loss 1.5428, batch acc 0.8700
17:49:45.375   Training iter 250, batch loss 1.5335, batch acc 0.8776
17:49:45.955   Training iter 300, batch loss 1.5261, batch acc 0.8844
17:49:46.535   Training iter 350, batch loss 1.5390, batch acc 0.8604
17:49:47.091   Training iter 400, batch loss 1.5296, batch acc 0.8860
17:49:47.636   Training iter 450, batch loss 1.5292, batch acc 0.8858
17:49:48.173   Training iter 500, batch loss 1.5294, batch acc 0.9066
17:49:48.703   Training iter 550, batch loss 1.5293, batch acc 0.9012
17:49:49.246   Training iter 600, batch loss 1.5376, batch acc 0.9032
17:49:49.248 Training @ 257 epoch...
17:49:49.777   Training iter 50, batch loss 1.5268, batch acc 0.9072
17:49:50.295   Training iter 100, batch loss 1.5240, batch acc 0.9092
17:49:50.824   Training iter 150, batch loss 1.5304, batch acc 0.8930
17:49:51.360   Training iter 200, batch loss 1.5339, batch acc 0.8926
17:49:51.878   Training iter 250, batch loss 1.5303, batch acc 0.8976
17:49:52.439   Training iter 300, batch loss 1.5300, batch acc 0.9008
17:49:53.009   Training iter 350, batch loss 1.5217, batch acc 0.9002
17:49:53.569   Training iter 400, batch loss 1.5226, batch acc 0.9180
17:49:54.136   Training iter 450, batch loss 1.5287, batch acc 0.9182
17:49:54.694   Training iter 500, batch loss 1.5429, batch acc 0.8890
17:49:55.265   Training iter 550, batch loss 1.5404, batch acc 0.8856
17:49:55.828   Training iter 600, batch loss 1.5278, batch acc 0.9082
17:49:55.830 Training @ 258 epoch...
17:49:56.421   Training iter 50, batch loss 1.5211, batch acc 0.9184
17:49:56.985   Training iter 100, batch loss 1.5182, batch acc 0.9202
17:49:57.544   Training iter 150, batch loss 1.5383, batch acc 0.8796
17:49:58.114   Training iter 200, batch loss 1.5344, batch acc 0.8982
17:49:58.662   Training iter 250, batch loss 1.5228, batch acc 0.9124
17:49:59.213   Training iter 300, batch loss 1.5365, batch acc 0.9024
17:49:59.727   Training iter 350, batch loss 1.5444, batch acc 0.8618
17:50:00.266   Training iter 400, batch loss 1.5317, batch acc 0.8948
17:50:00.796   Training iter 450, batch loss 1.5296, batch acc 0.8958
17:50:01.355   Training iter 500, batch loss 1.5392, batch acc 0.8736
17:50:01.940   Training iter 550, batch loss 1.5146, batch acc 0.9198
17:50:02.521   Training iter 600, batch loss 1.5211, batch acc 0.9072
17:50:02.523 Training @ 259 epoch...
17:50:03.080   Training iter 50, batch loss 1.5449, batch acc 0.8598
17:50:03.654   Training iter 100, batch loss 1.5249, batch acc 0.9204
17:50:04.206   Training iter 150, batch loss 1.5259, batch acc 0.9170
17:50:04.749   Training iter 200, batch loss 1.5217, batch acc 0.9112
17:50:05.275   Training iter 250, batch loss 1.5282, batch acc 0.8968
17:50:05.814   Training iter 300, batch loss 1.5259, batch acc 0.9080
17:50:06.347   Training iter 350, batch loss 1.5211, batch acc 0.9132
17:50:06.850   Training iter 400, batch loss 1.5257, batch acc 0.9162
17:50:07.384   Training iter 450, batch loss 1.5247, batch acc 0.9114
17:50:07.933   Training iter 500, batch loss 1.5206, batch acc 0.9192
17:50:08.499   Training iter 550, batch loss 1.5269, batch acc 0.9126
17:50:09.077   Training iter 600, batch loss 1.5334, batch acc 0.8974
17:50:09.079 Training @ 260 epoch...
17:50:09.648   Training iter 50, batch loss 1.5264, batch acc 0.9044
17:50:10.210   Training iter 100, batch loss 1.5239, batch acc 0.9112
17:50:10.774   Training iter 150, batch loss 1.5176, batch acc 0.9296
17:50:11.355   Training iter 200, batch loss 1.5332, batch acc 0.9188
17:50:11.918   Training iter 250, batch loss 1.5343, batch acc 0.9048
17:50:12.478   Training iter 300, batch loss 1.5310, batch acc 0.8954
17:50:13.052   Training iter 350, batch loss 1.5211, batch acc 0.9174
17:50:13.623   Training iter 400, batch loss 1.5343, batch acc 0.9102
17:50:14.200   Training iter 450, batch loss 1.5216, batch acc 0.9122
17:50:14.757   Training iter 500, batch loss 1.5216, batch acc 0.9122
17:50:15.289   Training iter 550, batch loss 1.5509, batch acc 0.8630
17:50:15.811   Training iter 600, batch loss 1.5377, batch acc 0.9006
17:50:15.813 Testing @ 260 epoch...
17:50:15.862     Testing, total mean loss 1.52824, total acc 0.89740
17:50:15.862 Training @ 261 epoch...
17:50:16.418   Training iter 50, batch loss 1.5415, batch acc 0.8468
17:50:16.963   Training iter 100, batch loss 1.5258, batch acc 0.8970
17:50:17.519   Training iter 150, batch loss 1.5237, batch acc 0.9062
17:50:18.066   Training iter 200, batch loss 1.5349, batch acc 0.9084
17:50:18.604   Training iter 250, batch loss 1.5334, batch acc 0.9120
17:50:19.166   Training iter 300, batch loss 1.5337, batch acc 0.8860
17:50:19.718   Training iter 350, batch loss 1.5350, batch acc 0.8758
17:50:20.295   Training iter 400, batch loss 1.5313, batch acc 0.8690
17:50:20.841   Training iter 450, batch loss 1.5474, batch acc 0.8688
17:50:21.391   Training iter 500, batch loss 1.5419, batch acc 0.8648
17:50:21.939   Training iter 550, batch loss 1.5206, batch acc 0.9006
17:50:22.491   Training iter 600, batch loss 1.5340, batch acc 0.8970
17:50:22.493 Training @ 262 epoch...
17:50:23.054   Training iter 50, batch loss 1.5410, batch acc 0.8996
17:50:23.615   Training iter 100, batch loss 1.5272, batch acc 0.8986
17:50:24.182   Training iter 150, batch loss 1.5384, batch acc 0.8842
17:50:24.763   Training iter 200, batch loss 1.5340, batch acc 0.8980
17:50:25.328   Training iter 250, batch loss 1.5342, batch acc 0.8860
17:50:25.897   Training iter 300, batch loss 1.5246, batch acc 0.9026
17:50:26.463   Training iter 350, batch loss 1.5234, batch acc 0.9082
17:50:27.051   Training iter 400, batch loss 1.5224, batch acc 0.9126
17:50:27.618   Training iter 450, batch loss 1.5279, batch acc 0.9062
17:50:28.165   Training iter 500, batch loss 1.5263, batch acc 0.9058
17:50:28.684   Training iter 550, batch loss 1.5261, batch acc 0.9030
17:50:29.226   Training iter 600, batch loss 1.5201, batch acc 0.9200
17:50:29.228 Training @ 263 epoch...
17:50:29.785   Training iter 50, batch loss 1.5264, batch acc 0.9110
17:50:30.336   Training iter 100, batch loss 1.5436, batch acc 0.8852
17:50:30.844   Training iter 150, batch loss 1.5326, batch acc 0.9018
17:50:31.372   Training iter 200, batch loss 1.5329, batch acc 0.9134
17:50:31.894   Training iter 250, batch loss 1.5307, batch acc 0.9104
17:50:32.421   Training iter 300, batch loss 1.5312, batch acc 0.9032
17:50:32.931   Training iter 350, batch loss 1.5245, batch acc 0.9156
17:50:33.448   Training iter 400, batch loss 1.5368, batch acc 0.8992
17:50:33.965   Training iter 450, batch loss 1.5354, batch acc 0.8976
17:50:34.471   Training iter 500, batch loss 1.5292, batch acc 0.8938
17:50:34.978   Training iter 550, batch loss 1.5324, batch acc 0.8906
17:50:35.479   Training iter 600, batch loss 1.5312, batch acc 0.8862
17:50:35.481 Training @ 264 epoch...
17:50:35.990   Training iter 50, batch loss 1.5415, batch acc 0.8960
17:50:36.505   Training iter 100, batch loss 1.5282, batch acc 0.9142
17:50:36.991   Training iter 150, batch loss 1.5253, batch acc 0.9086
17:50:37.505   Training iter 200, batch loss 1.5267, batch acc 0.9046
17:50:38.031   Training iter 250, batch loss 1.5313, batch acc 0.8912
17:50:38.571   Training iter 300, batch loss 1.5268, batch acc 0.9082
17:50:39.096   Training iter 350, batch loss 1.5189, batch acc 0.9162
17:50:39.637   Training iter 400, batch loss 1.5297, batch acc 0.9056
17:50:40.175   Training iter 450, batch loss 1.5347, batch acc 0.8846
17:50:40.712   Training iter 500, batch loss 1.5263, batch acc 0.8984
17:50:41.289   Training iter 550, batch loss 1.5290, batch acc 0.8912
17:50:41.855   Training iter 600, batch loss 1.5240, batch acc 0.9108
17:50:41.857 Training @ 265 epoch...
17:50:42.429   Training iter 50, batch loss 1.5218, batch acc 0.9090
17:50:42.978   Training iter 100, batch loss 1.5219, batch acc 0.9176
17:50:43.530   Training iter 150, batch loss 1.5406, batch acc 0.9038
17:50:44.095   Training iter 200, batch loss 1.5267, batch acc 0.9082
17:50:44.661   Training iter 250, batch loss 1.5172, batch acc 0.9224
17:50:45.188   Training iter 300, batch loss 1.5192, batch acc 0.9124
17:50:45.717   Training iter 350, batch loss 1.5323, batch acc 0.9112
17:50:46.302   Training iter 400, batch loss 1.5246, batch acc 0.9172
17:50:46.883   Training iter 450, batch loss 1.5341, batch acc 0.9068
17:50:47.498   Training iter 500, batch loss 1.5396, batch acc 0.8748
17:50:48.102   Training iter 550, batch loss 1.5257, batch acc 0.9120
17:50:48.693   Training iter 600, batch loss 1.5420, batch acc 0.8710
17:50:48.695 Testing @ 265 epoch...
17:50:48.747     Testing, total mean loss 1.52702, total acc 0.89770
17:50:48.747 Training @ 266 epoch...
17:50:49.282   Training iter 50, batch loss 1.5262, batch acc 0.8988
17:50:49.797   Training iter 100, batch loss 1.5322, batch acc 0.8964
17:50:50.332   Training iter 150, batch loss 1.5219, batch acc 0.9160
17:50:50.871   Training iter 200, batch loss 1.5458, batch acc 0.8478
17:50:51.409   Training iter 250, batch loss 1.5257, batch acc 0.9034
17:50:51.936   Training iter 300, batch loss 1.5305, batch acc 0.8812
17:50:52.474   Training iter 350, batch loss 1.5302, batch acc 0.8912
17:50:52.998   Training iter 400, batch loss 1.5490, batch acc 0.8788
17:50:53.497   Training iter 450, batch loss 1.5342, batch acc 0.8966
17:50:54.005   Training iter 500, batch loss 1.5364, batch acc 0.8768
17:50:54.515   Training iter 550, batch loss 1.5408, batch acc 0.8816
17:50:55.029   Training iter 600, batch loss 1.5300, batch acc 0.9110
17:50:55.031 Training @ 267 epoch...
17:50:55.628   Training iter 50, batch loss 1.5537, batch acc 0.8788
17:50:56.227   Training iter 100, batch loss 1.5367, batch acc 0.8744
17:50:56.864   Training iter 150, batch loss 1.5308, batch acc 0.8606
17:50:57.511   Training iter 200, batch loss 1.5282, batch acc 0.8924
17:50:58.124   Training iter 250, batch loss 1.5302, batch acc 0.8748
17:50:58.754   Training iter 300, batch loss 1.5345, batch acc 0.8766
17:50:59.381   Training iter 350, batch loss 1.5378, batch acc 0.8754
17:50:59.996   Training iter 400, batch loss 1.5386, batch acc 0.8794
17:51:00.605   Training iter 450, batch loss 1.5170, batch acc 0.9120
17:51:01.212   Training iter 500, batch loss 1.5234, batch acc 0.9098
17:51:01.864   Training iter 550, batch loss 1.5204, batch acc 0.9160
17:51:02.515   Training iter 600, batch loss 1.5221, batch acc 0.9130
17:51:02.517 Training @ 268 epoch...
17:51:03.136   Training iter 50, batch loss 1.5175, batch acc 0.9206
17:51:03.633   Training iter 100, batch loss 1.5378, batch acc 0.8670
17:51:04.137   Training iter 150, batch loss 1.5417, batch acc 0.8816
17:51:04.673   Training iter 200, batch loss 1.5223, batch acc 0.9174
17:51:05.220   Training iter 250, batch loss 1.5206, batch acc 0.9168
17:51:05.778   Training iter 300, batch loss 1.5172, batch acc 0.9172
17:51:06.352   Training iter 350, batch loss 1.5333, batch acc 0.9180
17:51:06.911   Training iter 400, batch loss 1.5321, batch acc 0.8952
17:51:07.468   Training iter 450, batch loss 1.5295, batch acc 0.9002
17:51:08.020   Training iter 500, batch loss 1.5289, batch acc 0.9078
17:51:08.549   Training iter 550, batch loss 1.5351, batch acc 0.9090
17:51:09.084   Training iter 600, batch loss 1.5148, batch acc 0.9180
17:51:09.086 Training @ 269 epoch...
17:51:09.629   Training iter 50, batch loss 1.5217, batch acc 0.9188
17:51:10.173   Training iter 100, batch loss 1.5451, batch acc 0.8938
17:51:10.714   Training iter 150, batch loss 1.5375, batch acc 0.9038
17:51:11.250   Training iter 200, batch loss 1.5428, batch acc 0.8830
17:51:11.778   Training iter 250, batch loss 1.5361, batch acc 0.8966
17:51:12.317   Training iter 300, batch loss 1.5374, batch acc 0.9032
17:51:12.885   Training iter 350, batch loss 1.5254, batch acc 0.9008
17:51:13.434   Training iter 400, batch loss 1.5258, batch acc 0.8884
17:51:13.987   Training iter 450, batch loss 1.5253, batch acc 0.8998
17:51:14.538   Training iter 500, batch loss 1.5350, batch acc 0.8948
17:51:15.086   Training iter 550, batch loss 1.5348, batch acc 0.9030
17:51:15.621   Training iter 600, batch loss 1.5354, batch acc 0.9010
17:51:15.623 Training @ 270 epoch...
17:51:16.167   Training iter 50, batch loss 1.5410, batch acc 0.8994
17:51:16.709   Training iter 100, batch loss 1.5386, batch acc 0.9088
17:51:17.246   Training iter 150, batch loss 1.5236, batch acc 0.9128
17:51:17.798   Training iter 200, batch loss 1.5321, batch acc 0.8840
17:51:18.332   Training iter 250, batch loss 1.5348, batch acc 0.8926
17:51:18.817   Training iter 300, batch loss 1.5335, batch acc 0.8902
17:51:19.312   Training iter 350, batch loss 1.5377, batch acc 0.8952
17:51:19.812   Training iter 400, batch loss 1.5418, batch acc 0.8934
17:51:20.319   Training iter 450, batch loss 1.5336, batch acc 0.9012
17:51:20.820   Training iter 500, batch loss 1.5256, batch acc 0.9026
17:51:21.348   Training iter 550, batch loss 1.5258, batch acc 0.9110
17:51:21.865   Training iter 600, batch loss 1.5418, batch acc 0.9012
17:51:21.867 Testing @ 270 epoch...
17:51:21.916     Testing, total mean loss 1.53748, total acc 0.90560
17:51:21.916 Training @ 271 epoch...
17:51:22.428   Training iter 50, batch loss 1.5438, batch acc 0.8946
17:51:22.923   Training iter 100, batch loss 1.5263, batch acc 0.8808
17:51:23.438   Training iter 150, batch loss 1.5309, batch acc 0.8916
17:51:23.948   Training iter 200, batch loss 1.5235, batch acc 0.9024
17:51:24.488   Training iter 250, batch loss 1.5308, batch acc 0.9008
17:51:25.013   Training iter 300, batch loss 1.5289, batch acc 0.9032
17:51:25.535   Training iter 350, batch loss 1.5258, batch acc 0.9010
17:51:26.047   Training iter 400, batch loss 1.5322, batch acc 0.8950
17:51:26.596   Training iter 450, batch loss 1.5302, batch acc 0.8966
17:51:27.129   Training iter 500, batch loss 1.5425, batch acc 0.8812
17:51:27.657   Training iter 550, batch loss 1.5307, batch acc 0.8940
17:51:28.169   Training iter 600, batch loss 1.5262, batch acc 0.9030
17:51:28.171 Training @ 272 epoch...
17:51:28.707   Training iter 50, batch loss 1.5248, batch acc 0.9084
17:51:29.265   Training iter 100, batch loss 1.5298, batch acc 0.9016
17:51:29.804   Training iter 150, batch loss 1.5339, batch acc 0.8920
17:51:30.356   Training iter 200, batch loss 1.5279, batch acc 0.8974
17:51:30.930   Training iter 250, batch loss 1.5242, batch acc 0.9212
17:51:31.514   Training iter 300, batch loss 1.5260, batch acc 0.9136
17:51:32.077   Training iter 350, batch loss 1.5197, batch acc 0.9160
17:51:32.649   Training iter 400, batch loss 1.5332, batch acc 0.8966
17:51:33.221   Training iter 450, batch loss 1.5395, batch acc 0.8924
17:51:33.763   Training iter 500, batch loss 1.5344, batch acc 0.8794
17:51:34.305   Training iter 550, batch loss 1.5292, batch acc 0.9134
17:51:34.825   Training iter 600, batch loss 1.5250, batch acc 0.9114
17:51:34.827 Training @ 273 epoch...
17:51:35.342   Training iter 50, batch loss 1.5372, batch acc 0.8756
17:51:35.859   Training iter 100, batch loss 1.5453, batch acc 0.8444
17:51:36.408   Training iter 150, batch loss 1.5364, batch acc 0.8752
17:51:36.957   Training iter 200, batch loss 1.5226, batch acc 0.9044
17:51:37.486   Training iter 250, batch loss 1.5278, batch acc 0.9098
17:51:38.016   Training iter 300, batch loss 1.5250, batch acc 0.9076
17:51:38.523   Training iter 350, batch loss 1.5475, batch acc 0.8982
17:51:39.017   Training iter 400, batch loss 1.5377, batch acc 0.8808
17:51:39.525   Training iter 450, batch loss 1.5247, batch acc 0.8958
17:51:40.030   Training iter 500, batch loss 1.5275, batch acc 0.8876
17:51:40.559   Training iter 550, batch loss 1.5199, batch acc 0.8984
17:51:41.120   Training iter 600, batch loss 1.5253, batch acc 0.8840
17:51:41.122 Training @ 274 epoch...
17:51:41.694   Training iter 50, batch loss 1.5195, batch acc 0.9112
17:51:42.281   Training iter 100, batch loss 1.5368, batch acc 0.8868
17:51:42.831   Training iter 150, batch loss 1.5243, batch acc 0.9110
17:51:43.373   Training iter 200, batch loss 1.5243, batch acc 0.9096
17:51:43.906   Training iter 250, batch loss 1.5282, batch acc 0.9076
17:51:44.458   Training iter 300, batch loss 1.5325, batch acc 0.9096
17:51:45.017   Training iter 350, batch loss 1.5490, batch acc 0.8776
17:51:45.557   Training iter 400, batch loss 1.5249, batch acc 0.9052
17:51:46.114   Training iter 450, batch loss 1.5292, batch acc 0.8972
17:51:46.667   Training iter 500, batch loss 1.5394, batch acc 0.8946
17:51:47.220   Training iter 550, batch loss 1.5318, batch acc 0.8756
17:51:47.767   Training iter 600, batch loss 1.5352, batch acc 0.8444
17:51:47.769 Training @ 275 epoch...
17:51:48.326   Training iter 50, batch loss 1.5245, batch acc 0.8938
17:51:48.878   Training iter 100, batch loss 1.5206, batch acc 0.9020
17:51:49.438   Training iter 150, batch loss 1.5292, batch acc 0.9008
17:51:49.991   Training iter 200, batch loss 1.5356, batch acc 0.8976
17:51:50.544   Training iter 250, batch loss 1.5265, batch acc 0.9118
17:51:51.100   Training iter 300, batch loss 1.5289, batch acc 0.9066
17:51:51.660   Training iter 350, batch loss 1.5378, batch acc 0.9036
17:51:52.223   Training iter 400, batch loss 1.5271, batch acc 0.9164
17:51:52.798   Training iter 450, batch loss 1.5238, batch acc 0.9170
17:51:53.363   Training iter 500, batch loss 1.5186, batch acc 0.9168
17:51:53.897   Training iter 550, batch loss 1.5290, batch acc 0.8898
17:51:54.445   Training iter 600, batch loss 1.5244, batch acc 0.8922
17:51:54.447 Testing @ 275 epoch...
17:51:54.495     Testing, total mean loss 1.56915, total acc 0.79780
17:51:54.495 Training @ 276 epoch...
17:51:55.059   Training iter 50, batch loss 1.5350, batch acc 0.8862
17:51:55.601   Training iter 100, batch loss 1.5227, batch acc 0.9128
17:51:56.150   Training iter 150, batch loss 1.5242, batch acc 0.9134
17:51:56.693   Training iter 200, batch loss 1.5382, batch acc 0.8656
17:51:57.244   Training iter 250, batch loss 1.5363, batch acc 0.8958
17:51:57.785   Training iter 300, batch loss 1.5326, batch acc 0.9030
17:51:58.335   Training iter 350, batch loss 1.5262, batch acc 0.9114
17:51:58.878   Training iter 400, batch loss 1.5268, batch acc 0.9060
17:51:59.448   Training iter 450, batch loss 1.5212, batch acc 0.9192
17:52:00.004   Training iter 500, batch loss 1.5308, batch acc 0.8948
17:52:00.562   Training iter 550, batch loss 1.5405, batch acc 0.8920
17:52:01.112   Training iter 600, batch loss 1.5279, batch acc 0.9144
17:52:01.114 Training @ 277 epoch...
17:52:01.683   Training iter 50, batch loss 1.5300, batch acc 0.9148
17:52:02.278   Training iter 100, batch loss 1.5365, batch acc 0.9020
17:52:02.854   Training iter 150, batch loss 1.5408, batch acc 0.8954
17:52:03.425   Training iter 200, batch loss 1.5369, batch acc 0.8916
17:52:04.018   Training iter 250, batch loss 1.5329, batch acc 0.9042
17:52:04.604   Training iter 300, batch loss 1.5192, batch acc 0.9178
17:52:05.173   Training iter 350, batch loss 1.5397, batch acc 0.8718
17:52:05.748   Training iter 400, batch loss 1.5350, batch acc 0.8876
17:52:06.313   Training iter 450, batch loss 1.5229, batch acc 0.9100
17:52:06.883   Training iter 500, batch loss 1.5334, batch acc 0.9074
17:52:07.482   Training iter 550, batch loss 1.5278, batch acc 0.9060
17:52:08.054   Training iter 600, batch loss 1.5387, batch acc 0.8762
17:52:08.056 Training @ 278 epoch...
17:52:08.611   Training iter 50, batch loss 1.5253, batch acc 0.8884
17:52:09.187   Training iter 100, batch loss 1.5257, batch acc 0.8900
17:52:09.735   Training iter 150, batch loss 1.5310, batch acc 0.8956
17:52:10.311   Training iter 200, batch loss 1.5308, batch acc 0.8976
17:52:10.881   Training iter 250, batch loss 1.5229, batch acc 0.9064
17:52:11.464   Training iter 300, batch loss 1.5253, batch acc 0.9130
17:52:12.026   Training iter 350, batch loss 1.5363, batch acc 0.8884
17:52:12.589   Training iter 400, batch loss 1.5351, batch acc 0.8866
17:52:13.168   Training iter 450, batch loss 1.5261, batch acc 0.9086
17:52:13.742   Training iter 500, batch loss 1.5250, batch acc 0.9072
17:52:14.335   Training iter 550, batch loss 1.5288, batch acc 0.9018
17:52:14.921   Training iter 600, batch loss 1.5221, batch acc 0.9004
17:52:14.923 Training @ 279 epoch...
17:52:15.495   Training iter 50, batch loss 1.5255, batch acc 0.9006
17:52:16.068   Training iter 100, batch loss 1.5283, batch acc 0.9194
17:52:16.679   Training iter 150, batch loss 1.5258, batch acc 0.9156
17:52:17.277   Training iter 200, batch loss 1.5225, batch acc 0.9200
17:52:17.863   Training iter 250, batch loss 1.5321, batch acc 0.9070
17:52:18.485   Training iter 300, batch loss 1.5267, batch acc 0.9062
17:52:19.083   Training iter 350, batch loss 1.5335, batch acc 0.8954
17:52:19.695   Training iter 400, batch loss 1.5394, batch acc 0.8846
17:52:20.285   Training iter 450, batch loss 1.5354, batch acc 0.8690
17:52:20.869   Training iter 500, batch loss 1.5262, batch acc 0.9046
17:52:21.463   Training iter 550, batch loss 1.5255, batch acc 0.8898
17:52:22.046   Training iter 600, batch loss 1.5361, batch acc 0.8738
17:52:22.048 Training @ 280 epoch...
17:52:22.610   Training iter 50, batch loss 1.5345, batch acc 0.8656
17:52:23.202   Training iter 100, batch loss 1.5318, batch acc 0.8878
17:52:23.786   Training iter 150, batch loss 1.5450, batch acc 0.8842
17:52:24.364   Training iter 200, batch loss 1.5332, batch acc 0.8748
17:52:24.937   Training iter 250, batch loss 1.5349, batch acc 0.8854
17:52:25.565   Training iter 300, batch loss 1.5228, batch acc 0.9150
17:52:26.146   Training iter 350, batch loss 1.5305, batch acc 0.8890
17:52:26.707   Training iter 400, batch loss 1.5347, batch acc 0.8840
17:52:27.283   Training iter 450, batch loss 1.5334, batch acc 0.8836
17:52:27.854   Training iter 500, batch loss 1.5204, batch acc 0.9090
17:52:28.404   Training iter 550, batch loss 1.5379, batch acc 0.8860
17:52:28.941   Training iter 600, batch loss 1.5303, batch acc 0.8946
17:52:28.943 Testing @ 280 epoch...
17:52:28.990     Testing, total mean loss 1.53076, total acc 0.89670
17:52:28.990 Training @ 281 epoch...
17:52:29.539   Training iter 50, batch loss 1.5395, batch acc 0.8672
17:52:30.083   Training iter 100, batch loss 1.5338, batch acc 0.8900
17:52:30.627   Training iter 150, batch loss 1.5241, batch acc 0.9124
17:52:31.369   Training iter 200, batch loss 1.5258, batch acc 0.9100
17:52:32.149   Training iter 250, batch loss 1.5373, batch acc 0.8864
17:52:32.885   Training iter 300, batch loss 1.5465, batch acc 0.8722
17:52:33.535   Training iter 350, batch loss 1.5327, batch acc 0.8772
17:52:34.155   Training iter 400, batch loss 1.5244, batch acc 0.9102
17:52:34.787   Training iter 450, batch loss 1.5338, batch acc 0.8846
17:52:35.415   Training iter 500, batch loss 1.5325, batch acc 0.9132
17:52:36.050   Training iter 550, batch loss 1.5379, batch acc 0.9050
17:52:36.648   Training iter 600, batch loss 1.5312, batch acc 0.8890
17:52:36.650 Training @ 282 epoch...
17:52:37.250   Training iter 50, batch loss 1.5401, batch acc 0.8754
17:52:37.817   Training iter 100, batch loss 1.5339, batch acc 0.8918
17:52:38.371   Training iter 150, batch loss 1.5293, batch acc 0.9038
17:52:38.890   Training iter 200, batch loss 1.5423, batch acc 0.8732
17:52:39.430   Training iter 250, batch loss 1.5242, batch acc 0.9104
17:52:39.959   Training iter 300, batch loss 1.5183, batch acc 0.9130
17:52:40.493   Training iter 350, batch loss 1.5271, batch acc 0.9038
17:52:41.024   Training iter 400, batch loss 1.5291, batch acc 0.9226
17:52:41.551   Training iter 450, batch loss 1.5465, batch acc 0.8682
17:52:42.093   Training iter 500, batch loss 1.5257, batch acc 0.8904
17:52:42.631   Training iter 550, batch loss 1.5305, batch acc 0.9036
17:52:43.144   Training iter 600, batch loss 1.5331, batch acc 0.8900
17:52:43.146 Training @ 283 epoch...
17:52:43.658   Training iter 50, batch loss 1.5214, batch acc 0.9228
17:52:44.167   Training iter 100, batch loss 1.5320, batch acc 0.9068
17:52:44.681   Training iter 150, batch loss 1.5276, batch acc 0.9202
17:52:45.195   Training iter 200, batch loss 1.5361, batch acc 0.9058
17:52:45.690   Training iter 250, batch loss 1.5219, batch acc 0.9066
17:52:46.196   Training iter 300, batch loss 1.5223, batch acc 0.9110
17:52:46.709   Training iter 350, batch loss 1.5421, batch acc 0.8850
17:52:47.220   Training iter 400, batch loss 1.5278, batch acc 0.9128
17:52:47.757   Training iter 450, batch loss 1.5287, batch acc 0.9106
17:52:48.272   Training iter 500, batch loss 1.5258, batch acc 0.9128
17:52:48.819   Training iter 550, batch loss 1.5503, batch acc 0.8902
17:52:49.373   Training iter 600, batch loss 1.5263, batch acc 0.9020
17:52:49.375 Training @ 284 epoch...
17:52:49.927   Training iter 50, batch loss 1.5476, batch acc 0.8966
17:52:50.473   Training iter 100, batch loss 1.5270, batch acc 0.8996
17:52:51.041   Training iter 150, batch loss 1.5301, batch acc 0.8952
17:52:51.631   Training iter 200, batch loss 1.5541, batch acc 0.8156
17:52:52.225   Training iter 250, batch loss 1.5374, batch acc 0.8498
17:52:52.785   Training iter 300, batch loss 1.5219, batch acc 0.8838
17:52:53.339   Training iter 350, batch loss 1.5303, batch acc 0.8856
17:52:53.883   Training iter 400, batch loss 1.5346, batch acc 0.8984
17:52:54.437   Training iter 450, batch loss 1.5303, batch acc 0.8932
17:52:54.956   Training iter 500, batch loss 1.5268, batch acc 0.8938
17:52:55.462   Training iter 550, batch loss 1.5409, batch acc 0.8852
17:52:55.976   Training iter 600, batch loss 1.5311, batch acc 0.8982
17:52:55.978 Training @ 285 epoch...
17:52:56.515   Training iter 50, batch loss 1.5264, batch acc 0.8960
17:52:57.049   Training iter 100, batch loss 1.5311, batch acc 0.9096
17:52:57.580   Training iter 150, batch loss 1.5329, batch acc 0.8978
17:52:58.121   Training iter 200, batch loss 1.5367, batch acc 0.8962
17:52:58.677   Training iter 250, batch loss 1.5402, batch acc 0.8846
17:52:59.244   Training iter 300, batch loss 1.5301, batch acc 0.8826
17:52:59.782   Training iter 350, batch loss 1.5364, batch acc 0.8884
17:53:00.310   Training iter 400, batch loss 1.5247, batch acc 0.9030
17:53:00.866   Training iter 450, batch loss 1.5287, batch acc 0.9192
17:53:01.483   Training iter 500, batch loss 1.5186, batch acc 0.9232
17:53:02.050   Training iter 550, batch loss 1.5324, batch acc 0.9170
17:53:02.590   Training iter 600, batch loss 1.5387, batch acc 0.8956
17:53:02.591 Testing @ 285 epoch...
17:53:02.640     Testing, total mean loss 1.53103, total acc 0.90150
17:53:02.640 Training @ 286 epoch...
17:53:03.211   Training iter 50, batch loss 1.5291, batch acc 0.9042
17:53:03.803   Training iter 100, batch loss 1.5287, batch acc 0.9074
17:53:04.419   Training iter 150, batch loss 1.5259, batch acc 0.9160
17:53:05.052   Training iter 200, batch loss 1.5267, batch acc 0.9174
17:53:05.690   Training iter 250, batch loss 1.5307, batch acc 0.9036
17:53:06.332   Training iter 300, batch loss 1.5258, batch acc 0.9158
17:53:06.960   Training iter 350, batch loss 1.5264, batch acc 0.9056
17:53:07.589   Training iter 400, batch loss 1.5234, batch acc 0.9054
17:53:08.178   Training iter 450, batch loss 1.5249, batch acc 0.9082
17:53:08.746   Training iter 500, batch loss 1.5309, batch acc 0.8892
17:53:09.324   Training iter 550, batch loss 1.5334, batch acc 0.9054
17:53:09.900   Training iter 600, batch loss 1.5296, batch acc 0.9062
17:53:09.901 Training @ 287 epoch...
17:53:10.498   Training iter 50, batch loss 1.5275, batch acc 0.8976
17:53:11.080   Training iter 100, batch loss 1.5244, batch acc 0.9034
17:53:11.663   Training iter 150, batch loss 1.5295, batch acc 0.8928
17:53:12.222   Training iter 200, batch loss 1.5263, batch acc 0.9046
17:53:12.766   Training iter 250, batch loss 1.5325, batch acc 0.8978
17:53:13.317   Training iter 300, batch loss 1.5368, batch acc 0.8984
17:53:13.861   Training iter 350, batch loss 1.5382, batch acc 0.8858
17:53:14.414   Training iter 400, batch loss 1.5347, batch acc 0.8900
17:53:14.952   Training iter 450, batch loss 1.5291, batch acc 0.9094
17:53:15.486   Training iter 500, batch loss 1.5417, batch acc 0.8812
17:53:16.033   Training iter 550, batch loss 1.5326, batch acc 0.8916
17:53:16.568   Training iter 600, batch loss 1.5240, batch acc 0.9124
17:53:16.570 Training @ 288 epoch...
17:53:17.121   Training iter 50, batch loss 1.5433, batch acc 0.8992
17:53:17.663   Training iter 100, batch loss 1.5290, batch acc 0.8934
17:53:18.202   Training iter 150, batch loss 1.5268, batch acc 0.9026
17:53:18.732   Training iter 200, batch loss 1.5276, batch acc 0.9120
17:53:19.264   Training iter 250, batch loss 1.5271, batch acc 0.9058
17:53:19.795   Training iter 300, batch loss 1.5329, batch acc 0.8874
17:53:20.327   Training iter 350, batch loss 1.5286, batch acc 0.8898
17:53:20.865   Training iter 400, batch loss 1.5313, batch acc 0.9010
17:53:21.399   Training iter 450, batch loss 1.5236, batch acc 0.9052
17:53:21.927   Training iter 500, batch loss 1.5309, batch acc 0.8914
17:53:22.469   Training iter 550, batch loss 1.5251, batch acc 0.9010
17:53:23.020   Training iter 600, batch loss 1.5371, batch acc 0.8776
17:53:23.022 Training @ 289 epoch...
17:53:23.565   Training iter 50, batch loss 1.5513, batch acc 0.8522
17:53:24.104   Training iter 100, batch loss 1.5372, batch acc 0.8748
17:53:24.650   Training iter 150, batch loss 1.5321, batch acc 0.8976
17:53:25.185   Training iter 200, batch loss 1.5229, batch acc 0.9018
17:53:25.704   Training iter 250, batch loss 1.5191, batch acc 0.9106
17:53:26.244   Training iter 300, batch loss 1.5211, batch acc 0.9118
17:53:26.776   Training iter 350, batch loss 1.5465, batch acc 0.8848
17:53:27.304   Training iter 400, batch loss 1.5326, batch acc 0.9016
17:53:27.847   Training iter 450, batch loss 1.5386, batch acc 0.8934
17:53:28.438   Training iter 500, batch loss 1.5361, batch acc 0.8946
17:53:28.966   Training iter 550, batch loss 1.5320, batch acc 0.8984
17:53:29.506   Training iter 600, batch loss 1.5388, batch acc 0.9044
17:53:29.508 Training @ 290 epoch...
17:53:30.052   Training iter 50, batch loss 1.5301, batch acc 0.9162
17:53:30.581   Training iter 100, batch loss 1.5436, batch acc 0.8740
17:53:31.122   Training iter 150, batch loss 1.5283, batch acc 0.8832
17:53:31.659   Training iter 200, batch loss 1.5249, batch acc 0.9048
17:53:32.187   Training iter 250, batch loss 1.5296, batch acc 0.8862
17:53:32.722   Training iter 300, batch loss 1.5303, batch acc 0.8982
17:53:33.242   Training iter 350, batch loss 1.5273, batch acc 0.8950
17:53:33.759   Training iter 400, batch loss 1.5206, batch acc 0.9180
17:53:34.241   Training iter 450, batch loss 1.5310, batch acc 0.8912
17:53:34.756   Training iter 500, batch loss 1.5594, batch acc 0.8522
17:53:35.277   Training iter 550, batch loss 1.5321, batch acc 0.8702
17:53:35.787   Training iter 600, batch loss 1.5312, batch acc 0.9094
17:53:35.789 Testing @ 290 epoch...
17:53:35.840     Testing, total mean loss 1.54911, total acc 0.89870
17:53:35.840 Training @ 291 epoch...
17:53:36.371   Training iter 50, batch loss 1.5312, batch acc 0.9130
17:53:36.894   Training iter 100, batch loss 1.5276, batch acc 0.9056
17:53:37.424   Training iter 150, batch loss 1.5239, batch acc 0.9156
17:53:37.960   Training iter 200, batch loss 1.5412, batch acc 0.8846
17:53:38.477   Training iter 250, batch loss 1.5235, batch acc 0.9060
17:53:38.983   Training iter 300, batch loss 1.5193, batch acc 0.9176
17:53:39.514   Training iter 350, batch loss 1.5258, batch acc 0.8960
17:53:40.036   Training iter 400, batch loss 1.5460, batch acc 0.8920
17:53:40.555   Training iter 450, batch loss 1.5246, batch acc 0.9040
17:53:41.081   Training iter 500, batch loss 1.5235, batch acc 0.9090
17:53:41.623   Training iter 550, batch loss 1.5269, batch acc 0.9072
17:53:42.208   Training iter 600, batch loss 1.5286, batch acc 0.9100
17:53:42.211 Training @ 292 epoch...
17:53:42.771   Training iter 50, batch loss 1.5351, batch acc 0.9068
17:53:43.311   Training iter 100, batch loss 1.5216, batch acc 0.9134
17:53:43.815   Training iter 150, batch loss 1.5301, batch acc 0.8914
17:53:44.335   Training iter 200, batch loss 1.5365, batch acc 0.8864
17:53:44.831   Training iter 250, batch loss 1.5327, batch acc 0.9002
17:53:45.328   Training iter 300, batch loss 1.5676, batch acc 0.8702
17:53:45.825   Training iter 350, batch loss 1.5414, batch acc 0.8754
17:53:46.328   Training iter 400, batch loss 1.5333, batch acc 0.9094
17:53:46.830   Training iter 450, batch loss 1.5436, batch acc 0.8940
17:53:47.327   Training iter 500, batch loss 1.5448, batch acc 0.8544
17:53:47.832   Training iter 550, batch loss 1.5367, batch acc 0.8740
17:53:48.319   Training iter 600, batch loss 1.5317, batch acc 0.8914
17:53:48.321 Training @ 293 epoch...
17:53:48.820   Training iter 50, batch loss 1.5302, batch acc 0.9012
17:53:49.357   Training iter 100, batch loss 1.5224, batch acc 0.9036
17:53:49.841   Training iter 150, batch loss 1.5231, batch acc 0.9078
17:53:50.341   Training iter 200, batch loss 1.5300, batch acc 0.8890
17:53:50.856   Training iter 250, batch loss 1.5266, batch acc 0.8988
17:53:51.388   Training iter 300, batch loss 1.5251, batch acc 0.9172
17:53:51.892   Training iter 350, batch loss 1.5391, batch acc 0.8934
17:53:52.400   Training iter 400, batch loss 1.5267, batch acc 0.8998
17:53:52.924   Training iter 450, batch loss 1.5248, batch acc 0.9150
17:53:53.436   Training iter 500, batch loss 1.5270, batch acc 0.9168
17:53:53.908   Training iter 550, batch loss 1.5313, batch acc 0.9092
17:53:54.404   Training iter 600, batch loss 1.5286, batch acc 0.9074
17:53:54.405 Training @ 294 epoch...
17:53:54.897   Training iter 50, batch loss 1.5291, batch acc 0.8968
17:53:55.374   Training iter 100, batch loss 1.5228, batch acc 0.9052
17:53:55.889   Training iter 150, batch loss 1.5180, batch acc 0.9232
17:53:56.435   Training iter 200, batch loss 1.5430, batch acc 0.8852
17:53:56.960   Training iter 250, batch loss 1.5289, batch acc 0.9072
17:53:57.486   Training iter 300, batch loss 1.5292, batch acc 0.9036
17:53:58.008   Training iter 350, batch loss 1.5272, batch acc 0.9168
17:53:58.528   Training iter 400, batch loss 1.5409, batch acc 0.8740
17:53:59.044   Training iter 450, batch loss 1.5220, batch acc 0.9202
17:53:59.566   Training iter 500, batch loss 1.5332, batch acc 0.9102
17:54:00.078   Training iter 550, batch loss 1.5274, batch acc 0.9166
17:54:00.591   Training iter 600, batch loss 1.5366, batch acc 0.9082
17:54:00.592 Training @ 295 epoch...
17:54:01.117   Training iter 50, batch loss 1.5360, batch acc 0.9046
17:54:01.670   Training iter 100, batch loss 1.5343, batch acc 0.9040
17:54:02.230   Training iter 150, batch loss 1.5257, batch acc 0.9178
17:54:02.771   Training iter 200, batch loss 1.5293, batch acc 0.9082
17:54:03.283   Training iter 250, batch loss 1.5272, batch acc 0.9024
17:54:03.826   Training iter 300, batch loss 1.5267, batch acc 0.9204
17:54:04.362   Training iter 350, batch loss 1.5291, batch acc 0.9162
17:54:04.903   Training iter 400, batch loss 1.5263, batch acc 0.9178
17:54:05.439   Training iter 450, batch loss 1.5302, batch acc 0.9050
17:54:05.956   Training iter 500, batch loss 1.5301, batch acc 0.9062
17:54:06.486   Training iter 550, batch loss 1.5264, batch acc 0.9086
17:54:07.010   Training iter 600, batch loss 1.5425, batch acc 0.8798
17:54:07.012 Testing @ 295 epoch...
17:54:07.059     Testing, total mean loss 1.54034, total acc 0.90850
17:54:07.059 Training @ 296 epoch...
17:54:07.612   Training iter 50, batch loss 1.5344, batch acc 0.8952
17:54:08.142   Training iter 100, batch loss 1.5307, batch acc 0.8948
17:54:08.658   Training iter 150, batch loss 1.5364, batch acc 0.8912
17:54:09.181   Training iter 200, batch loss 1.5261, batch acc 0.9114
17:54:09.713   Training iter 250, batch loss 1.5316, batch acc 0.9150
17:54:10.241   Training iter 300, batch loss 1.5317, batch acc 0.9006
17:54:10.763   Training iter 350, batch loss 1.5376, batch acc 0.8860
17:54:11.290   Training iter 400, batch loss 1.5267, batch acc 0.9078
17:54:11.794   Training iter 450, batch loss 1.5205, batch acc 0.9158
17:54:12.314   Training iter 500, batch loss 1.5222, batch acc 0.9184
17:54:12.829   Training iter 550, batch loss 1.5202, batch acc 0.9200
17:54:13.347   Training iter 600, batch loss 1.5338, batch acc 0.8894
17:54:13.349 Training @ 297 epoch...
17:54:13.863   Training iter 50, batch loss 1.5405, batch acc 0.8982
17:54:14.396   Training iter 100, batch loss 1.5183, batch acc 0.9214
17:54:14.883   Training iter 150, batch loss 1.5265, batch acc 0.8978
17:54:15.374   Training iter 200, batch loss 1.5255, batch acc 0.8982
17:54:15.892   Training iter 250, batch loss 1.5292, batch acc 0.8970
17:54:16.483   Training iter 300, batch loss 1.5329, batch acc 0.9084
17:54:17.079   Training iter 350, batch loss 1.5180, batch acc 0.9248
17:54:17.658   Training iter 400, batch loss 1.5239, batch acc 0.9142
17:54:18.205   Training iter 450, batch loss 1.5484, batch acc 0.8658
17:54:18.750   Training iter 500, batch loss 1.5376, batch acc 0.8964
17:54:19.293   Training iter 550, batch loss 1.5288, batch acc 0.8910
17:54:19.846   Training iter 600, batch loss 1.5388, batch acc 0.8690
17:54:19.848 Training @ 298 epoch...
17:54:20.396   Training iter 50, batch loss 1.5268, batch acc 0.9002
17:54:20.936   Training iter 100, batch loss 1.5306, batch acc 0.9042
17:54:21.501   Training iter 150, batch loss 1.5447, batch acc 0.8952
17:54:22.055   Training iter 200, batch loss 1.5439, batch acc 0.8696
17:54:22.598   Training iter 250, batch loss 1.5318, batch acc 0.8620
17:54:23.138   Training iter 300, batch loss 1.5215, batch acc 0.9130
17:54:23.686   Training iter 350, batch loss 1.5287, batch acc 0.9010
17:54:24.238   Training iter 400, batch loss 1.5314, batch acc 0.8948
17:54:24.783   Training iter 450, batch loss 1.5374, batch acc 0.8766
17:54:25.347   Training iter 500, batch loss 1.5240, batch acc 0.9050
17:54:25.909   Training iter 550, batch loss 1.5391, batch acc 0.8900
17:54:26.465   Training iter 600, batch loss 1.5258, batch acc 0.9028
17:54:26.467 Training @ 299 epoch...
17:54:27.021   Training iter 50, batch loss 1.5267, batch acc 0.9066
17:54:27.582   Training iter 100, batch loss 1.5388, batch acc 0.9106
17:54:28.148   Training iter 150, batch loss 1.5287, batch acc 0.9172
17:54:28.709   Training iter 200, batch loss 1.5350, batch acc 0.9096
17:54:29.284   Training iter 250, batch loss 1.5325, batch acc 0.9038
17:54:29.842   Training iter 300, batch loss 1.5327, batch acc 0.8988
17:54:30.400   Training iter 350, batch loss 1.5384, batch acc 0.9024
17:54:30.953   Training iter 400, batch loss 1.5342, batch acc 0.8894
17:54:31.512   Training iter 450, batch loss 1.5263, batch acc 0.9148
17:54:32.059   Training iter 500, batch loss 1.5214, batch acc 0.9082
17:54:32.600   Training iter 550, batch loss 1.5201, batch acc 0.9000
17:54:33.154   Training iter 600, batch loss 1.5291, batch acc 0.9018
17:54:33.156 Training @ 300 epoch...
17:54:33.698   Training iter 50, batch loss 1.5259, batch acc 0.9022
17:54:34.248   Training iter 100, batch loss 1.5256, batch acc 0.9062
17:54:34.799   Training iter 150, batch loss 1.5294, batch acc 0.9110
17:54:35.333   Training iter 200, batch loss 1.5290, batch acc 0.9078
17:54:35.869   Training iter 250, batch loss 1.5234, batch acc 0.9130
17:54:36.417   Training iter 300, batch loss 1.5407, batch acc 0.8694
17:54:36.977   Training iter 350, batch loss 1.5459, batch acc 0.8376
17:54:37.531   Training iter 400, batch loss 1.5384, batch acc 0.8620
17:54:38.089   Training iter 450, batch loss 1.5232, batch acc 0.8948
17:54:38.649   Training iter 500, batch loss 1.5281, batch acc 0.8822
17:54:39.206   Training iter 550, batch loss 1.5237, batch acc 0.9076
17:54:39.760   Training iter 600, batch loss 1.5338, batch acc 0.8808
17:54:39.761 Testing @ 300 epoch...
17:54:39.812     Testing, total mean loss 1.52455, total acc 0.90930
17:54:39.812 Plot @ 300 epoch...
17:54:39.812 Training @ 301 epoch...
17:54:40.378   Training iter 50, batch loss 1.5321, batch acc 0.8802
17:54:40.948   Training iter 100, batch loss 1.5455, batch acc 0.8634
17:54:41.512   Training iter 150, batch loss 1.5277, batch acc 0.8914
17:54:42.064   Training iter 200, batch loss 1.5339, batch acc 0.8790
17:54:42.621   Training iter 250, batch loss 1.5269, batch acc 0.9026
17:54:43.172   Training iter 300, batch loss 1.5250, batch acc 0.9124
17:54:43.707   Training iter 350, batch loss 1.5448, batch acc 0.8644
17:54:44.249   Training iter 400, batch loss 1.5310, batch acc 0.8876
17:54:44.792   Training iter 450, batch loss 1.5427, batch acc 0.8640
17:54:45.324   Training iter 500, batch loss 1.5387, batch acc 0.8746
17:54:45.858   Training iter 550, batch loss 1.5327, batch acc 0.8920
17:54:46.389   Training iter 600, batch loss 1.5190, batch acc 0.9010
17:54:46.391 Training @ 302 epoch...
17:54:46.928   Training iter 50, batch loss 1.5412, batch acc 0.8672
17:54:47.477   Training iter 100, batch loss 1.5356, batch acc 0.8562
17:54:48.028   Training iter 150, batch loss 1.5217, batch acc 0.8978
17:54:48.565   Training iter 200, batch loss 1.5271, batch acc 0.8952
17:54:49.092   Training iter 250, batch loss 1.5504, batch acc 0.8862
17:54:49.616   Training iter 300, batch loss 1.5444, batch acc 0.8684
17:54:50.098   Training iter 350, batch loss 1.5247, batch acc 0.8980
17:54:50.618   Training iter 400, batch loss 1.5201, batch acc 0.9212
17:54:51.174   Training iter 450, batch loss 1.5331, batch acc 0.9152
17:54:51.729   Training iter 500, batch loss 1.5288, batch acc 0.9108
17:54:52.423   Training iter 550, batch loss 1.5326, batch acc 0.9018
17:54:52.956   Training iter 600, batch loss 1.5407, batch acc 0.8916
17:54:52.957 Training @ 303 epoch...
17:54:53.474   Training iter 50, batch loss 1.5269, batch acc 0.9108
17:54:53.951   Training iter 100, batch loss 1.5207, batch acc 0.9160
17:54:54.458   Training iter 150, batch loss 1.5204, batch acc 0.9138
17:54:54.943   Training iter 200, batch loss 1.5233, batch acc 0.9178
17:54:55.410   Training iter 250, batch loss 1.5291, batch acc 0.8922
17:54:55.882   Training iter 300, batch loss 1.5243, batch acc 0.9170
17:54:56.422   Training iter 350, batch loss 1.5415, batch acc 0.8860
17:54:57.010   Training iter 400, batch loss 1.5356, batch acc 0.9000
17:54:57.585   Training iter 450, batch loss 1.5302, batch acc 0.9002
17:54:58.161   Training iter 500, batch loss 1.5284, batch acc 0.9090
17:54:58.732   Training iter 550, batch loss 1.5213, batch acc 0.9160
17:54:59.306   Training iter 600, batch loss 1.5315, batch acc 0.9028
17:54:59.308 Training @ 304 epoch...
17:54:59.882   Training iter 50, batch loss 1.5254, batch acc 0.9106
17:55:00.450   Training iter 100, batch loss 1.5362, batch acc 0.8946
17:55:01.021   Training iter 150, batch loss 1.5263, batch acc 0.8994
17:55:01.652   Training iter 200, batch loss 1.5337, batch acc 0.8924
17:55:02.298   Training iter 250, batch loss 1.5318, batch acc 0.8990
17:55:02.851   Training iter 300, batch loss 1.5335, batch acc 0.9054
17:55:03.393   Training iter 350, batch loss 1.5292, batch acc 0.9144
17:55:03.940   Training iter 400, batch loss 1.5240, batch acc 0.9160
17:55:04.497   Training iter 450, batch loss 1.5217, batch acc 0.9132
17:55:05.051   Training iter 500, batch loss 1.5121, batch acc 0.9222
17:55:05.598   Training iter 550, batch loss 1.5390, batch acc 0.8736
17:55:06.146   Training iter 600, batch loss 1.5377, batch acc 0.8930
17:55:06.148 Training @ 305 epoch...
17:55:06.695   Training iter 50, batch loss 1.5381, batch acc 0.8820
17:55:07.227   Training iter 100, batch loss 1.5267, batch acc 0.8992
17:55:07.750   Training iter 150, batch loss 1.5286, batch acc 0.8942
17:55:08.281   Training iter 200, batch loss 1.5437, batch acc 0.8968
17:55:08.809   Training iter 250, batch loss 1.5366, batch acc 0.8950
17:55:09.340   Training iter 300, batch loss 1.5304, batch acc 0.8988
17:55:09.862   Training iter 350, batch loss 1.5338, batch acc 0.8992
17:55:10.431   Training iter 400, batch loss 1.5342, batch acc 0.9082
17:55:11.017   Training iter 450, batch loss 1.5311, batch acc 0.8914
17:55:11.599   Training iter 500, batch loss 1.5316, batch acc 0.8986
17:55:12.151   Training iter 550, batch loss 1.5315, batch acc 0.9064
17:55:12.696   Training iter 600, batch loss 1.5240, batch acc 0.9146
17:55:12.698 Testing @ 305 epoch...
17:55:12.746     Testing, total mean loss 1.52403, total acc 0.92160
17:55:12.746 Training @ 306 epoch...
17:55:13.298   Training iter 50, batch loss 1.5222, batch acc 0.9270
17:55:13.839   Training iter 100, batch loss 1.5298, batch acc 0.9140
17:55:14.471   Training iter 150, batch loss 1.5291, batch acc 0.9048
17:55:15.022   Training iter 200, batch loss 1.5335, batch acc 0.8960
17:55:15.580   Training iter 250, batch loss 1.5317, batch acc 0.8906
17:55:16.124   Training iter 300, batch loss 1.5356, batch acc 0.8770
17:55:16.705   Training iter 350, batch loss 1.5436, batch acc 0.8686
17:55:17.248   Training iter 400, batch loss 1.5249, batch acc 0.9146
17:55:17.799   Training iter 450, batch loss 1.5317, batch acc 0.8926
17:55:18.351   Training iter 500, batch loss 1.5284, batch acc 0.9046
17:55:18.877   Training iter 550, batch loss 1.5333, batch acc 0.8854
17:55:19.399   Training iter 600, batch loss 1.5288, batch acc 0.9008
17:55:19.401 Training @ 307 epoch...
17:55:19.933   Training iter 50, batch loss 1.5308, batch acc 0.8742
17:55:20.435   Training iter 100, batch loss 1.5389, batch acc 0.8686
17:55:20.929   Training iter 150, batch loss 1.5375, batch acc 0.8764
17:55:21.432   Training iter 200, batch loss 1.5321, batch acc 0.8810
17:55:21.920   Training iter 250, batch loss 1.5272, batch acc 0.8926
17:55:22.413   Training iter 300, batch loss 1.5200, batch acc 0.9154
17:55:22.905   Training iter 350, batch loss 1.5323, batch acc 0.8914
17:55:23.438   Training iter 400, batch loss 1.5401, batch acc 0.8848
17:55:23.965   Training iter 450, batch loss 1.5271, batch acc 0.9072
17:55:24.522   Training iter 500, batch loss 1.5359, batch acc 0.8920
17:55:25.085   Training iter 550, batch loss 1.5222, batch acc 0.9168
17:55:25.654   Training iter 600, batch loss 1.5294, batch acc 0.9116
17:55:25.656 Training @ 308 epoch...
17:55:26.221   Training iter 50, batch loss 1.5194, batch acc 0.9212
17:55:26.789   Training iter 100, batch loss 1.5174, batch acc 0.9196
17:55:27.336   Training iter 150, batch loss 1.5243, batch acc 0.8906
17:55:27.864   Training iter 200, batch loss 1.5410, batch acc 0.8786
17:55:28.391   Training iter 250, batch loss 1.5296, batch acc 0.8872
17:55:28.913   Training iter 300, batch loss 1.5275, batch acc 0.8914
17:55:29.454   Training iter 350, batch loss 1.5280, batch acc 0.8934
17:55:29.982   Training iter 400, batch loss 1.5289, batch acc 0.8986
17:55:30.520   Training iter 450, batch loss 1.5249, batch acc 0.9080
17:55:31.035   Training iter 500, batch loss 1.5245, batch acc 0.9220
17:55:31.526   Training iter 550, batch loss 1.5332, batch acc 0.8898
17:55:32.025   Training iter 600, batch loss 1.5297, batch acc 0.8992
17:55:32.026 Training @ 309 epoch...
17:55:32.522   Training iter 50, batch loss 1.5334, batch acc 0.8820
17:55:33.032   Training iter 100, batch loss 1.5276, batch acc 0.8902
17:55:33.534   Training iter 150, batch loss 1.5365, batch acc 0.8596
17:55:34.077   Training iter 200, batch loss 1.5263, batch acc 0.8808
17:55:34.684   Training iter 250, batch loss 1.5244, batch acc 0.8938
17:55:35.189   Training iter 300, batch loss 1.5277, batch acc 0.8906
17:55:35.734   Training iter 350, batch loss 1.5283, batch acc 0.8866
17:55:36.288   Training iter 400, batch loss 1.5280, batch acc 0.9076
17:55:36.928   Training iter 450, batch loss 1.5288, batch acc 0.8960
17:55:37.449   Training iter 500, batch loss 1.5170, batch acc 0.9166
17:55:37.991   Training iter 550, batch loss 1.5216, batch acc 0.9126
17:55:38.543   Training iter 600, batch loss 1.5323, batch acc 0.8932
17:55:38.545 Training @ 310 epoch...
17:55:39.122   Training iter 50, batch loss 1.5336, batch acc 0.8888
17:55:39.648   Training iter 100, batch loss 1.5439, batch acc 0.8598
17:55:40.176   Training iter 150, batch loss 1.5295, batch acc 0.8890
17:55:40.829   Training iter 200, batch loss 1.5383, batch acc 0.8810
17:55:41.396   Training iter 250, batch loss 1.5333, batch acc 0.8818
17:55:42.002   Training iter 300, batch loss 1.5287, batch acc 0.8728
17:55:42.551   Training iter 350, batch loss 1.5282, batch acc 0.8874
17:55:43.111   Training iter 400, batch loss 1.5337, batch acc 0.9006
17:55:43.668   Training iter 450, batch loss 1.5180, batch acc 0.9104
17:55:44.219   Training iter 500, batch loss 1.5242, batch acc 0.9028
17:55:44.764   Training iter 550, batch loss 1.5298, batch acc 0.9018
17:55:45.310   Training iter 600, batch loss 1.5341, batch acc 0.8940
17:55:45.312 Testing @ 310 epoch...
17:55:45.360     Testing, total mean loss 1.54428, total acc 0.86130
17:55:45.360 Training @ 311 epoch...
17:55:45.918   Training iter 50, batch loss 1.5384, batch acc 0.8796
17:55:46.465   Training iter 100, batch loss 1.5239, batch acc 0.9098
17:55:47.003   Training iter 150, batch loss 1.5462, batch acc 0.8692
17:55:47.541   Training iter 200, batch loss 1.5444, batch acc 0.8886
17:55:48.090   Training iter 250, batch loss 1.5214, batch acc 0.8994
17:55:48.617   Training iter 300, batch loss 1.5185, batch acc 0.9150
17:55:49.148   Training iter 350, batch loss 1.5334, batch acc 0.8962
17:55:49.697   Training iter 400, batch loss 1.5284, batch acc 0.9128
17:55:50.250   Training iter 450, batch loss 1.5266, batch acc 0.9164
17:55:50.796   Training iter 500, batch loss 1.5461, batch acc 0.8436
17:55:51.377   Training iter 550, batch loss 1.5503, batch acc 0.8444
17:55:51.930   Training iter 600, batch loss 1.5286, batch acc 0.9000
17:55:51.932 Training @ 312 epoch...
17:55:52.465   Training iter 50, batch loss 1.5338, batch acc 0.8926
17:55:52.998   Training iter 100, batch loss 1.5320, batch acc 0.8960
17:55:53.551   Training iter 150, batch loss 1.5322, batch acc 0.8986
17:55:54.142   Training iter 200, batch loss 1.5417, batch acc 0.8842
17:55:54.719   Training iter 250, batch loss 1.5310, batch acc 0.8966
17:55:55.298   Training iter 300, batch loss 1.5259, batch acc 0.8906
17:55:55.837   Training iter 350, batch loss 1.5280, batch acc 0.9078
17:55:56.391   Training iter 400, batch loss 1.5254, batch acc 0.9100
17:55:56.940   Training iter 450, batch loss 1.5277, batch acc 0.8964
17:55:57.500   Training iter 500, batch loss 1.5283, batch acc 0.9056
17:55:58.041   Training iter 550, batch loss 1.5314, batch acc 0.9016
17:55:58.576   Training iter 600, batch loss 1.5299, batch acc 0.9032
17:55:58.578 Training @ 313 epoch...
17:55:59.125   Training iter 50, batch loss 1.5238, batch acc 0.9172
17:55:59.658   Training iter 100, batch loss 1.5281, batch acc 0.9174
17:56:00.203   Training iter 150, batch loss 1.5329, batch acc 0.8936
17:56:00.762   Training iter 200, batch loss 1.5268, batch acc 0.9056
17:56:01.306   Training iter 250, batch loss 1.5358, batch acc 0.8744
17:56:01.861   Training iter 300, batch loss 1.5286, batch acc 0.8918
17:56:02.454   Training iter 350, batch loss 1.5342, batch acc 0.8716
17:56:03.032   Training iter 400, batch loss 1.5444, batch acc 0.8802
17:56:03.593   Training iter 450, batch loss 1.5246, batch acc 0.9000
17:56:04.153   Training iter 500, batch loss 1.5291, batch acc 0.8904
17:56:04.704   Training iter 550, batch loss 1.5262, batch acc 0.9076
17:56:05.265   Training iter 600, batch loss 1.5249, batch acc 0.9024
17:56:05.267 Training @ 314 epoch...
17:56:05.793   Training iter 50, batch loss 1.5325, batch acc 0.8732
17:56:06.340   Training iter 100, batch loss 1.5279, batch acc 0.8914
17:56:06.864   Training iter 150, batch loss 1.5257, batch acc 0.9030
17:56:07.401   Training iter 200, batch loss 1.5242, batch acc 0.9046
17:56:07.936   Training iter 250, batch loss 1.5286, batch acc 0.9002
17:56:08.483   Training iter 300, batch loss 1.5316, batch acc 0.8884
17:56:09.038   Training iter 350, batch loss 1.5306, batch acc 0.9024
17:56:09.590   Training iter 400, batch loss 1.5282, batch acc 0.9018
17:56:10.088   Training iter 450, batch loss 1.5590, batch acc 0.8248
17:56:10.601   Training iter 500, batch loss 1.5386, batch acc 0.8848
17:56:11.118   Training iter 550, batch loss 1.5276, batch acc 0.9010
17:56:11.621   Training iter 600, batch loss 1.5258, batch acc 0.9030
17:56:11.622 Training @ 315 epoch...
17:56:12.123   Training iter 50, batch loss 1.5231, batch acc 0.9020
17:56:12.621   Training iter 100, batch loss 1.5251, batch acc 0.9052
17:56:13.139   Training iter 150, batch loss 1.5526, batch acc 0.8366
17:56:13.681   Training iter 200, batch loss 1.5514, batch acc 0.8668
17:56:14.218   Training iter 250, batch loss 1.5398, batch acc 0.8892
17:56:14.753   Training iter 300, batch loss 1.5289, batch acc 0.8898
17:56:15.291   Training iter 350, batch loss 1.5287, batch acc 0.8814
17:56:15.852   Training iter 400, batch loss 1.5423, batch acc 0.8854
17:56:16.390   Training iter 450, batch loss 1.5389, batch acc 0.8608
17:56:16.945   Training iter 500, batch loss 1.5348, batch acc 0.8738
17:56:17.477   Training iter 550, batch loss 1.5252, batch acc 0.8964
17:56:18.002   Training iter 600, batch loss 1.5411, batch acc 0.8542
17:56:18.004 Testing @ 315 epoch...
17:56:18.055     Testing, total mean loss 1.53421, total acc 0.87930
17:56:18.055 Training @ 316 epoch...
17:56:18.607   Training iter 50, batch loss 1.5273, batch acc 0.8856
17:56:19.144   Training iter 100, batch loss 1.5271, batch acc 0.9030
17:56:19.658   Training iter 150, batch loss 1.5297, batch acc 0.9018
17:56:20.170   Training iter 200, batch loss 1.5509, batch acc 0.8816
17:56:20.688   Training iter 250, batch loss 1.5309, batch acc 0.8922
17:56:21.210   Training iter 300, batch loss 1.5242, batch acc 0.9128
17:56:21.739   Training iter 350, batch loss 1.5336, batch acc 0.9014
17:56:22.290   Training iter 400, batch loss 1.5116, batch acc 0.9318
17:56:22.800   Training iter 450, batch loss 1.5204, batch acc 0.9106
17:56:23.348   Training iter 500, batch loss 1.5492, batch acc 0.8430
17:56:23.893   Training iter 550, batch loss 1.5419, batch acc 0.8452
17:56:24.448   Training iter 600, batch loss 1.5461, batch acc 0.8642
17:56:24.450 Training @ 317 epoch...
17:56:24.992   Training iter 50, batch loss 1.5389, batch acc 0.8898
17:56:25.554   Training iter 100, batch loss 1.5322, batch acc 0.8870
17:56:26.114   Training iter 150, batch loss 1.5206, batch acc 0.9062
17:56:26.665   Training iter 200, batch loss 1.5228, batch acc 0.9016
17:56:27.228   Training iter 250, batch loss 1.5346, batch acc 0.8642
17:56:27.837   Training iter 300, batch loss 1.5202, batch acc 0.9038
17:56:28.380   Training iter 350, batch loss 1.5295, batch acc 0.8988
17:56:28.959   Training iter 400, batch loss 1.5253, batch acc 0.9038
17:56:29.530   Training iter 450, batch loss 1.5333, batch acc 0.9054
17:56:30.100   Training iter 500, batch loss 1.5252, batch acc 0.8950
17:56:30.652   Training iter 550, batch loss 1.5222, batch acc 0.8948
17:56:31.207   Training iter 600, batch loss 1.5327, batch acc 0.8844
17:56:31.209 Training @ 318 epoch...
17:56:31.758   Training iter 50, batch loss 1.5299, batch acc 0.8994
17:56:32.333   Training iter 100, batch loss 1.5298, batch acc 0.8904
17:56:32.910   Training iter 150, batch loss 1.5370, batch acc 0.8720
17:56:33.482   Training iter 200, batch loss 1.5297, batch acc 0.8800
17:56:34.047   Training iter 250, batch loss 1.5483, batch acc 0.8574
17:56:34.623   Training iter 300, batch loss 1.5255, batch acc 0.9040
17:56:35.186   Training iter 350, batch loss 1.5353, batch acc 0.8890
17:56:35.729   Training iter 400, batch loss 1.5240, batch acc 0.9058
17:56:36.296   Training iter 450, batch loss 1.5280, batch acc 0.8788
17:56:36.859   Training iter 500, batch loss 1.5255, batch acc 0.9086
17:56:37.423   Training iter 550, batch loss 1.5314, batch acc 0.8732
17:56:37.970   Training iter 600, batch loss 1.5271, batch acc 0.9044
17:56:37.971 Training @ 319 epoch...
17:56:38.527   Training iter 50, batch loss 1.5274, batch acc 0.9058
17:56:39.078   Training iter 100, batch loss 1.5198, batch acc 0.9074
17:56:39.659   Training iter 150, batch loss 1.5289, batch acc 0.8954
17:56:40.219   Training iter 200, batch loss 1.5174, batch acc 0.9160
17:56:40.770   Training iter 250, batch loss 1.5324, batch acc 0.8990
17:56:41.356   Training iter 300, batch loss 1.5245, batch acc 0.9120
17:56:41.921   Training iter 350, batch loss 1.5301, batch acc 0.9062
17:56:42.522   Training iter 400, batch loss 1.5281, batch acc 0.8982
17:56:43.129   Training iter 450, batch loss 1.5310, batch acc 0.8902
17:56:43.866   Training iter 500, batch loss 1.5369, batch acc 0.8702
17:56:44.475   Training iter 550, batch loss 1.5417, batch acc 0.8846
17:56:45.045   Training iter 600, batch loss 1.5315, batch acc 0.8972
17:56:45.047 Training @ 320 epoch...
17:56:45.619   Training iter 50, batch loss 1.5404, batch acc 0.8702
17:56:46.189   Training iter 100, batch loss 1.5355, batch acc 0.8820
17:56:46.757   Training iter 150, batch loss 1.5152, batch acc 0.9198
17:56:47.308   Training iter 200, batch loss 1.5321, batch acc 0.8942
17:56:47.894   Training iter 250, batch loss 1.5342, batch acc 0.8928
17:56:48.458   Training iter 300, batch loss 1.5337, batch acc 0.9072
17:56:49.030   Training iter 350, batch loss 1.5396, batch acc 0.8854
17:56:49.594   Training iter 400, batch loss 1.5454, batch acc 0.8668
17:56:50.155   Training iter 450, batch loss 1.5398, batch acc 0.8814
17:56:50.725   Training iter 500, batch loss 1.5382, batch acc 0.8872
17:56:51.275   Training iter 550, batch loss 1.5276, batch acc 0.8932
17:56:51.827   Training iter 600, batch loss 1.5322, batch acc 0.8946
17:56:51.829 Testing @ 320 epoch...
17:56:51.877     Testing, total mean loss 1.54313, total acc 0.88640
17:56:51.877 Training @ 321 epoch...
17:56:52.447   Training iter 50, batch loss 1.5309, batch acc 0.8888
17:56:53.012   Training iter 100, batch loss 1.5275, batch acc 0.9008
17:56:53.583   Training iter 150, batch loss 1.5338, batch acc 0.8892
17:56:54.160   Training iter 200, batch loss 1.5200, batch acc 0.9196
17:56:54.732   Training iter 250, batch loss 1.5253, batch acc 0.9110
17:56:55.307   Training iter 300, batch loss 1.5285, batch acc 0.8982
17:56:55.881   Training iter 350, batch loss 1.5284, batch acc 0.9040
17:56:56.465   Training iter 400, batch loss 1.5285, batch acc 0.9106
17:56:57.037   Training iter 450, batch loss 1.5211, batch acc 0.9052
17:56:57.603   Training iter 500, batch loss 1.5399, batch acc 0.8862
17:56:58.231   Training iter 550, batch loss 1.5244, batch acc 0.9062
17:56:58.764   Training iter 600, batch loss 1.5286, batch acc 0.9076
17:56:58.766 Training @ 322 epoch...
17:56:59.320   Training iter 50, batch loss 1.5260, batch acc 0.9056
17:56:59.851   Training iter 100, batch loss 1.5245, batch acc 0.9162
17:57:00.387   Training iter 150, batch loss 1.5317, batch acc 0.9018
17:57:00.929   Training iter 200, batch loss 1.5271, batch acc 0.9166
17:57:01.508   Training iter 250, batch loss 1.5435, batch acc 0.8976
17:57:02.067   Training iter 300, batch loss 1.5266, batch acc 0.8896
17:57:02.571   Training iter 350, batch loss 1.5309, batch acc 0.8578
17:57:03.067   Training iter 400, batch loss 1.5269, batch acc 0.8778
17:57:03.580   Training iter 450, batch loss 1.5379, batch acc 0.8714
17:57:04.096   Training iter 500, batch loss 1.5324, batch acc 0.8678
17:57:04.624   Training iter 550, batch loss 1.5284, batch acc 0.8898
17:57:05.154   Training iter 600, batch loss 1.5365, batch acc 0.8628
17:57:05.156 Training @ 323 epoch...
17:57:05.706   Training iter 50, batch loss 1.5291, batch acc 0.8838
17:57:06.258   Training iter 100, batch loss 1.5270, batch acc 0.8900
17:57:06.812   Training iter 150, batch loss 1.5326, batch acc 0.8874
17:57:07.374   Training iter 200, batch loss 1.5271, batch acc 0.8998
17:57:07.914   Training iter 250, batch loss 1.5209, batch acc 0.8996
17:57:08.460   Training iter 300, batch loss 1.5320, batch acc 0.9010
17:57:09.022   Training iter 350, batch loss 1.5256, batch acc 0.9076
17:57:09.557   Training iter 400, batch loss 1.5233, batch acc 0.9046
17:57:10.049   Training iter 450, batch loss 1.5360, batch acc 0.8932
17:57:10.538   Training iter 500, batch loss 1.5259, batch acc 0.8996
17:57:11.032   Training iter 550, batch loss 1.5293, batch acc 0.8904
17:57:11.534   Training iter 600, batch loss 1.5246, batch acc 0.9030
17:57:11.536 Training @ 324 epoch...
17:57:12.079   Training iter 50, batch loss 1.5425, batch acc 0.8706
17:57:12.613   Training iter 100, batch loss 1.5331, batch acc 0.8852
17:57:13.142   Training iter 150, batch loss 1.5275, batch acc 0.9090
17:57:13.681   Training iter 200, batch loss 1.5183, batch acc 0.9084
17:57:14.230   Training iter 250, batch loss 1.5299, batch acc 0.8806
17:57:14.762   Training iter 300, batch loss 1.5238, batch acc 0.9102
17:57:15.297   Training iter 350, batch loss 1.5391, batch acc 0.8876
17:57:15.851   Training iter 400, batch loss 1.5295, batch acc 0.8986
17:57:16.409   Training iter 450, batch loss 1.5177, batch acc 0.9198
17:57:16.963   Training iter 500, batch loss 1.5369, batch acc 0.8796
17:57:17.496   Training iter 550, batch loss 1.5281, batch acc 0.8962
17:57:18.014   Training iter 600, batch loss 1.5237, batch acc 0.9014
17:57:18.016 Training @ 325 epoch...
17:57:18.537   Training iter 50, batch loss 1.5331, batch acc 0.8888
17:57:19.083   Training iter 100, batch loss 1.5281, batch acc 0.9098
17:57:19.615   Training iter 150, batch loss 1.5379, batch acc 0.8892
17:57:20.135   Training iter 200, batch loss 1.5265, batch acc 0.9148
17:57:20.643   Training iter 250, batch loss 1.5213, batch acc 0.9194
17:57:21.143   Training iter 300, batch loss 1.5255, batch acc 0.9108
17:57:21.648   Training iter 350, batch loss 1.5307, batch acc 0.9046
17:57:22.160   Training iter 400, batch loss 1.5272, batch acc 0.9068
17:57:22.652   Training iter 450, batch loss 1.5239, batch acc 0.9054
17:57:23.152   Training iter 500, batch loss 1.5211, batch acc 0.9186
17:57:23.632   Training iter 550, batch loss 1.5217, batch acc 0.9156
17:57:24.123   Training iter 600, batch loss 1.5317, batch acc 0.9020
17:57:24.125 Testing @ 325 epoch...
17:57:24.173     Testing, total mean loss 1.52858, total acc 0.91100
17:57:24.173 Training @ 326 epoch...
17:57:24.680   Training iter 50, batch loss 1.5354, batch acc 0.9072
17:57:25.179   Training iter 100, batch loss 1.5227, batch acc 0.9074
17:57:25.687   Training iter 150, batch loss 1.5295, batch acc 0.9052
17:57:26.206   Training iter 200, batch loss 1.5374, batch acc 0.8914
17:57:26.742   Training iter 250, batch loss 1.5357, batch acc 0.8946
17:57:27.257   Training iter 300, batch loss 1.5346, batch acc 0.8890
17:57:27.782   Training iter 350, batch loss 1.5346, batch acc 0.8832
17:57:28.326   Training iter 400, batch loss 1.5323, batch acc 0.8872
17:57:28.871   Training iter 450, batch loss 1.5224, batch acc 0.9166
17:57:29.463   Training iter 500, batch loss 1.5272, batch acc 0.9044
17:57:30.090   Training iter 550, batch loss 1.5363, batch acc 0.8908
17:57:30.717   Training iter 600, batch loss 1.5337, batch acc 0.8904
17:57:30.718 Training @ 327 epoch...
17:57:31.340   Training iter 50, batch loss 1.5238, batch acc 0.9152
17:57:31.910   Training iter 100, batch loss 1.5400, batch acc 0.8698
17:57:32.459   Training iter 150, batch loss 1.5378, batch acc 0.8758
17:57:32.996   Training iter 200, batch loss 1.5327, batch acc 0.8904
17:57:33.545   Training iter 250, batch loss 1.5323, batch acc 0.8884
17:57:34.089   Training iter 300, batch loss 1.5331, batch acc 0.8792
17:57:34.631   Training iter 350, batch loss 1.5203, batch acc 0.9114
17:57:35.156   Training iter 400, batch loss 1.5244, batch acc 0.9132
17:57:35.691   Training iter 450, batch loss 1.5274, batch acc 0.9094
17:57:36.250   Training iter 500, batch loss 1.5255, batch acc 0.9124
17:57:36.833   Training iter 550, batch loss 1.5289, batch acc 0.9118
17:57:37.408   Training iter 600, batch loss 1.5226, batch acc 0.9166
17:57:37.410 Training @ 328 epoch...
17:57:37.949   Training iter 50, batch loss 1.5239, batch acc 0.9146
17:57:38.475   Training iter 100, batch loss 1.5327, batch acc 0.8830
17:57:39.011   Training iter 150, batch loss 1.5296, batch acc 0.8868
17:57:39.538   Training iter 200, batch loss 1.5248, batch acc 0.9032
17:57:40.061   Training iter 250, batch loss 1.5337, batch acc 0.8722
17:57:40.585   Training iter 300, batch loss 1.5310, batch acc 0.8808
17:57:41.149   Training iter 350, batch loss 1.5374, batch acc 0.8848
17:57:41.705   Training iter 400, batch loss 1.5330, batch acc 0.8982
17:57:42.229   Training iter 450, batch loss 1.5284, batch acc 0.9054
17:57:42.742   Training iter 500, batch loss 1.5306, batch acc 0.8932
17:57:43.276   Training iter 550, batch loss 1.5233, batch acc 0.8964
17:57:43.800   Training iter 600, batch loss 1.5200, batch acc 0.9136
17:57:43.802 Training @ 329 epoch...
17:57:44.336   Training iter 50, batch loss 1.5226, batch acc 0.9104
17:57:44.857   Training iter 100, batch loss 1.5368, batch acc 0.8840
17:57:45.382   Training iter 150, batch loss 1.5273, batch acc 0.9010
17:57:45.898   Training iter 200, batch loss 1.5462, batch acc 0.8864
17:57:46.422   Training iter 250, batch loss 1.5269, batch acc 0.9120
17:57:46.936   Training iter 300, batch loss 1.5202, batch acc 0.9230
17:57:47.424   Training iter 350, batch loss 1.5265, batch acc 0.9202
17:57:47.906   Training iter 400, batch loss 1.5398, batch acc 0.9048
17:57:48.413   Training iter 450, batch loss 1.5200, batch acc 0.9194
17:57:48.922   Training iter 500, batch loss 1.5236, batch acc 0.8988
17:57:49.441   Training iter 550, batch loss 1.5305, batch acc 0.9004
17:57:49.946   Training iter 600, batch loss 1.5341, batch acc 0.8922
17:57:49.948 Training @ 330 epoch...
17:57:50.502   Training iter 50, batch loss 1.5210, batch acc 0.9220
17:57:51.046   Training iter 100, batch loss 1.5296, batch acc 0.8996
17:57:51.616   Training iter 150, batch loss 1.5470, batch acc 0.8572
17:57:52.168   Training iter 200, batch loss 1.5322, batch acc 0.8728
17:57:52.720   Training iter 250, batch loss 1.5371, batch acc 0.8936
17:57:53.281   Training iter 300, batch loss 1.5284, batch acc 0.8816
17:57:53.835   Training iter 350, batch loss 1.5347, batch acc 0.8920
17:57:54.383   Training iter 400, batch loss 1.5290, batch acc 0.8990
17:57:54.936   Training iter 450, batch loss 1.5334, batch acc 0.8886
17:57:55.494   Training iter 500, batch loss 1.5302, batch acc 0.9022
17:57:56.059   Training iter 550, batch loss 1.5298, batch acc 0.9104
17:57:56.681   Training iter 600, batch loss 1.5177, batch acc 0.9186
17:57:56.682 Testing @ 330 epoch...
17:57:56.734     Testing, total mean loss 1.52316, total acc 0.91670
17:57:56.734 Training @ 331 epoch...
17:57:57.305   Training iter 50, batch loss 1.5192, batch acc 0.9220
17:57:57.822   Training iter 100, batch loss 1.5290, batch acc 0.9150
17:57:58.327   Training iter 150, batch loss 1.5196, batch acc 0.9138
17:57:58.828   Training iter 200, batch loss 1.5260, batch acc 0.9192
17:57:59.355   Training iter 250, batch loss 1.5376, batch acc 0.9080
17:57:59.856   Training iter 300, batch loss 1.5400, batch acc 0.8870
17:58:00.379   Training iter 350, batch loss 1.5376, batch acc 0.8778
17:58:00.898   Training iter 400, batch loss 1.5295, batch acc 0.9072
17:58:01.449   Training iter 450, batch loss 1.5264, batch acc 0.9172
17:58:02.128   Training iter 500, batch loss 1.5197, batch acc 0.9036
17:58:02.727   Training iter 550, batch loss 1.5337, batch acc 0.8762
17:58:03.286   Training iter 600, batch loss 1.5218, batch acc 0.8916
17:58:03.288 Training @ 332 epoch...
17:58:03.827   Training iter 50, batch loss 1.5303, batch acc 0.8938
17:58:04.361   Training iter 100, batch loss 1.5300, batch acc 0.8962
17:58:04.935   Training iter 150, batch loss 1.5475, batch acc 0.8696
17:58:05.507   Training iter 200, batch loss 1.5295, batch acc 0.9070
17:58:06.056   Training iter 250, batch loss 1.5196, batch acc 0.9084
17:58:06.615   Training iter 300, batch loss 1.5236, batch acc 0.9108
17:58:07.155   Training iter 350, batch loss 1.5382, batch acc 0.8768
17:58:07.699   Training iter 400, batch loss 1.5370, batch acc 0.8932
17:58:08.284   Training iter 450, batch loss 1.5162, batch acc 0.9272
17:58:08.858   Training iter 500, batch loss 1.5340, batch acc 0.8992
17:58:09.434   Training iter 550, batch loss 1.5456, batch acc 0.8734
17:58:10.004   Training iter 600, batch loss 1.5285, batch acc 0.9082
17:58:10.006 Training @ 333 epoch...
17:58:10.582   Training iter 50, batch loss 1.5220, batch acc 0.9198
17:58:11.160   Training iter 100, batch loss 1.5248, batch acc 0.9064
17:58:11.731   Training iter 150, batch loss 1.5306, batch acc 0.9062
17:58:12.305   Training iter 200, batch loss 1.5302, batch acc 0.9118
17:58:12.873   Training iter 250, batch loss 1.5313, batch acc 0.8972
17:58:13.433   Training iter 300, batch loss 1.5232, batch acc 0.9176
17:58:13.995   Training iter 350, batch loss 1.5459, batch acc 0.8884
17:58:14.550   Training iter 400, batch loss 1.5353, batch acc 0.8926
17:58:15.104   Training iter 450, batch loss 1.5312, batch acc 0.8976
17:58:15.638   Training iter 500, batch loss 1.5290, batch acc 0.9018
17:58:16.180   Training iter 550, batch loss 1.5329, batch acc 0.8876
17:58:16.721   Training iter 600, batch loss 1.5325, batch acc 0.8980
17:58:16.723 Training @ 334 epoch...
17:58:17.272   Training iter 50, batch loss 1.5389, batch acc 0.8828
17:58:17.804   Training iter 100, batch loss 1.5274, batch acc 0.9090
17:58:18.351   Training iter 150, batch loss 1.5387, batch acc 0.9018
17:58:18.942   Training iter 200, batch loss 1.5379, batch acc 0.8794
17:58:19.529   Training iter 250, batch loss 1.5162, batch acc 0.9216
17:58:20.087   Training iter 300, batch loss 1.5230, batch acc 0.9066
17:58:20.624   Training iter 350, batch loss 1.5228, batch acc 0.9162
17:58:21.177   Training iter 400, batch loss 1.5272, batch acc 0.8980
17:58:21.687   Training iter 450, batch loss 1.5213, batch acc 0.9162
17:58:22.218   Training iter 500, batch loss 1.5172, batch acc 0.9240
17:58:22.762   Training iter 550, batch loss 1.5280, batch acc 0.9114
17:58:23.300   Training iter 600, batch loss 1.5430, batch acc 0.8946
17:58:23.302 Training @ 335 epoch...
17:58:23.831   Training iter 50, batch loss 1.5342, batch acc 0.9000
17:58:24.377   Training iter 100, batch loss 1.5290, batch acc 0.9016
17:58:24.942   Training iter 150, batch loss 1.5319, batch acc 0.8984
17:58:25.523   Training iter 200, batch loss 1.5240, batch acc 0.9006
17:58:26.099   Training iter 250, batch loss 1.5494, batch acc 0.8692
17:58:26.673   Training iter 300, batch loss 1.5318, batch acc 0.8926
17:58:27.252   Training iter 350, batch loss 1.5198, batch acc 0.9240
17:58:27.830   Training iter 400, batch loss 1.5300, batch acc 0.9022
17:58:28.392   Training iter 450, batch loss 1.5397, batch acc 0.8990
17:58:28.959   Training iter 500, batch loss 1.5241, batch acc 0.9208
17:58:29.529   Training iter 550, batch loss 1.5182, batch acc 0.9184
17:58:30.078   Training iter 600, batch loss 1.5230, batch acc 0.9076
17:58:30.080 Testing @ 335 epoch...
17:58:30.128     Testing, total mean loss 1.52250, total acc 0.91350
17:58:30.128 Training @ 336 epoch...
17:58:30.637   Training iter 50, batch loss 1.5246, batch acc 0.9070
17:58:31.149   Training iter 100, batch loss 1.5319, batch acc 0.8922
17:58:31.657   Training iter 150, batch loss 1.5335, batch acc 0.8868
17:58:32.170   Training iter 200, batch loss 1.5204, batch acc 0.9100
17:58:32.678   Training iter 250, batch loss 1.5325, batch acc 0.9108
17:58:33.187   Training iter 300, batch loss 1.5282, batch acc 0.9158
17:58:33.691   Training iter 350, batch loss 1.5209, batch acc 0.9162
17:58:34.200   Training iter 400, batch loss 1.5388, batch acc 0.8780
17:58:34.709   Training iter 450, batch loss 1.5395, batch acc 0.8714
17:58:35.214   Training iter 500, batch loss 1.5367, batch acc 0.8876
17:58:35.729   Training iter 550, batch loss 1.5327, batch acc 0.8862
17:58:36.236   Training iter 600, batch loss 1.5214, batch acc 0.8982
17:58:36.238 Training @ 337 epoch...
17:58:36.745   Training iter 50, batch loss 1.5242, batch acc 0.8996
17:58:37.271   Training iter 100, batch loss 1.5199, batch acc 0.9136
17:58:37.792   Training iter 150, batch loss 1.5242, batch acc 0.9150
17:58:38.315   Training iter 200, batch loss 1.5326, batch acc 0.9020
17:58:38.826   Training iter 250, batch loss 1.5225, batch acc 0.9198
17:58:39.358   Training iter 300, batch loss 1.5516, batch acc 0.8682
17:58:39.873   Training iter 350, batch loss 1.5339, batch acc 0.8930
17:58:40.427   Training iter 400, batch loss 1.5215, batch acc 0.9100
17:58:40.980   Training iter 450, batch loss 1.5200, batch acc 0.9134
17:58:41.536   Training iter 500, batch loss 1.5291, batch acc 0.9094
17:58:42.091   Training iter 550, batch loss 1.5305, batch acc 0.8962
17:58:42.649   Training iter 600, batch loss 1.5295, batch acc 0.8992
17:58:42.651 Training @ 338 epoch...
17:58:43.229   Training iter 50, batch loss 1.5275, batch acc 0.9094
17:58:43.789   Training iter 100, batch loss 1.5229, batch acc 0.9166
17:58:44.353   Training iter 150, batch loss 1.5311, batch acc 0.9022
17:58:44.885   Training iter 200, batch loss 1.5438, batch acc 0.8842
17:58:45.414   Training iter 250, batch loss 1.5281, batch acc 0.9142
17:58:45.943   Training iter 300, batch loss 1.5383, batch acc 0.8934
17:58:46.487   Training iter 350, batch loss 1.5286, batch acc 0.9100
17:58:47.009   Training iter 400, batch loss 1.5310, batch acc 0.9012
17:58:47.509   Training iter 450, batch loss 1.5340, batch acc 0.8724
17:58:47.986   Training iter 500, batch loss 1.5322, batch acc 0.8864
17:58:48.468   Training iter 550, batch loss 1.5328, batch acc 0.9014
17:58:48.943   Training iter 600, batch loss 1.5190, batch acc 0.9166
17:58:48.944 Training @ 339 epoch...
17:58:49.428   Training iter 50, batch loss 1.5331, batch acc 0.8860
17:58:49.912   Training iter 100, batch loss 1.5499, batch acc 0.8584
17:58:50.423   Training iter 150, batch loss 1.5344, batch acc 0.8988
17:58:50.943   Training iter 200, batch loss 1.5391, batch acc 0.9016
17:58:51.473   Training iter 250, batch loss 1.5264, batch acc 0.9190
17:58:52.005   Training iter 300, batch loss 1.5251, batch acc 0.9124
17:58:52.548   Training iter 350, batch loss 1.5311, batch acc 0.8916
17:58:53.086   Training iter 400, batch loss 1.5418, batch acc 0.8850
17:58:53.614   Training iter 450, batch loss 1.5374, batch acc 0.8860
17:58:54.135   Training iter 500, batch loss 1.5326, batch acc 0.8974
17:58:54.665   Training iter 550, batch loss 1.5281, batch acc 0.9042
17:58:55.192   Training iter 600, batch loss 1.5772, batch acc 0.8562
17:58:55.194 Training @ 340 epoch...
17:58:55.726   Training iter 50, batch loss 1.5358, batch acc 0.9104
17:58:56.264   Training iter 100, batch loss 1.5286, batch acc 0.8960
17:58:56.834   Training iter 150, batch loss 1.5222, batch acc 0.9094
17:58:57.405   Training iter 200, batch loss 1.5203, batch acc 0.9160
17:58:57.974   Training iter 250, batch loss 1.5359, batch acc 0.9082
17:58:58.552   Training iter 300, batch loss 1.5410, batch acc 0.8884
17:58:59.120   Training iter 350, batch loss 1.5214, batch acc 0.9104
17:58:59.688   Training iter 400, batch loss 1.5258, batch acc 0.9098
17:59:00.248   Training iter 450, batch loss 1.5308, batch acc 0.9078
17:59:00.808   Training iter 500, batch loss 1.5232, batch acc 0.9018
17:59:01.378   Training iter 550, batch loss 1.5280, batch acc 0.8838
17:59:01.967   Training iter 600, batch loss 1.5273, batch acc 0.8846
17:59:01.969 Testing @ 340 epoch...
17:59:02.019     Testing, total mean loss 1.53084, total acc 0.89690
17:59:02.019 Training @ 341 epoch...
17:59:02.602   Training iter 50, batch loss 1.5193, batch acc 0.9118
17:59:03.160   Training iter 100, batch loss 1.5336, batch acc 0.8664
17:59:03.714   Training iter 150, batch loss 1.5331, batch acc 0.8956
17:59:04.282   Training iter 200, batch loss 1.5360, batch acc 0.9122
17:59:04.859   Training iter 250, batch loss 1.5234, batch acc 0.9092
17:59:05.445   Training iter 300, batch loss 1.5235, batch acc 0.9070
17:59:06.014   Training iter 350, batch loss 1.5359, batch acc 0.8864
17:59:06.573   Training iter 400, batch loss 1.5331, batch acc 0.8998
17:59:07.112   Training iter 450, batch loss 1.5316, batch acc 0.8626
17:59:07.667   Training iter 500, batch loss 1.5497, batch acc 0.8378
17:59:08.212   Training iter 550, batch loss 1.5321, batch acc 0.8718
17:59:08.741   Training iter 600, batch loss 1.5277, batch acc 0.8840
17:59:08.742 Training @ 342 epoch...
17:59:09.275   Training iter 50, batch loss 1.5219, batch acc 0.9110
17:59:09.802   Training iter 100, batch loss 1.5306, batch acc 0.8936
17:59:10.351   Training iter 150, batch loss 1.5316, batch acc 0.9090
17:59:10.892   Training iter 200, batch loss 1.5249, batch acc 0.9144
17:59:11.416   Training iter 250, batch loss 1.5348, batch acc 0.8844
17:59:11.927   Training iter 300, batch loss 1.5291, batch acc 0.9076
17:59:12.472   Training iter 350, batch loss 1.5356, batch acc 0.8856
17:59:13.017   Training iter 400, batch loss 1.5241, batch acc 0.9076
17:59:13.542   Training iter 450, batch loss 1.5231, batch acc 0.9150
17:59:14.070   Training iter 500, batch loss 1.5380, batch acc 0.8984
17:59:14.650   Training iter 550, batch loss 1.5271, batch acc 0.9146
17:59:15.248   Training iter 600, batch loss 1.5422, batch acc 0.8816
17:59:15.249 Training @ 343 epoch...
17:59:15.844   Training iter 50, batch loss 1.5300, batch acc 0.9044
17:59:16.411   Training iter 100, batch loss 1.5341, batch acc 0.8966
17:59:16.963   Training iter 150, batch loss 1.5551, batch acc 0.8764
17:59:17.521   Training iter 200, batch loss 1.5316, batch acc 0.8928
17:59:18.071   Training iter 250, batch loss 1.5388, batch acc 0.8864
17:59:18.609   Training iter 300, batch loss 1.5410, batch acc 0.8912
17:59:19.157   Training iter 350, batch loss 1.5269, batch acc 0.9060
17:59:19.713   Training iter 400, batch loss 1.5267, batch acc 0.8998
17:59:20.266   Training iter 450, batch loss 1.5213, batch acc 0.9194
17:59:20.804   Training iter 500, batch loss 1.5326, batch acc 0.9022
17:59:21.356   Training iter 550, batch loss 1.5286, batch acc 0.9190
17:59:21.909   Training iter 600, batch loss 1.5389, batch acc 0.8950
17:59:21.911 Training @ 344 epoch...
17:59:22.461   Training iter 50, batch loss 1.5292, batch acc 0.9036
17:59:22.996   Training iter 100, batch loss 1.5384, batch acc 0.8884
17:59:23.518   Training iter 150, batch loss 1.5295, batch acc 0.9004
17:59:24.054   Training iter 200, batch loss 1.5222, batch acc 0.9180
17:59:24.597   Training iter 250, batch loss 1.5329, batch acc 0.9070
17:59:25.132   Training iter 300, batch loss 1.5506, batch acc 0.8446
17:59:25.677   Training iter 350, batch loss 1.5404, batch acc 0.8740
17:59:26.231   Training iter 400, batch loss 1.5228, batch acc 0.9108
17:59:26.784   Training iter 450, batch loss 1.5222, batch acc 0.9208
17:59:27.342   Training iter 500, batch loss 1.5362, batch acc 0.9140
17:59:27.888   Training iter 550, batch loss 1.5296, batch acc 0.9124
17:59:28.431   Training iter 600, batch loss 1.5331, batch acc 0.9202
17:59:28.433 Training @ 345 epoch...
17:59:28.989   Training iter 50, batch loss 1.5319, batch acc 0.8984
17:59:29.558   Training iter 100, batch loss 1.5183, batch acc 0.9076
17:59:30.116   Training iter 150, batch loss 1.5298, batch acc 0.9074
17:59:30.666   Training iter 200, batch loss 1.5332, batch acc 0.8986
17:59:31.237   Training iter 250, batch loss 1.5217, batch acc 0.9060
17:59:31.791   Training iter 300, batch loss 1.5276, batch acc 0.9056
17:59:32.358   Training iter 350, batch loss 1.5282, batch acc 0.9102
17:59:32.918   Training iter 400, batch loss 1.5308, batch acc 0.9018
17:59:33.467   Training iter 450, batch loss 1.5368, batch acc 0.8884
17:59:34.001   Training iter 500, batch loss 1.5324, batch acc 0.8890
17:59:34.553   Training iter 550, batch loss 1.5221, batch acc 0.9112
17:59:35.095   Training iter 600, batch loss 1.5328, batch acc 0.8976
17:59:35.097 Testing @ 345 epoch...
17:59:35.148     Testing, total mean loss 1.53035, total acc 0.89230
17:59:35.148 Training @ 346 epoch...
17:59:35.710   Training iter 50, batch loss 1.5288, batch acc 0.8910
17:59:36.263   Training iter 100, batch loss 1.5286, batch acc 0.8960
17:59:36.817   Training iter 150, batch loss 1.5295, batch acc 0.9046
17:59:37.362   Training iter 200, batch loss 1.5471, batch acc 0.8646
17:59:37.918   Training iter 250, batch loss 1.5285, batch acc 0.9116
17:59:38.448   Training iter 300, batch loss 1.5363, batch acc 0.9034
17:59:38.975   Training iter 350, batch loss 1.5280, batch acc 0.9014
17:59:39.514   Training iter 400, batch loss 1.5216, batch acc 0.9114
17:59:40.026   Training iter 450, batch loss 1.5219, batch acc 0.9018
17:59:40.565   Training iter 500, batch loss 1.5223, batch acc 0.9076
17:59:41.082   Training iter 550, batch loss 1.5233, batch acc 0.9188
17:59:41.618   Training iter 600, batch loss 1.5234, batch acc 0.9146
17:59:41.620 Training @ 347 epoch...
17:59:42.163   Training iter 50, batch loss 1.5377, batch acc 0.9020
17:59:42.711   Training iter 100, batch loss 1.5312, batch acc 0.8890
17:59:43.256   Training iter 150, batch loss 1.5341, batch acc 0.9116
17:59:43.793   Training iter 200, batch loss 1.5208, batch acc 0.9248
17:59:44.344   Training iter 250, batch loss 1.5301, batch acc 0.9082
17:59:44.886   Training iter 300, batch loss 1.5261, batch acc 0.9172
17:59:45.434   Training iter 350, batch loss 1.5242, batch acc 0.9074
17:59:45.986   Training iter 400, batch loss 1.5468, batch acc 0.9008
17:59:46.538   Training iter 450, batch loss 1.5336, batch acc 0.9082
17:59:47.067   Training iter 500, batch loss 1.5269, batch acc 0.9192
17:59:47.599   Training iter 550, batch loss 1.5279, batch acc 0.9026
17:59:48.165   Training iter 600, batch loss 1.5505, batch acc 0.8834
17:59:48.167 Training @ 348 epoch...
17:59:48.715   Training iter 50, batch loss 1.5315, batch acc 0.9014
17:59:49.250   Training iter 100, batch loss 1.5255, batch acc 0.9134
17:59:49.796   Training iter 150, batch loss 1.5222, batch acc 0.9206
17:59:50.349   Training iter 200, batch loss 1.5293, batch acc 0.9126
17:59:50.923   Training iter 250, batch loss 1.5244, batch acc 0.8984
17:59:51.468   Training iter 300, batch loss 1.5361, batch acc 0.8868
17:59:52.015   Training iter 350, batch loss 1.5337, batch acc 0.9058
17:59:52.578   Training iter 400, batch loss 1.5316, batch acc 0.8980
17:59:53.114   Training iter 450, batch loss 1.5237, batch acc 0.9202
17:59:53.635   Training iter 500, batch loss 1.5293, batch acc 0.9132
17:59:54.162   Training iter 550, batch loss 1.5282, batch acc 0.9072
17:59:54.657   Training iter 600, batch loss 1.5340, batch acc 0.9082
17:59:54.659 Training @ 349 epoch...
17:59:55.162   Training iter 50, batch loss 1.5325, batch acc 0.8982
17:59:55.669   Training iter 100, batch loss 1.5264, batch acc 0.9048
17:59:56.182   Training iter 150, batch loss 1.5356, batch acc 0.9012
17:59:56.699   Training iter 200, batch loss 1.5252, batch acc 0.9114
17:59:57.251   Training iter 250, batch loss 1.5291, batch acc 0.9094
17:59:57.811   Training iter 300, batch loss 1.5411, batch acc 0.8950
17:59:58.359   Training iter 350, batch loss 1.5344, batch acc 0.8992
17:59:58.912   Training iter 400, batch loss 1.5242, batch acc 0.9112
17:59:59.468   Training iter 450, batch loss 1.5266, batch acc 0.9214
18:00:00.043   Training iter 500, batch loss 1.5644, batch acc 0.8460
18:00:00.637   Training iter 550, batch loss 1.5304, batch acc 0.9042
18:00:01.231   Training iter 600, batch loss 1.5375, batch acc 0.8856
18:00:01.233 Training @ 350 epoch...
18:00:01.841   Training iter 50, batch loss 1.5360, batch acc 0.9038
18:00:02.431   Training iter 100, batch loss 1.5292, batch acc 0.9104
18:00:03.003   Training iter 150, batch loss 1.5279, batch acc 0.9156
18:00:03.586   Training iter 200, batch loss 1.5329, batch acc 0.9082
18:00:04.167   Training iter 250, batch loss 1.5335, batch acc 0.9102
18:00:04.739   Training iter 300, batch loss 1.5371, batch acc 0.9042
18:00:05.321   Training iter 350, batch loss 1.5315, batch acc 0.9030
18:00:05.903   Training iter 400, batch loss 1.5253, batch acc 0.9068
18:00:06.464   Training iter 450, batch loss 1.5466, batch acc 0.8702
18:00:07.022   Training iter 500, batch loss 1.5353, batch acc 0.8956
18:00:07.574   Training iter 550, batch loss 1.5288, batch acc 0.9010
18:00:08.129   Training iter 600, batch loss 1.5319, batch acc 0.8866
18:00:08.131 Testing @ 350 epoch...
18:00:08.180     Testing, total mean loss 1.52579, total acc 0.91050
18:00:08.180 Training @ 351 epoch...
18:00:08.733   Training iter 50, batch loss 1.5575, batch acc 0.8658
18:00:09.289   Training iter 100, batch loss 1.5217, batch acc 0.9034
18:00:09.888   Training iter 150, batch loss 1.5338, batch acc 0.9106
18:00:10.479   Training iter 200, batch loss 1.5255, batch acc 0.9106
18:00:11.087   Training iter 250, batch loss 1.5268, batch acc 0.8992
18:00:11.638   Training iter 300, batch loss 1.5290, batch acc 0.9134
18:00:12.198   Training iter 350, batch loss 1.5282, batch acc 0.9134
18:00:12.739   Training iter 400, batch loss 1.5306, batch acc 0.9016
18:00:13.300   Training iter 450, batch loss 1.5246, batch acc 0.9108
18:00:13.862   Training iter 500, batch loss 1.5188, batch acc 0.9182
18:00:14.459   Training iter 550, batch loss 1.5418, batch acc 0.8832
18:00:15.007   Training iter 600, batch loss 1.5221, batch acc 0.9200
18:00:15.009 Training @ 352 epoch...
18:00:15.546   Training iter 50, batch loss 1.5289, batch acc 0.9186
18:00:16.078   Training iter 100, batch loss 1.5364, batch acc 0.8918
18:00:16.612   Training iter 150, batch loss 1.5285, batch acc 0.9034
18:00:17.144   Training iter 200, batch loss 1.5214, batch acc 0.9110
18:00:17.672   Training iter 250, batch loss 1.5173, batch acc 0.9092
18:00:18.206   Training iter 300, batch loss 1.5278, batch acc 0.8900
18:00:18.718   Training iter 350, batch loss 1.5250, batch acc 0.8970
18:00:19.205   Training iter 400, batch loss 1.5238, batch acc 0.9094
18:00:19.694   Training iter 450, batch loss 1.5226, batch acc 0.9100
18:00:20.213   Training iter 500, batch loss 1.5287, batch acc 0.8956
18:00:20.742   Training iter 550, batch loss 1.5383, batch acc 0.8868
18:00:21.264   Training iter 600, batch loss 1.5258, batch acc 0.9006
18:00:21.266 Training @ 353 epoch...
18:00:21.793   Training iter 50, batch loss 1.5249, batch acc 0.9116
18:00:22.318   Training iter 100, batch loss 1.5239, batch acc 0.9142
18:00:22.838   Training iter 150, batch loss 1.5239, batch acc 0.9148
18:00:23.361   Training iter 200, batch loss 1.5328, batch acc 0.9068
18:00:23.879   Training iter 250, batch loss 1.5271, batch acc 0.9082
18:00:24.415   Training iter 300, batch loss 1.5499, batch acc 0.8914
18:00:24.931   Training iter 350, batch loss 1.5261, batch acc 0.9168
18:00:25.462   Training iter 400, batch loss 1.5286, batch acc 0.9060
18:00:25.984   Training iter 450, batch loss 1.5302, batch acc 0.8952
18:00:26.507   Training iter 500, batch loss 1.5209, batch acc 0.9102
18:00:27.026   Training iter 550, batch loss 1.5381, batch acc 0.8968
18:00:27.560   Training iter 600, batch loss 1.5199, batch acc 0.9084
18:00:27.562 Training @ 354 epoch...
18:00:28.113   Training iter 50, batch loss 1.5445, batch acc 0.8548
18:00:28.648   Training iter 100, batch loss 1.5361, batch acc 0.8774
18:00:29.185   Training iter 150, batch loss 1.5216, batch acc 0.9118
18:00:29.718   Training iter 200, batch loss 1.5277, batch acc 0.8994
18:00:30.250   Training iter 250, batch loss 1.5287, batch acc 0.9026
18:00:30.775   Training iter 300, batch loss 1.5377, batch acc 0.8902
18:00:31.298   Training iter 350, batch loss 1.5276, batch acc 0.9136
18:00:31.828   Training iter 400, batch loss 1.5336, batch acc 0.8870
18:00:32.354   Training iter 450, batch loss 1.5242, batch acc 0.8984
18:00:32.889   Training iter 500, batch loss 1.5411, batch acc 0.8680
18:00:33.424   Training iter 550, batch loss 1.5355, batch acc 0.9024
18:00:33.964   Training iter 600, batch loss 1.5235, batch acc 0.9158
18:00:33.966 Training @ 355 epoch...
18:00:34.533   Training iter 50, batch loss 1.5220, batch acc 0.9076
18:00:35.092   Training iter 100, batch loss 1.5470, batch acc 0.8776
18:00:35.637   Training iter 150, batch loss 1.5405, batch acc 0.8902
18:00:36.184   Training iter 200, batch loss 1.5259, batch acc 0.9002
18:00:36.750   Training iter 250, batch loss 1.5264, batch acc 0.8988
18:00:37.308   Training iter 300, batch loss 1.5226, batch acc 0.9150
18:00:37.846   Training iter 350, batch loss 1.5151, batch acc 0.9246
18:00:38.352   Training iter 400, batch loss 1.5312, batch acc 0.8874
18:00:38.833   Training iter 450, batch loss 1.5307, batch acc 0.8948
18:00:39.316   Training iter 500, batch loss 1.5400, batch acc 0.8596
18:00:39.778   Training iter 550, batch loss 1.5213, batch acc 0.9070
18:00:40.256   Training iter 600, batch loss 1.5228, batch acc 0.9230
18:00:40.258 Testing @ 355 epoch...
18:00:40.305     Testing, total mean loss 1.52467, total acc 0.91630
18:00:40.305 Training @ 356 epoch...
18:00:40.820   Training iter 50, batch loss 1.5271, batch acc 0.9142
18:00:41.349   Training iter 100, batch loss 1.5195, batch acc 0.9296
18:00:41.867   Training iter 150, batch loss 1.5272, batch acc 0.9158
18:00:42.386   Training iter 200, batch loss 1.5239, batch acc 0.9164
18:00:42.915   Training iter 250, batch loss 1.5272, batch acc 0.9026
18:00:43.436   Training iter 300, batch loss 1.5263, batch acc 0.9138
18:00:43.963   Training iter 350, batch loss 1.5705, batch acc 0.8738
18:00:44.497   Training iter 400, batch loss 1.5340, batch acc 0.9042
18:00:45.023   Training iter 450, batch loss 1.5349, batch acc 0.8886
18:00:45.545   Training iter 500, batch loss 1.5302, batch acc 0.9066
18:00:46.087   Training iter 550, batch loss 1.5387, batch acc 0.9024
18:00:46.616   Training iter 600, batch loss 1.5360, batch acc 0.9052
18:00:46.617 Training @ 357 epoch...
18:00:47.114   Training iter 50, batch loss 1.5375, batch acc 0.8974
18:00:47.627   Training iter 100, batch loss 1.5331, batch acc 0.8988
18:00:48.147   Training iter 150, batch loss 1.5356, batch acc 0.9002
18:00:48.658   Training iter 200, batch loss 1.5279, batch acc 0.9096
18:00:49.170   Training iter 250, batch loss 1.5398, batch acc 0.8712
18:00:49.690   Training iter 300, batch loss 1.5270, batch acc 0.9022
18:00:50.197   Training iter 350, batch loss 1.5313, batch acc 0.8922
18:00:50.711   Training iter 400, batch loss 1.5229, batch acc 0.9094
18:00:51.236   Training iter 450, batch loss 1.5250, batch acc 0.9070
18:00:51.769   Training iter 500, batch loss 1.5394, batch acc 0.8772
18:00:52.319   Training iter 550, batch loss 1.5287, batch acc 0.8916
18:00:52.822   Training iter 600, batch loss 1.5276, batch acc 0.9000
18:00:52.824 Training @ 358 epoch...
18:00:53.324   Training iter 50, batch loss 1.5210, batch acc 0.9186
18:00:53.813   Training iter 100, batch loss 1.5282, batch acc 0.9136
18:00:54.312   Training iter 150, batch loss 1.5200, batch acc 0.9208
18:00:54.806   Training iter 200, batch loss 1.5321, batch acc 0.8896
18:00:55.318   Training iter 250, batch loss 1.5440, batch acc 0.8738
18:00:55.825   Training iter 300, batch loss 1.5278, batch acc 0.9060
18:00:56.335   Training iter 350, batch loss 1.5238, batch acc 0.9170
18:00:56.916   Training iter 400, batch loss 1.5238, batch acc 0.9190
18:00:57.457   Training iter 450, batch loss 1.5287, batch acc 0.9036
18:00:58.085   Training iter 500, batch loss 1.5400, batch acc 0.9032
18:00:58.641   Training iter 550, batch loss 1.5317, batch acc 0.8882
18:00:59.229   Training iter 600, batch loss 1.5309, batch acc 0.9008
18:00:59.231 Training @ 359 epoch...
18:00:59.826   Training iter 50, batch loss 1.5313, batch acc 0.9064
18:01:00.364   Training iter 100, batch loss 1.5303, batch acc 0.9120
18:01:00.907   Training iter 150, batch loss 1.5232, batch acc 0.9112
18:01:01.484   Training iter 200, batch loss 1.5244, batch acc 0.9118
18:01:02.062   Training iter 250, batch loss 1.5294, batch acc 0.8838
18:01:02.629   Training iter 300, batch loss 1.5385, batch acc 0.8944
18:01:03.189   Training iter 350, batch loss 1.5320, batch acc 0.8986
18:01:03.743   Training iter 400, batch loss 1.5269, batch acc 0.8990
18:01:04.295   Training iter 450, batch loss 1.5202, batch acc 0.9186
18:01:04.860   Training iter 500, batch loss 1.5289, batch acc 0.9020
18:01:05.425   Training iter 550, batch loss 1.5275, batch acc 0.8982
18:01:05.973   Training iter 600, batch loss 1.5189, batch acc 0.9158
18:01:05.975 Training @ 360 epoch...
18:01:06.540   Training iter 50, batch loss 1.5219, batch acc 0.9204
18:01:07.086   Training iter 100, batch loss 1.5303, batch acc 0.9088
18:01:07.594   Training iter 150, batch loss 1.5297, batch acc 0.9102
18:01:08.121   Training iter 200, batch loss 1.5668, batch acc 0.8658
18:01:08.620   Training iter 250, batch loss 1.5459, batch acc 0.8716
18:01:09.114   Training iter 300, batch loss 1.5237, batch acc 0.9072
18:01:09.611   Training iter 350, batch loss 1.5277, batch acc 0.8884
18:01:10.090   Training iter 400, batch loss 1.5297, batch acc 0.8810
18:01:10.576   Training iter 450, batch loss 1.5237, batch acc 0.8992
18:01:11.082   Training iter 500, batch loss 1.5312, batch acc 0.8844
18:01:11.589   Training iter 550, batch loss 1.5341, batch acc 0.8766
18:01:12.077   Training iter 600, batch loss 1.5364, batch acc 0.8652
18:01:12.078 Testing @ 360 epoch...
18:01:12.127     Testing, total mean loss 1.52849, total acc 0.88910
18:01:12.127 Training @ 361 epoch...
18:01:12.633   Training iter 50, batch loss 1.5390, batch acc 0.8844
18:01:13.143   Training iter 100, batch loss 1.5439, batch acc 0.8676
18:01:13.653   Training iter 150, batch loss 1.5462, batch acc 0.8656
18:01:14.164   Training iter 200, batch loss 1.5310, batch acc 0.9042
18:01:14.678   Training iter 250, batch loss 1.5293, batch acc 0.8976
18:01:15.191   Training iter 300, batch loss 1.5369, batch acc 0.8748
18:01:15.706   Training iter 350, batch loss 1.5259, batch acc 0.9036
18:01:16.231   Training iter 400, batch loss 1.5223, batch acc 0.9158
18:01:16.786   Training iter 450, batch loss 1.5251, batch acc 0.9100
18:01:17.313   Training iter 500, batch loss 1.5169, batch acc 0.9150
18:01:17.823   Training iter 550, batch loss 1.5194, batch acc 0.9088
18:01:18.351   Training iter 600, batch loss 1.5276, batch acc 0.9088
18:01:18.353 Training @ 362 epoch...
18:01:18.869   Training iter 50, batch loss 1.5267, batch acc 0.8976
18:01:19.383   Training iter 100, batch loss 1.5174, batch acc 0.9136
18:01:19.897   Training iter 150, batch loss 1.5424, batch acc 0.8620
18:01:20.498   Training iter 200, batch loss 1.5208, batch acc 0.9168
18:01:21.018   Training iter 250, batch loss 1.5289, batch acc 0.8994
18:01:21.543   Training iter 300, batch loss 1.5225, batch acc 0.9118
18:01:22.066   Training iter 350, batch loss 1.5402, batch acc 0.8868
18:01:22.576   Training iter 400, batch loss 1.5324, batch acc 0.8978
18:01:23.087   Training iter 450, batch loss 1.5261, batch acc 0.9092
18:01:23.617   Training iter 500, batch loss 1.5288, batch acc 0.9012
18:01:24.129   Training iter 550, batch loss 1.5306, batch acc 0.8904
18:01:24.654   Training iter 600, batch loss 1.5438, batch acc 0.8916
18:01:24.656 Training @ 363 epoch...
18:01:25.179   Training iter 50, batch loss 1.5283, batch acc 0.9096
18:01:25.695   Training iter 100, batch loss 1.5300, batch acc 0.8996
18:01:26.243   Training iter 150, batch loss 1.5283, batch acc 0.8760
18:01:26.788   Training iter 200, batch loss 1.5314, batch acc 0.8870
18:01:27.336   Training iter 250, batch loss 1.5320, batch acc 0.8846
18:01:27.879   Training iter 300, batch loss 1.5291, batch acc 0.8952
18:01:28.428   Training iter 350, batch loss 1.5337, batch acc 0.8872
18:01:28.968   Training iter 400, batch loss 1.5382, batch acc 0.8880
18:01:29.512   Training iter 450, batch loss 1.5262, batch acc 0.8832
18:01:30.049   Training iter 500, batch loss 1.5237, batch acc 0.9044
18:01:30.618   Training iter 550, batch loss 1.5452, batch acc 0.8492
18:01:31.364   Training iter 600, batch loss 1.5700, batch acc 0.8576
18:01:31.366 Training @ 364 epoch...
18:01:32.141   Training iter 50, batch loss 1.5352, batch acc 0.8858
18:01:32.824   Training iter 100, batch loss 1.5286, batch acc 0.8850
18:01:33.391   Training iter 150, batch loss 1.5246, batch acc 0.9170
18:01:33.986   Training iter 200, batch loss 1.5330, batch acc 0.8564
18:01:34.554   Training iter 250, batch loss 1.5385, batch acc 0.8746
18:01:35.093   Training iter 300, batch loss 1.5298, batch acc 0.8832
18:01:35.616   Training iter 350, batch loss 1.5319, batch acc 0.8916
18:01:36.164   Training iter 400, batch loss 1.5341, batch acc 0.8776
18:01:36.701   Training iter 450, batch loss 1.5272, batch acc 0.8922
18:01:37.244   Training iter 500, batch loss 1.5262, batch acc 0.9032
18:01:37.781   Training iter 550, batch loss 1.5198, batch acc 0.9190
18:01:38.324   Training iter 600, batch loss 1.5306, batch acc 0.9012
18:01:38.326 Training @ 365 epoch...
18:01:38.873   Training iter 50, batch loss 1.5273, batch acc 0.9138
18:01:39.439   Training iter 100, batch loss 1.5318, batch acc 0.8970
18:01:39.985   Training iter 150, batch loss 1.5297, batch acc 0.9080
18:01:40.538   Training iter 200, batch loss 1.5260, batch acc 0.9136
18:01:41.086   Training iter 250, batch loss 1.5333, batch acc 0.9180
18:01:41.645   Training iter 300, batch loss 1.5338, batch acc 0.9142
18:01:42.171   Training iter 350, batch loss 1.5307, batch acc 0.9096
18:01:42.699   Training iter 400, batch loss 1.5212, batch acc 0.9136
18:01:43.236   Training iter 450, batch loss 1.5260, batch acc 0.9136
18:01:43.758   Training iter 500, batch loss 1.5314, batch acc 0.9104
18:01:44.300   Training iter 550, batch loss 1.5298, batch acc 0.9058
18:01:44.815   Training iter 600, batch loss 1.5323, batch acc 0.8866
18:01:44.817 Testing @ 365 epoch...
18:01:44.865     Testing, total mean loss 1.52649, total acc 0.90230
18:01:44.865 Training @ 366 epoch...
18:01:45.357   Training iter 50, batch loss 1.5233, batch acc 0.9182
18:01:45.870   Training iter 100, batch loss 1.5320, batch acc 0.8926
18:01:46.413   Training iter 150, batch loss 1.5315, batch acc 0.8996
18:01:46.948   Training iter 200, batch loss 1.5194, batch acc 0.9172
18:01:47.482   Training iter 250, batch loss 1.5324, batch acc 0.8920
18:01:48.013   Training iter 300, batch loss 1.5285, batch acc 0.8950
18:01:48.545   Training iter 350, batch loss 1.5238, batch acc 0.9172
18:01:49.058   Training iter 400, batch loss 1.5210, batch acc 0.9138
18:01:49.598   Training iter 450, batch loss 1.5341, batch acc 0.8910
18:01:50.123   Training iter 500, batch loss 1.5297, batch acc 0.9064
18:01:50.652   Training iter 550, batch loss 1.5334, batch acc 0.8900
18:01:51.202   Training iter 600, batch loss 1.5271, batch acc 0.8978
18:01:51.204 Training @ 367 epoch...
18:01:51.746   Training iter 50, batch loss 1.5179, batch acc 0.9126
18:01:52.308   Training iter 100, batch loss 1.5322, batch acc 0.9114
18:01:52.861   Training iter 150, batch loss 1.5262, batch acc 0.9104
18:01:53.414   Training iter 200, batch loss 1.5260, batch acc 0.9014
18:01:53.959   Training iter 250, batch loss 1.5252, batch acc 0.8972
18:01:54.524   Training iter 300, batch loss 1.5368, batch acc 0.8832
18:01:55.089   Training iter 350, batch loss 1.5214, batch acc 0.9072
18:01:55.644   Training iter 400, batch loss 1.5383, batch acc 0.8814
18:01:56.198   Training iter 450, batch loss 1.5270, batch acc 0.8882
18:01:56.741   Training iter 500, batch loss 1.5286, batch acc 0.9004
18:01:57.287   Training iter 550, batch loss 1.5323, batch acc 0.8820
18:01:57.827   Training iter 600, batch loss 1.5357, batch acc 0.8574
18:01:57.828 Training @ 368 epoch...
18:01:58.327   Training iter 50, batch loss 1.5336, batch acc 0.8710
18:01:58.817   Training iter 100, batch loss 1.5501, batch acc 0.8550
18:01:59.340   Training iter 150, batch loss 1.5464, batch acc 0.8610
18:01:59.883   Training iter 200, batch loss 1.5177, batch acc 0.9112
18:02:00.429   Training iter 250, batch loss 1.5392, batch acc 0.8928
18:02:00.966   Training iter 300, batch loss 1.5415, batch acc 0.9000
18:02:01.535   Training iter 350, batch loss 1.5249, batch acc 0.9088
18:02:02.113   Training iter 400, batch loss 1.5233, batch acc 0.9136
18:02:02.667   Training iter 450, batch loss 1.5275, batch acc 0.9006
18:02:03.234   Training iter 500, batch loss 1.5273, batch acc 0.9028
18:02:03.818   Training iter 550, batch loss 1.5227, batch acc 0.8946
18:02:04.453   Training iter 600, batch loss 1.5301, batch acc 0.8778
18:02:04.455 Training @ 369 epoch...
18:02:05.043   Training iter 50, batch loss 1.5477, batch acc 0.8338
18:02:05.600   Training iter 100, batch loss 1.5408, batch acc 0.8694
18:02:06.171   Training iter 150, batch loss 1.5358, batch acc 0.8798
18:02:06.742   Training iter 200, batch loss 1.5264, batch acc 0.9102
18:02:07.312   Training iter 250, batch loss 1.5195, batch acc 0.9170
18:02:07.847   Training iter 300, batch loss 1.5228, batch acc 0.9108
18:02:08.399   Training iter 350, batch loss 1.5315, batch acc 0.9124
18:02:08.952   Training iter 400, batch loss 1.5348, batch acc 0.9012
18:02:09.517   Training iter 450, batch loss 1.5404, batch acc 0.8696
18:02:10.072   Training iter 500, batch loss 1.5353, batch acc 0.8850
18:02:10.633   Training iter 550, batch loss 1.5206, batch acc 0.9162
18:02:11.198   Training iter 600, batch loss 1.5393, batch acc 0.8648
18:02:11.199 Training @ 370 epoch...
18:02:11.757   Training iter 50, batch loss 1.5359, batch acc 0.8736
18:02:12.318   Training iter 100, batch loss 1.5358, batch acc 0.8790
18:02:12.882   Training iter 150, batch loss 1.5427, batch acc 0.8674
18:02:13.444   Training iter 200, batch loss 1.5370, batch acc 0.8690
18:02:14.015   Training iter 250, batch loss 1.5474, batch acc 0.8772
18:02:14.573   Training iter 300, batch loss 1.5333, batch acc 0.9016
18:02:15.097   Training iter 350, batch loss 1.5191, batch acc 0.9112
18:02:15.623   Training iter 400, batch loss 1.5267, batch acc 0.9058
18:02:16.164   Training iter 450, batch loss 1.5305, batch acc 0.8944
18:02:16.691   Training iter 500, batch loss 1.5332, batch acc 0.9016
18:02:17.226   Training iter 550, batch loss 1.5225, batch acc 0.9070
18:02:17.764   Training iter 600, batch loss 1.5253, batch acc 0.9028
18:02:17.765 Testing @ 370 epoch...
18:02:17.812     Testing, total mean loss 1.52358, total acc 0.90750
18:02:17.812 Training @ 371 epoch...
18:02:18.353   Training iter 50, batch loss 1.5312, batch acc 0.8920
18:02:18.881   Training iter 100, batch loss 1.5267, batch acc 0.8878
18:02:19.424   Training iter 150, batch loss 1.5436, batch acc 0.8516
18:02:19.990   Training iter 200, batch loss 1.5540, batch acc 0.8570
18:02:20.557   Training iter 250, batch loss 1.5318, batch acc 0.8754
18:02:21.121   Training iter 300, batch loss 1.5218, batch acc 0.8978
18:02:21.660   Training iter 350, batch loss 1.5229, batch acc 0.9062
18:02:22.219   Training iter 400, batch loss 1.5345, batch acc 0.8986
18:02:22.779   Training iter 450, batch loss 1.5214, batch acc 0.9184
18:02:23.358   Training iter 500, batch loss 1.5191, batch acc 0.9146
18:02:23.929   Training iter 550, batch loss 1.5193, batch acc 0.9138
18:02:24.510   Training iter 600, batch loss 1.5234, batch acc 0.9104
18:02:24.512 Training @ 372 epoch...
18:02:25.093   Training iter 50, batch loss 1.5264, batch acc 0.8930
18:02:25.639   Training iter 100, batch loss 1.5308, batch acc 0.8970
18:02:26.191   Training iter 150, batch loss 1.5437, batch acc 0.8988
18:02:26.781   Training iter 200, batch loss 1.5289, batch acc 0.9116
18:02:27.361   Training iter 250, batch loss 1.5318, batch acc 0.9066
18:02:27.960   Training iter 300, batch loss 1.5312, batch acc 0.9084
18:02:28.556   Training iter 350, batch loss 1.5235, batch acc 0.9108
18:02:29.155   Training iter 400, batch loss 1.5413, batch acc 0.8972
18:02:29.730   Training iter 450, batch loss 1.5250, batch acc 0.9156
18:02:30.306   Training iter 500, batch loss 1.5256, batch acc 0.9202
18:02:30.853   Training iter 550, batch loss 1.5219, batch acc 0.9150
18:02:31.402   Training iter 600, batch loss 1.5263, batch acc 0.9076
18:02:31.404 Training @ 373 epoch...
18:02:31.949   Training iter 50, batch loss 1.5308, batch acc 0.9126
18:02:32.475   Training iter 100, batch loss 1.5312, batch acc 0.9124
18:02:33.028   Training iter 150, batch loss 1.5268, batch acc 0.9044
18:02:33.570   Training iter 200, batch loss 1.5249, batch acc 0.9098
18:02:34.118   Training iter 250, batch loss 1.5368, batch acc 0.8994
18:02:34.649   Training iter 300, batch loss 1.5252, batch acc 0.9100
18:02:35.196   Training iter 350, batch loss 1.5306, batch acc 0.8992
18:02:35.734   Training iter 400, batch loss 1.5301, batch acc 0.8912
18:02:36.266   Training iter 450, batch loss 1.5255, batch acc 0.9056
18:02:36.786   Training iter 500, batch loss 1.5302, batch acc 0.8896
18:02:37.317   Training iter 550, batch loss 1.5612, batch acc 0.8332
18:02:37.835   Training iter 600, batch loss 1.5422, batch acc 0.8856
18:02:37.837 Training @ 374 epoch...
18:02:38.362   Training iter 50, batch loss 1.5380, batch acc 0.9038
18:02:38.885   Training iter 100, batch loss 1.5446, batch acc 0.8954
18:02:39.419   Training iter 150, batch loss 1.5420, batch acc 0.8848
18:02:39.925   Training iter 200, batch loss 1.5326, batch acc 0.8976
18:02:40.437   Training iter 250, batch loss 1.5302, batch acc 0.9112
18:02:40.938   Training iter 300, batch loss 1.5297, batch acc 0.8970
18:02:41.437   Training iter 350, batch loss 1.5323, batch acc 0.8924
18:02:41.958   Training iter 400, batch loss 1.5260, batch acc 0.9044
18:02:42.474   Training iter 450, batch loss 1.5161, batch acc 0.9258
18:02:42.973   Training iter 500, batch loss 1.5259, batch acc 0.9108
18:02:43.497   Training iter 550, batch loss 1.5452, batch acc 0.8414
18:02:44.021   Training iter 600, batch loss 1.5417, batch acc 0.8766
18:02:44.022 Training @ 375 epoch...
18:02:44.577   Training iter 50, batch loss 1.5311, batch acc 0.9028
18:02:45.196   Training iter 100, batch loss 1.5385, batch acc 0.8752
18:02:45.765   Training iter 150, batch loss 1.5196, batch acc 0.9004
18:02:46.326   Training iter 200, batch loss 1.5397, batch acc 0.8670
18:02:46.904   Training iter 250, batch loss 1.5387, batch acc 0.8710
18:02:47.447   Training iter 300, batch loss 1.5305, batch acc 0.8836
18:02:47.998   Training iter 350, batch loss 1.5377, batch acc 0.8914
18:02:48.540   Training iter 400, batch loss 1.5337, batch acc 0.8984
18:02:49.074   Training iter 450, batch loss 1.5258, batch acc 0.9064
18:02:49.611   Training iter 500, batch loss 1.5387, batch acc 0.8792
18:02:50.175   Training iter 550, batch loss 1.5398, batch acc 0.8844
18:02:50.729   Training iter 600, batch loss 1.5316, batch acc 0.9094
18:02:50.731 Testing @ 375 epoch...
18:02:50.781     Testing, total mean loss 1.53058, total acc 0.91620
18:02:50.781 Training @ 376 epoch...
18:02:51.364   Training iter 50, batch loss 1.5316, batch acc 0.8862
18:02:51.909   Training iter 100, batch loss 1.5289, batch acc 0.8906
18:02:52.447   Training iter 150, batch loss 1.5218, batch acc 0.9092
18:02:52.985   Training iter 200, batch loss 1.5301, batch acc 0.9114
18:02:53.521   Training iter 250, batch loss 1.5398, batch acc 0.9026
18:02:54.058   Training iter 300, batch loss 1.5323, batch acc 0.8984
18:02:54.592   Training iter 350, batch loss 1.5335, batch acc 0.8948
18:02:55.136   Training iter 400, batch loss 1.5499, batch acc 0.8524
18:02:55.654   Training iter 450, batch loss 1.5412, batch acc 0.8558
18:02:56.197   Training iter 500, batch loss 1.5378, batch acc 0.8774
18:02:56.736   Training iter 550, batch loss 1.5309, batch acc 0.9018
18:02:57.326   Training iter 600, batch loss 1.5394, batch acc 0.8812
18:02:57.327 Training @ 377 epoch...
18:02:57.869   Training iter 50, batch loss 1.5279, batch acc 0.8856
18:02:58.395   Training iter 100, batch loss 1.5391, batch acc 0.8668
18:02:58.923   Training iter 150, batch loss 1.5434, batch acc 0.8814
18:02:59.455   Training iter 200, batch loss 1.5370, batch acc 0.8894
18:02:59.980   Training iter 250, batch loss 1.5314, batch acc 0.8826
18:03:00.507   Training iter 300, batch loss 1.5176, batch acc 0.9022
18:03:01.041   Training iter 350, batch loss 1.5250, batch acc 0.9018
18:03:01.634   Training iter 400, batch loss 1.5269, batch acc 0.8984
18:03:02.244   Training iter 450, batch loss 1.5230, batch acc 0.9096
18:03:02.802   Training iter 500, batch loss 1.5215, batch acc 0.9026
18:03:03.349   Training iter 550, batch loss 1.5275, batch acc 0.8994
18:03:03.902   Training iter 600, batch loss 1.5296, batch acc 0.8942
18:03:03.904 Training @ 378 epoch...
18:03:04.470   Training iter 50, batch loss 1.5272, batch acc 0.8910
18:03:05.039   Training iter 100, batch loss 1.5238, batch acc 0.9060
18:03:05.592   Training iter 150, batch loss 1.5212, batch acc 0.9090
18:03:06.151   Training iter 200, batch loss 1.5229, batch acc 0.9046
18:03:06.697   Training iter 250, batch loss 1.5394, batch acc 0.8930
18:03:07.236   Training iter 300, batch loss 1.5348, batch acc 0.8872
18:03:07.765   Training iter 350, batch loss 1.5336, batch acc 0.8928
18:03:08.293   Training iter 400, batch loss 1.5252, batch acc 0.8886
18:03:08.825   Training iter 450, batch loss 1.5338, batch acc 0.8956
18:03:09.369   Training iter 500, batch loss 1.5417, batch acc 0.9036
18:03:09.896   Training iter 550, batch loss 1.5313, batch acc 0.9070
18:03:10.434   Training iter 600, batch loss 1.5205, batch acc 0.9034
18:03:10.436 Training @ 379 epoch...
18:03:10.980   Training iter 50, batch loss 1.5258, batch acc 0.9088
18:03:11.489   Training iter 100, batch loss 1.5334, batch acc 0.9050
18:03:11.986   Training iter 150, batch loss 1.5266, batch acc 0.9112
18:03:12.509   Training iter 200, batch loss 1.5410, batch acc 0.8818
18:03:13.037   Training iter 250, batch loss 1.5277, batch acc 0.9008
18:03:13.606   Training iter 300, batch loss 1.5314, batch acc 0.8896
18:03:14.184   Training iter 350, batch loss 1.5224, batch acc 0.9130
18:03:14.760   Training iter 400, batch loss 1.5317, batch acc 0.8936
18:03:15.252   Training iter 450, batch loss 1.5344, batch acc 0.8868
18:03:15.737   Training iter 500, batch loss 1.5268, batch acc 0.9024
18:03:16.235   Training iter 550, batch loss 1.5321, batch acc 0.9076
18:03:16.741   Training iter 600, batch loss 1.5325, batch acc 0.9088
18:03:16.743 Training @ 380 epoch...
18:03:17.269   Training iter 50, batch loss 1.5376, batch acc 0.8960
18:03:17.845   Training iter 100, batch loss 1.5218, batch acc 0.9068
18:03:18.325   Training iter 150, batch loss 1.5296, batch acc 0.9004
18:03:18.811   Training iter 200, batch loss 1.5286, batch acc 0.8962
18:03:19.312   Training iter 250, batch loss 1.5360, batch acc 0.8868
18:03:19.784   Training iter 300, batch loss 1.5203, batch acc 0.9072
18:03:20.267   Training iter 350, batch loss 1.5299, batch acc 0.8966
18:03:20.753   Training iter 400, batch loss 1.5344, batch acc 0.8986
18:03:21.232   Training iter 450, batch loss 1.5337, batch acc 0.9070
18:03:21.709   Training iter 500, batch loss 1.5251, batch acc 0.9152
18:03:22.202   Training iter 550, batch loss 1.5345, batch acc 0.9044
18:03:22.688   Training iter 600, batch loss 1.5359, batch acc 0.9002
18:03:22.690 Testing @ 380 epoch...
18:03:22.737     Testing, total mean loss 1.52346, total acc 0.91050
18:03:22.737 Training @ 381 epoch...
18:03:23.282   Training iter 50, batch loss 1.5235, batch acc 0.9052
18:03:23.807   Training iter 100, batch loss 1.5231, batch acc 0.9084
18:03:24.339   Training iter 150, batch loss 1.5414, batch acc 0.8812
18:03:24.871   Training iter 200, batch loss 1.5273, batch acc 0.9030
18:03:25.397   Training iter 250, batch loss 1.5317, batch acc 0.9016
18:03:25.929   Training iter 300, batch loss 1.5249, batch acc 0.9080
18:03:26.459   Training iter 350, batch loss 1.5221, batch acc 0.9056
18:03:26.979   Training iter 400, batch loss 1.5301, batch acc 0.8820
18:03:27.514   Training iter 450, batch loss 1.5314, batch acc 0.8932
18:03:28.057   Training iter 500, batch loss 1.5317, batch acc 0.8808
18:03:28.633   Training iter 550, batch loss 1.5332, batch acc 0.8962
18:03:29.224   Training iter 600, batch loss 1.5340, batch acc 0.8876
18:03:29.225 Training @ 382 epoch...
18:03:29.794   Training iter 50, batch loss 1.5155, batch acc 0.9166
18:03:30.353   Training iter 100, batch loss 1.5176, batch acc 0.9116
18:03:30.903   Training iter 150, batch loss 1.5183, batch acc 0.9150
18:03:31.454   Training iter 200, batch loss 1.5489, batch acc 0.8742
18:03:32.010   Training iter 250, batch loss 1.5446, batch acc 0.8802
18:03:32.581   Training iter 300, batch loss 1.5343, batch acc 0.8930
18:03:33.130   Training iter 350, batch loss 1.5305, batch acc 0.9078
18:03:33.665   Training iter 400, batch loss 1.5328, batch acc 0.9112
18:03:34.196   Training iter 450, batch loss 1.5436, batch acc 0.8734
18:03:34.697   Training iter 500, batch loss 1.5355, batch acc 0.8814
18:03:35.205   Training iter 550, batch loss 1.5311, batch acc 0.8876
18:03:35.712   Training iter 600, batch loss 1.5429, batch acc 0.8608
18:03:35.713 Training @ 383 epoch...
18:03:36.236   Training iter 50, batch loss 1.5243, batch acc 0.9056
18:03:36.746   Training iter 100, batch loss 1.5211, batch acc 0.9052
18:03:37.267   Training iter 150, batch loss 1.5240, batch acc 0.9046
18:03:37.785   Training iter 200, batch loss 1.5226, batch acc 0.9098
18:03:38.314   Training iter 250, batch loss 1.5308, batch acc 0.8914
18:03:38.854   Training iter 300, batch loss 1.5303, batch acc 0.9020
18:03:39.417   Training iter 350, batch loss 1.5478, batch acc 0.8862
18:03:39.954   Training iter 400, batch loss 1.5390, batch acc 0.8950
18:03:40.504   Training iter 450, batch loss 1.5418, batch acc 0.8986
18:03:41.020   Training iter 500, batch loss 1.5366, batch acc 0.8970
18:03:41.534   Training iter 550, batch loss 1.5311, batch acc 0.9076
18:03:42.049   Training iter 600, batch loss 1.5302, batch acc 0.9106
18:03:42.051 Training @ 384 epoch...
18:03:42.587   Training iter 50, batch loss 1.5464, batch acc 0.8818
18:03:43.138   Training iter 100, batch loss 1.5318, batch acc 0.8932
18:03:43.688   Training iter 150, batch loss 1.5341, batch acc 0.8834
18:03:44.225   Training iter 200, batch loss 1.5317, batch acc 0.9102
18:03:44.772   Training iter 250, batch loss 1.5296, batch acc 0.9088
18:03:45.323   Training iter 300, batch loss 1.5282, batch acc 0.9168
18:03:45.863   Training iter 350, batch loss 1.5271, batch acc 0.8954
18:03:46.413   Training iter 400, batch loss 1.5295, batch acc 0.8872
18:03:46.953   Training iter 450, batch loss 1.5262, batch acc 0.8996
18:03:47.498   Training iter 500, batch loss 1.5329, batch acc 0.8982
18:03:48.031   Training iter 550, batch loss 1.5331, batch acc 0.8872
18:03:48.549   Training iter 600, batch loss 1.5419, batch acc 0.8862
18:03:48.551 Training @ 385 epoch...
18:03:49.064   Training iter 50, batch loss 1.5328, batch acc 0.8744
18:03:49.571   Training iter 100, batch loss 1.5326, batch acc 0.8884
18:03:50.082   Training iter 150, batch loss 1.5272, batch acc 0.9054
18:03:50.581   Training iter 200, batch loss 1.5324, batch acc 0.9088
18:03:51.071   Training iter 250, batch loss 1.5419, batch acc 0.8874
18:03:51.555   Training iter 300, batch loss 1.5339, batch acc 0.8716
18:03:52.040   Training iter 350, batch loss 1.5321, batch acc 0.8800
18:03:52.527   Training iter 400, batch loss 1.5352, batch acc 0.8922
18:03:53.022   Training iter 450, batch loss 1.5228, batch acc 0.9140
18:03:53.520   Training iter 500, batch loss 1.5271, batch acc 0.9054
18:03:54.037   Training iter 550, batch loss 1.5254, batch acc 0.9086
18:03:54.540   Training iter 600, batch loss 1.5218, batch acc 0.9074
18:03:54.542 Testing @ 385 epoch...
18:03:54.589     Testing, total mean loss 1.52409, total acc 0.91030
18:03:54.589 Training @ 386 epoch...
18:03:55.090   Training iter 50, batch loss 1.5381, batch acc 0.8658
18:03:55.574   Training iter 100, batch loss 1.5328, batch acc 0.9058
18:03:56.054   Training iter 150, batch loss 1.5275, batch acc 0.8956
18:03:56.546   Training iter 200, batch loss 1.5325, batch acc 0.8918
18:03:57.048   Training iter 250, batch loss 1.5296, batch acc 0.8956
18:03:57.581   Training iter 300, batch loss 1.5339, batch acc 0.8786
18:03:58.108   Training iter 350, batch loss 1.5260, batch acc 0.9012
18:03:58.629   Training iter 400, batch loss 1.5215, batch acc 0.9130
18:03:59.148   Training iter 450, batch loss 1.5421, batch acc 0.8788
18:03:59.666   Training iter 500, batch loss 1.5326, batch acc 0.8950
18:04:00.155   Training iter 550, batch loss 1.5327, batch acc 0.8836
18:04:00.659   Training iter 600, batch loss 1.5442, batch acc 0.8808
18:04:00.661 Training @ 387 epoch...
18:04:01.162   Training iter 50, batch loss 1.5238, batch acc 0.9146
18:04:01.704   Training iter 100, batch loss 1.5238, batch acc 0.9174
18:04:02.252   Training iter 150, batch loss 1.5301, batch acc 0.8942
18:04:02.778   Training iter 200, batch loss 1.5374, batch acc 0.8948
18:04:03.336   Training iter 250, batch loss 1.5283, batch acc 0.9092
18:04:03.889   Training iter 300, batch loss 1.5267, batch acc 0.9154
18:04:04.477   Training iter 350, batch loss 1.5363, batch acc 0.8936
18:04:05.057   Training iter 400, batch loss 1.5217, batch acc 0.9234
18:04:05.647   Training iter 450, batch loss 1.5374, batch acc 0.9116
18:04:06.219   Training iter 500, batch loss 1.5278, batch acc 0.9134
18:04:06.786   Training iter 550, batch loss 1.5282, batch acc 0.9122
18:04:07.304   Training iter 600, batch loss 1.5306, batch acc 0.8998
18:04:07.306 Training @ 388 epoch...
18:04:07.888   Training iter 50, batch loss 1.5583, batch acc 0.8370
18:04:08.455   Training iter 100, batch loss 1.5284, batch acc 0.8838
18:04:08.983   Training iter 150, batch loss 1.5220, batch acc 0.9024
18:04:09.489   Training iter 200, batch loss 1.5270, batch acc 0.8934
18:04:10.011   Training iter 250, batch loss 1.5350, batch acc 0.9114
18:04:10.526   Training iter 300, batch loss 1.5356, batch acc 0.9092
18:04:11.055   Training iter 350, batch loss 1.5247, batch acc 0.9134
18:04:11.585   Training iter 400, batch loss 1.5309, batch acc 0.8998
18:04:12.119   Training iter 450, batch loss 1.5252, batch acc 0.9198
18:04:12.661   Training iter 500, batch loss 1.5348, batch acc 0.8968
18:04:13.203   Training iter 550, batch loss 1.5263, batch acc 0.9032
18:04:13.742   Training iter 600, batch loss 1.5215, batch acc 0.8978
18:04:13.744 Training @ 389 epoch...
18:04:14.295   Training iter 50, batch loss 1.5283, batch acc 0.8934
18:04:14.822   Training iter 100, batch loss 1.5323, batch acc 0.9068
18:04:15.336   Training iter 150, batch loss 1.5282, batch acc 0.9098
18:04:15.853   Training iter 200, batch loss 1.5451, batch acc 0.8476
18:04:16.391   Training iter 250, batch loss 1.5418, batch acc 0.8484
18:04:16.892   Training iter 300, batch loss 1.5570, batch acc 0.8546
18:04:17.399   Training iter 350, batch loss 1.5381, batch acc 0.8900
18:04:17.902   Training iter 400, batch loss 1.5345, batch acc 0.8826
18:04:18.411   Training iter 450, batch loss 1.5337, batch acc 0.8642
18:04:18.942   Training iter 500, batch loss 1.5343, batch acc 0.9052
18:04:19.460   Training iter 550, batch loss 1.5423, batch acc 0.8852
18:04:19.984   Training iter 600, batch loss 1.5281, batch acc 0.9014
18:04:19.986 Training @ 390 epoch...
18:04:20.534   Training iter 50, batch loss 1.5324, batch acc 0.8912
18:04:21.069   Training iter 100, batch loss 1.5339, batch acc 0.8786
18:04:21.591   Training iter 150, batch loss 1.5251, batch acc 0.9146
18:04:22.127   Training iter 200, batch loss 1.5202, batch acc 0.9230
18:04:22.644   Training iter 250, batch loss 1.5237, batch acc 0.8932
18:04:23.169   Training iter 300, batch loss 1.5323, batch acc 0.8978
18:04:23.702   Training iter 350, batch loss 1.5278, batch acc 0.8964
18:04:24.257   Training iter 400, batch loss 1.5472, batch acc 0.8618
18:04:24.816   Training iter 450, batch loss 1.5430, batch acc 0.8940
18:04:25.370   Training iter 500, batch loss 1.5300, batch acc 0.8798
18:04:25.910   Training iter 550, batch loss 1.5241, batch acc 0.9034
18:04:26.441   Training iter 600, batch loss 1.5279, batch acc 0.9004
18:04:26.443 Testing @ 390 epoch...
18:04:26.491     Testing, total mean loss 1.52236, total acc 0.91520
18:04:26.491 Training @ 391 epoch...
18:04:27.011   Training iter 50, batch loss 1.5209, batch acc 0.9024
18:04:27.541   Training iter 100, batch loss 1.5288, batch acc 0.9038
18:04:28.069   Training iter 150, batch loss 1.5246, batch acc 0.8974
18:04:28.590   Training iter 200, batch loss 1.5325, batch acc 0.8922
18:04:29.127   Training iter 250, batch loss 1.5270, batch acc 0.9092
18:04:29.678   Training iter 300, batch loss 1.5444, batch acc 0.8578
18:04:30.222   Training iter 350, batch loss 1.5304, batch acc 0.8886
18:04:30.763   Training iter 400, batch loss 1.5279, batch acc 0.9026
18:04:31.316   Training iter 450, batch loss 1.5204, batch acc 0.9108
18:04:31.846   Training iter 500, batch loss 1.5237, batch acc 0.9072
18:04:32.370   Training iter 550, batch loss 1.5245, batch acc 0.9124
18:04:32.927   Training iter 600, batch loss 1.5289, batch acc 0.9060
18:04:32.928 Training @ 392 epoch...
18:04:33.493   Training iter 50, batch loss 1.5315, batch acc 0.8992
18:04:34.053   Training iter 100, batch loss 1.5180, batch acc 0.9200
18:04:34.614   Training iter 150, batch loss 1.5327, batch acc 0.8860
18:04:35.137   Training iter 200, batch loss 1.5300, batch acc 0.9000
18:04:35.677   Training iter 250, batch loss 1.5175, batch acc 0.9182
18:04:36.248   Training iter 300, batch loss 1.5300, batch acc 0.9110
18:04:36.777   Training iter 350, batch loss 1.5289, batch acc 0.9008
18:04:37.343   Training iter 400, batch loss 1.5286, batch acc 0.8644
18:04:37.892   Training iter 450, batch loss 1.5293, batch acc 0.8758
18:04:38.424   Training iter 500, batch loss 1.5246, batch acc 0.8954
18:04:38.943   Training iter 550, batch loss 1.5250, batch acc 0.8858
18:04:39.461   Training iter 600, batch loss 1.5367, batch acc 0.8820
18:04:39.463 Training @ 393 epoch...
18:04:39.986   Training iter 50, batch loss 1.5272, batch acc 0.8910
18:04:40.501   Training iter 100, batch loss 1.5243, batch acc 0.8956
18:04:41.003   Training iter 150, batch loss 1.5168, batch acc 0.9034
18:04:41.518   Training iter 200, batch loss 1.5254, batch acc 0.9070
18:04:42.077   Training iter 250, batch loss 1.5290, batch acc 0.8988
18:04:42.648   Training iter 300, batch loss 1.5314, batch acc 0.9002
18:04:43.217   Training iter 350, batch loss 1.5263, batch acc 0.9078
18:04:43.747   Training iter 400, batch loss 1.5277, batch acc 0.8950
18:04:44.323   Training iter 450, batch loss 1.5243, batch acc 0.9102
18:04:44.864   Training iter 500, batch loss 1.5279, batch acc 0.9020
18:04:45.428   Training iter 550, batch loss 1.5287, batch acc 0.9000
18:04:45.949   Training iter 600, batch loss 1.5228, batch acc 0.9032
18:04:45.950 Training @ 394 epoch...
18:04:46.486   Training iter 50, batch loss 1.5288, batch acc 0.8886
18:04:46.992   Training iter 100, batch loss 1.5183, batch acc 0.9190
18:04:47.500   Training iter 150, batch loss 1.5184, batch acc 0.9270
18:04:48.020   Training iter 200, batch loss 1.5364, batch acc 0.9028
18:04:48.554   Training iter 250, batch loss 1.5281, batch acc 0.9102
18:04:49.119   Training iter 300, batch loss 1.5358, batch acc 0.8958
18:04:49.672   Training iter 350, batch loss 1.5307, batch acc 0.9092
18:04:50.214   Training iter 400, batch loss 1.5248, batch acc 0.9126
18:04:50.776   Training iter 450, batch loss 1.5363, batch acc 0.8830
18:04:51.347   Training iter 500, batch loss 1.5350, batch acc 0.8692
18:04:51.913   Training iter 550, batch loss 1.5483, batch acc 0.8822
18:04:52.463   Training iter 600, batch loss 1.5396, batch acc 0.8894
18:04:52.464 Training @ 395 epoch...
18:04:53.031   Training iter 50, batch loss 1.5252, batch acc 0.9024
18:04:53.587   Training iter 100, batch loss 1.5290, batch acc 0.9054
18:04:54.136   Training iter 150, batch loss 1.5281, batch acc 0.9132
18:04:54.674   Training iter 200, batch loss 1.5334, batch acc 0.9080
18:04:55.197   Training iter 250, batch loss 1.5293, batch acc 0.9200
18:04:55.721   Training iter 300, batch loss 1.5241, batch acc 0.9122
18:04:56.239   Training iter 350, batch loss 1.5387, batch acc 0.9014
18:04:56.766   Training iter 400, batch loss 1.5355, batch acc 0.9036
18:04:57.309   Training iter 450, batch loss 1.5386, batch acc 0.8932
18:04:57.846   Training iter 500, batch loss 1.5284, batch acc 0.8954
18:04:58.430   Training iter 550, batch loss 1.5295, batch acc 0.8900
18:04:59.012   Training iter 600, batch loss 1.5300, batch acc 0.8968
18:04:59.014 Testing @ 395 epoch...
18:04:59.062     Testing, total mean loss 1.52998, total acc 0.89750
18:04:59.062 Training @ 396 epoch...
18:04:59.648   Training iter 50, batch loss 1.5302, batch acc 0.9058
18:05:00.178   Training iter 100, batch loss 1.5507, batch acc 0.8902
18:05:00.698   Training iter 150, batch loss 1.5183, batch acc 0.9186
18:05:01.212   Training iter 200, batch loss 1.5166, batch acc 0.9214
18:05:01.763   Training iter 250, batch loss 1.5318, batch acc 0.8814
18:05:02.332   Training iter 300, batch loss 1.5247, batch acc 0.9098
18:05:02.886   Training iter 350, batch loss 1.5234, batch acc 0.9138
18:05:03.443   Training iter 400, batch loss 1.5290, batch acc 0.9022
18:05:03.994   Training iter 450, batch loss 1.5333, batch acc 0.9090
18:05:04.544   Training iter 500, batch loss 1.5410, batch acc 0.8874
18:05:05.100   Training iter 550, batch loss 1.5427, batch acc 0.8694
18:05:05.652   Training iter 600, batch loss 1.5263, batch acc 0.8942
18:05:05.653 Training @ 397 epoch...
18:05:06.232   Training iter 50, batch loss 1.5282, batch acc 0.8992
18:05:06.784   Training iter 100, batch loss 1.5291, batch acc 0.8888
18:05:07.339   Training iter 150, batch loss 1.5290, batch acc 0.8950
18:05:07.892   Training iter 200, batch loss 1.5239, batch acc 0.9028
18:05:08.484   Training iter 250, batch loss 1.5440, batch acc 0.9068
18:05:09.014   Training iter 300, batch loss 1.5249, batch acc 0.9080
18:05:09.546   Training iter 350, batch loss 1.5271, batch acc 0.9088
18:05:10.061   Training iter 400, batch loss 1.5336, batch acc 0.8894
18:05:10.588   Training iter 450, batch loss 1.5391, batch acc 0.8996
18:05:11.126   Training iter 500, batch loss 1.5403, batch acc 0.8848
18:05:11.677   Training iter 550, batch loss 1.5271, batch acc 0.8906
18:05:12.209   Training iter 600, batch loss 1.5197, batch acc 0.9138
18:05:12.211 Training @ 398 epoch...
18:05:12.762   Training iter 50, batch loss 1.5291, batch acc 0.8936
18:05:13.283   Training iter 100, batch loss 1.5364, batch acc 0.8922
18:05:13.795   Training iter 150, batch loss 1.5240, batch acc 0.9060
18:05:14.316   Training iter 200, batch loss 1.5337, batch acc 0.8792
18:05:14.838   Training iter 250, batch loss 1.5341, batch acc 0.8922
18:05:15.364   Training iter 300, batch loss 1.5273, batch acc 0.8934
18:05:15.880   Training iter 350, batch loss 1.5241, batch acc 0.9038
18:05:16.394   Training iter 400, batch loss 1.5229, batch acc 0.9084
18:05:16.913   Training iter 450, batch loss 1.5250, batch acc 0.9110
18:05:17.444   Training iter 500, batch loss 1.5224, batch acc 0.9088
18:05:17.962   Training iter 550, batch loss 1.5280, batch acc 0.9024
18:05:18.480   Training iter 600, batch loss 1.5394, batch acc 0.8750
18:05:18.481 Training @ 399 epoch...
18:05:19.017   Training iter 50, batch loss 1.5320, batch acc 0.8946
18:05:19.552   Training iter 100, batch loss 1.5309, batch acc 0.9030
18:05:20.081   Training iter 150, batch loss 1.5215, batch acc 0.9130
18:05:20.617   Training iter 200, batch loss 1.5311, batch acc 0.9022
18:05:21.167   Training iter 250, batch loss 1.5320, batch acc 0.8928
18:05:21.732   Training iter 300, batch loss 1.5260, batch acc 0.8964
18:05:22.307   Training iter 350, batch loss 1.5244, batch acc 0.9076
18:05:22.878   Training iter 400, batch loss 1.5341, batch acc 0.8832
18:05:23.455   Training iter 450, batch loss 1.5258, batch acc 0.9018
18:05:24.080   Training iter 500, batch loss 1.5275, batch acc 0.9092
18:05:24.654   Training iter 550, batch loss 1.5246, batch acc 0.9052
18:05:25.205   Training iter 600, batch loss 1.5295, batch acc 0.9040
18:05:25.206 Training @ 400 epoch...
18:05:25.745   Training iter 50, batch loss 1.5229, batch acc 0.9180
18:05:26.305   Training iter 100, batch loss 1.5160, batch acc 0.9262
18:05:26.852   Training iter 150, batch loss 1.5279, batch acc 0.9056
18:05:27.385   Training iter 200, batch loss 1.5299, batch acc 0.9002
18:05:27.918   Training iter 250, batch loss 1.5259, batch acc 0.9086
18:05:28.472   Training iter 300, batch loss 1.5267, batch acc 0.9086
18:05:29.022   Training iter 350, batch loss 1.5203, batch acc 0.9076
18:05:29.568   Training iter 400, batch loss 1.5307, batch acc 0.9072
18:05:30.112   Training iter 450, batch loss 1.5342, batch acc 0.9002
18:05:30.642   Training iter 500, batch loss 1.5258, batch acc 0.9038
18:05:31.172   Training iter 550, batch loss 1.5276, batch acc 0.9140
18:05:31.690   Training iter 600, batch loss 1.5315, batch acc 0.9126
18:05:31.691 Testing @ 400 epoch...
18:05:31.738     Testing, total mean loss 1.52762, total acc 0.90970
18:05:31.738 Plot @ 400 epoch...
18:05:31.738 Training @ 401 epoch...
18:05:32.292   Training iter 50, batch loss 1.5321, batch acc 0.9020
18:05:32.829   Training iter 100, batch loss 1.5236, batch acc 0.9066
18:05:33.327   Training iter 150, batch loss 1.5436, batch acc 0.8628
18:05:33.782   Training iter 200, batch loss 1.5261, batch acc 0.9010
18:05:34.279   Training iter 250, batch loss 1.5194, batch acc 0.9202
18:05:34.802   Training iter 300, batch loss 1.5413, batch acc 0.9048
18:05:35.332   Training iter 350, batch loss 1.5486, batch acc 0.8802
18:05:35.834   Training iter 400, batch loss 1.5315, batch acc 0.8816
18:05:36.331   Training iter 450, batch loss 1.5243, batch acc 0.8938
18:05:36.880   Training iter 500, batch loss 1.5363, batch acc 0.8822
18:05:37.432   Training iter 550, batch loss 1.5303, batch acc 0.8970
18:05:37.992   Training iter 600, batch loss 1.5318, batch acc 0.9072
18:05:37.994 Training @ 402 epoch...
18:05:38.535   Training iter 50, batch loss 1.5342, batch acc 0.9138
18:05:39.066   Training iter 100, batch loss 1.5225, batch acc 0.9064
18:05:39.602   Training iter 150, batch loss 1.5169, batch acc 0.9192
18:05:40.145   Training iter 200, batch loss 1.5200, batch acc 0.9132
18:05:40.708   Training iter 250, batch loss 1.5226, batch acc 0.9170
18:05:41.257   Training iter 300, batch loss 1.5384, batch acc 0.8936
18:05:41.809   Training iter 350, batch loss 1.5458, batch acc 0.8824
18:05:42.396   Training iter 400, batch loss 1.5284, batch acc 0.8944
18:05:42.973   Training iter 450, batch loss 1.5349, batch acc 0.8976
18:05:43.539   Training iter 500, batch loss 1.5278, batch acc 0.9052
18:05:44.103   Training iter 550, batch loss 1.5350, batch acc 0.8908
18:05:44.763   Training iter 600, batch loss 1.5239, batch acc 0.9130
18:05:44.766 Training @ 403 epoch...
18:05:45.389   Training iter 50, batch loss 1.5356, batch acc 0.9012
18:05:45.981   Training iter 100, batch loss 1.5395, batch acc 0.8968
18:05:46.573   Training iter 150, batch loss 1.5320, batch acc 0.9132
18:05:47.163   Training iter 200, batch loss 1.5353, batch acc 0.8912
18:05:47.756   Training iter 250, batch loss 1.5534, batch acc 0.8796
18:05:48.299   Training iter 300, batch loss 1.5375, batch acc 0.8874
18:05:48.813   Training iter 350, batch loss 1.5298, batch acc 0.9052
18:05:49.325   Training iter 400, batch loss 1.5360, batch acc 0.8958
18:05:49.833   Training iter 450, batch loss 1.5274, batch acc 0.8814
18:05:50.357   Training iter 500, batch loss 1.5246, batch acc 0.9026
18:05:50.917   Training iter 550, batch loss 1.5197, batch acc 0.9172
18:05:51.476   Training iter 600, batch loss 1.5250, batch acc 0.9190
18:05:51.478 Training @ 404 epoch...
18:05:52.057   Training iter 50, batch loss 1.5405, batch acc 0.8954
18:05:52.654   Training iter 100, batch loss 1.5356, batch acc 0.8896
18:05:53.280   Training iter 150, batch loss 1.5369, batch acc 0.8886
18:05:53.867   Training iter 200, batch loss 1.5280, batch acc 0.8968
18:05:54.480   Training iter 250, batch loss 1.5301, batch acc 0.8842
18:05:55.092   Training iter 300, batch loss 1.5225, batch acc 0.9060
18:05:55.702   Training iter 350, batch loss 1.5446, batch acc 0.8640
18:05:56.292   Training iter 400, batch loss 1.5332, batch acc 0.8808
18:05:56.869   Training iter 450, batch loss 1.5299, batch acc 0.9040
18:05:57.437   Training iter 500, batch loss 1.5462, batch acc 0.8892
18:05:57.987   Training iter 550, batch loss 1.5214, batch acc 0.9132
18:05:58.533   Training iter 600, batch loss 1.5233, batch acc 0.9100
18:05:58.535 Training @ 405 epoch...
18:05:59.066   Training iter 50, batch loss 1.5357, batch acc 0.9000
18:05:59.602   Training iter 100, batch loss 1.5235, batch acc 0.9162
18:06:00.140   Training iter 150, batch loss 1.5201, batch acc 0.9186
18:06:00.645   Training iter 200, batch loss 1.5312, batch acc 0.9140
18:06:01.145   Training iter 250, batch loss 1.5237, batch acc 0.9136
18:06:01.702   Training iter 300, batch loss 1.5270, batch acc 0.9094
18:06:02.246   Training iter 350, batch loss 1.5222, batch acc 0.9078
18:06:02.756   Training iter 400, batch loss 1.5277, batch acc 0.9116
18:06:03.278   Training iter 450, batch loss 1.5275, batch acc 0.9084
18:06:03.794   Training iter 500, batch loss 1.5453, batch acc 0.8840
18:06:04.314   Training iter 550, batch loss 1.5291, batch acc 0.9036
18:06:04.844   Training iter 600, batch loss 1.5334, batch acc 0.8936
18:06:04.846 Testing @ 405 epoch...
18:06:04.894     Testing, total mean loss 1.54516, total acc 0.88090
18:06:04.894 Training @ 406 epoch...
18:06:05.457   Training iter 50, batch loss 1.5247, batch acc 0.9092
18:06:05.999   Training iter 100, batch loss 1.5318, batch acc 0.8972
18:06:06.544   Training iter 150, batch loss 1.5320, batch acc 0.8950
18:06:07.106   Training iter 200, batch loss 1.5364, batch acc 0.8928
18:06:07.681   Training iter 250, batch loss 1.5396, batch acc 0.8964
18:06:08.221   Training iter 300, batch loss 1.5232, batch acc 0.9124
18:06:08.769   Training iter 350, batch loss 1.5295, batch acc 0.9126
18:06:09.332   Training iter 400, batch loss 1.5264, batch acc 0.9060
18:06:09.901   Training iter 450, batch loss 1.5254, batch acc 0.9080
18:06:10.503   Training iter 500, batch loss 1.5322, batch acc 0.9060
18:06:11.075   Training iter 550, batch loss 1.5332, batch acc 0.8898
18:06:11.636   Training iter 600, batch loss 1.5326, batch acc 0.8938
18:06:11.638 Training @ 407 epoch...
18:06:12.212   Training iter 50, batch loss 1.5317, batch acc 0.9010
18:06:12.775   Training iter 100, batch loss 1.5633, batch acc 0.8540
18:06:13.322   Training iter 150, batch loss 1.5364, batch acc 0.8938
18:06:13.850   Training iter 200, batch loss 1.5434, batch acc 0.8872
18:06:14.387   Training iter 250, batch loss 1.5435, batch acc 0.8820
18:06:14.908   Training iter 300, batch loss 1.5315, batch acc 0.8930
18:06:15.434   Training iter 350, batch loss 1.5365, batch acc 0.8940
18:06:15.964   Training iter 400, batch loss 1.5327, batch acc 0.9076
18:06:16.517   Training iter 450, batch loss 1.5440, batch acc 0.8778
18:06:17.065   Training iter 500, batch loss 1.5344, batch acc 0.8762
18:06:17.608   Training iter 550, batch loss 1.5311, batch acc 0.8940
18:06:18.151   Training iter 600, batch loss 1.5302, batch acc 0.8988
18:06:18.153 Training @ 408 epoch...
18:06:18.694   Training iter 50, batch loss 1.5342, batch acc 0.9028
18:06:19.247   Training iter 100, batch loss 1.5344, batch acc 0.8944
18:06:19.792   Training iter 150, batch loss 1.5200, batch acc 0.9182
18:06:20.351   Training iter 200, batch loss 1.5378, batch acc 0.8942
18:06:20.902   Training iter 250, batch loss 1.5323, batch acc 0.9002
18:06:21.468   Training iter 300, batch loss 1.5346, batch acc 0.8838
18:06:22.022   Training iter 350, batch loss 1.5441, batch acc 0.8582
18:06:22.597   Training iter 400, batch loss 1.5318, batch acc 0.8658
18:06:23.165   Training iter 450, batch loss 1.5393, batch acc 0.8716
18:06:23.746   Training iter 500, batch loss 1.5391, batch acc 0.8730
18:06:24.316   Training iter 550, batch loss 1.5259, batch acc 0.8978
18:06:24.887   Training iter 600, batch loss 1.5343, batch acc 0.8980
18:06:24.888 Training @ 409 epoch...
18:06:25.475   Training iter 50, batch loss 1.5320, batch acc 0.8972
18:06:26.076   Training iter 100, batch loss 1.5321, batch acc 0.8982
18:06:26.686   Training iter 150, batch loss 1.5254, batch acc 0.8926
18:06:27.273   Training iter 200, batch loss 1.5429, batch acc 0.8842
18:06:27.859   Training iter 250, batch loss 1.5260, batch acc 0.9032
18:06:28.449   Training iter 300, batch loss 1.5194, batch acc 0.9124
18:06:29.030   Training iter 350, batch loss 1.5368, batch acc 0.8938
18:06:29.591   Training iter 400, batch loss 1.5320, batch acc 0.9064
18:06:30.143   Training iter 450, batch loss 1.5166, batch acc 0.9108
18:06:30.708   Training iter 500, batch loss 1.5257, batch acc 0.9102
18:06:31.233   Training iter 550, batch loss 1.5280, batch acc 0.9122
18:06:31.778   Training iter 600, batch loss 1.5247, batch acc 0.9104
18:06:31.779 Training @ 410 epoch...
18:06:32.325   Training iter 50, batch loss 1.5293, batch acc 0.9002
18:06:32.857   Training iter 100, batch loss 1.5227, batch acc 0.9172
18:06:33.418   Training iter 150, batch loss 1.5286, batch acc 0.9102
18:06:33.971   Training iter 200, batch loss 1.5294, batch acc 0.9072
18:06:34.542   Training iter 250, batch loss 1.5370, batch acc 0.8858
18:06:35.103   Training iter 300, batch loss 1.5239, batch acc 0.9058
18:06:35.669   Training iter 350, batch loss 1.5372, batch acc 0.8860
18:06:36.250   Training iter 400, batch loss 1.5269, batch acc 0.9050
18:06:36.814   Training iter 450, batch loss 1.5233, batch acc 0.9178
18:06:37.380   Training iter 500, batch loss 1.5293, batch acc 0.9020
18:06:37.930   Training iter 550, batch loss 1.5289, batch acc 0.9090
18:06:38.482   Training iter 600, batch loss 1.5233, batch acc 0.9176
18:06:38.484 Testing @ 410 epoch...
18:06:38.534     Testing, total mean loss 1.52463, total acc 0.92280
18:06:38.534 Training @ 411 epoch...
18:06:39.100   Training iter 50, batch loss 1.5408, batch acc 0.8956
18:06:39.676   Training iter 100, batch loss 1.5278, batch acc 0.9158
18:06:40.259   Training iter 150, batch loss 1.5293, batch acc 0.9038
18:06:40.791   Training iter 200, batch loss 1.5430, batch acc 0.8980
18:06:41.325   Training iter 250, batch loss 1.5460, batch acc 0.8924
18:06:41.866   Training iter 300, batch loss 1.5307, batch acc 0.9020
18:06:42.429   Training iter 350, batch loss 1.5241, batch acc 0.9014
18:06:42.963   Training iter 400, batch loss 1.5272, batch acc 0.9022
18:06:43.513   Training iter 450, batch loss 1.5283, batch acc 0.9092
18:06:44.057   Training iter 500, batch loss 1.5273, batch acc 0.9060
18:06:44.624   Training iter 550, batch loss 1.5507, batch acc 0.8528
18:06:45.186   Training iter 600, batch loss 1.5230, batch acc 0.8964
18:06:45.188 Training @ 412 epoch...
18:06:45.760   Training iter 50, batch loss 1.5271, batch acc 0.9048
18:06:46.326   Training iter 100, batch loss 1.5344, batch acc 0.9146
18:06:46.881   Training iter 150, batch loss 1.5233, batch acc 0.9144
18:06:47.418   Training iter 200, batch loss 1.5290, batch acc 0.9044
18:06:47.969   Training iter 250, batch loss 1.5214, batch acc 0.9156
18:06:48.534   Training iter 300, batch loss 1.5284, batch acc 0.9076
18:06:49.096   Training iter 350, batch loss 1.5268, batch acc 0.9100
18:06:49.634   Training iter 400, batch loss 1.5229, batch acc 0.9098
18:06:50.171   Training iter 450, batch loss 1.5339, batch acc 0.8930
18:06:50.739   Training iter 500, batch loss 1.5303, batch acc 0.8964
18:06:51.295   Training iter 550, batch loss 1.5345, batch acc 0.8824
18:06:51.872   Training iter 600, batch loss 1.5251, batch acc 0.9092
18:06:51.874 Training @ 413 epoch...
18:06:52.443   Training iter 50, batch loss 1.5277, batch acc 0.8962
18:06:53.013   Training iter 100, batch loss 1.5293, batch acc 0.8796
18:06:53.609   Training iter 150, batch loss 1.5356, batch acc 0.8846
18:06:54.198   Training iter 200, batch loss 1.5371, batch acc 0.8732
18:06:54.774   Training iter 250, batch loss 1.5296, batch acc 0.8882
18:06:55.339   Training iter 300, batch loss 1.5334, batch acc 0.8944
18:06:55.903   Training iter 350, batch loss 1.5326, batch acc 0.9006
18:06:56.530   Training iter 400, batch loss 1.5343, batch acc 0.9114
18:06:57.356   Training iter 450, batch loss 1.5306, batch acc 0.9040
18:06:58.183   Training iter 500, batch loss 1.5360, batch acc 0.9066
18:06:59.008   Training iter 550, batch loss 1.5277, batch acc 0.9036
18:06:59.661   Training iter 600, batch loss 1.5225, batch acc 0.9072
18:06:59.662 Training @ 414 epoch...
18:07:00.300   Training iter 50, batch loss 1.5259, batch acc 0.9046
18:07:00.910   Training iter 100, batch loss 1.5271, batch acc 0.9036
18:07:01.587   Training iter 150, batch loss 1.5285, batch acc 0.9020
18:07:02.285   Training iter 200, batch loss 1.5354, batch acc 0.8770
18:07:02.889   Training iter 250, batch loss 1.5309, batch acc 0.8976
18:07:03.456   Training iter 300, batch loss 1.5358, batch acc 0.8884
18:07:04.032   Training iter 350, batch loss 1.5312, batch acc 0.8898
18:07:04.716   Training iter 400, batch loss 1.5282, batch acc 0.8958
18:07:05.395   Training iter 450, batch loss 1.5377, batch acc 0.9088
18:07:06.032   Training iter 500, batch loss 1.5286, batch acc 0.9122
18:07:06.740   Training iter 550, batch loss 1.5265, batch acc 0.9084
18:07:07.430   Training iter 600, batch loss 1.5238, batch acc 0.9108
18:07:07.432 Training @ 415 epoch...
18:07:08.022   Training iter 50, batch loss 1.5223, batch acc 0.9230
18:07:08.536   Training iter 100, batch loss 1.5267, batch acc 0.9086
18:07:09.035   Training iter 150, batch loss 1.5277, batch acc 0.9106
18:07:09.533   Training iter 200, batch loss 1.5347, batch acc 0.8950
18:07:10.033   Training iter 250, batch loss 1.5501, batch acc 0.8602
18:07:10.556   Training iter 300, batch loss 1.5421, batch acc 0.8558
18:07:11.065   Training iter 350, batch loss 1.5278, batch acc 0.8936
18:07:11.568   Training iter 400, batch loss 1.5235, batch acc 0.9116
18:07:12.072   Training iter 450, batch loss 1.5222, batch acc 0.9144
18:07:12.620   Training iter 500, batch loss 1.5250, batch acc 0.9070
18:07:13.190   Training iter 550, batch loss 1.5364, batch acc 0.9144
18:07:13.774   Training iter 600, batch loss 1.5285, batch acc 0.8958
18:07:13.775 Testing @ 415 epoch...
18:07:13.824     Testing, total mean loss 1.53712, total acc 0.89240
18:07:13.824 Training @ 416 epoch...
18:07:14.436   Training iter 50, batch loss 1.5283, batch acc 0.9034
18:07:15.027   Training iter 100, batch loss 1.5156, batch acc 0.9286
18:07:15.615   Training iter 150, batch loss 1.5405, batch acc 0.8514
18:07:16.229   Training iter 200, batch loss 1.5276, batch acc 0.8766
18:07:16.839   Training iter 250, batch loss 1.5356, batch acc 0.8800
18:07:17.453   Training iter 300, batch loss 1.5239, batch acc 0.9082
18:07:18.029   Training iter 350, batch loss 1.5441, batch acc 0.8700
18:07:18.588   Training iter 400, batch loss 1.5484, batch acc 0.8572
18:07:19.156   Training iter 450, batch loss 1.5280, batch acc 0.8978
18:07:19.696   Training iter 500, batch loss 1.5305, batch acc 0.9114
18:07:20.217   Training iter 550, batch loss 1.5352, batch acc 0.8918
18:07:20.741   Training iter 600, batch loss 1.5309, batch acc 0.8804
18:07:20.743 Training @ 417 epoch...
18:07:21.283   Training iter 50, batch loss 1.5413, batch acc 0.8632
18:07:21.827   Training iter 100, batch loss 1.5320, batch acc 0.8836
18:07:22.366   Training iter 150, batch loss 1.5281, batch acc 0.9094
18:07:22.883   Training iter 200, batch loss 1.5313, batch acc 0.9052
18:07:23.405   Training iter 250, batch loss 1.5282, batch acc 0.8766
18:07:23.923   Training iter 300, batch loss 1.5285, batch acc 0.9110
18:07:24.451   Training iter 350, batch loss 1.5251, batch acc 0.9144
18:07:24.970   Training iter 400, batch loss 1.5276, batch acc 0.8978
18:07:25.498   Training iter 450, batch loss 1.5310, batch acc 0.9064
18:07:26.012   Training iter 500, batch loss 1.5224, batch acc 0.9216
18:07:26.569   Training iter 550, batch loss 1.5348, batch acc 0.9146
18:07:27.127   Training iter 600, batch loss 1.5302, batch acc 0.9140
18:07:27.129 Training @ 418 epoch...
18:07:27.671   Training iter 50, batch loss 1.5241, batch acc 0.9096
18:07:28.216   Training iter 100, batch loss 1.5596, batch acc 0.8324
18:07:28.753   Training iter 150, batch loss 1.5368, batch acc 0.8936
18:07:29.306   Training iter 200, batch loss 1.5242, batch acc 0.9098
18:07:29.881   Training iter 250, batch loss 1.5296, batch acc 0.8972
18:07:30.458   Training iter 300, batch loss 1.5270, batch acc 0.9184
18:07:31.050   Training iter 350, batch loss 1.5280, batch acc 0.9108
18:07:31.656   Training iter 400, batch loss 1.5386, batch acc 0.8824
18:07:32.241   Training iter 450, batch loss 1.5372, batch acc 0.8552
18:07:32.834   Training iter 500, batch loss 1.5468, batch acc 0.8604
18:07:33.409   Training iter 550, batch loss 1.5347, batch acc 0.8698
18:07:33.995   Training iter 600, batch loss 1.5315, batch acc 0.8752
18:07:33.997 Training @ 419 epoch...
18:07:34.593   Training iter 50, batch loss 1.5347, batch acc 0.8832
18:07:35.160   Training iter 100, batch loss 1.5372, batch acc 0.8948
18:07:35.698   Training iter 150, batch loss 1.5328, batch acc 0.9002
18:07:36.261   Training iter 200, batch loss 1.5261, batch acc 0.9078
18:07:36.823   Training iter 250, batch loss 1.5360, batch acc 0.8962
18:07:37.361   Training iter 300, batch loss 1.5430, batch acc 0.8806
18:07:37.888   Training iter 350, batch loss 1.5324, batch acc 0.8956
18:07:38.430   Training iter 400, batch loss 1.5469, batch acc 0.8734
18:07:38.952   Training iter 450, batch loss 1.5327, batch acc 0.8896
18:07:39.489   Training iter 500, batch loss 1.5270, batch acc 0.8988
18:07:40.015   Training iter 550, batch loss 1.5333, batch acc 0.8946
18:07:40.558   Training iter 600, batch loss 1.5310, batch acc 0.9144
18:07:40.560 Training @ 420 epoch...
18:07:41.112   Training iter 50, batch loss 1.5381, batch acc 0.8936
18:07:41.640   Training iter 100, batch loss 1.5184, batch acc 0.9140
18:07:42.176   Training iter 150, batch loss 1.5226, batch acc 0.9090
18:07:42.717   Training iter 200, batch loss 1.5247, batch acc 0.9166
18:07:43.260   Training iter 250, batch loss 1.5188, batch acc 0.9176
18:07:43.767   Training iter 300, batch loss 1.5213, batch acc 0.9196
18:07:44.301   Training iter 350, batch loss 1.5275, batch acc 0.9148
18:07:44.829   Training iter 400, batch loss 1.5310, batch acc 0.8974
18:07:45.352   Training iter 450, batch loss 1.5283, batch acc 0.9116
18:07:45.896   Training iter 500, batch loss 1.5304, batch acc 0.9088
18:07:46.474   Training iter 550, batch loss 1.5436, batch acc 0.8804
18:07:47.035   Training iter 600, batch loss 1.5294, batch acc 0.9086
18:07:47.037 Testing @ 420 epoch...
18:07:47.086     Testing, total mean loss 1.52526, total acc 0.91370
18:07:47.086 Training @ 421 epoch...
18:07:47.641   Training iter 50, batch loss 1.5300, batch acc 0.8956
18:07:48.204   Training iter 100, batch loss 1.5200, batch acc 0.9174
18:07:48.758   Training iter 150, batch loss 1.5203, batch acc 0.9228
18:07:49.328   Training iter 200, batch loss 1.5170, batch acc 0.9196
18:07:49.870   Training iter 250, batch loss 1.5348, batch acc 0.8876
18:07:50.393   Training iter 300, batch loss 1.5232, batch acc 0.9134
18:07:50.940   Training iter 350, batch loss 1.5348, batch acc 0.8944
18:07:51.494   Training iter 400, batch loss 1.5495, batch acc 0.8596
18:07:52.041   Training iter 450, batch loss 1.5408, batch acc 0.8956
18:07:52.570   Training iter 500, batch loss 1.5577, batch acc 0.8608
18:07:53.097   Training iter 550, batch loss 1.5358, batch acc 0.9022
18:07:53.619   Training iter 600, batch loss 1.5365, batch acc 0.8996
18:07:53.621 Training @ 422 epoch...
18:07:54.155   Training iter 50, batch loss 1.5399, batch acc 0.8860
18:07:54.697   Training iter 100, batch loss 1.5244, batch acc 0.9052
18:07:55.231   Training iter 150, batch loss 1.5336, batch acc 0.8652
18:07:55.764   Training iter 200, batch loss 1.5246, batch acc 0.8920
18:07:56.304   Training iter 250, batch loss 1.5269, batch acc 0.8852
18:07:56.822   Training iter 300, batch loss 1.5235, batch acc 0.8874
18:07:57.392   Training iter 350, batch loss 1.5216, batch acc 0.9050
18:07:57.951   Training iter 400, batch loss 1.5240, batch acc 0.9032
18:07:58.491   Training iter 450, batch loss 1.5329, batch acc 0.8982
18:07:59.045   Training iter 500, batch loss 1.5256, batch acc 0.9106
18:07:59.596   Training iter 550, batch loss 1.5252, batch acc 0.9144
18:08:00.159   Training iter 600, batch loss 1.5350, batch acc 0.9006
18:08:00.161 Training @ 423 epoch...
18:08:00.712   Training iter 50, batch loss 1.5364, batch acc 0.9040
18:08:01.285   Training iter 100, batch loss 1.5269, batch acc 0.9090
18:08:01.894   Training iter 150, batch loss 1.5230, batch acc 0.9148
18:08:02.488   Training iter 200, batch loss 1.5267, batch acc 0.9234
18:08:03.081   Training iter 250, batch loss 1.5390, batch acc 0.8900
18:08:03.683   Training iter 300, batch loss 1.5326, batch acc 0.9004
18:08:04.277   Training iter 350, batch loss 1.5284, batch acc 0.9088
18:08:04.865   Training iter 400, batch loss 1.5298, batch acc 0.9094
18:08:05.463   Training iter 450, batch loss 1.5316, batch acc 0.9036
18:08:06.036   Training iter 500, batch loss 1.5310, batch acc 0.9026
18:08:06.615   Training iter 550, batch loss 1.5392, batch acc 0.8934
18:08:07.196   Training iter 600, batch loss 1.5409, batch acc 0.8944
18:08:07.198 Training @ 424 epoch...
18:08:07.749   Training iter 50, batch loss 1.5314, batch acc 0.9018
18:08:08.298   Training iter 100, batch loss 1.5306, batch acc 0.9066
18:08:08.843   Training iter 150, batch loss 1.5284, batch acc 0.9006
18:08:09.398   Training iter 200, batch loss 1.5223, batch acc 0.9192
18:08:09.949   Training iter 250, batch loss 1.5292, batch acc 0.8960
18:08:10.504   Training iter 300, batch loss 1.5397, batch acc 0.8650
18:08:11.065   Training iter 350, batch loss 1.5244, batch acc 0.8998
18:08:11.606   Training iter 400, batch loss 1.5221, batch acc 0.9146
18:08:12.149   Training iter 450, batch loss 1.5355, batch acc 0.8748
18:08:12.746   Training iter 500, batch loss 1.5224, batch acc 0.9140
18:08:13.336   Training iter 550, batch loss 1.5239, batch acc 0.9144
18:08:13.934   Training iter 600, batch loss 1.5191, batch acc 0.9206
18:08:13.936 Training @ 425 epoch...
18:08:14.498   Training iter 50, batch loss 1.5315, batch acc 0.8918
18:08:15.032   Training iter 100, batch loss 1.5280, batch acc 0.8734
18:08:15.598   Training iter 150, batch loss 1.5324, batch acc 0.8794
18:08:16.181   Training iter 200, batch loss 1.5444, batch acc 0.8812
18:08:16.767   Training iter 250, batch loss 1.5469, batch acc 0.8656
18:08:17.322   Training iter 300, batch loss 1.5308, batch acc 0.8870
18:08:17.882   Training iter 350, batch loss 1.5247, batch acc 0.8994
18:08:18.446   Training iter 400, batch loss 1.5244, batch acc 0.9056
18:08:18.994   Training iter 450, batch loss 1.5260, batch acc 0.9012
18:08:19.603   Training iter 500, batch loss 1.5229, batch acc 0.9128
18:08:20.371   Training iter 550, batch loss 1.5337, batch acc 0.9018
18:08:21.308   Training iter 600, batch loss 1.5272, batch acc 0.8990
18:08:21.310 Testing @ 425 epoch...
18:08:21.359     Testing, total mean loss 1.51985, total acc 0.91440
18:08:21.359 Training @ 426 epoch...
18:08:21.921   Training iter 50, batch loss 1.5291, batch acc 0.9080
18:08:22.490   Training iter 100, batch loss 1.5243, batch acc 0.9050
18:08:23.058   Training iter 150, batch loss 1.5365, batch acc 0.8904
18:08:23.649   Training iter 200, batch loss 1.5495, batch acc 0.8776
18:08:24.246   Training iter 250, batch loss 1.5351, batch acc 0.9052
18:08:24.891   Training iter 300, batch loss 1.5332, batch acc 0.8948
18:08:25.487   Training iter 350, batch loss 1.5203, batch acc 0.9170
18:08:26.058   Training iter 400, batch loss 1.5207, batch acc 0.9080
18:08:26.624   Training iter 450, batch loss 1.5423, batch acc 0.8838
18:08:27.200   Training iter 500, batch loss 1.5349, batch acc 0.8934
18:08:27.785   Training iter 550, batch loss 1.5255, batch acc 0.8910
18:08:28.322   Training iter 600, batch loss 1.5343, batch acc 0.8954
18:08:28.324 Training @ 427 epoch...
18:08:28.894   Training iter 50, batch loss 1.5248, batch acc 0.9150
18:08:29.446   Training iter 100, batch loss 1.5254, batch acc 0.9080
18:08:30.019   Training iter 150, batch loss 1.5247, batch acc 0.9072
18:08:30.587   Training iter 200, batch loss 1.5363, batch acc 0.8692
18:08:31.147   Training iter 250, batch loss 1.5295, batch acc 0.9064
18:08:31.728   Training iter 300, batch loss 1.5278, batch acc 0.8946
18:08:32.293   Training iter 350, batch loss 1.5310, batch acc 0.9050
18:08:32.851   Training iter 400, batch loss 1.5367, batch acc 0.8960
18:08:33.454   Training iter 450, batch loss 1.5357, batch acc 0.8956
18:08:34.075   Training iter 500, batch loss 1.5231, batch acc 0.9120
18:08:34.694   Training iter 550, batch loss 1.5183, batch acc 0.9126
18:08:35.322   Training iter 600, batch loss 1.5281, batch acc 0.8996
18:08:35.324 Training @ 428 epoch...
18:08:35.943   Training iter 50, batch loss 1.5243, batch acc 0.9082
18:08:36.567   Training iter 100, batch loss 1.5238, batch acc 0.9114
18:08:37.187   Training iter 150, batch loss 1.5355, batch acc 0.9006
18:08:37.799   Training iter 200, batch loss 1.5214, batch acc 0.9174
18:08:38.395   Training iter 250, batch loss 1.5319, batch acc 0.8896
18:08:38.995   Training iter 300, batch loss 1.5231, batch acc 0.9100
18:08:39.563   Training iter 350, batch loss 1.5171, batch acc 0.9168
18:08:40.123   Training iter 400, batch loss 1.5378, batch acc 0.9024
18:08:40.707   Training iter 450, batch loss 1.5506, batch acc 0.8742
18:08:41.310   Training iter 500, batch loss 1.5403, batch acc 0.8966
18:08:41.887   Training iter 550, batch loss 1.5339, batch acc 0.8982
18:08:42.466   Training iter 600, batch loss 1.5347, batch acc 0.9064
18:08:42.468 Training @ 429 epoch...
18:08:43.043   Training iter 50, batch loss 1.5362, batch acc 0.8990
18:08:43.598   Training iter 100, batch loss 1.5380, batch acc 0.8912
18:08:44.150   Training iter 150, batch loss 1.5250, batch acc 0.9164
18:08:44.703   Training iter 200, batch loss 1.5224, batch acc 0.9116
18:08:45.197   Training iter 250, batch loss 1.5224, batch acc 0.9164
18:08:45.695   Training iter 300, batch loss 1.5282, batch acc 0.9034
18:08:46.193   Training iter 350, batch loss 1.5544, batch acc 0.8888
18:08:46.661   Training iter 400, batch loss 1.5486, batch acc 0.8344
18:08:47.147   Training iter 450, batch loss 1.5335, batch acc 0.8640
18:08:47.614   Training iter 500, batch loss 1.5270, batch acc 0.8812
18:08:48.088   Training iter 550, batch loss 1.5411, batch acc 0.8802
18:08:48.568   Training iter 600, batch loss 1.5305, batch acc 0.8988
18:08:48.570 Training @ 430 epoch...
18:08:49.134   Training iter 50, batch loss 1.5216, batch acc 0.9052
18:08:49.628   Training iter 100, batch loss 1.5192, batch acc 0.9100
18:08:50.091   Training iter 150, batch loss 1.5390, batch acc 0.8786
18:08:50.564   Training iter 200, batch loss 1.5264, batch acc 0.8980
18:08:51.027   Training iter 250, batch loss 1.5520, batch acc 0.8574
18:08:51.499   Training iter 300, batch loss 1.5199, batch acc 0.9044
18:08:51.951   Training iter 350, batch loss 1.5256, batch acc 0.8966
18:08:52.428   Training iter 400, batch loss 1.5253, batch acc 0.9024
18:08:52.903   Training iter 450, batch loss 1.5345, batch acc 0.9100
18:08:53.378   Training iter 500, batch loss 1.5270, batch acc 0.9034
18:08:53.822   Training iter 550, batch loss 1.5339, batch acc 0.8990
18:08:54.242   Training iter 600, batch loss 1.5360, batch acc 0.8984
18:08:54.243 Testing @ 430 epoch...
18:08:54.290     Testing, total mean loss 1.55672, total acc 0.84580
18:08:54.290 Training @ 431 epoch...
18:08:54.714   Training iter 50, batch loss 1.5404, batch acc 0.8882
18:08:55.143   Training iter 100, batch loss 1.5420, batch acc 0.8840
18:08:55.607   Training iter 150, batch loss 1.5293, batch acc 0.9162
18:08:56.077   Training iter 200, batch loss 1.5218, batch acc 0.9276
18:08:56.551   Training iter 250, batch loss 1.5223, batch acc 0.9180
18:08:57.047   Training iter 300, batch loss 1.5237, batch acc 0.9130
18:08:57.507   Training iter 350, batch loss 1.5274, batch acc 0.9010
18:08:57.962   Training iter 400, batch loss 1.5181, batch acc 0.9116
18:08:58.427   Training iter 450, batch loss 1.5460, batch acc 0.8612
18:08:58.887   Training iter 500, batch loss 1.5358, batch acc 0.8852
18:08:59.383   Training iter 550, batch loss 1.5261, batch acc 0.9064
18:08:59.830   Training iter 600, batch loss 1.5359, batch acc 0.8802
18:08:59.832 Training @ 432 epoch...
18:09:00.306   Training iter 50, batch loss 1.5386, batch acc 0.8822
18:09:00.849   Training iter 100, batch loss 1.5297, batch acc 0.8866
18:09:01.415   Training iter 150, batch loss 1.5354, batch acc 0.8840
18:09:01.994   Training iter 200, batch loss 1.5382, batch acc 0.8828
18:09:02.573   Training iter 250, batch loss 1.5264, batch acc 0.9058
18:09:03.117   Training iter 300, batch loss 1.5223, batch acc 0.9130
18:09:03.632   Training iter 350, batch loss 1.5228, batch acc 0.9096
18:09:04.150   Training iter 400, batch loss 1.5329, batch acc 0.8942
18:09:04.691   Training iter 450, batch loss 1.5294, batch acc 0.8948
18:09:05.231   Training iter 500, batch loss 1.5283, batch acc 0.8968
18:09:05.767   Training iter 550, batch loss 1.5359, batch acc 0.8806
18:09:06.306   Training iter 600, batch loss 1.5320, batch acc 0.8914
18:09:06.307 Training @ 433 epoch...
18:09:06.830   Training iter 50, batch loss 1.5413, batch acc 0.8690
18:09:07.363   Training iter 100, batch loss 1.5225, batch acc 0.8842
18:09:07.869   Training iter 150, batch loss 1.5249, batch acc 0.9002
18:09:08.440   Training iter 200, batch loss 1.5250, batch acc 0.9050
18:09:08.992   Training iter 250, batch loss 1.5344, batch acc 0.8890
18:09:09.549   Training iter 300, batch loss 1.5315, batch acc 0.9052
18:09:10.102   Training iter 350, batch loss 1.5244, batch acc 0.9038
18:09:10.637   Training iter 400, batch loss 1.5219, batch acc 0.9120
18:09:11.185   Training iter 450, batch loss 1.5482, batch acc 0.8698
18:09:11.714   Training iter 500, batch loss 1.5375, batch acc 0.8718
18:09:12.227   Training iter 550, batch loss 1.5312, batch acc 0.9050
18:09:12.753   Training iter 600, batch loss 1.5417, batch acc 0.8772
18:09:12.755 Training @ 434 epoch...
18:09:13.286   Training iter 50, batch loss 1.5323, batch acc 0.8870
18:09:13.796   Training iter 100, batch loss 1.5389, batch acc 0.8776
18:09:14.322   Training iter 150, batch loss 1.5444, batch acc 0.8794
18:09:14.858   Training iter 200, batch loss 1.5273, batch acc 0.9078
18:09:15.377   Training iter 250, batch loss 1.5262, batch acc 0.9032
18:09:15.893   Training iter 300, batch loss 1.5252, batch acc 0.9104
18:09:16.426   Training iter 350, batch loss 1.5232, batch acc 0.9082
18:09:16.964   Training iter 400, batch loss 1.5237, batch acc 0.8890
18:09:17.496   Training iter 450, batch loss 1.5336, batch acc 0.8794
18:09:18.019   Training iter 500, batch loss 1.5256, batch acc 0.8908
18:09:18.550   Training iter 550, batch loss 1.5184, batch acc 0.9066
18:09:19.062   Training iter 600, batch loss 1.5445, batch acc 0.8910
18:09:19.063 Training @ 435 epoch...
18:09:19.584   Training iter 50, batch loss 1.5355, batch acc 0.9058
18:09:20.109   Training iter 100, batch loss 1.5251, batch acc 0.9110
18:09:20.647   Training iter 150, batch loss 1.5243, batch acc 0.9042
18:09:21.165   Training iter 200, batch loss 1.5194, batch acc 0.9190
18:09:21.685   Training iter 250, batch loss 1.5282, batch acc 0.9074
18:09:22.196   Training iter 300, batch loss 1.5230, batch acc 0.9146
18:09:22.669   Training iter 350, batch loss 1.5209, batch acc 0.9168
18:09:23.161   Training iter 400, batch loss 1.5203, batch acc 0.9220
18:09:23.660   Training iter 450, batch loss 1.5253, batch acc 0.9118
18:09:24.134   Training iter 500, batch loss 1.5142, batch acc 0.9230
18:09:24.652   Training iter 550, batch loss 1.5299, batch acc 0.9094
18:09:25.311   Training iter 600, batch loss 1.5258, batch acc 0.9136
18:09:25.314 Testing @ 435 epoch...
18:09:25.394     Testing, total mean loss 1.52287, total acc 0.92150
18:09:25.394 Training @ 436 epoch...
18:09:26.210   Training iter 50, batch loss 1.5194, batch acc 0.9272
18:09:27.008   Training iter 100, batch loss 1.5296, batch acc 0.9146
18:09:27.809   Training iter 150, batch loss 1.5231, batch acc 0.9180
18:09:28.527   Training iter 200, batch loss 1.5159, batch acc 0.9244
18:09:29.106   Training iter 250, batch loss 1.5237, batch acc 0.9150
18:09:29.667   Training iter 300, batch loss 1.5247, batch acc 0.9150
18:09:30.217   Training iter 350, batch loss 1.5338, batch acc 0.9058
18:09:30.767   Training iter 400, batch loss 1.5466, batch acc 0.8448
18:09:31.301   Training iter 450, batch loss 1.5505, batch acc 0.8494
18:09:31.832   Training iter 500, batch loss 1.5271, batch acc 0.8908
18:09:32.370   Training iter 550, batch loss 1.5222, batch acc 0.9052
18:09:32.919   Training iter 600, batch loss 1.5377, batch acc 0.8918
18:09:32.921 Training @ 437 epoch...
18:09:33.480   Training iter 50, batch loss 1.5522, batch acc 0.8704
18:09:34.034   Training iter 100, batch loss 1.5373, batch acc 0.8768
18:09:34.590   Training iter 150, batch loss 1.5302, batch acc 0.8774
18:09:35.132   Training iter 200, batch loss 1.5226, batch acc 0.8900
18:09:35.684   Training iter 250, batch loss 1.5207, batch acc 0.9014
18:09:36.222   Training iter 300, batch loss 1.5261, batch acc 0.9014
18:09:36.769   Training iter 350, batch loss 1.5238, batch acc 0.9054
18:09:37.315   Training iter 400, batch loss 1.5310, batch acc 0.9014
18:09:37.837   Training iter 450, batch loss 1.5291, batch acc 0.9042
18:09:38.364   Training iter 500, batch loss 1.5254, batch acc 0.8976
18:09:38.876   Training iter 550, batch loss 1.5303, batch acc 0.8926
18:09:39.393   Training iter 600, batch loss 1.5297, batch acc 0.8816
18:09:39.395 Training @ 438 epoch...
18:09:39.912   Training iter 50, batch loss 1.5411, batch acc 0.8606
18:09:40.429   Training iter 100, batch loss 1.5181, batch acc 0.9084
18:09:40.964   Training iter 150, batch loss 1.5187, batch acc 0.9164
18:09:41.497   Training iter 200, batch loss 1.5294, batch acc 0.8808
18:09:42.012   Training iter 250, batch loss 1.5438, batch acc 0.8338
18:09:42.530   Training iter 300, batch loss 1.5324, batch acc 0.8740
18:09:43.084   Training iter 350, batch loss 1.5257, batch acc 0.9006
18:09:43.589   Training iter 400, batch loss 1.5401, batch acc 0.8900
18:09:44.265   Training iter 450, batch loss 1.5243, batch acc 0.8938
18:09:44.958   Training iter 500, batch loss 1.5244, batch acc 0.9080
18:09:45.480   Training iter 550, batch loss 1.5294, batch acc 0.9040
18:09:45.962   Training iter 600, batch loss 1.5278, batch acc 0.9010
18:09:45.964 Training @ 439 epoch...
18:09:46.500   Training iter 50, batch loss 1.5368, batch acc 0.9054
18:09:47.008   Training iter 100, batch loss 1.5330, batch acc 0.8974
18:09:47.525   Training iter 150, batch loss 1.5291, batch acc 0.9080
18:09:48.038   Training iter 200, batch loss 1.5291, batch acc 0.8908
18:09:48.553   Training iter 250, batch loss 1.5208, batch acc 0.9024
18:09:49.056   Training iter 300, batch loss 1.5300, batch acc 0.9044
18:09:49.560   Training iter 350, batch loss 1.5236, batch acc 0.9076
18:09:50.044   Training iter 400, batch loss 1.5256, batch acc 0.8866
18:09:50.553   Training iter 450, batch loss 1.5188, batch acc 0.9212
18:09:51.092   Training iter 500, batch loss 1.5201, batch acc 0.9242
18:09:51.636   Training iter 550, batch loss 1.5219, batch acc 0.9038
18:09:52.177   Training iter 600, batch loss 1.5222, batch acc 0.9190
18:09:52.179 Training @ 440 epoch...
18:09:52.733   Training iter 50, batch loss 1.5234, batch acc 0.8946
18:09:53.278   Training iter 100, batch loss 1.5289, batch acc 0.8822
18:09:53.818   Training iter 150, batch loss 1.5281, batch acc 0.8900
18:09:54.365   Training iter 200, batch loss 1.5268, batch acc 0.9048
18:09:54.888   Training iter 250, batch loss 1.5183, batch acc 0.9216
18:09:55.443   Training iter 300, batch loss 1.5175, batch acc 0.9202
18:09:55.983   Training iter 350, batch loss 1.5237, batch acc 0.9176
18:09:56.545   Training iter 400, batch loss 1.5240, batch acc 0.9202
18:09:57.119   Training iter 450, batch loss 1.5253, batch acc 0.9162
18:09:57.683   Training iter 500, batch loss 1.5315, batch acc 0.9098
18:09:58.257   Training iter 550, batch loss 1.5275, batch acc 0.9150
18:09:58.837   Training iter 600, batch loss 1.5224, batch acc 0.9096
18:09:58.839 Testing @ 440 epoch...
18:09:58.890     Testing, total mean loss 1.52862, total acc 0.90180
18:09:58.890 Training @ 441 epoch...
18:09:59.493   Training iter 50, batch loss 1.5256, batch acc 0.9100
18:10:00.087   Training iter 100, batch loss 1.5296, batch acc 0.9134
18:10:00.682   Training iter 150, batch loss 1.5278, batch acc 0.9174
18:10:01.284   Training iter 200, batch loss 1.5298, batch acc 0.9098
18:10:01.855   Training iter 250, batch loss 1.5295, batch acc 0.9078
18:10:02.449   Training iter 300, batch loss 1.5154, batch acc 0.9112
18:10:03.025   Training iter 350, batch loss 1.5227, batch acc 0.9112
18:10:03.619   Training iter 400, batch loss 1.5234, batch acc 0.9146
18:10:04.209   Training iter 450, batch loss 1.5197, batch acc 0.9168
18:10:04.795   Training iter 500, batch loss 1.5171, batch acc 0.9206
18:10:05.377   Training iter 550, batch loss 1.5284, batch acc 0.9134
18:10:05.958   Training iter 600, batch loss 1.5298, batch acc 0.9042
18:10:05.960 Training @ 442 epoch...
18:10:06.534   Training iter 50, batch loss 1.5249, batch acc 0.9108
18:10:07.077   Training iter 100, batch loss 1.5207, batch acc 0.9212
18:10:07.627   Training iter 150, batch loss 1.5311, batch acc 0.9160
18:10:08.178   Training iter 200, batch loss 1.5225, batch acc 0.9184
18:10:08.690   Training iter 250, batch loss 1.5273, batch acc 0.9162
18:10:09.211   Training iter 300, batch loss 1.5192, batch acc 0.9278
18:10:09.740   Training iter 350, batch loss 1.5299, batch acc 0.9132
18:10:10.253   Training iter 400, batch loss 1.5193, batch acc 0.9252
18:10:10.774   Training iter 450, batch loss 1.5420, batch acc 0.8972
18:10:11.295   Training iter 500, batch loss 1.5481, batch acc 0.8854
18:10:11.810   Training iter 550, batch loss 1.5326, batch acc 0.9006
18:10:12.343   Training iter 600, batch loss 1.5277, batch acc 0.9096
18:10:12.345 Training @ 443 epoch...
18:10:12.880   Training iter 50, batch loss 1.5277, batch acc 0.9086
18:10:13.405   Training iter 100, batch loss 1.5226, batch acc 0.9258
18:10:13.920   Training iter 150, batch loss 1.5138, batch acc 0.9238
18:10:14.459   Training iter 200, batch loss 1.5340, batch acc 0.9086
18:10:15.005   Training iter 250, batch loss 1.5246, batch acc 0.9022
18:10:15.537   Training iter 300, batch loss 1.5229, batch acc 0.9134
18:10:16.075   Training iter 350, batch loss 1.5253, batch acc 0.9158
18:10:16.612   Training iter 400, batch loss 1.5386, batch acc 0.9086
18:10:17.159   Training iter 450, batch loss 1.5204, batch acc 0.9210
18:10:17.693   Training iter 500, batch loss 1.5247, batch acc 0.9168
18:10:18.208   Training iter 550, batch loss 1.5428, batch acc 0.8776
18:10:18.714   Training iter 600, batch loss 1.5326, batch acc 0.9008
18:10:18.715 Training @ 444 epoch...
18:10:19.248   Training iter 50, batch loss 1.5197, batch acc 0.9200
18:10:19.793   Training iter 100, batch loss 1.5219, batch acc 0.9086
18:10:20.358   Training iter 150, batch loss 1.5211, batch acc 0.9040
18:10:20.910   Training iter 200, batch loss 1.5267, batch acc 0.8916
18:10:21.461   Training iter 250, batch loss 1.5252, batch acc 0.8978
18:10:22.018   Training iter 300, batch loss 1.5258, batch acc 0.9164
18:10:22.569   Training iter 350, batch loss 1.5457, batch acc 0.9128
18:10:23.122   Training iter 400, batch loss 1.5430, batch acc 0.9004
18:10:23.652   Training iter 450, batch loss 1.5374, batch acc 0.8750
18:10:24.166   Training iter 500, batch loss 1.5336, batch acc 0.8742
18:10:24.696   Training iter 550, batch loss 1.5257, batch acc 0.8980
18:10:25.225   Training iter 600, batch loss 1.5325, batch acc 0.9062
18:10:25.227 Training @ 445 epoch...
18:10:25.742   Training iter 50, batch loss 1.5292, batch acc 0.8966
18:10:26.252   Training iter 100, batch loss 1.5248, batch acc 0.8868
18:10:26.744   Training iter 150, batch loss 1.5390, batch acc 0.8874
18:10:27.256   Training iter 200, batch loss 1.5191, batch acc 0.9190
18:10:27.815   Training iter 250, batch loss 1.5253, batch acc 0.8958
18:10:28.366   Training iter 300, batch loss 1.5368, batch acc 0.8934
18:10:28.890   Training iter 350, batch loss 1.5328, batch acc 0.8828
18:10:29.427   Training iter 400, batch loss 1.5284, batch acc 0.8864
18:10:29.987   Training iter 450, batch loss 1.5304, batch acc 0.8870
18:10:30.569   Training iter 500, batch loss 1.5417, batch acc 0.8710
18:10:31.138   Training iter 550, batch loss 1.5237, batch acc 0.8946
18:10:31.708   Training iter 600, batch loss 1.5347, batch acc 0.8984
18:10:31.710 Testing @ 445 epoch...
18:10:31.758     Testing, total mean loss 1.53165, total acc 0.90720
18:10:31.758 Training @ 446 epoch...
18:10:32.325   Training iter 50, batch loss 1.5331, batch acc 0.9012
18:10:32.879   Training iter 100, batch loss 1.5371, batch acc 0.8894
18:10:33.439   Training iter 150, batch loss 1.5426, batch acc 0.8680
18:10:33.947   Training iter 200, batch loss 1.5310, batch acc 0.9004
18:10:34.462   Training iter 250, batch loss 1.5413, batch acc 0.8962
18:10:34.991   Training iter 300, batch loss 1.5350, batch acc 0.8996
18:10:35.516   Training iter 350, batch loss 1.5176, batch acc 0.9168
18:10:36.041   Training iter 400, batch loss 1.5247, batch acc 0.9040
18:10:36.555   Training iter 450, batch loss 1.5236, batch acc 0.9096
18:10:37.093   Training iter 500, batch loss 1.5272, batch acc 0.9074
18:10:37.643   Training iter 550, batch loss 1.5276, batch acc 0.9000
18:10:38.174   Training iter 600, batch loss 1.5253, batch acc 0.9084
18:10:38.176 Training @ 447 epoch...
18:10:38.720   Training iter 50, batch loss 1.5346, batch acc 0.9114
18:10:39.259   Training iter 100, batch loss 1.5245, batch acc 0.9134
18:10:39.801   Training iter 150, batch loss 1.5367, batch acc 0.8974
18:10:40.349   Training iter 200, batch loss 1.5318, batch acc 0.8920
18:10:40.881   Training iter 250, batch loss 1.5373, batch acc 0.8630
18:10:41.370   Training iter 300, batch loss 1.5309, batch acc 0.8800
18:10:41.864   Training iter 350, batch loss 1.5266, batch acc 0.8948
18:10:42.375   Training iter 400, batch loss 1.5437, batch acc 0.8736
18:10:42.892   Training iter 450, batch loss 1.5271, batch acc 0.9098
18:10:43.408   Training iter 500, batch loss 1.5293, batch acc 0.9066
18:10:43.935   Training iter 550, batch loss 1.5361, batch acc 0.8780
18:10:44.482   Training iter 600, batch loss 1.5324, batch acc 0.8856
18:10:44.484 Training @ 448 epoch...
18:10:45.038   Training iter 50, batch loss 1.5232, batch acc 0.9034
18:10:45.586   Training iter 100, batch loss 1.5245, batch acc 0.9116
18:10:46.144   Training iter 150, batch loss 1.5244, batch acc 0.9184
18:10:46.686   Training iter 200, batch loss 1.5308, batch acc 0.8972
18:10:47.238   Training iter 250, batch loss 1.5266, batch acc 0.9018
18:10:47.785   Training iter 300, batch loss 1.5242, batch acc 0.8976
18:10:48.325   Training iter 350, batch loss 1.5233, batch acc 0.9138
18:10:48.845   Training iter 400, batch loss 1.5218, batch acc 0.9160
18:10:49.390   Training iter 450, batch loss 1.5252, batch acc 0.9196
18:10:49.934   Training iter 500, batch loss 1.5225, batch acc 0.9176
18:10:50.502   Training iter 550, batch loss 1.5275, batch acc 0.9016
18:10:51.056   Training iter 600, batch loss 1.5281, batch acc 0.9024
18:10:51.058 Training @ 449 epoch...
18:10:51.622   Training iter 50, batch loss 1.5262, batch acc 0.9038
18:10:52.173   Training iter 100, batch loss 1.5234, batch acc 0.9086
18:10:52.709   Training iter 150, batch loss 1.5362, batch acc 0.8684
18:10:53.240   Training iter 200, batch loss 1.5448, batch acc 0.8642
18:10:53.768   Training iter 250, batch loss 1.5477, batch acc 0.8556
18:10:54.275   Training iter 300, batch loss 1.5374, batch acc 0.8972
18:10:54.784   Training iter 350, batch loss 1.5390, batch acc 0.8790
18:10:55.299   Training iter 400, batch loss 1.5308, batch acc 0.9002
18:10:55.797   Training iter 450, batch loss 1.5380, batch acc 0.8866
18:10:56.332   Training iter 500, batch loss 1.5358, batch acc 0.8892
18:10:56.859   Training iter 550, batch loss 1.5242, batch acc 0.9092
18:10:57.408   Training iter 600, batch loss 1.5352, batch acc 0.8784
18:10:57.410 Training @ 450 epoch...
18:10:57.978   Training iter 50, batch loss 1.5258, batch acc 0.9048
18:10:58.550   Training iter 100, batch loss 1.5198, batch acc 0.9186
18:10:59.125   Training iter 150, batch loss 1.5282, batch acc 0.9050
18:10:59.727   Training iter 200, batch loss 1.5218, batch acc 0.9150
18:11:00.291   Training iter 250, batch loss 1.5214, batch acc 0.9128
18:11:00.862   Training iter 300, batch loss 1.5396, batch acc 0.8682
18:11:01.473   Training iter 350, batch loss 1.5400, batch acc 0.8872
18:11:02.129   Training iter 400, batch loss 1.5201, batch acc 0.9080
18:11:02.722   Training iter 450, batch loss 1.5251, batch acc 0.9112
18:11:03.274   Training iter 500, batch loss 1.5287, batch acc 0.9020
18:11:03.815   Training iter 550, batch loss 1.5169, batch acc 0.9210
18:11:04.368   Training iter 600, batch loss 1.5125, batch acc 0.9226
18:11:04.369 Testing @ 450 epoch...
18:11:04.420     Testing, total mean loss 1.52004, total acc 0.92000
18:11:04.420 Training @ 451 epoch...
18:11:05.036   Training iter 50, batch loss 1.5313, batch acc 0.9024
18:11:05.598   Training iter 100, batch loss 1.5228, batch acc 0.9198
18:11:06.141   Training iter 150, batch loss 1.5223, batch acc 0.9216
18:11:06.679   Training iter 200, batch loss 1.5222, batch acc 0.9280
18:11:07.178   Training iter 250, batch loss 1.5377, batch acc 0.9090
18:11:07.680   Training iter 300, batch loss 1.5457, batch acc 0.8838
18:11:08.178   Training iter 350, batch loss 1.5412, batch acc 0.8860
18:11:08.676   Training iter 400, batch loss 1.5277, batch acc 0.8970
18:11:09.171   Training iter 450, batch loss 1.5193, batch acc 0.9162
18:11:09.671   Training iter 500, batch loss 1.5390, batch acc 0.8824
18:11:10.183   Training iter 550, batch loss 1.5408, batch acc 0.8936
18:11:10.689   Training iter 600, batch loss 1.5347, batch acc 0.8934
18:11:10.691 Training @ 452 epoch...
18:11:11.210   Training iter 50, batch loss 1.5238, batch acc 0.9134
18:11:11.725   Training iter 100, batch loss 1.5339, batch acc 0.8976
18:11:12.230   Training iter 150, batch loss 1.5373, batch acc 0.9026
18:11:12.747   Training iter 200, batch loss 1.5263, batch acc 0.8960
18:11:13.277   Training iter 250, batch loss 1.5183, batch acc 0.9116
18:11:13.784   Training iter 300, batch loss 1.5206, batch acc 0.9170
18:11:14.296   Training iter 350, batch loss 1.5260, batch acc 0.9064
18:11:14.821   Training iter 400, batch loss 1.5354, batch acc 0.8702
18:11:15.385   Training iter 450, batch loss 1.5409, batch acc 0.8592
18:11:15.949   Training iter 500, batch loss 1.5236, batch acc 0.9042
18:11:16.506   Training iter 550, batch loss 1.5201, batch acc 0.9106
18:11:17.066   Training iter 600, batch loss 1.5561, batch acc 0.8708
18:11:17.068 Training @ 453 epoch...
18:11:17.625   Training iter 50, batch loss 1.5276, batch acc 0.9030
18:11:18.181   Training iter 100, batch loss 1.5384, batch acc 0.8718
18:11:18.749   Training iter 150, batch loss 1.5316, batch acc 0.8964
18:11:19.318   Training iter 200, batch loss 1.5373, batch acc 0.9084
18:11:19.892   Training iter 250, batch loss 1.5257, batch acc 0.9110
18:11:20.473   Training iter 300, batch loss 1.5237, batch acc 0.9188
18:11:21.059   Training iter 350, batch loss 1.5314, batch acc 0.9040
18:11:21.611   Training iter 400, batch loss 1.5232, batch acc 0.9110
18:11:22.171   Training iter 450, batch loss 1.5223, batch acc 0.8952
18:11:22.737   Training iter 500, batch loss 1.5359, batch acc 0.8730
18:11:23.301   Training iter 550, batch loss 1.5186, batch acc 0.9164
18:11:23.864   Training iter 600, batch loss 1.5203, batch acc 0.9218
18:11:23.865 Training @ 454 epoch...
18:11:24.435   Training iter 50, batch loss 1.5213, batch acc 0.9178
18:11:24.993   Training iter 100, batch loss 1.5212, batch acc 0.9232
18:11:25.555   Training iter 150, batch loss 1.5366, batch acc 0.8972
18:11:26.126   Training iter 200, batch loss 1.5268, batch acc 0.9152
18:11:26.687   Training iter 250, batch loss 1.5279, batch acc 0.9004
18:11:27.255   Training iter 300, batch loss 1.5343, batch acc 0.8920
18:11:27.795   Training iter 350, batch loss 1.5265, batch acc 0.9100
18:11:28.325   Training iter 400, batch loss 1.5235, batch acc 0.9180
18:11:28.850   Training iter 450, batch loss 1.5228, batch acc 0.9210
18:11:29.386   Training iter 500, batch loss 1.5230, batch acc 0.9132
18:11:29.931   Training iter 550, batch loss 1.5334, batch acc 0.8934
18:11:30.482   Training iter 600, batch loss 1.5238, batch acc 0.9008
18:11:30.484 Training @ 455 epoch...
18:11:31.047   Training iter 50, batch loss 1.5461, batch acc 0.8980
18:11:31.596   Training iter 100, batch loss 1.5334, batch acc 0.9084
18:11:32.149   Training iter 150, batch loss 1.5183, batch acc 0.9146
18:11:32.716   Training iter 200, batch loss 1.5233, batch acc 0.9104
18:11:33.305   Training iter 250, batch loss 1.5268, batch acc 0.9154
18:11:33.869   Training iter 300, batch loss 1.5246, batch acc 0.9092
18:11:34.425   Training iter 350, batch loss 1.5206, batch acc 0.9160
18:11:35.001   Training iter 400, batch loss 1.5373, batch acc 0.8974
18:11:35.562   Training iter 450, batch loss 1.5199, batch acc 0.9158
18:11:36.143   Training iter 500, batch loss 1.5238, batch acc 0.9174
18:11:36.694   Training iter 550, batch loss 1.5270, batch acc 0.9134
18:11:37.254   Training iter 600, batch loss 1.5201, batch acc 0.9272
18:11:37.255 Testing @ 455 epoch...
18:11:37.303     Testing, total mean loss 1.52308, total acc 0.91960
18:11:37.303 Training @ 456 epoch...
18:11:37.858   Training iter 50, batch loss 1.5245, batch acc 0.9198
18:11:38.407   Training iter 100, batch loss 1.5289, batch acc 0.9130
18:11:38.927   Training iter 150, batch loss 1.5202, batch acc 0.9222
18:11:39.465   Training iter 200, batch loss 1.5285, batch acc 0.9110
18:11:39.984   Training iter 250, batch loss 1.5305, batch acc 0.9094
18:11:40.518   Training iter 300, batch loss 1.5243, batch acc 0.9076
18:11:41.044   Training iter 350, batch loss 1.5202, batch acc 0.9148
18:11:41.583   Training iter 400, batch loss 1.5288, batch acc 0.8930
18:11:42.116   Training iter 450, batch loss 1.5317, batch acc 0.8810
18:11:42.642   Training iter 500, batch loss 1.5280, batch acc 0.8994
18:11:43.182   Training iter 550, batch loss 1.5247, batch acc 0.9172
18:11:43.740   Training iter 600, batch loss 1.5282, batch acc 0.9168
18:11:43.742 Training @ 457 epoch...
18:11:44.355   Training iter 50, batch loss 1.5215, batch acc 0.9096
18:11:44.956   Training iter 100, batch loss 1.5285, batch acc 0.8966
18:11:45.552   Training iter 150, batch loss 1.5227, batch acc 0.9056
18:11:46.119   Training iter 200, batch loss 1.5400, batch acc 0.8606
18:11:46.663   Training iter 250, batch loss 1.5382, batch acc 0.8590
18:11:47.200   Training iter 300, batch loss 1.5258, batch acc 0.9070
18:11:47.768   Training iter 350, batch loss 1.5240, batch acc 0.9076
18:11:48.318   Training iter 400, batch loss 1.5376, batch acc 0.8820
18:11:48.858   Training iter 450, batch loss 1.5249, batch acc 0.9090
18:11:49.367   Training iter 500, batch loss 1.5332, batch acc 0.8934
18:11:49.883   Training iter 550, batch loss 1.5450, batch acc 0.8900
18:11:50.407   Training iter 600, batch loss 1.5262, batch acc 0.9086
18:11:50.409 Training @ 458 epoch...
18:11:50.932   Training iter 50, batch loss 1.5357, batch acc 0.8880
18:11:51.449   Training iter 100, batch loss 1.5271, batch acc 0.9138
18:11:51.972   Training iter 150, batch loss 1.5214, batch acc 0.9156
18:11:52.513   Training iter 200, batch loss 1.5432, batch acc 0.8846
18:11:53.047   Training iter 250, batch loss 1.5311, batch acc 0.8890
18:11:53.563   Training iter 300, batch loss 1.5205, batch acc 0.9124
18:11:54.060   Training iter 350, batch loss 1.5360, batch acc 0.8902
18:11:54.566   Training iter 400, batch loss 1.5296, batch acc 0.9072
18:11:55.085   Training iter 450, batch loss 1.5222, batch acc 0.9132
18:11:55.593   Training iter 500, batch loss 1.5228, batch acc 0.9186
18:11:56.056   Training iter 550, batch loss 1.5433, batch acc 0.8708
18:11:56.543   Training iter 600, batch loss 1.5388, batch acc 0.8646
18:11:56.545 Training @ 459 epoch...
18:11:57.056   Training iter 50, batch loss 1.5349, batch acc 0.8858
18:11:57.592   Training iter 100, batch loss 1.5196, batch acc 0.8988
18:11:58.136   Training iter 150, batch loss 1.5349, batch acc 0.8926
18:11:58.626   Training iter 200, batch loss 1.5586, batch acc 0.8620
18:11:59.127   Training iter 250, batch loss 1.5323, batch acc 0.8872
18:11:59.652   Training iter 300, batch loss 1.5450, batch acc 0.8516
18:12:00.182   Training iter 350, batch loss 1.5308, batch acc 0.8892
18:12:00.713   Training iter 400, batch loss 1.5232, batch acc 0.8946
18:12:01.216   Training iter 450, batch loss 1.5273, batch acc 0.8898
18:12:01.745   Training iter 500, batch loss 1.5235, batch acc 0.9050
18:12:02.326   Training iter 550, batch loss 1.5258, batch acc 0.8990
18:12:02.903   Training iter 600, batch loss 1.5578, batch acc 0.8214
18:12:02.904 Training @ 460 epoch...
18:12:03.472   Training iter 50, batch loss 1.5273, batch acc 0.8702
18:12:04.024   Training iter 100, batch loss 1.5241, batch acc 0.8904
18:12:04.606   Training iter 150, batch loss 1.5232, batch acc 0.8946
18:12:05.195   Training iter 200, batch loss 1.5248, batch acc 0.9086
18:12:05.767   Training iter 250, batch loss 1.5277, batch acc 0.9044
18:12:06.341   Training iter 300, batch loss 1.5299, batch acc 0.8842
18:12:06.901   Training iter 350, batch loss 1.5281, batch acc 0.9020
18:12:07.502   Training iter 400, batch loss 1.5297, batch acc 0.8924
18:12:08.096   Training iter 450, batch loss 1.5293, batch acc 0.9028
18:12:08.674   Training iter 500, batch loss 1.5203, batch acc 0.9208
18:12:09.248   Training iter 550, batch loss 1.5197, batch acc 0.9182
18:12:09.802   Training iter 600, batch loss 1.5214, batch acc 0.9162
18:12:09.804 Testing @ 460 epoch...
18:12:09.852     Testing, total mean loss 1.52252, total acc 0.91460
18:12:09.852 Training @ 461 epoch...
18:12:10.425   Training iter 50, batch loss 1.5308, batch acc 0.9112
18:12:10.972   Training iter 100, batch loss 1.5604, batch acc 0.8878
18:12:11.534   Training iter 150, batch loss 1.5405, batch acc 0.8836
18:12:12.084   Training iter 200, batch loss 1.5243, batch acc 0.8980
18:12:12.652   Training iter 250, batch loss 1.5287, batch acc 0.8858
18:12:13.212   Training iter 300, batch loss 1.5364, batch acc 0.8862
18:12:13.765   Training iter 350, batch loss 1.5275, batch acc 0.8998
18:12:14.325   Training iter 400, batch loss 1.5445, batch acc 0.8536
18:12:14.919   Training iter 450, batch loss 1.5284, batch acc 0.8922
18:12:15.509   Training iter 500, batch loss 1.5200, batch acc 0.9078
18:12:16.102   Training iter 550, batch loss 1.5271, batch acc 0.9048
18:12:16.646   Training iter 600, batch loss 1.5242, batch acc 0.9122
18:12:16.648 Training @ 462 epoch...
18:12:17.164   Training iter 50, batch loss 1.5233, batch acc 0.9090
18:12:17.692   Training iter 100, batch loss 1.5285, batch acc 0.9066
18:12:18.244   Training iter 150, batch loss 1.5258, batch acc 0.8932
18:12:18.781   Training iter 200, batch loss 1.5258, batch acc 0.8982
18:12:19.337   Training iter 250, batch loss 1.5275, batch acc 0.8988
18:12:19.862   Training iter 300, batch loss 1.5266, batch acc 0.8958
18:12:20.395   Training iter 350, batch loss 1.5244, batch acc 0.9156
18:12:20.925   Training iter 400, batch loss 1.5235, batch acc 0.9026
18:12:21.450   Training iter 450, batch loss 1.5300, batch acc 0.8990
18:12:21.971   Training iter 500, batch loss 1.5362, batch acc 0.8860
18:12:22.494   Training iter 550, batch loss 1.5274, batch acc 0.9074
18:12:23.021   Training iter 600, batch loss 1.5239, batch acc 0.9060
18:12:23.023 Training @ 463 epoch...
18:12:23.540   Training iter 50, batch loss 1.5482, batch acc 0.8376
18:12:24.057   Training iter 100, batch loss 1.5458, batch acc 0.8524
18:12:24.576   Training iter 150, batch loss 1.5297, batch acc 0.8960
18:12:25.092   Training iter 200, batch loss 1.5204, batch acc 0.9092
18:12:25.579   Training iter 250, batch loss 1.5242, batch acc 0.9092
18:12:26.073   Training iter 300, batch loss 1.5324, batch acc 0.8796
18:12:26.585   Training iter 350, batch loss 1.5277, batch acc 0.8864
18:12:27.101   Training iter 400, batch loss 1.5431, batch acc 0.8692
18:12:27.621   Training iter 450, batch loss 1.5267, batch acc 0.9164
18:12:28.126   Training iter 500, batch loss 1.5295, batch acc 0.9036
18:12:28.629   Training iter 550, batch loss 1.5398, batch acc 0.8830
18:12:29.122   Training iter 600, batch loss 1.5373, batch acc 0.8902
18:12:29.124 Training @ 464 epoch...
18:12:29.638   Training iter 50, batch loss 1.5213, batch acc 0.9154
18:12:30.191   Training iter 100, batch loss 1.5280, batch acc 0.9072
18:12:30.751   Training iter 150, batch loss 1.5324, batch acc 0.9106
18:12:31.298   Training iter 200, batch loss 1.5362, batch acc 0.8912
18:12:31.832   Training iter 250, batch loss 1.5243, batch acc 0.9130
18:12:32.385   Training iter 300, batch loss 1.5237, batch acc 0.9090
18:12:32.924   Training iter 350, batch loss 1.5199, batch acc 0.9200
18:12:33.459   Training iter 400, batch loss 1.5338, batch acc 0.9016
18:12:33.969   Training iter 450, batch loss 1.5314, batch acc 0.9014
18:12:34.485   Training iter 500, batch loss 1.5369, batch acc 0.8686
18:12:35.005   Training iter 550, batch loss 1.5317, batch acc 0.8982
18:12:35.554   Training iter 600, batch loss 1.5418, batch acc 0.8698
18:12:35.556 Training @ 465 epoch...
18:12:36.116   Training iter 50, batch loss 1.5286, batch acc 0.8992
18:12:36.669   Training iter 100, batch loss 1.5272, batch acc 0.9028
18:12:37.232   Training iter 150, batch loss 1.5241, batch acc 0.8946
18:12:37.783   Training iter 200, batch loss 1.5357, batch acc 0.8940
18:12:38.359   Training iter 250, batch loss 1.5261, batch acc 0.9018
18:12:38.941   Training iter 300, batch loss 1.5212, batch acc 0.9178
18:12:39.527   Training iter 350, batch loss 1.5265, batch acc 0.9020
18:12:40.107   Training iter 400, batch loss 1.5228, batch acc 0.9196
18:12:40.672   Training iter 450, batch loss 1.5307, batch acc 0.9106
18:12:41.210   Training iter 500, batch loss 1.5190, batch acc 0.9186
18:12:41.737   Training iter 550, batch loss 1.5203, batch acc 0.9256
18:12:42.265   Training iter 600, batch loss 1.5346, batch acc 0.8812
18:12:42.267 Testing @ 465 epoch...
18:12:42.317     Testing, total mean loss 1.52707, total acc 0.90060
18:12:42.317 Training @ 466 epoch...
18:12:42.857   Training iter 50, batch loss 1.5446, batch acc 0.8860
18:12:43.393   Training iter 100, batch loss 1.5211, batch acc 0.9172
18:12:43.935   Training iter 150, batch loss 1.5424, batch acc 0.8946
18:12:44.481   Training iter 200, batch loss 1.5264, batch acc 0.9022
18:12:45.013   Training iter 250, batch loss 1.5326, batch acc 0.8948
18:12:45.540   Training iter 300, batch loss 1.5172, batch acc 0.9150
18:12:46.056   Training iter 350, batch loss 1.5237, batch acc 0.9166
18:12:46.572   Training iter 400, batch loss 1.5382, batch acc 0.9004
18:12:47.094   Training iter 450, batch loss 1.5296, batch acc 0.8990
18:12:47.623   Training iter 500, batch loss 1.5307, batch acc 0.8996
18:12:48.159   Training iter 550, batch loss 1.5278, batch acc 0.9080
18:12:48.679   Training iter 600, batch loss 1.5298, batch acc 0.8848
18:12:48.681 Training @ 467 epoch...
18:12:49.190   Training iter 50, batch loss 1.5320, batch acc 0.8816
18:12:49.697   Training iter 100, batch loss 1.5329, batch acc 0.8928
18:12:50.178   Training iter 150, batch loss 1.5283, batch acc 0.9120
18:12:50.670   Training iter 200, batch loss 1.5278, batch acc 0.9144
18:12:51.203   Training iter 250, batch loss 1.5328, batch acc 0.9110
18:12:51.764   Training iter 300, batch loss 1.5437, batch acc 0.9048
18:12:52.326   Training iter 350, batch loss 1.5260, batch acc 0.9160
18:12:52.904   Training iter 400, batch loss 1.5247, batch acc 0.9222
18:12:53.471   Training iter 450, batch loss 1.5289, batch acc 0.9132
18:12:54.031   Training iter 500, batch loss 1.5259, batch acc 0.9082
18:12:54.567   Training iter 550, batch loss 1.5291, batch acc 0.9064
18:12:55.100   Training iter 600, batch loss 1.5433, batch acc 0.8942
18:12:55.102 Training @ 468 epoch...
18:12:55.661   Training iter 50, batch loss 1.5339, batch acc 0.9148
18:12:56.203   Training iter 100, batch loss 1.5253, batch acc 0.9130
18:12:56.735   Training iter 150, batch loss 1.5279, batch acc 0.9046
18:12:57.270   Training iter 200, batch loss 1.5215, batch acc 0.9114
18:12:57.805   Training iter 250, batch loss 1.5239, batch acc 0.9060
18:12:58.356   Training iter 300, batch loss 1.5318, batch acc 0.8982
18:12:58.897   Training iter 350, batch loss 1.5307, batch acc 0.9068
18:12:59.436   Training iter 400, batch loss 1.5367, batch acc 0.8868
18:12:59.973   Training iter 450, batch loss 1.5200, batch acc 0.9210
18:13:00.549   Training iter 500, batch loss 1.5271, batch acc 0.8972
18:13:01.118   Training iter 550, batch loss 1.5240, batch acc 0.8964
18:13:01.686   Training iter 600, batch loss 1.5281, batch acc 0.9086
18:13:01.688 Training @ 469 epoch...
18:13:02.306   Training iter 50, batch loss 1.5282, batch acc 0.9128
18:13:02.855   Training iter 100, batch loss 1.5236, batch acc 0.9114
18:13:03.417   Training iter 150, batch loss 1.5152, batch acc 0.9258
18:13:03.941   Training iter 200, batch loss 1.5286, batch acc 0.9160
18:13:04.474   Training iter 250, batch loss 1.5205, batch acc 0.9174
18:13:05.062   Training iter 300, batch loss 1.5313, batch acc 0.9052
18:13:05.643   Training iter 350, batch loss 1.5405, batch acc 0.8798
18:13:06.230   Training iter 400, batch loss 1.5301, batch acc 0.8936
18:13:06.795   Training iter 450, batch loss 1.5308, batch acc 0.9000
18:13:07.365   Training iter 500, batch loss 1.5254, batch acc 0.9020
18:13:07.926   Training iter 550, batch loss 1.5246, batch acc 0.9054
18:13:08.495   Training iter 600, batch loss 1.5278, batch acc 0.9002
18:13:08.497 Training @ 470 epoch...
18:13:09.074   Training iter 50, batch loss 1.5295, batch acc 0.9040
18:13:09.620   Training iter 100, batch loss 1.5174, batch acc 0.9218
18:13:10.136   Training iter 150, batch loss 1.5353, batch acc 0.8820
18:13:10.657   Training iter 200, batch loss 1.5459, batch acc 0.8726
18:13:11.170   Training iter 250, batch loss 1.5404, batch acc 0.8742
18:13:11.684   Training iter 300, batch loss 1.5296, batch acc 0.8952
18:13:12.219   Training iter 350, batch loss 1.5245, batch acc 0.8978
18:13:12.796   Training iter 400, batch loss 1.5295, batch acc 0.9000
18:13:13.413   Training iter 450, batch loss 1.5452, batch acc 0.8722
18:13:13.990   Training iter 500, batch loss 1.5221, batch acc 0.9000
18:13:14.544   Training iter 550, batch loss 1.5163, batch acc 0.9086
18:13:15.078   Training iter 600, batch loss 1.5199, batch acc 0.9078
18:13:15.080 Testing @ 470 epoch...
18:13:15.127     Testing, total mean loss 1.52137, total acc 0.91490
18:13:15.127 Training @ 471 epoch...
18:13:15.654   Training iter 50, batch loss 1.5158, batch acc 0.9178
18:13:16.169   Training iter 100, batch loss 1.5280, batch acc 0.8880
18:13:16.696   Training iter 150, batch loss 1.5165, batch acc 0.9250
18:13:17.223   Training iter 200, batch loss 1.5361, batch acc 0.8900
18:13:17.733   Training iter 250, batch loss 1.5264, batch acc 0.9026
18:13:18.245   Training iter 300, batch loss 1.5237, batch acc 0.9146
18:13:18.744   Training iter 350, batch loss 1.5276, batch acc 0.9084
18:13:19.261   Training iter 400, batch loss 1.5345, batch acc 0.8826
18:13:19.780   Training iter 450, batch loss 1.5267, batch acc 0.9122
18:13:20.308   Training iter 500, batch loss 1.5297, batch acc 0.9056
18:13:20.822   Training iter 550, batch loss 1.5465, batch acc 0.8660
18:13:21.335   Training iter 600, batch loss 1.5347, batch acc 0.8938
18:13:21.337 Training @ 472 epoch...
18:13:21.857   Training iter 50, batch loss 1.5255, batch acc 0.9116
18:13:22.400   Training iter 100, batch loss 1.5285, batch acc 0.8912
18:13:22.941   Training iter 150, batch loss 1.5264, batch acc 0.9086
18:13:23.520   Training iter 200, batch loss 1.5285, batch acc 0.9118
18:13:24.109   Training iter 250, batch loss 1.5227, batch acc 0.9184
18:13:24.713   Training iter 300, batch loss 1.5410, batch acc 0.8824
18:13:25.323   Training iter 350, batch loss 1.5402, batch acc 0.8860
18:13:25.904   Training iter 400, batch loss 1.5218, batch acc 0.9220
18:13:26.528   Training iter 450, batch loss 1.5272, batch acc 0.8898
18:13:27.134   Training iter 500, batch loss 1.5332, batch acc 0.8852
18:13:27.732   Training iter 550, batch loss 1.5212, batch acc 0.9114
18:13:28.315   Training iter 600, batch loss 1.5209, batch acc 0.9170
18:13:28.317 Training @ 473 epoch...
18:13:28.896   Training iter 50, batch loss 1.5212, batch acc 0.9178
18:13:29.452   Training iter 100, batch loss 1.5560, batch acc 0.8520
18:13:29.994   Training iter 150, batch loss 1.5517, batch acc 0.8440
18:13:30.572   Training iter 200, batch loss 1.5318, batch acc 0.8940
18:13:31.158   Training iter 250, batch loss 1.5198, batch acc 0.9154
18:13:31.710   Training iter 300, batch loss 1.5270, batch acc 0.9082
18:13:32.251   Training iter 350, batch loss 1.5355, batch acc 0.8834
18:13:32.791   Training iter 400, batch loss 1.5392, batch acc 0.8768
18:13:33.365   Training iter 450, batch loss 1.5317, batch acc 0.8916
18:13:33.910   Training iter 500, batch loss 1.5366, batch acc 0.8844
18:13:34.461   Training iter 550, batch loss 1.5400, batch acc 0.8830
18:13:35.006   Training iter 600, batch loss 1.5280, batch acc 0.8938
18:13:35.008 Training @ 474 epoch...
18:13:35.585   Training iter 50, batch loss 1.5396, batch acc 0.8754
18:13:36.148   Training iter 100, batch loss 1.5293, batch acc 0.8824
18:13:36.711   Training iter 150, batch loss 1.5207, batch acc 0.8986
18:13:37.277   Training iter 200, batch loss 1.5274, batch acc 0.8980
18:13:37.840   Training iter 250, batch loss 1.5224, batch acc 0.9066
18:13:38.414   Training iter 300, batch loss 1.5349, batch acc 0.8948
18:13:38.976   Training iter 350, batch loss 1.5335, batch acc 0.9084
18:13:39.564   Training iter 400, batch loss 1.5271, batch acc 0.9102
18:13:40.161   Training iter 450, batch loss 1.5369, batch acc 0.8932
18:13:40.753   Training iter 500, batch loss 1.5287, batch acc 0.8924
18:13:41.352   Training iter 550, batch loss 1.5299, batch acc 0.8946
18:13:41.928   Training iter 600, batch loss 1.5256, batch acc 0.9070
18:13:41.930 Training @ 475 epoch...
18:13:42.527   Training iter 50, batch loss 1.5217, batch acc 0.9048
18:13:43.090   Training iter 100, batch loss 1.5334, batch acc 0.9044
18:13:43.644   Training iter 150, batch loss 1.5248, batch acc 0.9136
18:13:44.199   Training iter 200, batch loss 1.5335, batch acc 0.8892
18:13:44.773   Training iter 250, batch loss 1.5209, batch acc 0.9212
18:13:45.320   Training iter 300, batch loss 1.5356, batch acc 0.9122
18:13:45.845   Training iter 350, batch loss 1.5345, batch acc 0.8856
18:13:46.377   Training iter 400, batch loss 1.5222, batch acc 0.9112
18:13:46.917   Training iter 450, batch loss 1.5327, batch acc 0.8948
18:13:47.451   Training iter 500, batch loss 1.5343, batch acc 0.8884
18:13:47.995   Training iter 550, batch loss 1.5330, batch acc 0.8904
18:13:48.541   Training iter 600, batch loss 1.5192, batch acc 0.9152
18:13:48.543 Testing @ 475 epoch...
18:13:48.590     Testing, total mean loss 1.53261, total acc 0.90640
18:13:48.590 Training @ 476 epoch...
18:13:49.149   Training iter 50, batch loss 1.5202, batch acc 0.9186
18:13:49.698   Training iter 100, batch loss 1.5370, batch acc 0.9072
18:13:50.207   Training iter 150, batch loss 1.5211, batch acc 0.9132
18:13:50.717   Training iter 200, batch loss 1.5222, batch acc 0.9084
18:13:51.229   Training iter 250, batch loss 1.5213, batch acc 0.9156
18:13:51.751   Training iter 300, batch loss 1.5210, batch acc 0.9320
18:13:52.272   Training iter 350, batch loss 1.5278, batch acc 0.9208
18:13:52.797   Training iter 400, batch loss 1.5204, batch acc 0.9304
18:13:53.315   Training iter 450, batch loss 1.5391, batch acc 0.8988
18:13:53.832   Training iter 500, batch loss 1.5423, batch acc 0.8950
18:13:54.363   Training iter 550, batch loss 1.5433, batch acc 0.8948
18:13:54.917   Training iter 600, batch loss 1.5291, batch acc 0.9016
18:13:54.919 Training @ 477 epoch...
18:13:55.480   Training iter 50, batch loss 1.5424, batch acc 0.9008
18:13:56.044   Training iter 100, batch loss 1.5256, batch acc 0.9094
18:13:56.611   Training iter 150, batch loss 1.5578, batch acc 0.8260
18:13:57.188   Training iter 200, batch loss 1.5372, batch acc 0.8682
18:13:57.794   Training iter 250, batch loss 1.5264, batch acc 0.8960
18:13:58.384   Training iter 300, batch loss 1.5240, batch acc 0.9020
18:13:58.963   Training iter 350, batch loss 1.5381, batch acc 0.8648
18:13:59.486   Training iter 400, batch loss 1.5290, batch acc 0.8986
18:14:00.014   Training iter 450, batch loss 1.5245, batch acc 0.9108
18:14:00.565   Training iter 500, batch loss 1.5272, batch acc 0.9038
18:14:01.125   Training iter 550, batch loss 1.5308, batch acc 0.8782
18:14:01.716   Training iter 600, batch loss 1.5520, batch acc 0.8354
18:14:01.718 Training @ 478 epoch...
18:14:02.299   Training iter 50, batch loss 1.5443, batch acc 0.8434
18:14:02.848   Training iter 100, batch loss 1.5388, batch acc 0.8612
18:14:03.408   Training iter 150, batch loss 1.5256, batch acc 0.8974
18:14:03.958   Training iter 200, batch loss 1.5379, batch acc 0.8836
18:14:04.518   Training iter 250, batch loss 1.5365, batch acc 0.8908
18:14:05.070   Training iter 300, batch loss 1.5394, batch acc 0.8990
18:14:05.601   Training iter 350, batch loss 1.5458, batch acc 0.8680
18:14:06.162   Training iter 400, batch loss 1.5254, batch acc 0.8936
18:14:06.752   Training iter 450, batch loss 1.5505, batch acc 0.8516
18:14:07.319   Training iter 500, batch loss 1.5317, batch acc 0.8856
18:14:07.887   Training iter 550, batch loss 1.5222, batch acc 0.9116
18:14:08.424   Training iter 600, batch loss 1.5366, batch acc 0.8950
18:14:08.426 Training @ 479 epoch...
18:14:08.951   Training iter 50, batch loss 1.5264, batch acc 0.8904
18:14:09.487   Training iter 100, batch loss 1.5283, batch acc 0.8838
18:14:10.027   Training iter 150, batch loss 1.5315, batch acc 0.8952
18:14:10.568   Training iter 200, batch loss 1.5308, batch acc 0.9044
18:14:11.106   Training iter 250, batch loss 1.5325, batch acc 0.9006
18:14:11.663   Training iter 300, batch loss 1.5277, batch acc 0.9142
18:14:12.224   Training iter 350, batch loss 1.5357, batch acc 0.8884
18:14:12.800   Training iter 400, batch loss 1.5319, batch acc 0.9048
18:14:13.333   Training iter 450, batch loss 1.5341, batch acc 0.8798
18:14:13.862   Training iter 500, batch loss 1.5379, batch acc 0.8460
18:14:14.432   Training iter 550, batch loss 1.5280, batch acc 0.8952
18:14:15.034   Training iter 600, batch loss 1.5339, batch acc 0.9010
18:14:15.036 Training @ 480 epoch...
18:14:15.657   Training iter 50, batch loss 1.5390, batch acc 0.8920
18:14:16.274   Training iter 100, batch loss 1.5432, batch acc 0.8740
18:14:16.883   Training iter 150, batch loss 1.5583, batch acc 0.8072
18:14:17.448   Training iter 200, batch loss 1.5593, batch acc 0.7996
18:14:18.004   Training iter 250, batch loss 1.5480, batch acc 0.8460
18:14:18.549   Training iter 300, batch loss 1.5203, batch acc 0.9242
18:14:19.121   Training iter 350, batch loss 1.5518, batch acc 0.8658
18:14:19.701   Training iter 400, batch loss 1.5350, batch acc 0.8992
18:14:20.269   Training iter 450, batch loss 1.5337, batch acc 0.8784
18:14:20.850   Training iter 500, batch loss 1.5271, batch acc 0.8976
18:14:21.436   Training iter 550, batch loss 1.5401, batch acc 0.8578
18:14:22.027   Training iter 600, batch loss 1.5271, batch acc 0.8866
18:14:22.029 Testing @ 480 epoch...
18:14:22.084     Testing, total mean loss 1.55192, total acc 0.88780
18:14:22.084 Training @ 481 epoch...
18:14:22.688   Training iter 50, batch loss 1.5294, batch acc 0.9022
18:14:23.283   Training iter 100, batch loss 1.5299, batch acc 0.8920
18:14:23.870   Training iter 150, batch loss 1.5275, batch acc 0.9012
18:14:24.469   Training iter 200, batch loss 1.5233, batch acc 0.9124
18:14:25.031   Training iter 250, batch loss 1.5297, batch acc 0.8992
18:14:25.568   Training iter 300, batch loss 1.5325, batch acc 0.8970
18:14:26.135   Training iter 350, batch loss 1.5262, batch acc 0.9026
18:14:26.715   Training iter 400, batch loss 1.5367, batch acc 0.8762
18:14:27.301   Training iter 450, batch loss 1.5361, batch acc 0.8846
18:14:27.930   Training iter 500, batch loss 1.5304, batch acc 0.9016
18:14:28.554   Training iter 550, batch loss 1.5241, batch acc 0.9170
18:14:29.177   Training iter 600, batch loss 1.5374, batch acc 0.9012
18:14:29.179 Training @ 482 epoch...
18:14:29.795   Training iter 50, batch loss 1.5291, batch acc 0.9084
18:14:30.411   Training iter 100, batch loss 1.5439, batch acc 0.8796
18:14:31.002   Training iter 150, batch loss 1.5473, batch acc 0.8752
18:14:31.599   Training iter 200, batch loss 1.5225, batch acc 0.9140
18:14:32.205   Training iter 250, batch loss 1.5189, batch acc 0.9184
18:14:32.814   Training iter 300, batch loss 1.5263, batch acc 0.9050
18:14:33.366   Training iter 350, batch loss 1.5372, batch acc 0.8870
18:14:33.923   Training iter 400, batch loss 1.5438, batch acc 0.8902
18:14:34.475   Training iter 450, batch loss 1.5494, batch acc 0.8518
18:14:35.016   Training iter 500, batch loss 1.5423, batch acc 0.8672
18:14:35.552   Training iter 550, batch loss 1.5334, batch acc 0.8988
18:14:36.101   Training iter 600, batch loss 1.5348, batch acc 0.9064
18:14:36.104 Training @ 483 epoch...
18:14:36.700   Training iter 50, batch loss 1.5453, batch acc 0.8688
18:14:37.275   Training iter 100, batch loss 1.5298, batch acc 0.9024
18:14:37.857   Training iter 150, batch loss 1.5329, batch acc 0.9006
18:14:38.406   Training iter 200, batch loss 1.5444, batch acc 0.8816
18:14:38.978   Training iter 250, batch loss 1.5544, batch acc 0.8694
18:14:39.561   Training iter 300, batch loss 1.5423, batch acc 0.8730
18:14:40.153   Training iter 350, batch loss 1.5277, batch acc 0.8860
18:14:40.748   Training iter 400, batch loss 1.5319, batch acc 0.8894
18:14:41.348   Training iter 450, batch loss 1.5277, batch acc 0.9036
18:14:41.944   Training iter 500, batch loss 1.5243, batch acc 0.9082
18:14:42.545   Training iter 550, batch loss 1.5298, batch acc 0.8964
18:14:43.151   Training iter 600, batch loss 1.5325, batch acc 0.8914
18:14:43.153 Training @ 484 epoch...
18:14:43.781   Training iter 50, batch loss 1.5418, batch acc 0.8878
18:14:44.413   Training iter 100, batch loss 1.5255, batch acc 0.8984
18:14:45.049   Training iter 150, batch loss 1.5295, batch acc 0.8948
18:14:45.691   Training iter 200, batch loss 1.5377, batch acc 0.8772
18:14:46.329   Training iter 250, batch loss 1.5380, batch acc 0.8778
18:14:46.968   Training iter 300, batch loss 1.5331, batch acc 0.8978
18:14:47.609   Training iter 350, batch loss 1.5307, batch acc 0.8928
18:14:48.251   Training iter 400, batch loss 1.5338, batch acc 0.8922
18:14:48.914   Training iter 450, batch loss 1.5238, batch acc 0.9040
18:14:49.575   Training iter 500, batch loss 1.5274, batch acc 0.9140
18:14:50.233   Training iter 550, batch loss 1.5276, batch acc 0.8968
18:14:50.827   Training iter 600, batch loss 1.5454, batch acc 0.8726
18:14:50.829 Training @ 485 epoch...
18:14:51.427   Training iter 50, batch loss 1.5330, batch acc 0.8816
18:14:52.023   Training iter 100, batch loss 1.5365, batch acc 0.8854
18:14:52.621   Training iter 150, batch loss 1.5411, batch acc 0.8582
18:14:53.214   Training iter 200, batch loss 1.5316, batch acc 0.8824
18:14:53.793   Training iter 250, batch loss 1.5190, batch acc 0.9118
18:14:54.381   Training iter 300, batch loss 1.5196, batch acc 0.9136
18:14:54.963   Training iter 350, batch loss 1.5283, batch acc 0.9128
18:14:55.570   Training iter 400, batch loss 1.5316, batch acc 0.9052
18:14:56.156   Training iter 450, batch loss 1.5226, batch acc 0.9112
18:14:56.760   Training iter 500, batch loss 1.5255, batch acc 0.9052
18:14:57.371   Training iter 550, batch loss 1.5343, batch acc 0.8884
18:14:57.977   Training iter 600, batch loss 1.5233, batch acc 0.9070
18:14:57.979 Testing @ 485 epoch...
18:14:58.036     Testing, total mean loss 1.52304, total acc 0.90290
18:14:58.036 Training @ 486 epoch...
18:14:58.634   Training iter 50, batch loss 1.5291, batch acc 0.8904
18:14:59.229   Training iter 100, batch loss 1.5293, batch acc 0.8832
18:14:59.852   Training iter 150, batch loss 1.5291, batch acc 0.8932
18:15:00.491   Training iter 200, batch loss 1.5458, batch acc 0.8732
18:15:01.117   Training iter 250, batch loss 1.5272, batch acc 0.9010
18:15:01.774   Training iter 300, batch loss 1.5243, batch acc 0.9132
18:15:02.421   Training iter 350, batch loss 1.5276, batch acc 0.9032
18:15:03.066   Training iter 400, batch loss 1.5309, batch acc 0.8950
18:15:03.697   Training iter 450, batch loss 1.5223, batch acc 0.8948
18:15:04.323   Training iter 500, batch loss 1.5349, batch acc 0.9026
18:15:04.954   Training iter 550, batch loss 1.5335, batch acc 0.8894
18:15:05.546   Training iter 600, batch loss 1.5316, batch acc 0.8962
18:15:05.548 Training @ 487 epoch...
18:15:06.081   Training iter 50, batch loss 1.5257, batch acc 0.9094
18:15:06.693   Training iter 100, batch loss 1.5273, batch acc 0.9190
18:15:07.281   Training iter 150, batch loss 1.5291, batch acc 0.9002
18:15:07.838   Training iter 200, batch loss 1.5223, batch acc 0.9046
18:15:08.386   Training iter 250, batch loss 1.5233, batch acc 0.8836
18:15:08.958   Training iter 300, batch loss 1.5227, batch acc 0.9006
18:15:09.526   Training iter 350, batch loss 1.5195, batch acc 0.9084
18:15:10.091   Training iter 400, batch loss 1.5246, batch acc 0.9038
18:15:10.649   Training iter 450, batch loss 1.5184, batch acc 0.9224
18:15:11.193   Training iter 500, batch loss 1.5282, batch acc 0.8988
18:15:11.765   Training iter 550, batch loss 1.5279, batch acc 0.8966
18:15:12.339   Training iter 600, batch loss 1.5210, batch acc 0.9084
18:15:12.341 Training @ 488 epoch...
18:15:12.907   Training iter 50, batch loss 1.5411, batch acc 0.8730
18:15:13.458   Training iter 100, batch loss 1.5321, batch acc 0.8900
18:15:14.021   Training iter 150, batch loss 1.5309, batch acc 0.8806
18:15:14.590   Training iter 200, batch loss 1.5301, batch acc 0.8864
18:15:15.167   Training iter 250, batch loss 1.5287, batch acc 0.8952
18:15:15.759   Training iter 300, batch loss 1.5273, batch acc 0.8994
18:15:16.374   Training iter 350, batch loss 1.5299, batch acc 0.8990
18:15:16.997   Training iter 400, batch loss 1.5265, batch acc 0.9026
18:15:17.625   Training iter 450, batch loss 1.5369, batch acc 0.8854
18:15:18.214   Training iter 500, batch loss 1.5236, batch acc 0.9058
18:15:18.818   Training iter 550, batch loss 1.5238, batch acc 0.9082
18:15:19.418   Training iter 600, batch loss 1.5197, batch acc 0.9236
18:15:19.420 Training @ 489 epoch...
18:15:20.018   Training iter 50, batch loss 1.5212, batch acc 0.9200
18:15:20.622   Training iter 100, batch loss 1.5406, batch acc 0.8924
18:15:21.223   Training iter 150, batch loss 1.5367, batch acc 0.8986
18:15:21.806   Training iter 200, batch loss 1.5234, batch acc 0.9142
18:15:22.363   Training iter 250, batch loss 1.5219, batch acc 0.9170
18:15:22.930   Training iter 300, batch loss 1.5220, batch acc 0.9178
18:15:23.486   Training iter 350, batch loss 1.5497, batch acc 0.8676
18:15:24.018   Training iter 400, batch loss 1.5549, batch acc 0.8592
18:15:24.569   Training iter 450, batch loss 1.5403, batch acc 0.8630
18:15:25.130   Training iter 500, batch loss 1.5472, batch acc 0.8622
18:15:25.687   Training iter 550, batch loss 1.5462, batch acc 0.8672
18:15:26.247   Training iter 600, batch loss 1.5433, batch acc 0.8894
18:15:26.249 Training @ 490 epoch...
18:15:26.825   Training iter 50, batch loss 1.5335, batch acc 0.8886
18:15:27.407   Training iter 100, batch loss 1.5216, batch acc 0.9046
18:15:27.978   Training iter 150, batch loss 1.5204, batch acc 0.9280
18:15:28.542   Training iter 200, batch loss 1.5394, batch acc 0.8646
18:15:29.117   Training iter 250, batch loss 1.5203, batch acc 0.9196
18:15:29.666   Training iter 300, batch loss 1.5238, batch acc 0.9082
18:15:30.214   Training iter 350, batch loss 1.5423, batch acc 0.8648
18:15:30.781   Training iter 400, batch loss 1.5392, batch acc 0.8822
18:15:31.348   Training iter 450, batch loss 1.5302, batch acc 0.8934
18:15:31.921   Training iter 500, batch loss 1.5264, batch acc 0.8866
18:15:32.528   Training iter 550, batch loss 1.5367, batch acc 0.8878
18:15:33.130   Training iter 600, batch loss 1.5349, batch acc 0.8658
18:15:33.132 Testing @ 490 epoch...
18:15:33.186     Testing, total mean loss 1.53572, total acc 0.88950
18:15:33.186 Training @ 491 epoch...
18:15:33.774   Training iter 50, batch loss 1.5351, batch acc 0.8780
18:15:34.361   Training iter 100, batch loss 1.5314, batch acc 0.8754
18:15:34.978   Training iter 150, batch loss 1.5254, batch acc 0.8924
18:15:35.612   Training iter 200, batch loss 1.5214, batch acc 0.9002
18:15:36.237   Training iter 250, batch loss 1.5286, batch acc 0.9068
18:15:36.871   Training iter 300, batch loss 1.5310, batch acc 0.9112
18:15:37.528   Training iter 350, batch loss 1.5235, batch acc 0.9128
18:15:38.130   Training iter 400, batch loss 1.5320, batch acc 0.8970
18:15:38.732   Training iter 450, batch loss 1.5302, batch acc 0.9138
18:15:39.319   Training iter 500, batch loss 1.5300, batch acc 0.9082
18:15:39.918   Training iter 550, batch loss 1.5235, batch acc 0.9080
18:15:40.524   Training iter 600, batch loss 1.5259, batch acc 0.9006
18:15:40.526 Training @ 492 epoch...
18:15:41.134   Training iter 50, batch loss 1.5177, batch acc 0.9226
18:15:41.741   Training iter 100, batch loss 1.5230, batch acc 0.9148
18:15:42.374   Training iter 150, batch loss 1.5311, batch acc 0.8938
18:15:42.969   Training iter 200, batch loss 1.5312, batch acc 0.9052
18:15:43.561   Training iter 250, batch loss 1.5221, batch acc 0.9160
18:15:44.181   Training iter 300, batch loss 1.5246, batch acc 0.9026
18:15:44.841   Training iter 350, batch loss 1.5263, batch acc 0.9012
18:15:45.509   Training iter 400, batch loss 1.5300, batch acc 0.8910
18:15:46.107   Training iter 450, batch loss 1.5292, batch acc 0.8896
18:15:46.684   Training iter 500, batch loss 1.5382, batch acc 0.8774
18:15:47.237   Training iter 550, batch loss 1.5275, batch acc 0.8850
18:15:47.816   Training iter 600, batch loss 1.5196, batch acc 0.9152
18:15:47.818 Training @ 493 epoch...
18:15:48.399   Training iter 50, batch loss 1.5301, batch acc 0.9098
18:15:48.957   Training iter 100, batch loss 1.5284, batch acc 0.8850
18:15:49.499   Training iter 150, batch loss 1.5340, batch acc 0.8874
18:15:50.070   Training iter 200, batch loss 1.5341, batch acc 0.8958
18:15:50.671   Training iter 250, batch loss 1.5265, batch acc 0.9116
18:15:51.231   Training iter 300, batch loss 1.5237, batch acc 0.9140
18:15:51.773   Training iter 350, batch loss 1.5264, batch acc 0.9006
18:15:52.330   Training iter 400, batch loss 1.5255, batch acc 0.9024
18:15:52.910   Training iter 450, batch loss 1.5382, batch acc 0.8828
18:15:53.470   Training iter 500, batch loss 1.5253, batch acc 0.9062
18:15:54.011   Training iter 550, batch loss 1.5298, batch acc 0.9088
18:15:54.558   Training iter 600, batch loss 1.5290, batch acc 0.9136
18:15:54.560 Training @ 494 epoch...
18:15:55.127   Training iter 50, batch loss 1.5268, batch acc 0.9136
18:15:55.730   Training iter 100, batch loss 1.5221, batch acc 0.9090
18:15:56.346   Training iter 150, batch loss 1.5270, batch acc 0.9164
18:15:56.940   Training iter 200, batch loss 1.5309, batch acc 0.9120
18:15:57.519   Training iter 250, batch loss 1.5252, batch acc 0.9128
18:15:58.082   Training iter 300, batch loss 1.5278, batch acc 0.9042
18:15:58.665   Training iter 350, batch loss 1.5270, batch acc 0.9068
18:15:59.243   Training iter 400, batch loss 1.5361, batch acc 0.8804
18:15:59.816   Training iter 450, batch loss 1.5299, batch acc 0.8994
18:16:00.394   Training iter 500, batch loss 1.5282, batch acc 0.8860
18:16:00.992   Training iter 550, batch loss 1.5233, batch acc 0.9064
18:16:01.606   Training iter 600, batch loss 1.5352, batch acc 0.8770
18:16:01.608 Training @ 495 epoch...
18:16:02.220   Training iter 50, batch loss 1.5212, batch acc 0.9036
18:16:02.778   Training iter 100, batch loss 1.5217, batch acc 0.9128
18:16:03.341   Training iter 150, batch loss 1.5254, batch acc 0.9098
18:16:03.937   Training iter 200, batch loss 1.5324, batch acc 0.8936
18:16:04.578   Training iter 250, batch loss 1.5290, batch acc 0.8986
18:16:05.201   Training iter 300, batch loss 1.5270, batch acc 0.9122
18:16:05.807   Training iter 350, batch loss 1.5322, batch acc 0.8866
18:16:06.443   Training iter 400, batch loss 1.5254, batch acc 0.9058
18:16:07.077   Training iter 450, batch loss 1.5200, batch acc 0.9156
18:16:07.702   Training iter 500, batch loss 1.5275, batch acc 0.9016
18:16:08.289   Training iter 550, batch loss 1.5353, batch acc 0.9058
18:16:08.821   Training iter 600, batch loss 1.5316, batch acc 0.9090
18:16:08.823 Testing @ 495 epoch...
18:16:08.870     Testing, total mean loss 1.55595, total acc 0.88400
18:16:08.870 Training @ 496 epoch...
18:16:09.409   Training iter 50, batch loss 1.5542, batch acc 0.8794
18:16:09.946   Training iter 100, batch loss 1.5295, batch acc 0.8904
18:16:10.539   Training iter 150, batch loss 1.5248, batch acc 0.8902
18:16:11.121   Training iter 200, batch loss 1.5247, batch acc 0.8940
18:16:11.701   Training iter 250, batch loss 1.5216, batch acc 0.9046
18:16:12.288   Training iter 300, batch loss 1.5256, batch acc 0.8946
18:16:12.873   Training iter 350, batch loss 1.5499, batch acc 0.8510
18:16:13.414   Training iter 400, batch loss 1.5266, batch acc 0.8998
18:16:13.947   Training iter 450, batch loss 1.5467, batch acc 0.8766
18:16:14.480   Training iter 500, batch loss 1.5448, batch acc 0.8770
18:16:15.009   Training iter 550, batch loss 1.5234, batch acc 0.9088
18:16:15.531   Training iter 600, batch loss 1.5225, batch acc 0.9048
18:16:15.533 Training @ 497 epoch...
18:16:16.077   Training iter 50, batch loss 1.5271, batch acc 0.8908
18:16:16.607   Training iter 100, batch loss 1.5355, batch acc 0.8946
18:16:17.146   Training iter 150, batch loss 1.5264, batch acc 0.9048
18:16:17.732   Training iter 200, batch loss 1.5234, batch acc 0.9068
18:16:18.275   Training iter 250, batch loss 1.5461, batch acc 0.8630
18:16:18.812   Training iter 300, batch loss 1.5335, batch acc 0.8908
18:16:19.348   Training iter 350, batch loss 1.5328, batch acc 0.8934
18:16:19.907   Training iter 400, batch loss 1.5367, batch acc 0.8822
18:16:20.476   Training iter 450, batch loss 1.5261, batch acc 0.9040
18:16:21.081   Training iter 500, batch loss 1.5299, batch acc 0.8846
18:16:21.694   Training iter 550, batch loss 1.5287, batch acc 0.9008
18:16:22.309   Training iter 600, batch loss 1.5341, batch acc 0.8694
18:16:22.311 Training @ 498 epoch...
18:16:22.919   Training iter 50, batch loss 1.5315, batch acc 0.8870
18:16:23.529   Training iter 100, batch loss 1.5265, batch acc 0.8914
18:16:24.130   Training iter 150, batch loss 1.5405, batch acc 0.8660
18:16:24.758   Training iter 200, batch loss 1.5291, batch acc 0.8878
18:16:25.363   Training iter 250, batch loss 1.5306, batch acc 0.8950
18:16:25.952   Training iter 300, batch loss 1.5344, batch acc 0.8660
18:16:26.532   Training iter 350, batch loss 1.5481, batch acc 0.8266
18:16:27.095   Training iter 400, batch loss 1.5269, batch acc 0.8890
18:16:27.683   Training iter 450, batch loss 1.5284, batch acc 0.8878
18:16:28.262   Training iter 500, batch loss 1.5238, batch acc 0.9078
18:16:28.813   Training iter 550, batch loss 1.5278, batch acc 0.8828
18:16:29.362   Training iter 600, batch loss 1.5247, batch acc 0.8948
18:16:29.364 Training @ 499 epoch...
18:16:29.903   Training iter 50, batch loss 1.5330, batch acc 0.8862
18:16:30.448   Training iter 100, batch loss 1.5396, batch acc 0.8718
18:16:30.987   Training iter 150, batch loss 1.5218, batch acc 0.9156
18:16:31.547   Training iter 200, batch loss 1.5264, batch acc 0.9114
18:16:32.077   Training iter 250, batch loss 1.5238, batch acc 0.9154
18:16:32.606   Training iter 300, batch loss 1.5343, batch acc 0.8880
18:16:33.154   Training iter 350, batch loss 1.5316, batch acc 0.8976
18:16:33.703   Training iter 400, batch loss 1.5165, batch acc 0.9214
18:16:34.246   Training iter 450, batch loss 1.5289, batch acc 0.9014
18:16:34.791   Training iter 500, batch loss 1.5267, batch acc 0.9088
18:16:35.326   Training iter 550, batch loss 1.5296, batch acc 0.9056
18:16:35.863   Training iter 600, batch loss 1.5349, batch acc 0.8990
======================================================
18:16:35.864 Testing @ final epoch...
18:16:35.912     Testing, total mean loss 1.57633, total acc 0.88030
training time: 3304 seconds
