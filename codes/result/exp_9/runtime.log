======================================================
learning_rate: 0.1
weight_decay: 0.0001
momentum: 0.1
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Relu Lin-10-10 Sigm
loss: Softmax
result dir: ./result/exp_9
======================================================
16:28:07.083 Training @ 0 epoch...
16:28:07.661   Training iter 50, batch loss 2.3022, batch acc 0.1108
16:28:08.209   Training iter 100, batch loss 2.3012, batch acc 0.1112
16:28:08.759   Training iter 150, batch loss 2.2976, batch acc 0.1462
16:28:09.316   Training iter 200, batch loss 2.2839, batch acc 0.1806
16:28:09.875   Training iter 250, batch loss 2.2468, batch acc 0.2130
16:28:10.438   Training iter 300, batch loss 2.1735, batch acc 0.3218
16:28:11.003   Training iter 350, batch loss 2.0877, batch acc 0.4192
16:28:11.559   Training iter 400, batch loss 1.9996, batch acc 0.5078
16:28:12.127   Training iter 450, batch loss 1.9401, batch acc 0.5504
16:28:12.686   Training iter 500, batch loss 1.9043, batch acc 0.5724
16:28:13.250   Training iter 550, batch loss 1.8667, batch acc 0.5908
16:28:13.872   Training iter 600, batch loss 1.8404, batch acc 0.5770
16:28:13.874 Testing @ 0 epoch...
16:28:13.928     Testing, total mean loss 1.83352, total acc 0.58590
16:28:13.928 Plot @ 0 epoch...
16:28:13.928 Training @ 1 epoch...
16:28:14.574   Training iter 50, batch loss 1.8336, batch acc 0.5814
16:28:15.339   Training iter 100, batch loss 1.8143, batch acc 0.5952
16:28:16.043   Training iter 150, batch loss 1.8020, batch acc 0.5998
16:28:16.589   Training iter 200, batch loss 1.7947, batch acc 0.5768
16:28:17.135   Training iter 250, batch loss 1.7863, batch acc 0.5838
16:28:17.688   Training iter 300, batch loss 1.7782, batch acc 0.5922
16:28:18.239   Training iter 350, batch loss 1.7686, batch acc 0.5996
16:28:18.788   Training iter 400, batch loss 1.7662, batch acc 0.5934
16:28:19.345   Training iter 450, batch loss 1.7600, batch acc 0.6070
16:28:19.916   Training iter 500, batch loss 1.7521, batch acc 0.5986
16:28:20.491   Training iter 550, batch loss 1.7530, batch acc 0.6134
16:28:21.033   Training iter 600, batch loss 1.7427, batch acc 0.6190
16:28:21.034 Training @ 2 epoch...
16:28:21.558   Training iter 50, batch loss 1.7413, batch acc 0.6402
16:28:22.082   Training iter 100, batch loss 1.7341, batch acc 0.6464
16:28:22.613   Training iter 150, batch loss 1.7294, batch acc 0.6530
16:28:23.137   Training iter 200, batch loss 1.7213, batch acc 0.6658
16:28:23.665   Training iter 250, batch loss 1.7189, batch acc 0.6600
16:28:24.188   Training iter 300, batch loss 1.7082, batch acc 0.6782
16:28:24.722   Training iter 350, batch loss 1.7115, batch acc 0.6784
16:28:25.255   Training iter 400, batch loss 1.6997, batch acc 0.6944
16:28:25.785   Training iter 450, batch loss 1.7006, batch acc 0.7002
16:28:26.298   Training iter 500, batch loss 1.6901, batch acc 0.7170
16:28:26.822   Training iter 550, batch loss 1.6919, batch acc 0.7192
16:28:27.347   Training iter 600, batch loss 1.6862, batch acc 0.7384
16:28:27.349 Training @ 3 epoch...
16:28:27.866   Training iter 50, batch loss 1.6786, batch acc 0.7594
16:28:28.404   Training iter 100, batch loss 1.6746, batch acc 0.7702
16:28:28.946   Training iter 150, batch loss 1.6652, batch acc 0.7660
16:28:29.509   Training iter 200, batch loss 1.6686, batch acc 0.7660
16:28:30.045   Training iter 250, batch loss 1.6622, batch acc 0.7606
16:28:30.598   Training iter 300, batch loss 1.6630, batch acc 0.7630
16:28:31.154   Training iter 350, batch loss 1.6579, batch acc 0.7630
16:28:31.696   Training iter 400, batch loss 1.6559, batch acc 0.7760
16:28:32.244   Training iter 450, batch loss 1.6540, batch acc 0.7688
16:28:32.794   Training iter 500, batch loss 1.6458, batch acc 0.7690
16:28:33.375   Training iter 550, batch loss 1.6495, batch acc 0.7824
16:28:33.938   Training iter 600, batch loss 1.6431, batch acc 0.7860
16:28:33.940 Training @ 4 epoch...
16:28:34.469   Training iter 50, batch loss 1.6416, batch acc 0.7852
16:28:35.015   Training iter 100, batch loss 1.6425, batch acc 0.7796
16:28:35.558   Training iter 150, batch loss 1.6440, batch acc 0.7860
16:28:36.132   Training iter 200, batch loss 1.6330, batch acc 0.7966
16:28:36.684   Training iter 250, batch loss 1.6336, batch acc 0.7894
16:28:37.262   Training iter 300, batch loss 1.6294, batch acc 0.7918
16:28:37.830   Training iter 350, batch loss 1.6346, batch acc 0.7890
16:28:38.416   Training iter 400, batch loss 1.6278, batch acc 0.7922
16:28:39.012   Training iter 450, batch loss 1.6261, batch acc 0.7928
16:28:39.592   Training iter 500, batch loss 1.6309, batch acc 0.7926
16:28:40.174   Training iter 550, batch loss 1.6250, batch acc 0.8008
16:28:40.750   Training iter 600, batch loss 1.6244, batch acc 0.7994
16:28:40.751 Training @ 5 epoch...
16:28:41.321   Training iter 50, batch loss 1.6259, batch acc 0.7916
16:28:41.868   Training iter 100, batch loss 1.6202, batch acc 0.8094
16:28:42.425   Training iter 150, batch loss 1.6228, batch acc 0.7902
16:28:42.965   Training iter 200, batch loss 1.6219, batch acc 0.7928
16:28:43.501   Training iter 250, batch loss 1.6202, batch acc 0.7942
16:28:44.048   Training iter 300, batch loss 1.6149, batch acc 0.7988
16:28:44.607   Training iter 350, batch loss 1.6168, batch acc 0.8000
16:28:45.161   Training iter 400, batch loss 1.6211, batch acc 0.7976
16:28:45.709   Training iter 450, batch loss 1.6157, batch acc 0.8038
16:28:46.266   Training iter 500, batch loss 1.6175, batch acc 0.7910
16:28:46.819   Training iter 550, batch loss 1.6195, batch acc 0.7952
16:28:47.374   Training iter 600, batch loss 1.6133, batch acc 0.8098
16:28:47.376 Testing @ 5 epoch...
16:28:47.423     Testing, total mean loss 1.61019, total acc 0.80560
16:28:47.423 Training @ 6 epoch...
16:28:47.977   Training iter 50, batch loss 1.6140, batch acc 0.8004
16:28:48.526   Training iter 100, batch loss 1.6130, batch acc 0.8016
16:28:49.088   Training iter 150, batch loss 1.6115, batch acc 0.8086
16:28:49.638   Training iter 200, batch loss 1.6098, batch acc 0.8084
16:28:50.197   Training iter 250, batch loss 1.6150, batch acc 0.8042
16:28:50.763   Training iter 300, batch loss 1.6058, batch acc 0.8034
16:28:51.307   Training iter 350, batch loss 1.6097, batch acc 0.8130
16:28:51.868   Training iter 400, batch loss 1.6059, batch acc 0.8136
16:28:52.460   Training iter 450, batch loss 1.6087, batch acc 0.8074
16:28:53.041   Training iter 500, batch loss 1.6089, batch acc 0.8046
16:28:53.616   Training iter 550, batch loss 1.6049, batch acc 0.8094
16:28:54.196   Training iter 600, batch loss 1.6107, batch acc 0.8012
16:28:54.198 Training @ 7 epoch...
16:28:54.780   Training iter 50, batch loss 1.6030, batch acc 0.8140
16:28:55.364   Training iter 100, batch loss 1.6057, batch acc 0.7978
16:28:55.933   Training iter 150, batch loss 1.6049, batch acc 0.8078
16:28:56.496   Training iter 200, batch loss 1.6070, batch acc 0.8016
16:28:56.999   Training iter 250, batch loss 1.6061, batch acc 0.8186
16:28:57.544   Training iter 300, batch loss 1.6087, batch acc 0.8024
16:28:58.057   Training iter 350, batch loss 1.6020, batch acc 0.7986
16:28:58.530   Training iter 400, batch loss 1.6015, batch acc 0.8192
16:28:59.013   Training iter 450, batch loss 1.5967, batch acc 0.8192
16:28:59.496   Training iter 500, batch loss 1.6002, batch acc 0.8016
16:28:59.984   Training iter 550, batch loss 1.6002, batch acc 0.8160
16:29:00.501   Training iter 600, batch loss 1.5999, batch acc 0.8162
16:29:00.503 Training @ 8 epoch...
16:29:01.016   Training iter 50, batch loss 1.6016, batch acc 0.8036
16:29:01.571   Training iter 100, batch loss 1.5967, batch acc 0.8106
16:29:02.137   Training iter 150, batch loss 1.5993, batch acc 0.8164
16:29:02.671   Training iter 200, batch loss 1.5981, batch acc 0.8164
16:29:03.186   Training iter 250, batch loss 1.5975, batch acc 0.8112
16:29:03.713   Training iter 300, batch loss 1.5963, batch acc 0.8128
16:29:04.266   Training iter 350, batch loss 1.5989, batch acc 0.8072
16:29:04.831   Training iter 400, batch loss 1.5928, batch acc 0.8214
16:29:05.399   Training iter 450, batch loss 1.5968, batch acc 0.8114
16:29:05.942   Training iter 500, batch loss 1.5986, batch acc 0.8074
16:29:06.466   Training iter 550, batch loss 1.5993, batch acc 0.8106
16:29:07.047   Training iter 600, batch loss 1.5965, batch acc 0.8100
16:29:07.049 Training @ 9 epoch...
16:29:07.614   Training iter 50, batch loss 1.5930, batch acc 0.8166
16:29:08.138   Training iter 100, batch loss 1.5924, batch acc 0.8184
16:29:08.676   Training iter 150, batch loss 1.5937, batch acc 0.8162
16:29:09.223   Training iter 200, batch loss 1.5938, batch acc 0.8156
16:29:09.790   Training iter 250, batch loss 1.5942, batch acc 0.8162
16:29:10.362   Training iter 300, batch loss 1.5953, batch acc 0.8108
16:29:10.925   Training iter 350, batch loss 1.5924, batch acc 0.8202
16:29:11.494   Training iter 400, batch loss 1.5941, batch acc 0.8158
16:29:12.068   Training iter 450, batch loss 1.5920, batch acc 0.8174
16:29:12.650   Training iter 500, batch loss 1.5921, batch acc 0.8146
16:29:13.191   Training iter 550, batch loss 1.5944, batch acc 0.8114
16:29:13.704   Training iter 600, batch loss 1.5934, batch acc 0.8134
16:29:13.706 Training @ 10 epoch...
16:29:14.242   Training iter 50, batch loss 1.5940, batch acc 0.8136
16:29:14.748   Training iter 100, batch loss 1.5958, batch acc 0.8172
16:29:15.286   Training iter 150, batch loss 1.5935, batch acc 0.8218
16:29:15.810   Training iter 200, batch loss 1.5916, batch acc 0.8172
16:29:16.340   Training iter 250, batch loss 1.5860, batch acc 0.8194
16:29:16.882   Training iter 300, batch loss 1.5900, batch acc 0.8180
16:29:17.443   Training iter 350, batch loss 1.5884, batch acc 0.8204
16:29:18.013   Training iter 400, batch loss 1.5866, batch acc 0.8304
16:29:18.579   Training iter 450, batch loss 1.5859, batch acc 0.8232
16:29:19.143   Training iter 500, batch loss 1.5873, batch acc 0.8240
16:29:19.683   Training iter 550, batch loss 1.5884, batch acc 0.8200
16:29:20.197   Training iter 600, batch loss 1.5908, batch acc 0.8182
16:29:20.199 Testing @ 10 epoch...
16:29:20.246     Testing, total mean loss 1.58446, total acc 0.82050
16:29:20.246 Training @ 11 epoch...
16:29:20.767   Training iter 50, batch loss 1.5903, batch acc 0.8164
16:29:21.277   Training iter 100, batch loss 1.5876, batch acc 0.8176
16:29:21.780   Training iter 150, batch loss 1.5904, batch acc 0.8204
16:29:22.301   Training iter 200, batch loss 1.5871, batch acc 0.8198
16:29:22.822   Training iter 250, batch loss 1.5910, batch acc 0.8222
16:29:23.437   Training iter 300, batch loss 1.5853, batch acc 0.8298
16:29:24.205   Training iter 350, batch loss 1.5861, batch acc 0.8290
16:29:24.985   Training iter 400, batch loss 1.5878, batch acc 0.8178
16:29:25.624   Training iter 450, batch loss 1.5882, batch acc 0.8226
16:29:26.206   Training iter 500, batch loss 1.5810, batch acc 0.8292
16:29:26.763   Training iter 550, batch loss 1.5858, batch acc 0.8332
16:29:27.343   Training iter 600, batch loss 1.5833, batch acc 0.8262
16:29:27.345 Training @ 12 epoch...
16:29:27.952   Training iter 50, batch loss 1.5814, batch acc 0.8296
16:29:28.497   Training iter 100, batch loss 1.5917, batch acc 0.8210
16:29:29.060   Training iter 150, batch loss 1.5861, batch acc 0.8202
16:29:29.629   Training iter 200, batch loss 1.5787, batch acc 0.8326
16:29:30.173   Training iter 250, batch loss 1.5843, batch acc 0.8328
16:29:30.712   Training iter 300, batch loss 1.5891, batch acc 0.8290
16:29:31.220   Training iter 350, batch loss 1.5837, batch acc 0.8324
16:29:31.719   Training iter 400, batch loss 1.5864, batch acc 0.8288
16:29:32.230   Training iter 450, batch loss 1.5834, batch acc 0.8356
16:29:32.760   Training iter 500, batch loss 1.5814, batch acc 0.8202
16:29:33.308   Training iter 550, batch loss 1.5863, batch acc 0.8304
16:29:33.885   Training iter 600, batch loss 1.5826, batch acc 0.8236
16:29:33.887 Training @ 13 epoch...
16:29:34.488   Training iter 50, batch loss 1.5786, batch acc 0.8438
16:29:35.058   Training iter 100, batch loss 1.5787, batch acc 0.8388
16:29:35.618   Training iter 150, batch loss 1.5807, batch acc 0.8234
16:29:36.165   Training iter 200, batch loss 1.5821, batch acc 0.8320
16:29:36.710   Training iter 250, batch loss 1.5834, batch acc 0.8356
16:29:37.269   Training iter 300, batch loss 1.5833, batch acc 0.8296
16:29:37.830   Training iter 350, batch loss 1.5796, batch acc 0.8276
16:29:38.381   Training iter 400, batch loss 1.5834, batch acc 0.8234
16:29:38.927   Training iter 450, batch loss 1.5837, batch acc 0.8276
16:29:39.476   Training iter 500, batch loss 1.5861, batch acc 0.8278
16:29:40.024   Training iter 550, batch loss 1.5831, batch acc 0.8322
16:29:40.580   Training iter 600, batch loss 1.5866, batch acc 0.8280
16:29:40.582 Training @ 14 epoch...
16:29:41.163   Training iter 50, batch loss 1.5811, batch acc 0.8250
16:29:41.728   Training iter 100, batch loss 1.5815, batch acc 0.8348
16:29:42.290   Training iter 150, batch loss 1.5845, batch acc 0.8292
16:29:42.865   Training iter 200, batch loss 1.5849, batch acc 0.8244
16:29:43.459   Training iter 250, batch loss 1.5798, batch acc 0.8296
16:29:44.048   Training iter 300, batch loss 1.5789, batch acc 0.8398
16:29:44.651   Training iter 350, batch loss 1.5766, batch acc 0.8466
16:29:45.221   Training iter 400, batch loss 1.5827, batch acc 0.8396
16:29:45.800   Training iter 450, batch loss 1.5802, batch acc 0.8382
16:29:46.361   Training iter 500, batch loss 1.5807, batch acc 0.8342
16:29:46.924   Training iter 550, batch loss 1.5768, batch acc 0.8432
16:29:47.485   Training iter 600, batch loss 1.5781, batch acc 0.8354
16:29:47.486 Training @ 15 epoch...
16:29:48.015   Training iter 50, batch loss 1.5722, batch acc 0.8496
16:29:48.539   Training iter 100, batch loss 1.5772, batch acc 0.8436
16:29:49.068   Training iter 150, batch loss 1.5755, batch acc 0.8358
16:29:49.613   Training iter 200, batch loss 1.5786, batch acc 0.8360
16:29:50.158   Training iter 250, batch loss 1.5803, batch acc 0.8370
16:29:50.705   Training iter 300, batch loss 1.5815, batch acc 0.8302
16:29:51.269   Training iter 350, batch loss 1.5829, batch acc 0.8276
16:29:51.834   Training iter 400, batch loss 1.5834, batch acc 0.8318
16:29:52.395   Training iter 450, batch loss 1.5775, batch acc 0.8428
16:29:52.976   Training iter 500, batch loss 1.5781, batch acc 0.8416
16:29:53.542   Training iter 550, batch loss 1.5791, batch acc 0.8406
16:29:54.089   Training iter 600, batch loss 1.5799, batch acc 0.8352
16:29:54.091 Testing @ 15 epoch...
16:29:54.140     Testing, total mean loss 1.57513, total acc 0.84020
16:29:54.140 Training @ 16 epoch...
16:29:54.712   Training iter 50, batch loss 1.5749, batch acc 0.8426
16:29:55.341   Training iter 100, batch loss 1.5797, batch acc 0.8468
16:29:56.005   Training iter 150, batch loss 1.5795, batch acc 0.8386
16:29:56.667   Training iter 200, batch loss 1.5803, batch acc 0.8386
16:29:57.335   Training iter 250, batch loss 1.5775, batch acc 0.8374
16:29:57.933   Training iter 300, batch loss 1.5792, batch acc 0.8352
16:29:58.474   Training iter 350, batch loss 1.5769, batch acc 0.8384
16:29:59.026   Training iter 400, batch loss 1.5637, batch acc 0.8614
16:29:59.565   Training iter 450, batch loss 1.5849, batch acc 0.8324
16:30:00.127   Training iter 500, batch loss 1.5782, batch acc 0.8376
16:30:00.690   Training iter 550, batch loss 1.5782, batch acc 0.8404
16:30:01.230   Training iter 600, batch loss 1.5754, batch acc 0.8516
16:30:01.232 Training @ 17 epoch...
16:30:01.818   Training iter 50, batch loss 1.5762, batch acc 0.8446
16:30:02.417   Training iter 100, batch loss 1.5760, batch acc 0.8416
16:30:02.976   Training iter 150, batch loss 1.5800, batch acc 0.8336
16:30:03.543   Training iter 200, batch loss 1.5703, batch acc 0.8548
16:30:04.143   Training iter 250, batch loss 1.5799, batch acc 0.8336
16:30:04.734   Training iter 300, batch loss 1.5766, batch acc 0.8492
16:30:05.341   Training iter 350, batch loss 1.5743, batch acc 0.8492
16:30:05.933   Training iter 400, batch loss 1.5839, batch acc 0.8370
16:30:06.532   Training iter 450, batch loss 1.5720, batch acc 0.8508
16:30:07.122   Training iter 500, batch loss 1.5774, batch acc 0.8454
16:30:07.688   Training iter 550, batch loss 1.5744, batch acc 0.8474
16:30:08.236   Training iter 600, batch loss 1.5707, batch acc 0.8598
16:30:08.238 Training @ 18 epoch...
16:30:08.778   Training iter 50, batch loss 1.5738, batch acc 0.8520
16:30:09.306   Training iter 100, batch loss 1.5711, batch acc 0.8544
16:30:09.830   Training iter 150, batch loss 1.5780, batch acc 0.8456
16:30:10.362   Training iter 200, batch loss 1.5707, batch acc 0.8552
16:30:10.907   Training iter 250, batch loss 1.5750, batch acc 0.8438
16:30:11.452   Training iter 300, batch loss 1.5746, batch acc 0.8386
16:30:11.971   Training iter 350, batch loss 1.5779, batch acc 0.8416
16:30:12.522   Training iter 400, batch loss 1.5802, batch acc 0.8432
16:30:13.064   Training iter 450, batch loss 1.5714, batch acc 0.8486
16:30:13.604   Training iter 500, batch loss 1.5745, batch acc 0.8506
16:30:14.144   Training iter 550, batch loss 1.5742, batch acc 0.8570
16:30:14.647   Training iter 600, batch loss 1.5748, batch acc 0.8526
16:30:14.649 Training @ 19 epoch...
16:30:15.136   Training iter 50, batch loss 1.5758, batch acc 0.8408
16:30:15.626   Training iter 100, batch loss 1.5734, batch acc 0.8534
16:30:16.108   Training iter 150, batch loss 1.5710, batch acc 0.8566
16:30:16.575   Training iter 200, batch loss 1.5718, batch acc 0.8572
16:30:17.075   Training iter 250, batch loss 1.5721, batch acc 0.8546
16:30:17.588   Training iter 300, batch loss 1.5755, batch acc 0.8486
16:30:18.095   Training iter 350, batch loss 1.5744, batch acc 0.8504
16:30:18.587   Training iter 400, batch loss 1.5713, batch acc 0.8576
16:30:19.069   Training iter 450, batch loss 1.5747, batch acc 0.8562
16:30:19.555   Training iter 500, batch loss 1.5763, batch acc 0.8498
16:30:20.031   Training iter 550, batch loss 1.5770, batch acc 0.8528
16:30:20.509   Training iter 600, batch loss 1.5707, batch acc 0.8556
16:30:20.511 Training @ 20 epoch...
16:30:20.992   Training iter 50, batch loss 1.5674, batch acc 0.8652
16:30:21.474   Training iter 100, batch loss 1.5763, batch acc 0.8452
16:30:21.963   Training iter 150, batch loss 1.5695, batch acc 0.8646
16:30:22.457   Training iter 200, batch loss 1.5707, batch acc 0.8632
16:30:22.957   Training iter 250, batch loss 1.5770, batch acc 0.8528
16:30:23.448   Training iter 300, batch loss 1.5755, batch acc 0.8410
16:30:23.941   Training iter 350, batch loss 1.5705, batch acc 0.8554
16:30:24.448   Training iter 400, batch loss 1.5778, batch acc 0.8482
16:30:24.957   Training iter 450, batch loss 1.5701, batch acc 0.8562
16:30:25.458   Training iter 500, batch loss 1.5718, batch acc 0.8576
16:30:25.945   Training iter 550, batch loss 1.5741, batch acc 0.8620
16:30:26.427   Training iter 600, batch loss 1.5697, batch acc 0.8570
16:30:26.429 Testing @ 20 epoch...
16:30:26.477     Testing, total mean loss 1.56925, total acc 0.86340
16:30:26.477 Training @ 21 epoch...
16:30:26.979   Training iter 50, batch loss 1.5745, batch acc 0.8596
16:30:27.488   Training iter 100, batch loss 1.5712, batch acc 0.8622
16:30:27.995   Training iter 150, batch loss 1.5705, batch acc 0.8604
16:30:28.512   Training iter 200, batch loss 1.5691, batch acc 0.8622
16:30:29.061   Training iter 250, batch loss 1.5707, batch acc 0.8632
16:30:29.591   Training iter 300, batch loss 1.5679, batch acc 0.8624
16:30:30.116   Training iter 350, batch loss 1.5763, batch acc 0.8540
16:30:30.638   Training iter 400, batch loss 1.5682, batch acc 0.8618
16:30:31.177   Training iter 450, batch loss 1.5773, batch acc 0.8588
16:30:31.706   Training iter 500, batch loss 1.5686, batch acc 0.8648
16:30:32.241   Training iter 550, batch loss 1.5738, batch acc 0.8662
16:30:32.818   Training iter 600, batch loss 1.5700, batch acc 0.8638
16:30:32.819 Training @ 22 epoch...
16:30:33.397   Training iter 50, batch loss 1.5690, batch acc 0.8680
16:30:33.954   Training iter 100, batch loss 1.5709, batch acc 0.8640
16:30:34.527   Training iter 150, batch loss 1.5637, batch acc 0.8710
16:30:35.050   Training iter 200, batch loss 1.5729, batch acc 0.8650
16:30:35.549   Training iter 250, batch loss 1.5739, batch acc 0.8608
16:30:36.047   Training iter 300, batch loss 1.5729, batch acc 0.8682
16:30:36.545   Training iter 350, batch loss 1.5717, batch acc 0.8664
16:30:37.058   Training iter 400, batch loss 1.5716, batch acc 0.8664
16:30:37.588   Training iter 450, batch loss 1.5703, batch acc 0.8680
16:30:38.117   Training iter 500, batch loss 1.5681, batch acc 0.8758
16:30:38.639   Training iter 550, batch loss 1.5689, batch acc 0.8706
16:30:39.153   Training iter 600, batch loss 1.5690, batch acc 0.8754
16:30:39.155 Training @ 23 epoch...
16:30:39.660   Training iter 50, batch loss 1.5660, batch acc 0.8802
16:30:40.165   Training iter 100, batch loss 1.5671, batch acc 0.8794
16:30:40.673   Training iter 150, batch loss 1.5705, batch acc 0.8734
16:30:41.151   Training iter 200, batch loss 1.5708, batch acc 0.8732
16:30:41.622   Training iter 250, batch loss 1.5691, batch acc 0.8784
16:30:42.165   Training iter 300, batch loss 1.5696, batch acc 0.8774
16:30:42.724   Training iter 350, batch loss 1.5732, batch acc 0.8672
16:30:43.313   Training iter 400, batch loss 1.5713, batch acc 0.8764
16:30:43.895   Training iter 450, batch loss 1.5687, batch acc 0.8812
16:30:44.530   Training iter 500, batch loss 1.5647, batch acc 0.8810
16:30:45.198   Training iter 550, batch loss 1.5668, batch acc 0.8752
16:30:45.797   Training iter 600, batch loss 1.5690, batch acc 0.8772
16:30:45.799 Training @ 24 epoch...
16:30:46.374   Training iter 50, batch loss 1.5644, batch acc 0.8848
16:30:46.921   Training iter 100, batch loss 1.5651, batch acc 0.8862
16:30:47.462   Training iter 150, batch loss 1.5721, batch acc 0.8730
16:30:48.002   Training iter 200, batch loss 1.5723, batch acc 0.8750
16:30:48.550   Training iter 250, batch loss 1.5681, batch acc 0.8842
16:30:49.086   Training iter 300, batch loss 1.5668, batch acc 0.8800
16:30:49.609   Training iter 350, batch loss 1.5713, batch acc 0.8796
16:30:50.132   Training iter 400, batch loss 1.5680, batch acc 0.8826
16:30:50.676   Training iter 450, batch loss 1.5653, batch acc 0.8806
16:30:51.205   Training iter 500, batch loss 1.5606, batch acc 0.8940
16:30:51.724   Training iter 550, batch loss 1.5677, batch acc 0.8882
16:30:52.253   Training iter 600, batch loss 1.5660, batch acc 0.8810
16:30:52.255 Training @ 25 epoch...
16:30:52.785   Training iter 50, batch loss 1.5648, batch acc 0.8918
16:30:53.319   Training iter 100, batch loss 1.5615, batch acc 0.8884
16:30:53.848   Training iter 150, batch loss 1.5705, batch acc 0.8894
16:30:54.370   Training iter 200, batch loss 1.5637, batch acc 0.8882
16:30:54.900   Training iter 250, batch loss 1.5711, batch acc 0.8812
16:30:55.459   Training iter 300, batch loss 1.5700, batch acc 0.8824
16:30:55.990   Training iter 350, batch loss 1.5666, batch acc 0.8890
16:30:56.522   Training iter 400, batch loss 1.5641, batch acc 0.8888
16:30:57.056   Training iter 450, batch loss 1.5611, batch acc 0.8916
16:30:57.598   Training iter 500, batch loss 1.5651, batch acc 0.8886
16:30:58.140   Training iter 550, batch loss 1.5645, batch acc 0.8870
16:30:58.674   Training iter 600, batch loss 1.5666, batch acc 0.8886
16:30:58.676 Testing @ 25 epoch...
16:30:58.724     Testing, total mean loss 1.56371, total acc 0.89170
16:30:58.724 Training @ 26 epoch...
16:30:59.260   Training iter 50, batch loss 1.5650, batch acc 0.8874
16:30:59.791   Training iter 100, batch loss 1.5649, batch acc 0.8946
16:31:00.342   Training iter 150, batch loss 1.5653, batch acc 0.8952
16:31:00.904   Training iter 200, batch loss 1.5611, batch acc 0.8940
16:31:01.509   Training iter 250, batch loss 1.5637, batch acc 0.8932
16:31:02.118   Training iter 300, batch loss 1.5637, batch acc 0.8930
16:31:02.685   Training iter 350, batch loss 1.5657, batch acc 0.8904
16:31:03.230   Training iter 400, batch loss 1.5625, batch acc 0.8912
16:31:03.781   Training iter 450, batch loss 1.5627, batch acc 0.8936
16:31:04.354   Training iter 500, batch loss 1.5668, batch acc 0.8822
16:31:04.918   Training iter 550, batch loss 1.5679, batch acc 0.8860
16:31:05.472   Training iter 600, batch loss 1.5668, batch acc 0.8892
16:31:05.474 Training @ 27 epoch...
16:31:06.061   Training iter 50, batch loss 1.5628, batch acc 0.8908
16:31:06.632   Training iter 100, batch loss 1.5655, batch acc 0.8932
16:31:07.179   Training iter 150, batch loss 1.5640, batch acc 0.8898
16:31:07.734   Training iter 200, batch loss 1.5623, batch acc 0.8948
16:31:08.289   Training iter 250, batch loss 1.5641, batch acc 0.8878
16:31:08.858   Training iter 300, batch loss 1.5606, batch acc 0.8930
16:31:09.431   Training iter 350, batch loss 1.5688, batch acc 0.8926
16:31:10.004   Training iter 400, batch loss 1.5607, batch acc 0.8976
16:31:10.580   Training iter 450, batch loss 1.5669, batch acc 0.8858
16:31:11.167   Training iter 500, batch loss 1.5595, batch acc 0.9010
16:31:11.735   Training iter 550, batch loss 1.5636, batch acc 0.8964
16:31:12.325   Training iter 600, batch loss 1.5643, batch acc 0.8910
16:31:12.327 Training @ 28 epoch...
16:31:12.932   Training iter 50, batch loss 1.5665, batch acc 0.8928
16:31:13.498   Training iter 100, batch loss 1.5610, batch acc 0.8918
16:31:14.072   Training iter 150, batch loss 1.5549, batch acc 0.9010
16:31:14.671   Training iter 200, batch loss 1.5610, batch acc 0.8956
16:31:15.284   Training iter 250, batch loss 1.5719, batch acc 0.8844
16:31:15.903   Training iter 300, batch loss 1.5633, batch acc 0.8958
16:31:16.527   Training iter 350, batch loss 1.5610, batch acc 0.8950
16:31:17.159   Training iter 400, batch loss 1.5640, batch acc 0.8922
16:31:17.780   Training iter 450, batch loss 1.5553, batch acc 0.8992
16:31:18.392   Training iter 500, batch loss 1.5641, batch acc 0.8966
16:31:19.004   Training iter 550, batch loss 1.5641, batch acc 0.8968
16:31:19.618   Training iter 600, batch loss 1.5657, batch acc 0.8866
16:31:19.620 Training @ 29 epoch...
16:31:20.234   Training iter 50, batch loss 1.5652, batch acc 0.8954
16:31:20.834   Training iter 100, batch loss 1.5615, batch acc 0.8926
16:31:21.439   Training iter 150, batch loss 1.5609, batch acc 0.8974
16:31:22.051   Training iter 200, batch loss 1.5603, batch acc 0.8932
16:31:22.666   Training iter 250, batch loss 1.5603, batch acc 0.8948
16:31:23.255   Training iter 300, batch loss 1.5629, batch acc 0.8918
16:31:23.885   Training iter 350, batch loss 1.5606, batch acc 0.8950
16:31:24.497   Training iter 400, batch loss 1.5628, batch acc 0.8956
16:31:25.104   Training iter 450, batch loss 1.5620, batch acc 0.8986
16:31:25.707   Training iter 500, batch loss 1.5616, batch acc 0.8950
16:31:26.315   Training iter 550, batch loss 1.5606, batch acc 0.8978
16:31:26.923   Training iter 600, batch loss 1.5648, batch acc 0.8938
16:31:26.925 Training @ 30 epoch...
16:31:27.550   Training iter 50, batch loss 1.5599, batch acc 0.8970
16:31:28.160   Training iter 100, batch loss 1.5633, batch acc 0.8912
16:31:28.784   Training iter 150, batch loss 1.5618, batch acc 0.8934
16:31:29.404   Training iter 200, batch loss 1.5589, batch acc 0.8932
16:31:30.031   Training iter 250, batch loss 1.5626, batch acc 0.8954
16:31:30.638   Training iter 300, batch loss 1.5585, batch acc 0.9022
16:31:31.249   Training iter 350, batch loss 1.5584, batch acc 0.8992
16:31:31.863   Training iter 400, batch loss 1.5622, batch acc 0.8958
16:31:32.477   Training iter 450, batch loss 1.5601, batch acc 0.8994
16:31:33.076   Training iter 500, batch loss 1.5619, batch acc 0.8970
16:31:33.663   Training iter 550, batch loss 1.5658, batch acc 0.8904
16:31:34.270   Training iter 600, batch loss 1.5610, batch acc 0.8970
16:31:34.272 Testing @ 30 epoch...
16:31:34.329     Testing, total mean loss 1.56079, total acc 0.90000
16:31:34.329 Training @ 31 epoch...
16:31:34.945   Training iter 50, batch loss 1.5599, batch acc 0.9012
16:31:35.546   Training iter 100, batch loss 1.5589, batch acc 0.8946
16:31:36.155   Training iter 150, batch loss 1.5613, batch acc 0.8928
16:31:36.769   Training iter 200, batch loss 1.5651, batch acc 0.8918
16:31:37.380   Training iter 250, batch loss 1.5647, batch acc 0.8966
16:31:38.018   Training iter 300, batch loss 1.5598, batch acc 0.8976
16:31:38.698   Training iter 350, batch loss 1.5568, batch acc 0.9014
16:31:39.363   Training iter 400, batch loss 1.5585, batch acc 0.8942
16:31:39.959   Training iter 450, batch loss 1.5629, batch acc 0.8940
16:31:40.532   Training iter 500, batch loss 1.5599, batch acc 0.9034
16:31:41.154   Training iter 550, batch loss 1.5618, batch acc 0.8940
16:31:41.762   Training iter 600, batch loss 1.5578, batch acc 0.9006
16:31:41.764 Training @ 32 epoch...
16:31:42.346   Training iter 50, batch loss 1.5586, batch acc 0.8960
16:31:42.925   Training iter 100, batch loss 1.5612, batch acc 0.8952
16:31:43.489   Training iter 150, batch loss 1.5601, batch acc 0.8976
16:31:44.053   Training iter 200, batch loss 1.5596, batch acc 0.9028
16:31:44.625   Training iter 250, batch loss 1.5633, batch acc 0.8918
16:31:45.214   Training iter 300, batch loss 1.5594, batch acc 0.8994
16:31:45.808   Training iter 350, batch loss 1.5615, batch acc 0.8962
16:31:46.392   Training iter 400, batch loss 1.5618, batch acc 0.8950
16:31:46.968   Training iter 450, batch loss 1.5603, batch acc 0.8968
16:31:47.536   Training iter 500, batch loss 1.5548, batch acc 0.9048
16:31:48.093   Training iter 550, batch loss 1.5581, batch acc 0.9026
16:31:48.658   Training iter 600, batch loss 1.5616, batch acc 0.8980
16:31:48.660 Training @ 33 epoch...
16:31:49.237   Training iter 50, batch loss 1.5604, batch acc 0.8958
16:31:49.793   Training iter 100, batch loss 1.5616, batch acc 0.8936
16:31:50.344   Training iter 150, batch loss 1.5528, batch acc 0.9074
16:31:50.837   Training iter 200, batch loss 1.5579, batch acc 0.9014
16:31:51.360   Training iter 250, batch loss 1.5621, batch acc 0.8968
16:31:51.919   Training iter 300, batch loss 1.5534, batch acc 0.9054
16:31:52.492   Training iter 350, batch loss 1.5572, batch acc 0.8988
16:31:53.060   Training iter 400, batch loss 1.5623, batch acc 0.8994
16:31:53.622   Training iter 450, batch loss 1.5581, batch acc 0.8990
16:31:54.191   Training iter 500, batch loss 1.5633, batch acc 0.8962
16:31:54.733   Training iter 550, batch loss 1.5644, batch acc 0.8916
16:31:55.259   Training iter 600, batch loss 1.5605, batch acc 0.8974
16:31:55.260 Training @ 34 epoch...
16:31:55.808   Training iter 50, batch loss 1.5617, batch acc 0.8950
16:31:56.361   Training iter 100, batch loss 1.5587, batch acc 0.8990
16:31:56.908   Training iter 150, batch loss 1.5657, batch acc 0.8904
16:31:57.450   Training iter 200, batch loss 1.5588, batch acc 0.8990
16:31:58.010   Training iter 250, batch loss 1.5627, batch acc 0.8968
16:31:58.552   Training iter 300, batch loss 1.5578, batch acc 0.8954
16:31:59.097   Training iter 350, batch loss 1.5586, batch acc 0.9002
16:31:59.638   Training iter 400, batch loss 1.5559, batch acc 0.9068
16:32:00.194   Training iter 450, batch loss 1.5583, batch acc 0.8972
16:32:00.759   Training iter 500, batch loss 1.5568, batch acc 0.9036
16:32:01.321   Training iter 550, batch loss 1.5545, batch acc 0.9028
16:32:01.898   Training iter 600, batch loss 1.5563, batch acc 0.9024
16:32:01.899 Training @ 35 epoch...
16:32:02.462   Training iter 50, batch loss 1.5592, batch acc 0.8996
16:32:03.011   Training iter 100, batch loss 1.5561, batch acc 0.8990
16:32:03.569   Training iter 150, batch loss 1.5549, batch acc 0.9034
16:32:04.148   Training iter 200, batch loss 1.5567, batch acc 0.8988
16:32:04.719   Training iter 250, batch loss 1.5610, batch acc 0.8964
16:32:05.283   Training iter 300, batch loss 1.5621, batch acc 0.8978
16:32:05.838   Training iter 350, batch loss 1.5569, batch acc 0.9020
16:32:06.402   Training iter 400, batch loss 1.5588, batch acc 0.9006
16:32:06.955   Training iter 450, batch loss 1.5600, batch acc 0.8974
16:32:07.497   Training iter 500, batch loss 1.5600, batch acc 0.8980
16:32:08.024   Training iter 550, batch loss 1.5566, batch acc 0.9036
16:32:08.546   Training iter 600, batch loss 1.5589, batch acc 0.9000
16:32:08.548 Testing @ 35 epoch...
16:32:08.594     Testing, total mean loss 1.55740, total acc 0.90250
16:32:08.594 Training @ 36 epoch...
16:32:09.090   Training iter 50, batch loss 1.5570, batch acc 0.8972
16:32:09.621   Training iter 100, batch loss 1.5579, batch acc 0.8984
16:32:10.146   Training iter 150, batch loss 1.5581, batch acc 0.8948
16:32:10.680   Training iter 200, batch loss 1.5564, batch acc 0.9012
16:32:11.217   Training iter 250, batch loss 1.5563, batch acc 0.8994
16:32:11.749   Training iter 300, batch loss 1.5559, batch acc 0.9058
16:32:12.246   Training iter 350, batch loss 1.5579, batch acc 0.9012
16:32:12.744   Training iter 400, batch loss 1.5561, batch acc 0.9012
16:32:13.231   Training iter 450, batch loss 1.5601, batch acc 0.8984
16:32:13.728   Training iter 500, batch loss 1.5614, batch acc 0.8952
16:32:14.220   Training iter 550, batch loss 1.5578, batch acc 0.9024
16:32:14.745   Training iter 600, batch loss 1.5594, batch acc 0.9018
16:32:14.746 Training @ 37 epoch...
16:32:15.290   Training iter 50, batch loss 1.5540, batch acc 0.9022
16:32:15.821   Training iter 100, batch loss 1.5575, batch acc 0.8946
16:32:16.316   Training iter 150, batch loss 1.5587, batch acc 0.8992
16:32:16.820   Training iter 200, batch loss 1.5614, batch acc 0.8944
16:32:17.308   Training iter 250, batch loss 1.5596, batch acc 0.9016
16:32:17.813   Training iter 300, batch loss 1.5520, batch acc 0.9082
16:32:18.318   Training iter 350, batch loss 1.5586, batch acc 0.8998
16:32:18.815   Training iter 400, batch loss 1.5604, batch acc 0.8968
16:32:19.307   Training iter 450, batch loss 1.5559, batch acc 0.9024
16:32:19.808   Training iter 500, batch loss 1.5537, batch acc 0.9064
16:32:20.336   Training iter 550, batch loss 1.5603, batch acc 0.8996
16:32:20.868   Training iter 600, batch loss 1.5568, batch acc 0.9024
16:32:20.870 Training @ 38 epoch...
16:32:21.408   Training iter 50, batch loss 1.5596, batch acc 0.8998
16:32:21.935   Training iter 100, batch loss 1.5600, batch acc 0.8974
16:32:22.466   Training iter 150, batch loss 1.5592, batch acc 0.8940
16:32:22.995   Training iter 200, batch loss 1.5504, batch acc 0.9118
16:32:23.515   Training iter 250, batch loss 1.5580, batch acc 0.8980
16:32:24.041   Training iter 300, batch loss 1.5520, batch acc 0.9066
16:32:24.574   Training iter 350, batch loss 1.5564, batch acc 0.9048
16:32:25.112   Training iter 400, batch loss 1.5532, batch acc 0.9050
16:32:25.698   Training iter 450, batch loss 1.5618, batch acc 0.8980
16:32:26.292   Training iter 500, batch loss 1.5569, batch acc 0.8996
16:32:26.858   Training iter 550, batch loss 1.5556, batch acc 0.9062
16:32:27.380   Training iter 600, batch loss 1.5606, batch acc 0.9000
16:32:27.382 Training @ 39 epoch...
16:32:27.919   Training iter 50, batch loss 1.5597, batch acc 0.8970
16:32:28.422   Training iter 100, batch loss 1.5522, batch acc 0.9068
16:32:28.938   Training iter 150, batch loss 1.5549, batch acc 0.9082
16:32:29.470   Training iter 200, batch loss 1.5605, batch acc 0.8924
16:32:29.952   Training iter 250, batch loss 1.5528, batch acc 0.9060
16:32:30.463   Training iter 300, batch loss 1.5551, batch acc 0.9058
16:32:31.002   Training iter 350, batch loss 1.5570, batch acc 0.8986
16:32:31.525   Training iter 400, batch loss 1.5559, batch acc 0.9082
16:32:32.053   Training iter 450, batch loss 1.5607, batch acc 0.9052
16:32:32.586   Training iter 500, batch loss 1.5512, batch acc 0.9040
16:32:33.149   Training iter 550, batch loss 1.5626, batch acc 0.8932
16:32:33.693   Training iter 600, batch loss 1.5571, batch acc 0.9010
16:32:33.694 Training @ 40 epoch...
16:32:34.250   Training iter 50, batch loss 1.5555, batch acc 0.9046
16:32:34.812   Training iter 100, batch loss 1.5564, batch acc 0.9002
16:32:35.341   Training iter 150, batch loss 1.5501, batch acc 0.9072
16:32:35.845   Training iter 200, batch loss 1.5566, batch acc 0.8986
16:32:36.333   Training iter 250, batch loss 1.5588, batch acc 0.9038
16:32:36.861   Training iter 300, batch loss 1.5587, batch acc 0.9004
16:32:37.422   Training iter 350, batch loss 1.5478, batch acc 0.9134
16:32:38.008   Training iter 400, batch loss 1.5584, batch acc 0.9002
16:32:38.565   Training iter 450, batch loss 1.5614, batch acc 0.8922
16:32:39.085   Training iter 500, batch loss 1.5578, batch acc 0.9028
16:32:39.606   Training iter 550, batch loss 1.5587, batch acc 0.8964
16:32:40.131   Training iter 600, batch loss 1.5535, batch acc 0.9064
16:32:40.132 Testing @ 40 epoch...
16:32:40.179     Testing, total mean loss 1.55595, total acc 0.90430
16:32:40.179 Training @ 41 epoch...
16:32:40.712   Training iter 50, batch loss 1.5569, batch acc 0.9046
16:32:41.234   Training iter 100, batch loss 1.5540, batch acc 0.9040
16:32:41.733   Training iter 150, batch loss 1.5548, batch acc 0.9052
16:32:42.253   Training iter 200, batch loss 1.5578, batch acc 0.8976
16:32:42.804   Training iter 250, batch loss 1.5571, batch acc 0.8986
16:32:43.333   Training iter 300, batch loss 1.5554, batch acc 0.9004
16:32:43.846   Training iter 350, batch loss 1.5508, batch acc 0.9072
16:32:44.361   Training iter 400, batch loss 1.5590, batch acc 0.9014
16:32:44.910   Training iter 450, batch loss 1.5529, batch acc 0.9078
16:32:45.474   Training iter 500, batch loss 1.5580, batch acc 0.8986
16:32:46.042   Training iter 550, batch loss 1.5601, batch acc 0.8952
16:32:46.599   Training iter 600, batch loss 1.5536, batch acc 0.9104
16:32:46.601 Training @ 42 epoch...
16:32:47.164   Training iter 50, batch loss 1.5555, batch acc 0.9082
16:32:47.726   Training iter 100, batch loss 1.5535, batch acc 0.9062
16:32:48.294   Training iter 150, batch loss 1.5530, batch acc 0.9044
16:32:48.852   Training iter 200, batch loss 1.5529, batch acc 0.9046
16:32:49.416   Training iter 250, batch loss 1.5563, batch acc 0.9040
16:32:49.967   Training iter 300, batch loss 1.5600, batch acc 0.8962
16:32:50.513   Training iter 350, batch loss 1.5579, batch acc 0.8994
16:32:51.051   Training iter 400, batch loss 1.5566, batch acc 0.9016
16:32:51.550   Training iter 450, batch loss 1.5520, batch acc 0.9070
16:32:52.047   Training iter 500, batch loss 1.5547, batch acc 0.9018
16:32:52.555   Training iter 550, batch loss 1.5566, batch acc 0.9032
16:32:53.153   Training iter 600, batch loss 1.5566, batch acc 0.9012
16:32:53.155 Training @ 43 epoch...
16:32:53.692   Training iter 50, batch loss 1.5552, batch acc 0.8996
16:32:54.221   Training iter 100, batch loss 1.5542, batch acc 0.9036
16:32:54.756   Training iter 150, batch loss 1.5578, batch acc 0.9002
16:32:55.314   Training iter 200, batch loss 1.5587, batch acc 0.9016
16:32:55.834   Training iter 250, batch loss 1.5595, batch acc 0.8986
16:32:56.363   Training iter 300, batch loss 1.5545, batch acc 0.9054
16:32:56.903   Training iter 350, batch loss 1.5548, batch acc 0.9054
16:32:57.469   Training iter 400, batch loss 1.5577, batch acc 0.9024
16:32:58.022   Training iter 450, batch loss 1.5518, batch acc 0.9050
16:32:58.580   Training iter 500, batch loss 1.5542, batch acc 0.9036
16:32:59.126   Training iter 550, batch loss 1.5509, batch acc 0.9128
16:32:59.660   Training iter 600, batch loss 1.5527, batch acc 0.9080
16:32:59.662 Training @ 44 epoch...
16:33:00.203   Training iter 50, batch loss 1.5584, batch acc 0.8964
16:33:00.756   Training iter 100, batch loss 1.5616, batch acc 0.8972
16:33:01.353   Training iter 150, batch loss 1.5478, batch acc 0.9160
16:33:01.979   Training iter 200, batch loss 1.5502, batch acc 0.9088
16:33:02.558   Training iter 250, batch loss 1.5558, batch acc 0.9062
16:33:03.131   Training iter 300, batch loss 1.5573, batch acc 0.9012
16:33:03.714   Training iter 350, batch loss 1.5519, batch acc 0.9052
16:33:04.294   Training iter 400, batch loss 1.5563, batch acc 0.9000
16:33:04.876   Training iter 450, batch loss 1.5547, batch acc 0.9038
16:33:05.467   Training iter 500, batch loss 1.5542, batch acc 0.9038
16:33:06.075   Training iter 550, batch loss 1.5587, batch acc 0.8970
16:33:06.672   Training iter 600, batch loss 1.5509, batch acc 0.9092
16:33:06.674 Training @ 45 epoch...
16:33:07.256   Training iter 50, batch loss 1.5552, batch acc 0.9050
16:33:07.814   Training iter 100, batch loss 1.5551, batch acc 0.9044
16:33:08.374   Training iter 150, batch loss 1.5516, batch acc 0.9112
16:33:08.937   Training iter 200, batch loss 1.5512, batch acc 0.9026
16:33:09.539   Training iter 250, batch loss 1.5505, batch acc 0.9042
16:33:10.126   Training iter 300, batch loss 1.5569, batch acc 0.8990
16:33:10.713   Training iter 350, batch loss 1.5553, batch acc 0.9024
16:33:11.322   Training iter 400, batch loss 1.5587, batch acc 0.8990
16:33:11.936   Training iter 450, batch loss 1.5573, batch acc 0.9036
16:33:12.539   Training iter 500, batch loss 1.5581, batch acc 0.8998
16:33:13.100   Training iter 550, batch loss 1.5515, batch acc 0.9064
16:33:13.663   Training iter 600, batch loss 1.5516, batch acc 0.9074
16:33:13.665 Testing @ 45 epoch...
16:33:13.712     Testing, total mean loss 1.55478, total acc 0.90540
16:33:13.713 Training @ 46 epoch...
16:33:14.302   Training iter 50, batch loss 1.5581, batch acc 0.8986
16:33:14.871   Training iter 100, batch loss 1.5485, batch acc 0.9130
16:33:15.402   Training iter 150, batch loss 1.5564, batch acc 0.9006
16:33:15.927   Training iter 200, batch loss 1.5514, batch acc 0.9042
16:33:16.500   Training iter 250, batch loss 1.5558, batch acc 0.9010
16:33:17.050   Training iter 300, batch loss 1.5513, batch acc 0.9056
16:33:17.601   Training iter 350, batch loss 1.5586, batch acc 0.9008
16:33:18.117   Training iter 400, batch loss 1.5595, batch acc 0.9026
16:33:18.628   Training iter 450, batch loss 1.5526, batch acc 0.9108
16:33:19.135   Training iter 500, batch loss 1.5521, batch acc 0.9100
16:33:19.631   Training iter 550, batch loss 1.5538, batch acc 0.9048
16:33:20.129   Training iter 600, batch loss 1.5506, batch acc 0.9060
16:33:20.131 Training @ 47 epoch...
16:33:20.637   Training iter 50, batch loss 1.5584, batch acc 0.9008
16:33:21.124   Training iter 100, batch loss 1.5479, batch acc 0.9134
16:33:21.626   Training iter 150, batch loss 1.5562, batch acc 0.8992
16:33:22.115   Training iter 200, batch loss 1.5496, batch acc 0.9106
16:33:22.589   Training iter 250, batch loss 1.5515, batch acc 0.9060
16:33:23.069   Training iter 300, batch loss 1.5557, batch acc 0.9028
16:33:23.574   Training iter 350, batch loss 1.5487, batch acc 0.9108
16:33:24.076   Training iter 400, batch loss 1.5545, batch acc 0.9030
16:33:24.574   Training iter 450, batch loss 1.5549, batch acc 0.9024
16:33:25.085   Training iter 500, batch loss 1.5580, batch acc 0.9012
16:33:25.648   Training iter 550, batch loss 1.5565, batch acc 0.9046
16:33:26.215   Training iter 600, batch loss 1.5535, batch acc 0.9050
16:33:26.217 Training @ 48 epoch...
16:33:26.769   Training iter 50, batch loss 1.5551, batch acc 0.9038
16:33:27.324   Training iter 100, batch loss 1.5521, batch acc 0.9066
16:33:27.877   Training iter 150, batch loss 1.5563, batch acc 0.9014
16:33:28.430   Training iter 200, batch loss 1.5512, batch acc 0.9104
16:33:29.002   Training iter 250, batch loss 1.5579, batch acc 0.8994
16:33:29.598   Training iter 300, batch loss 1.5536, batch acc 0.9070
16:33:30.167   Training iter 350, batch loss 1.5504, batch acc 0.9102
16:33:30.720   Training iter 400, batch loss 1.5530, batch acc 0.9060
16:33:31.240   Training iter 450, batch loss 1.5506, batch acc 0.9048
16:33:31.783   Training iter 500, batch loss 1.5489, batch acc 0.9098
16:33:32.317   Training iter 550, batch loss 1.5582, batch acc 0.9040
16:33:32.833   Training iter 600, batch loss 1.5540, batch acc 0.9052
16:33:32.835 Training @ 49 epoch...
16:33:33.355   Training iter 50, batch loss 1.5452, batch acc 0.9132
16:33:33.865   Training iter 100, batch loss 1.5539, batch acc 0.9044
16:33:34.379   Training iter 150, batch loss 1.5542, batch acc 0.8976
16:33:34.903   Training iter 200, batch loss 1.5532, batch acc 0.9054
16:33:35.391   Training iter 250, batch loss 1.5561, batch acc 0.9044
16:33:35.880   Training iter 300, batch loss 1.5505, batch acc 0.9092
16:33:36.372   Training iter 350, batch loss 1.5554, batch acc 0.9058
16:33:36.865   Training iter 400, batch loss 1.5570, batch acc 0.9026
16:33:37.404   Training iter 450, batch loss 1.5463, batch acc 0.9144
16:33:37.928   Training iter 500, batch loss 1.5532, batch acc 0.9076
16:33:38.450   Training iter 550, batch loss 1.5569, batch acc 0.9000
16:33:38.966   Training iter 600, batch loss 1.5558, batch acc 0.9022
16:33:38.968 Training @ 50 epoch...
16:33:39.491   Training iter 50, batch loss 1.5544, batch acc 0.9002
16:33:40.020   Training iter 100, batch loss 1.5488, batch acc 0.9106
16:33:40.527   Training iter 150, batch loss 1.5479, batch acc 0.9124
16:33:41.046   Training iter 200, batch loss 1.5562, batch acc 0.9018
16:33:41.589   Training iter 250, batch loss 1.5531, batch acc 0.9054
16:33:42.146   Training iter 300, batch loss 1.5517, batch acc 0.9054
16:33:42.703   Training iter 350, batch loss 1.5504, batch acc 0.9092
16:33:43.268   Training iter 400, batch loss 1.5547, batch acc 0.9038
16:33:43.823   Training iter 450, batch loss 1.5572, batch acc 0.9036
16:33:44.383   Training iter 500, batch loss 1.5501, batch acc 0.9082
16:33:44.954   Training iter 550, batch loss 1.5554, batch acc 0.9070
16:33:45.528   Training iter 600, batch loss 1.5541, batch acc 0.9052
16:33:45.529 Testing @ 50 epoch...
16:33:45.577     Testing, total mean loss 1.55386, total acc 0.90610
16:33:45.577 Training @ 51 epoch...
16:33:46.142   Training iter 50, batch loss 1.5552, batch acc 0.9032
16:33:46.696   Training iter 100, batch loss 1.5472, batch acc 0.9134
16:33:47.227   Training iter 150, batch loss 1.5501, batch acc 0.9054
16:33:47.750   Training iter 200, batch loss 1.5576, batch acc 0.9028
16:33:48.286   Training iter 250, batch loss 1.5524, batch acc 0.9098
16:33:48.794   Training iter 300, batch loss 1.5520, batch acc 0.9106
16:33:49.319   Training iter 350, batch loss 1.5514, batch acc 0.9090
16:33:49.840   Training iter 400, batch loss 1.5515, batch acc 0.9104
16:33:50.360   Training iter 450, batch loss 1.5568, batch acc 0.8998
16:33:50.876   Training iter 500, batch loss 1.5536, batch acc 0.9038
16:33:51.406   Training iter 550, batch loss 1.5518, batch acc 0.9114
16:33:51.921   Training iter 600, batch loss 1.5520, batch acc 0.9036
16:33:51.923 Training @ 52 epoch...
16:33:52.457   Training iter 50, batch loss 1.5478, batch acc 0.9146
16:33:52.986   Training iter 100, batch loss 1.5521, batch acc 0.9056
16:33:53.519   Training iter 150, batch loss 1.5575, batch acc 0.9026
16:33:54.053   Training iter 200, batch loss 1.5559, batch acc 0.9034
16:33:54.602   Training iter 250, batch loss 1.5536, batch acc 0.9048
16:33:55.148   Training iter 300, batch loss 1.5491, batch acc 0.9142
16:33:55.676   Training iter 350, batch loss 1.5514, batch acc 0.9052
16:33:56.199   Training iter 400, batch loss 1.5496, batch acc 0.9082
16:33:56.727   Training iter 450, batch loss 1.5514, batch acc 0.9056
16:33:57.282   Training iter 500, batch loss 1.5568, batch acc 0.9034
16:33:57.859   Training iter 550, batch loss 1.5511, batch acc 0.9072
16:33:58.422   Training iter 600, batch loss 1.5510, batch acc 0.9086
16:33:58.424 Training @ 53 epoch...
16:33:58.996   Training iter 50, batch loss 1.5563, batch acc 0.9016
16:33:59.582   Training iter 100, batch loss 1.5480, batch acc 0.9086
16:34:00.182   Training iter 150, batch loss 1.5532, batch acc 0.9056
16:34:00.788   Training iter 200, batch loss 1.5526, batch acc 0.9054
16:34:01.400   Training iter 250, batch loss 1.5529, batch acc 0.9062
16:34:02.033   Training iter 300, batch loss 1.5500, batch acc 0.9120
16:34:02.636   Training iter 350, batch loss 1.5559, batch acc 0.9000
16:34:03.217   Training iter 400, batch loss 1.5512, batch acc 0.9042
16:34:03.791   Training iter 450, batch loss 1.5493, batch acc 0.9102
16:34:04.396   Training iter 500, batch loss 1.5515, batch acc 0.9068
16:34:04.979   Training iter 550, batch loss 1.5514, batch acc 0.9070
16:34:05.559   Training iter 600, batch loss 1.5520, batch acc 0.9088
16:34:05.560 Training @ 54 epoch...
16:34:06.158   Training iter 50, batch loss 1.5461, batch acc 0.9132
16:34:06.747   Training iter 100, batch loss 1.5549, batch acc 0.8990
16:34:07.349   Training iter 150, batch loss 1.5558, batch acc 0.9052
16:34:07.958   Training iter 200, batch loss 1.5496, batch acc 0.9102
16:34:08.555   Training iter 250, batch loss 1.5497, batch acc 0.9102
16:34:09.110   Training iter 300, batch loss 1.5558, batch acc 0.9022
16:34:09.721   Training iter 350, batch loss 1.5472, batch acc 0.9138
16:34:10.277   Training iter 400, batch loss 1.5555, batch acc 0.8980
16:34:10.832   Training iter 450, batch loss 1.5556, batch acc 0.9030
16:34:11.379   Training iter 500, batch loss 1.5495, batch acc 0.9134
16:34:11.937   Training iter 550, batch loss 1.5514, batch acc 0.9116
16:34:12.494   Training iter 600, batch loss 1.5495, batch acc 0.9146
16:34:12.496 Training @ 55 epoch...
16:34:12.988   Training iter 50, batch loss 1.5522, batch acc 0.9072
16:34:13.477   Training iter 100, batch loss 1.5472, batch acc 0.9076
16:34:13.958   Training iter 150, batch loss 1.5546, batch acc 0.9032
16:34:14.466   Training iter 200, batch loss 1.5464, batch acc 0.9154
16:34:14.984   Training iter 250, batch loss 1.5500, batch acc 0.9144
16:34:15.478   Training iter 300, batch loss 1.5554, batch acc 0.9040
16:34:16.004   Training iter 350, batch loss 1.5495, batch acc 0.9064
16:34:16.529   Training iter 400, batch loss 1.5509, batch acc 0.9122
16:34:17.085   Training iter 450, batch loss 1.5567, batch acc 0.8992
16:34:17.649   Training iter 500, batch loss 1.5524, batch acc 0.9054
16:34:18.196   Training iter 550, batch loss 1.5561, batch acc 0.9044
16:34:18.833   Training iter 600, batch loss 1.5452, batch acc 0.9116
16:34:18.836 Testing @ 55 epoch...
16:34:18.896     Testing, total mean loss 1.55226, total acc 0.90910
16:34:18.896 Training @ 56 epoch...
16:34:19.516   Training iter 50, batch loss 1.5528, batch acc 0.9062
16:34:20.044   Training iter 100, batch loss 1.5528, batch acc 0.9074
16:34:20.574   Training iter 150, batch loss 1.5512, batch acc 0.9126
16:34:21.098   Training iter 200, batch loss 1.5481, batch acc 0.9086
16:34:21.616   Training iter 250, batch loss 1.5482, batch acc 0.9082
16:34:22.144   Training iter 300, batch loss 1.5524, batch acc 0.9072
16:34:22.644   Training iter 350, batch loss 1.5509, batch acc 0.9086
16:34:23.154   Training iter 400, batch loss 1.5511, batch acc 0.9004
16:34:23.663   Training iter 450, batch loss 1.5510, batch acc 0.9120
16:34:24.161   Training iter 500, batch loss 1.5507, batch acc 0.9098
16:34:24.659   Training iter 550, batch loss 1.5499, batch acc 0.9108
16:34:25.163   Training iter 600, batch loss 1.5537, batch acc 0.9026
16:34:25.165 Training @ 57 epoch...
16:34:25.702   Training iter 50, batch loss 1.5489, batch acc 0.9110
16:34:26.233   Training iter 100, batch loss 1.5526, batch acc 0.9118
16:34:26.774   Training iter 150, batch loss 1.5531, batch acc 0.9006
16:34:27.339   Training iter 200, batch loss 1.5544, batch acc 0.9048
16:34:27.941   Training iter 250, batch loss 1.5459, batch acc 0.9128
16:34:28.450   Training iter 300, batch loss 1.5500, batch acc 0.9070
16:34:29.036   Training iter 350, batch loss 1.5533, batch acc 0.9058
16:34:29.700   Training iter 400, batch loss 1.5460, batch acc 0.9152
16:34:30.280   Training iter 450, batch loss 1.5511, batch acc 0.9122
16:34:30.850   Training iter 500, batch loss 1.5588, batch acc 0.8974
16:34:31.424   Training iter 550, batch loss 1.5508, batch acc 0.9080
16:34:32.022   Training iter 600, batch loss 1.5454, batch acc 0.9144
16:34:32.023 Training @ 58 epoch...
16:34:32.599   Training iter 50, batch loss 1.5504, batch acc 0.9118
16:34:33.184   Training iter 100, batch loss 1.5475, batch acc 0.9146
16:34:33.757   Training iter 150, batch loss 1.5553, batch acc 0.9028
16:34:34.319   Training iter 200, batch loss 1.5492, batch acc 0.9038
16:34:34.857   Training iter 250, batch loss 1.5470, batch acc 0.9124
16:34:35.370   Training iter 300, batch loss 1.5517, batch acc 0.9074
16:34:35.877   Training iter 350, batch loss 1.5525, batch acc 0.9068
16:34:36.384   Training iter 400, batch loss 1.5461, batch acc 0.9124
16:34:36.898   Training iter 450, batch loss 1.5510, batch acc 0.9076
16:34:37.439   Training iter 500, batch loss 1.5498, batch acc 0.9120
16:34:37.982   Training iter 550, batch loss 1.5513, batch acc 0.9078
16:34:38.513   Training iter 600, batch loss 1.5549, batch acc 0.9064
16:34:38.514 Training @ 59 epoch...
16:34:39.055   Training iter 50, batch loss 1.5469, batch acc 0.9108
16:34:39.595   Training iter 100, batch loss 1.5478, batch acc 0.9166
16:34:40.150   Training iter 150, batch loss 1.5544, batch acc 0.9012
16:34:40.689   Training iter 200, batch loss 1.5488, batch acc 0.9086
16:34:41.242   Training iter 250, batch loss 1.5471, batch acc 0.9124
16:34:41.774   Training iter 300, batch loss 1.5534, batch acc 0.9010
16:34:42.301   Training iter 350, batch loss 1.5531, batch acc 0.9058
16:34:42.827   Training iter 400, batch loss 1.5512, batch acc 0.9112
16:34:43.343   Training iter 450, batch loss 1.5459, batch acc 0.9134
16:34:43.858   Training iter 500, batch loss 1.5541, batch acc 0.9082
16:34:44.375   Training iter 550, batch loss 1.5492, batch acc 0.9104
16:34:44.910   Training iter 600, batch loss 1.5526, batch acc 0.9056
16:34:44.912 Training @ 60 epoch...
16:34:45.444   Training iter 50, batch loss 1.5499, batch acc 0.9068
16:34:45.935   Training iter 100, batch loss 1.5538, batch acc 0.9084
16:34:46.449   Training iter 150, batch loss 1.5503, batch acc 0.9054
16:34:46.961   Training iter 200, batch loss 1.5464, batch acc 0.9154
16:34:47.468   Training iter 250, batch loss 1.5564, batch acc 0.9066
16:34:47.954   Training iter 300, batch loss 1.5523, batch acc 0.9040
16:34:48.445   Training iter 350, batch loss 1.5503, batch acc 0.9110
16:34:48.976   Training iter 400, batch loss 1.5441, batch acc 0.9156
16:34:49.524   Training iter 450, batch loss 1.5490, batch acc 0.9148
16:34:50.061   Training iter 500, batch loss 1.5527, batch acc 0.9030
16:34:50.610   Training iter 550, batch loss 1.5487, batch acc 0.9122
16:34:51.168   Training iter 600, batch loss 1.5483, batch acc 0.9100
16:34:51.170 Testing @ 60 epoch...
16:34:51.221     Testing, total mean loss 1.55134, total acc 0.90860
16:34:51.221 Training @ 61 epoch...
16:34:51.762   Training iter 50, batch loss 1.5531, batch acc 0.9048
16:34:52.305   Training iter 100, batch loss 1.5506, batch acc 0.9072
16:34:52.841   Training iter 150, batch loss 1.5506, batch acc 0.9098
16:34:53.379   Training iter 200, batch loss 1.5521, batch acc 0.9054
16:34:53.917   Training iter 250, batch loss 1.5477, batch acc 0.9110
16:34:54.437   Training iter 300, batch loss 1.5492, batch acc 0.9106
16:34:54.961   Training iter 350, batch loss 1.5524, batch acc 0.9082
16:34:55.518   Training iter 400, batch loss 1.5478, batch acc 0.9088
16:34:56.102   Training iter 450, batch loss 1.5460, batch acc 0.9116
16:34:56.676   Training iter 500, batch loss 1.5479, batch acc 0.9084
16:34:57.256   Training iter 550, batch loss 1.5489, batch acc 0.9130
16:34:57.754   Training iter 600, batch loss 1.5523, batch acc 0.9096
16:34:57.756 Training @ 62 epoch...
16:34:58.263   Training iter 50, batch loss 1.5473, batch acc 0.9132
16:34:58.765   Training iter 100, batch loss 1.5494, batch acc 0.9050
16:34:59.288   Training iter 150, batch loss 1.5496, batch acc 0.9124
16:34:59.817   Training iter 200, batch loss 1.5495, batch acc 0.9098
16:35:00.359   Training iter 250, batch loss 1.5522, batch acc 0.9092
16:35:00.863   Training iter 300, batch loss 1.5466, batch acc 0.9118
16:35:01.397   Training iter 350, batch loss 1.5486, batch acc 0.9120
16:35:02.006   Training iter 400, batch loss 1.5500, batch acc 0.9106
16:35:02.612   Training iter 450, batch loss 1.5483, batch acc 0.9076
16:35:03.214   Training iter 500, batch loss 1.5488, batch acc 0.9074
16:35:03.760   Training iter 550, batch loss 1.5534, batch acc 0.9046
16:35:04.349   Training iter 600, batch loss 1.5509, batch acc 0.9118
16:35:04.351 Training @ 63 epoch...
16:35:04.911   Training iter 50, batch loss 1.5510, batch acc 0.9062
16:35:05.485   Training iter 100, batch loss 1.5484, batch acc 0.9096
16:35:06.042   Training iter 150, batch loss 1.5470, batch acc 0.9160
16:35:06.576   Training iter 200, batch loss 1.5519, batch acc 0.9106
16:35:07.115   Training iter 250, batch loss 1.5461, batch acc 0.9116
16:35:07.657   Training iter 300, batch loss 1.5471, batch acc 0.9112
16:35:08.184   Training iter 350, batch loss 1.5480, batch acc 0.9110
16:35:08.703   Training iter 400, batch loss 1.5497, batch acc 0.9102
16:35:09.246   Training iter 450, batch loss 1.5549, batch acc 0.9020
16:35:09.791   Training iter 500, batch loss 1.5513, batch acc 0.9116
16:35:10.349   Training iter 550, batch loss 1.5502, batch acc 0.9076
16:35:10.892   Training iter 600, batch loss 1.5471, batch acc 0.9124
16:35:10.894 Training @ 64 epoch...
16:35:11.463   Training iter 50, batch loss 1.5524, batch acc 0.9078
16:35:12.074   Training iter 100, batch loss 1.5500, batch acc 0.9098
16:35:12.626   Training iter 150, batch loss 1.5486, batch acc 0.9112
16:35:13.158   Training iter 200, batch loss 1.5507, batch acc 0.9104
16:35:13.692   Training iter 250, batch loss 1.5486, batch acc 0.9096
16:35:14.239   Training iter 300, batch loss 1.5494, batch acc 0.9096
16:35:14.799   Training iter 350, batch loss 1.5452, batch acc 0.9126
16:35:15.349   Training iter 400, batch loss 1.5493, batch acc 0.9118
16:35:15.899   Training iter 450, batch loss 1.5470, batch acc 0.9130
16:35:16.457   Training iter 500, batch loss 1.5493, batch acc 0.9128
16:35:17.014   Training iter 550, batch loss 1.5492, batch acc 0.9102
16:35:17.571   Training iter 600, batch loss 1.5493, batch acc 0.9074
16:35:17.573 Training @ 65 epoch...
16:35:18.126   Training iter 50, batch loss 1.5476, batch acc 0.9116
16:35:18.666   Training iter 100, batch loss 1.5486, batch acc 0.9072
16:35:19.228   Training iter 150, batch loss 1.5482, batch acc 0.9122
16:35:19.753   Training iter 200, batch loss 1.5498, batch acc 0.9120
16:35:20.266   Training iter 250, batch loss 1.5495, batch acc 0.9112
16:35:20.792   Training iter 300, batch loss 1.5518, batch acc 0.9044
16:35:21.298   Training iter 350, batch loss 1.5487, batch acc 0.9120
16:35:21.805   Training iter 400, batch loss 1.5450, batch acc 0.9186
16:35:22.341   Training iter 450, batch loss 1.5503, batch acc 0.9100
16:35:22.884   Training iter 500, batch loss 1.5495, batch acc 0.9096
16:35:23.408   Training iter 550, batch loss 1.5517, batch acc 0.9076
16:35:23.920   Training iter 600, batch loss 1.5456, batch acc 0.9138
16:35:23.922 Testing @ 65 epoch...
16:35:23.970     Testing, total mean loss 1.55016, total acc 0.91050
16:35:23.970 Training @ 66 epoch...
16:35:24.533   Training iter 50, batch loss 1.5520, batch acc 0.9082
16:35:25.095   Training iter 100, batch loss 1.5529, batch acc 0.9058
16:35:25.658   Training iter 150, batch loss 1.5511, batch acc 0.9098
16:35:26.242   Training iter 200, batch loss 1.5490, batch acc 0.9074
16:35:26.825   Training iter 250, batch loss 1.5450, batch acc 0.9176
16:35:27.415   Training iter 300, batch loss 1.5450, batch acc 0.9194
16:35:28.005   Training iter 350, batch loss 1.5441, batch acc 0.9168
16:35:28.574   Training iter 400, batch loss 1.5502, batch acc 0.9086
16:35:29.105   Training iter 450, batch loss 1.5497, batch acc 0.9084
16:35:29.627   Training iter 500, batch loss 1.5479, batch acc 0.9074
16:35:30.158   Training iter 550, batch loss 1.5540, batch acc 0.9020
16:35:30.690   Training iter 600, batch loss 1.5431, batch acc 0.9176
16:35:30.692 Training @ 67 epoch...
16:35:31.224   Training iter 50, batch loss 1.5486, batch acc 0.9128
16:35:31.763   Training iter 100, batch loss 1.5511, batch acc 0.9088
16:35:32.313   Training iter 150, batch loss 1.5485, batch acc 0.9098
16:35:32.863   Training iter 200, batch loss 1.5483, batch acc 0.9112
16:35:33.422   Training iter 250, batch loss 1.5505, batch acc 0.9056
16:35:33.965   Training iter 300, batch loss 1.5479, batch acc 0.9116
16:35:34.505   Training iter 350, batch loss 1.5456, batch acc 0.9178
16:35:35.038   Training iter 400, batch loss 1.5493, batch acc 0.9102
16:35:35.554   Training iter 450, batch loss 1.5438, batch acc 0.9154
16:35:36.076   Training iter 500, batch loss 1.5521, batch acc 0.9078
16:35:36.589   Training iter 550, batch loss 1.5472, batch acc 0.9114
16:35:37.127   Training iter 600, batch loss 1.5491, batch acc 0.9080
16:35:37.129 Training @ 68 epoch...
16:35:37.674   Training iter 50, batch loss 1.5485, batch acc 0.9116
16:35:38.189   Training iter 100, batch loss 1.5486, batch acc 0.9102
16:35:38.690   Training iter 150, batch loss 1.5489, batch acc 0.9096
16:35:39.198   Training iter 200, batch loss 1.5453, batch acc 0.9192
16:35:39.719   Training iter 250, batch loss 1.5487, batch acc 0.9108
16:35:40.258   Training iter 300, batch loss 1.5431, batch acc 0.9170
16:35:40.778   Training iter 350, batch loss 1.5494, batch acc 0.9108
16:35:41.309   Training iter 400, batch loss 1.5504, batch acc 0.9052
16:35:41.834   Training iter 450, batch loss 1.5541, batch acc 0.9034
16:35:42.347   Training iter 500, batch loss 1.5516, batch acc 0.9088
16:35:42.861   Training iter 550, batch loss 1.5454, batch acc 0.9126
16:35:43.374   Training iter 600, batch loss 1.5443, batch acc 0.9156
16:35:43.376 Training @ 69 epoch...
16:35:43.894   Training iter 50, batch loss 1.5496, batch acc 0.9102
16:35:44.404   Training iter 100, batch loss 1.5493, batch acc 0.9088
16:35:44.938   Training iter 150, batch loss 1.5465, batch acc 0.9130
16:35:45.460   Training iter 200, batch loss 1.5448, batch acc 0.9110
16:35:45.979   Training iter 250, batch loss 1.5446, batch acc 0.9184
16:35:46.495   Training iter 300, batch loss 1.5489, batch acc 0.9100
16:35:47.019   Training iter 350, batch loss 1.5515, batch acc 0.9064
16:35:47.557   Training iter 400, batch loss 1.5500, batch acc 0.9080
16:35:48.074   Training iter 450, batch loss 1.5556, batch acc 0.9018
16:35:48.594   Training iter 500, batch loss 1.5458, batch acc 0.9166
16:35:49.174   Training iter 550, batch loss 1.5425, batch acc 0.9150
16:35:49.768   Training iter 600, batch loss 1.5469, batch acc 0.9096
16:35:49.770 Training @ 70 epoch...
16:35:50.355   Training iter 50, batch loss 1.5428, batch acc 0.9142
16:35:50.942   Training iter 100, batch loss 1.5458, batch acc 0.9138
16:35:51.550   Training iter 150, batch loss 1.5483, batch acc 0.9120
16:35:52.132   Training iter 200, batch loss 1.5456, batch acc 0.9090
16:35:52.709   Training iter 250, batch loss 1.5524, batch acc 0.9064
16:35:53.279   Training iter 300, batch loss 1.5513, batch acc 0.9080
16:35:53.837   Training iter 350, batch loss 1.5485, batch acc 0.9148
16:35:54.397   Training iter 400, batch loss 1.5477, batch acc 0.9164
16:35:54.963   Training iter 450, batch loss 1.5474, batch acc 0.9116
16:35:55.520   Training iter 500, batch loss 1.5530, batch acc 0.9058
16:35:56.049   Training iter 550, batch loss 1.5450, batch acc 0.9148
16:35:56.587   Training iter 600, batch loss 1.5448, batch acc 0.9126
16:35:56.588 Testing @ 70 epoch...
16:35:56.637     Testing, total mean loss 1.54907, total acc 0.91300
16:35:56.638 Training @ 71 epoch...
16:35:57.197   Training iter 50, batch loss 1.5459, batch acc 0.9128
16:35:57.741   Training iter 100, batch loss 1.5457, batch acc 0.9124
16:35:58.270   Training iter 150, batch loss 1.5489, batch acc 0.9104
16:35:58.795   Training iter 200, batch loss 1.5455, batch acc 0.9162
16:35:59.322   Training iter 250, batch loss 1.5449, batch acc 0.9128
16:35:59.857   Training iter 300, batch loss 1.5502, batch acc 0.9124
16:36:00.394   Training iter 350, batch loss 1.5494, batch acc 0.9090
16:36:00.899   Training iter 400, batch loss 1.5518, batch acc 0.9154
16:36:01.401   Training iter 450, batch loss 1.5473, batch acc 0.9070
16:36:01.934   Training iter 500, batch loss 1.5478, batch acc 0.9052
16:36:02.497   Training iter 550, batch loss 1.5452, batch acc 0.9174
16:36:03.070   Training iter 600, batch loss 1.5487, batch acc 0.9100
16:36:03.072 Training @ 72 epoch...
16:36:03.667   Training iter 50, batch loss 1.5389, batch acc 0.9218
16:36:04.223   Training iter 100, batch loss 1.5484, batch acc 0.9086
16:36:04.783   Training iter 150, batch loss 1.5492, batch acc 0.9104
16:36:05.351   Training iter 200, batch loss 1.5471, batch acc 0.9122
16:36:05.938   Training iter 250, batch loss 1.5493, batch acc 0.9090
16:36:06.540   Training iter 300, batch loss 1.5480, batch acc 0.9140
16:36:07.134   Training iter 350, batch loss 1.5460, batch acc 0.9132
16:36:07.718   Training iter 400, batch loss 1.5446, batch acc 0.9184
16:36:08.283   Training iter 450, batch loss 1.5489, batch acc 0.9084
16:36:08.837   Training iter 500, batch loss 1.5514, batch acc 0.9094
16:36:09.399   Training iter 550, batch loss 1.5441, batch acc 0.9144
16:36:09.955   Training iter 600, batch loss 1.5520, batch acc 0.9006
16:36:09.956 Training @ 73 epoch...
16:36:10.523   Training iter 50, batch loss 1.5495, batch acc 0.9108
16:36:11.086   Training iter 100, batch loss 1.5472, batch acc 0.9144
16:36:11.632   Training iter 150, batch loss 1.5503, batch acc 0.9110
16:36:12.173   Training iter 200, batch loss 1.5466, batch acc 0.9136
16:36:12.729   Training iter 250, batch loss 1.5449, batch acc 0.9150
16:36:13.259   Training iter 300, batch loss 1.5432, batch acc 0.9132
16:36:13.783   Training iter 350, batch loss 1.5465, batch acc 0.9090
16:36:14.321   Training iter 400, batch loss 1.5514, batch acc 0.9116
16:36:14.855   Training iter 450, batch loss 1.5462, batch acc 0.9136
16:36:15.382   Training iter 500, batch loss 1.5439, batch acc 0.9164
16:36:15.910   Training iter 550, batch loss 1.5497, batch acc 0.9094
16:36:16.451   Training iter 600, batch loss 1.5465, batch acc 0.9102
16:36:16.453 Training @ 74 epoch...
16:36:17.003   Training iter 50, batch loss 1.5485, batch acc 0.9060
16:36:17.546   Training iter 100, batch loss 1.5441, batch acc 0.9128
16:36:18.087   Training iter 150, batch loss 1.5514, batch acc 0.9036
16:36:18.642   Training iter 200, batch loss 1.5455, batch acc 0.9146
16:36:19.183   Training iter 250, batch loss 1.5490, batch acc 0.9106
16:36:19.739   Training iter 300, batch loss 1.5504, batch acc 0.9128
16:36:20.278   Training iter 350, batch loss 1.5445, batch acc 0.9154
16:36:20.808   Training iter 400, batch loss 1.5487, batch acc 0.9128
16:36:21.342   Training iter 450, batch loss 1.5506, batch acc 0.9096
16:36:21.887   Training iter 500, batch loss 1.5436, batch acc 0.9160
16:36:22.450   Training iter 550, batch loss 1.5481, batch acc 0.9078
16:36:22.996   Training iter 600, batch loss 1.5394, batch acc 0.9204
16:36:22.998 Training @ 75 epoch...
16:36:23.543   Training iter 50, batch loss 1.5433, batch acc 0.9144
16:36:24.076   Training iter 100, batch loss 1.5452, batch acc 0.9134
16:36:24.621   Training iter 150, batch loss 1.5493, batch acc 0.9128
16:36:25.166   Training iter 200, batch loss 1.5478, batch acc 0.9092
16:36:25.704   Training iter 250, batch loss 1.5461, batch acc 0.9182
16:36:26.249   Training iter 300, batch loss 1.5444, batch acc 0.9148
16:36:26.792   Training iter 350, batch loss 1.5536, batch acc 0.9044
16:36:27.336   Training iter 400, batch loss 1.5472, batch acc 0.9094
16:36:27.850   Training iter 450, batch loss 1.5496, batch acc 0.9088
16:36:28.365   Training iter 500, batch loss 1.5446, batch acc 0.9142
16:36:28.885   Training iter 550, batch loss 1.5447, batch acc 0.9162
16:36:29.409   Training iter 600, batch loss 1.5448, batch acc 0.9172
16:36:29.411 Testing @ 75 epoch...
16:36:29.459     Testing, total mean loss 1.54892, total acc 0.91230
16:36:29.459 Training @ 76 epoch...
16:36:29.982   Training iter 50, batch loss 1.5450, batch acc 0.9152
16:36:30.509   Training iter 100, batch loss 1.5443, batch acc 0.9148
16:36:31.041   Training iter 150, batch loss 1.5425, batch acc 0.9154
16:36:31.571   Training iter 200, batch loss 1.5499, batch acc 0.9132
16:36:32.102   Training iter 250, batch loss 1.5487, batch acc 0.9094
16:36:32.627   Training iter 300, batch loss 1.5465, batch acc 0.9190
16:36:33.152   Training iter 350, batch loss 1.5430, batch acc 0.9114
16:36:33.698   Training iter 400, batch loss 1.5446, batch acc 0.9172
16:36:34.266   Training iter 450, batch loss 1.5431, batch acc 0.9142
16:36:34.803   Training iter 500, batch loss 1.5494, batch acc 0.9090
16:36:35.285   Training iter 550, batch loss 1.5492, batch acc 0.9082
16:36:35.758   Training iter 600, batch loss 1.5522, batch acc 0.9090
16:36:35.760 Training @ 77 epoch...
16:36:36.236   Training iter 50, batch loss 1.5449, batch acc 0.9148
16:36:36.710   Training iter 100, batch loss 1.5466, batch acc 0.9136
16:36:37.197   Training iter 150, batch loss 1.5452, batch acc 0.9176
16:36:37.731   Training iter 200, batch loss 1.5525, batch acc 0.9066
16:36:38.259   Training iter 250, batch loss 1.5422, batch acc 0.9204
16:36:38.785   Training iter 300, batch loss 1.5457, batch acc 0.9108
16:36:39.326   Training iter 350, batch loss 1.5485, batch acc 0.9090
16:36:39.869   Training iter 400, batch loss 1.5461, batch acc 0.9072
16:36:40.418   Training iter 450, batch loss 1.5496, batch acc 0.9072
16:36:40.944   Training iter 500, batch loss 1.5493, batch acc 0.9174
16:36:41.478   Training iter 550, batch loss 1.5473, batch acc 0.9118
16:36:42.014   Training iter 600, batch loss 1.5381, batch acc 0.9202
16:36:42.016 Training @ 78 epoch...
16:36:42.558   Training iter 50, batch loss 1.5460, batch acc 0.9140
16:36:43.086   Training iter 100, batch loss 1.5410, batch acc 0.9206
16:36:43.610   Training iter 150, batch loss 1.5457, batch acc 0.9154
16:36:44.139   Training iter 200, batch loss 1.5481, batch acc 0.9096
16:36:44.653   Training iter 250, batch loss 1.5506, batch acc 0.9094
16:36:45.162   Training iter 300, batch loss 1.5451, batch acc 0.9160
16:36:45.673   Training iter 350, batch loss 1.5500, batch acc 0.9064
16:36:46.192   Training iter 400, batch loss 1.5450, batch acc 0.9134
16:36:46.706   Training iter 450, batch loss 1.5510, batch acc 0.9086
16:36:47.220   Training iter 500, batch loss 1.5416, batch acc 0.9150
16:36:47.728   Training iter 550, batch loss 1.5483, batch acc 0.9118
16:36:48.244   Training iter 600, batch loss 1.5409, batch acc 0.9170
16:36:48.246 Training @ 79 epoch...
16:36:48.764   Training iter 50, batch loss 1.5445, batch acc 0.9162
16:36:49.280   Training iter 100, batch loss 1.5490, batch acc 0.9104
16:36:49.805   Training iter 150, batch loss 1.5488, batch acc 0.9120
16:36:50.322   Training iter 200, batch loss 1.5442, batch acc 0.9116
16:36:50.846   Training iter 250, batch loss 1.5481, batch acc 0.9094
16:36:51.368   Training iter 300, batch loss 1.5421, batch acc 0.9166
16:36:51.889   Training iter 350, batch loss 1.5499, batch acc 0.9116
16:36:52.412   Training iter 400, batch loss 1.5414, batch acc 0.9180
16:36:52.954   Training iter 450, batch loss 1.5475, batch acc 0.9106
16:36:53.514   Training iter 500, batch loss 1.5426, batch acc 0.9184
16:36:54.071   Training iter 550, batch loss 1.5424, batch acc 0.9182
16:36:54.624   Training iter 600, batch loss 1.5516, batch acc 0.9092
16:36:54.626 Training @ 80 epoch...
16:36:55.191   Training iter 50, batch loss 1.5456, batch acc 0.9174
16:36:55.754   Training iter 100, batch loss 1.5464, batch acc 0.9134
16:36:56.317   Training iter 150, batch loss 1.5468, batch acc 0.9118
16:36:56.882   Training iter 200, batch loss 1.5447, batch acc 0.9130
16:36:57.455   Training iter 250, batch loss 1.5456, batch acc 0.9100
16:36:58.018   Training iter 300, batch loss 1.5474, batch acc 0.9118
16:36:58.571   Training iter 350, batch loss 1.5422, batch acc 0.9148
16:36:59.122   Training iter 400, batch loss 1.5459, batch acc 0.9120
16:36:59.648   Training iter 450, batch loss 1.5466, batch acc 0.9138
16:37:00.179   Training iter 500, batch loss 1.5463, batch acc 0.9156
16:37:00.717   Training iter 550, batch loss 1.5473, batch acc 0.9110
16:37:01.254   Training iter 600, batch loss 1.5454, batch acc 0.9186
16:37:01.256 Testing @ 80 epoch...
16:37:01.303     Testing, total mean loss 1.54854, total acc 0.91320
16:37:01.303 Training @ 81 epoch...
16:37:01.864   Training iter 50, batch loss 1.5434, batch acc 0.9182
16:37:02.422   Training iter 100, batch loss 1.5428, batch acc 0.9164
16:37:02.962   Training iter 150, batch loss 1.5506, batch acc 0.9028
16:37:03.511   Training iter 200, batch loss 1.5410, batch acc 0.9224
16:37:04.043   Training iter 250, batch loss 1.5469, batch acc 0.9124
16:37:04.598   Training iter 300, batch loss 1.5458, batch acc 0.9114
16:37:05.143   Training iter 350, batch loss 1.5465, batch acc 0.9134
16:37:05.669   Training iter 400, batch loss 1.5462, batch acc 0.9162
16:37:06.190   Training iter 450, batch loss 1.5473, batch acc 0.9090
16:37:06.717   Training iter 500, batch loss 1.5412, batch acc 0.9192
16:37:07.245   Training iter 550, batch loss 1.5439, batch acc 0.9168
16:37:07.776   Training iter 600, batch loss 1.5521, batch acc 0.9076
16:37:07.778 Training @ 82 epoch...
16:37:08.315   Training iter 50, batch loss 1.5458, batch acc 0.9130
16:37:08.860   Training iter 100, batch loss 1.5529, batch acc 0.9070
16:37:09.450   Training iter 150, batch loss 1.5465, batch acc 0.9122
16:37:10.051   Training iter 200, batch loss 1.5472, batch acc 0.9116
16:37:10.645   Training iter 250, batch loss 1.5447, batch acc 0.9120
16:37:11.191   Training iter 300, batch loss 1.5439, batch acc 0.9174
16:37:11.750   Training iter 350, batch loss 1.5450, batch acc 0.9142
16:37:12.327   Training iter 400, batch loss 1.5464, batch acc 0.9136
16:37:12.923   Training iter 450, batch loss 1.5470, batch acc 0.9112
16:37:13.531   Training iter 500, batch loss 1.5394, batch acc 0.9210
16:37:14.134   Training iter 550, batch loss 1.5434, batch acc 0.9206
16:37:14.708   Training iter 600, batch loss 1.5426, batch acc 0.9178
16:37:14.710 Training @ 83 epoch...
16:37:15.269   Training iter 50, batch loss 1.5423, batch acc 0.9188
16:37:15.812   Training iter 100, batch loss 1.5412, batch acc 0.9124
16:37:16.360   Training iter 150, batch loss 1.5493, batch acc 0.9086
16:37:16.906   Training iter 200, batch loss 1.5481, batch acc 0.9094
16:37:17.451   Training iter 250, batch loss 1.5455, batch acc 0.9164
16:37:17.992   Training iter 300, batch loss 1.5459, batch acc 0.9152
16:37:18.523   Training iter 350, batch loss 1.5459, batch acc 0.9178
16:37:19.083   Training iter 400, batch loss 1.5475, batch acc 0.9120
16:37:19.637   Training iter 450, batch loss 1.5449, batch acc 0.9120
16:37:20.171   Training iter 500, batch loss 1.5431, batch acc 0.9140
16:37:20.691   Training iter 550, batch loss 1.5441, batch acc 0.9144
16:37:21.223   Training iter 600, batch loss 1.5449, batch acc 0.9144
16:37:21.225 Training @ 84 epoch...
16:37:21.753   Training iter 50, batch loss 1.5442, batch acc 0.9160
16:37:22.274   Training iter 100, batch loss 1.5409, batch acc 0.9210
16:37:22.796   Training iter 150, batch loss 1.5446, batch acc 0.9172
16:37:23.319   Training iter 200, batch loss 1.5468, batch acc 0.9132
16:37:23.847   Training iter 250, batch loss 1.5489, batch acc 0.9078
16:37:24.389   Training iter 300, batch loss 1.5451, batch acc 0.9148
16:37:24.935   Training iter 350, batch loss 1.5400, batch acc 0.9190
16:37:25.508   Training iter 400, batch loss 1.5461, batch acc 0.9160
16:37:26.084   Training iter 450, batch loss 1.5466, batch acc 0.9084
16:37:26.657   Training iter 500, batch loss 1.5470, batch acc 0.9162
16:37:27.227   Training iter 550, batch loss 1.5419, batch acc 0.9192
16:37:27.798   Training iter 600, batch loss 1.5494, batch acc 0.9078
16:37:27.799 Training @ 85 epoch...
16:37:28.343   Training iter 50, batch loss 1.5464, batch acc 0.9124
16:37:28.881   Training iter 100, batch loss 1.5413, batch acc 0.9198
16:37:29.432   Training iter 150, batch loss 1.5457, batch acc 0.9152
16:37:29.984   Training iter 200, batch loss 1.5416, batch acc 0.9154
16:37:30.543   Training iter 250, batch loss 1.5455, batch acc 0.9146
16:37:31.115   Training iter 300, batch loss 1.5502, batch acc 0.9084
16:37:31.635   Training iter 350, batch loss 1.5429, batch acc 0.9192
16:37:32.166   Training iter 400, batch loss 1.5445, batch acc 0.9146
16:37:32.698   Training iter 450, batch loss 1.5441, batch acc 0.9150
16:37:33.224   Training iter 500, batch loss 1.5423, batch acc 0.9158
16:37:33.773   Training iter 550, batch loss 1.5441, batch acc 0.9160
16:37:34.372   Training iter 600, batch loss 1.5501, batch acc 0.9082
16:37:34.374 Testing @ 85 epoch...
16:37:34.422     Testing, total mean loss 1.54670, total acc 0.91440
16:37:34.422 Training @ 86 epoch...
16:37:34.964   Training iter 50, batch loss 1.5521, batch acc 0.9076
16:37:35.507   Training iter 100, batch loss 1.5422, batch acc 0.9174
16:37:36.059   Training iter 150, batch loss 1.5400, batch acc 0.9186
16:37:36.593   Training iter 200, batch loss 1.5439, batch acc 0.9102
16:37:37.133   Training iter 250, batch loss 1.5447, batch acc 0.9152
16:37:37.696   Training iter 300, batch loss 1.5431, batch acc 0.9180
16:37:38.239   Training iter 350, batch loss 1.5427, batch acc 0.9166
16:37:38.792   Training iter 400, batch loss 1.5427, batch acc 0.9142
16:37:39.338   Training iter 450, batch loss 1.5502, batch acc 0.9076
16:37:39.880   Training iter 500, batch loss 1.5445, batch acc 0.9158
16:37:40.426   Training iter 550, batch loss 1.5462, batch acc 0.9176
16:37:40.977   Training iter 600, batch loss 1.5438, batch acc 0.9192
16:37:40.979 Training @ 87 epoch...
16:37:41.551   Training iter 50, batch loss 1.5420, batch acc 0.9170
16:37:42.113   Training iter 100, batch loss 1.5395, batch acc 0.9192
16:37:42.681   Training iter 150, batch loss 1.5406, batch acc 0.9192
16:37:43.256   Training iter 200, batch loss 1.5484, batch acc 0.9134
16:37:43.805   Training iter 250, batch loss 1.5470, batch acc 0.9156
16:37:44.366   Training iter 300, batch loss 1.5446, batch acc 0.9160
16:37:44.930   Training iter 350, batch loss 1.5473, batch acc 0.9098
16:37:45.489   Training iter 400, batch loss 1.5427, batch acc 0.9128
16:37:46.051   Training iter 450, batch loss 1.5421, batch acc 0.9176
16:37:46.627   Training iter 500, batch loss 1.5466, batch acc 0.9136
16:37:47.181   Training iter 550, batch loss 1.5454, batch acc 0.9148
16:37:47.736   Training iter 600, batch loss 1.5493, batch acc 0.9072
16:37:47.738 Training @ 88 epoch...
16:37:48.296   Training iter 50, batch loss 1.5514, batch acc 0.9084
16:37:48.834   Training iter 100, batch loss 1.5427, batch acc 0.9140
16:37:49.389   Training iter 150, batch loss 1.5423, batch acc 0.9174
16:37:49.951   Training iter 200, batch loss 1.5472, batch acc 0.9104
16:37:50.516   Training iter 250, batch loss 1.5438, batch acc 0.9238
16:37:51.077   Training iter 300, batch loss 1.5431, batch acc 0.9138
16:37:51.648   Training iter 350, batch loss 1.5415, batch acc 0.9178
16:37:52.220   Training iter 400, batch loss 1.5404, batch acc 0.9134
16:37:52.787   Training iter 450, batch loss 1.5416, batch acc 0.9198
16:37:53.322   Training iter 500, batch loss 1.5466, batch acc 0.9096
16:37:53.830   Training iter 550, batch loss 1.5496, batch acc 0.9094
16:37:54.335   Training iter 600, batch loss 1.5432, batch acc 0.9156
16:37:54.337 Training @ 89 epoch...
16:37:54.845   Training iter 50, batch loss 1.5447, batch acc 0.9136
16:37:55.372   Training iter 100, batch loss 1.5506, batch acc 0.9046
16:37:55.868   Training iter 150, batch loss 1.5426, batch acc 0.9128
16:37:56.381   Training iter 200, batch loss 1.5453, batch acc 0.9132
16:37:56.885   Training iter 250, batch loss 1.5456, batch acc 0.9116
16:37:57.434   Training iter 300, batch loss 1.5431, batch acc 0.9160
16:37:57.977   Training iter 350, batch loss 1.5445, batch acc 0.9136
16:37:58.514   Training iter 400, batch loss 1.5428, batch acc 0.9188
16:37:59.039   Training iter 450, batch loss 1.5432, batch acc 0.9184
16:37:59.558   Training iter 500, batch loss 1.5419, batch acc 0.9194
16:38:00.099   Training iter 550, batch loss 1.5373, batch acc 0.9236
16:38:00.665   Training iter 600, batch loss 1.5480, batch acc 0.9112
16:38:00.667 Training @ 90 epoch...
16:38:01.233   Training iter 50, batch loss 1.5462, batch acc 0.9132
16:38:01.796   Training iter 100, batch loss 1.5459, batch acc 0.9126
16:38:02.347   Training iter 150, batch loss 1.5492, batch acc 0.9062
16:38:02.885   Training iter 200, batch loss 1.5416, batch acc 0.9150
16:38:03.394   Training iter 250, batch loss 1.5433, batch acc 0.9148
16:38:03.908   Training iter 300, batch loss 1.5419, batch acc 0.9198
16:38:04.447   Training iter 350, batch loss 1.5465, batch acc 0.9128
16:38:04.985   Training iter 400, batch loss 1.5446, batch acc 0.9132
16:38:05.518   Training iter 450, batch loss 1.5463, batch acc 0.9152
16:38:06.055   Training iter 500, batch loss 1.5417, batch acc 0.9202
16:38:06.572   Training iter 550, batch loss 1.5387, batch acc 0.9164
16:38:07.130   Training iter 600, batch loss 1.5444, batch acc 0.9166
16:38:07.132 Testing @ 90 epoch...
16:38:07.179     Testing, total mean loss 1.54628, total acc 0.91380
16:38:07.179 Training @ 91 epoch...
16:38:07.725   Training iter 50, batch loss 1.5429, batch acc 0.9158
16:38:08.255   Training iter 100, batch loss 1.5432, batch acc 0.9136
16:38:08.804   Training iter 150, batch loss 1.5426, batch acc 0.9156
16:38:09.376   Training iter 200, batch loss 1.5440, batch acc 0.9150
16:38:09.951   Training iter 250, batch loss 1.5444, batch acc 0.9140
16:38:10.550   Training iter 300, batch loss 1.5423, batch acc 0.9154
16:38:11.085   Training iter 350, batch loss 1.5455, batch acc 0.9146
16:38:11.617   Training iter 400, batch loss 1.5478, batch acc 0.9094
16:38:12.154   Training iter 450, batch loss 1.5435, batch acc 0.9142
16:38:12.685   Training iter 500, batch loss 1.5426, batch acc 0.9186
16:38:13.250   Training iter 550, batch loss 1.5441, batch acc 0.9186
16:38:13.831   Training iter 600, batch loss 1.5433, batch acc 0.9154
16:38:13.833 Training @ 92 epoch...
16:38:14.420   Training iter 50, batch loss 1.5485, batch acc 0.9092
16:38:14.969   Training iter 100, batch loss 1.5407, batch acc 0.9158
16:38:15.504   Training iter 150, batch loss 1.5463, batch acc 0.9118
16:38:16.039   Training iter 200, batch loss 1.5446, batch acc 0.9166
16:38:16.573   Training iter 250, batch loss 1.5417, batch acc 0.9166
16:38:17.123   Training iter 300, batch loss 1.5429, batch acc 0.9188
16:38:17.676   Training iter 350, batch loss 1.5477, batch acc 0.9118
16:38:18.203   Training iter 400, batch loss 1.5422, batch acc 0.9152
16:38:18.730   Training iter 450, batch loss 1.5490, batch acc 0.9134
16:38:19.250   Training iter 500, batch loss 1.5411, batch acc 0.9184
16:38:19.797   Training iter 550, batch loss 1.5404, batch acc 0.9162
16:38:20.312   Training iter 600, batch loss 1.5399, batch acc 0.9192
16:38:20.314 Training @ 93 epoch...
16:38:20.831   Training iter 50, batch loss 1.5461, batch acc 0.9114
16:38:21.350   Training iter 100, batch loss 1.5408, batch acc 0.9188
16:38:21.864   Training iter 150, batch loss 1.5454, batch acc 0.9114
16:38:22.375   Training iter 200, batch loss 1.5453, batch acc 0.9184
16:38:22.893   Training iter 250, batch loss 1.5429, batch acc 0.9128
16:38:23.410   Training iter 300, batch loss 1.5417, batch acc 0.9148
16:38:23.926   Training iter 350, batch loss 1.5455, batch acc 0.9152
16:38:24.462   Training iter 400, batch loss 1.5409, batch acc 0.9182
16:38:24.991   Training iter 450, batch loss 1.5431, batch acc 0.9160
16:38:25.558   Training iter 500, batch loss 1.5415, batch acc 0.9184
16:38:26.290   Training iter 550, batch loss 1.5456, batch acc 0.9162
16:38:26.834   Training iter 600, batch loss 1.5446, batch acc 0.9108
16:38:26.836 Training @ 94 epoch...
16:38:27.425   Training iter 50, batch loss 1.5395, batch acc 0.9210
16:38:27.984   Training iter 100, batch loss 1.5428, batch acc 0.9124
16:38:28.535   Training iter 150, batch loss 1.5415, batch acc 0.9152
16:38:29.087   Training iter 200, batch loss 1.5433, batch acc 0.9176
16:38:29.682   Training iter 250, batch loss 1.5387, batch acc 0.9232
16:38:30.287   Training iter 300, batch loss 1.5444, batch acc 0.9154
16:38:30.867   Training iter 350, batch loss 1.5487, batch acc 0.9074
16:38:31.441   Training iter 400, batch loss 1.5434, batch acc 0.9166
16:38:32.015   Training iter 450, batch loss 1.5454, batch acc 0.9122
16:38:32.586   Training iter 500, batch loss 1.5461, batch acc 0.9142
16:38:33.168   Training iter 550, batch loss 1.5472, batch acc 0.9094
16:38:33.737   Training iter 600, batch loss 1.5418, batch acc 0.9172
16:38:33.739 Training @ 95 epoch...
16:38:34.320   Training iter 50, batch loss 1.5444, batch acc 0.9184
16:38:34.888   Training iter 100, batch loss 1.5454, batch acc 0.9146
16:38:35.460   Training iter 150, batch loss 1.5450, batch acc 0.9108
16:38:36.012   Training iter 200, batch loss 1.5450, batch acc 0.9144
16:38:36.558   Training iter 250, batch loss 1.5433, batch acc 0.9160
16:38:37.111   Training iter 300, batch loss 1.5401, batch acc 0.9178
16:38:37.667   Training iter 350, batch loss 1.5429, batch acc 0.9140
16:38:38.219   Training iter 400, batch loss 1.5410, batch acc 0.9186
16:38:38.776   Training iter 450, batch loss 1.5459, batch acc 0.9108
16:38:39.340   Training iter 500, batch loss 1.5383, batch acc 0.9200
16:38:39.908   Training iter 550, batch loss 1.5421, batch acc 0.9176
16:38:40.479   Training iter 600, batch loss 1.5462, batch acc 0.9116
16:38:40.481 Testing @ 95 epoch...
16:38:40.530     Testing, total mean loss 1.54573, total acc 0.91490
16:38:40.530 Training @ 96 epoch...
16:38:41.095   Training iter 50, batch loss 1.5409, batch acc 0.9212
16:38:41.638   Training iter 100, batch loss 1.5465, batch acc 0.9114
16:38:42.180   Training iter 150, batch loss 1.5451, batch acc 0.9112
16:38:42.718   Training iter 200, batch loss 1.5412, batch acc 0.9180
16:38:43.265   Training iter 250, batch loss 1.5445, batch acc 0.9150
16:38:43.801   Training iter 300, batch loss 1.5393, batch acc 0.9228
16:38:44.349   Training iter 350, batch loss 1.5417, batch acc 0.9178
16:38:44.882   Training iter 400, batch loss 1.5439, batch acc 0.9142
16:38:45.434   Training iter 450, batch loss 1.5447, batch acc 0.9142
16:38:45.981   Training iter 500, batch loss 1.5446, batch acc 0.9126
16:38:46.524   Training iter 550, batch loss 1.5390, batch acc 0.9206
16:38:47.077   Training iter 600, batch loss 1.5475, batch acc 0.9120
16:38:47.078 Training @ 97 epoch...
16:38:47.656   Training iter 50, batch loss 1.5372, batch acc 0.9242
16:38:48.226   Training iter 100, batch loss 1.5466, batch acc 0.9108
16:38:48.790   Training iter 150, batch loss 1.5418, batch acc 0.9160
16:38:49.355   Training iter 200, batch loss 1.5419, batch acc 0.9158
16:38:49.926   Training iter 250, batch loss 1.5455, batch acc 0.9148
16:38:50.489   Training iter 300, batch loss 1.5458, batch acc 0.9106
16:38:51.045   Training iter 350, batch loss 1.5389, batch acc 0.9200
16:38:51.587   Training iter 400, batch loss 1.5491, batch acc 0.9126
16:38:52.121   Training iter 450, batch loss 1.5402, batch acc 0.9200
16:38:52.659   Training iter 500, batch loss 1.5410, batch acc 0.9168
16:38:53.209   Training iter 550, batch loss 1.5428, batch acc 0.9158
16:38:53.749   Training iter 600, batch loss 1.5464, batch acc 0.9118
16:38:53.751 Training @ 98 epoch...
16:38:54.300   Training iter 50, batch loss 1.5487, batch acc 0.9048
16:38:54.817   Training iter 100, batch loss 1.5383, batch acc 0.9228
16:38:55.346   Training iter 150, batch loss 1.5415, batch acc 0.9174
16:38:55.860   Training iter 200, batch loss 1.5424, batch acc 0.9172
16:38:56.385   Training iter 250, batch loss 1.5408, batch acc 0.9162
16:38:56.927   Training iter 300, batch loss 1.5437, batch acc 0.9122
16:38:57.490   Training iter 350, batch loss 1.5435, batch acc 0.9190
16:38:58.047   Training iter 400, batch loss 1.5439, batch acc 0.9158
16:38:58.589   Training iter 450, batch loss 1.5439, batch acc 0.9162
16:38:59.114   Training iter 500, batch loss 1.5440, batch acc 0.9154
16:38:59.654   Training iter 550, batch loss 1.5441, batch acc 0.9150
16:39:00.197   Training iter 600, batch loss 1.5411, batch acc 0.9184
16:39:00.199 Training @ 99 epoch...
16:39:00.752   Training iter 50, batch loss 1.5410, batch acc 0.9176
16:39:01.306   Training iter 100, batch loss 1.5436, batch acc 0.9176
16:39:01.899   Training iter 150, batch loss 1.5438, batch acc 0.9158
16:39:02.492   Training iter 200, batch loss 1.5496, batch acc 0.9088
16:39:03.110   Training iter 250, batch loss 1.5424, batch acc 0.9158
16:39:03.722   Training iter 300, batch loss 1.5406, batch acc 0.9170
16:39:04.362   Training iter 350, batch loss 1.5394, batch acc 0.9242
16:39:04.956   Training iter 400, batch loss 1.5390, batch acc 0.9198
16:39:05.544   Training iter 450, batch loss 1.5451, batch acc 0.9108
16:39:06.120   Training iter 500, batch loss 1.5420, batch acc 0.9150
16:39:06.655   Training iter 550, batch loss 1.5467, batch acc 0.9140
16:39:07.181   Training iter 600, batch loss 1.5400, batch acc 0.9198
16:39:07.183 Training @ 100 epoch...
16:39:07.698   Training iter 50, batch loss 1.5425, batch acc 0.9146
16:39:08.186   Training iter 100, batch loss 1.5403, batch acc 0.9184
16:39:08.685   Training iter 150, batch loss 1.5428, batch acc 0.9156
16:39:09.206   Training iter 200, batch loss 1.5437, batch acc 0.9144
16:39:09.748   Training iter 250, batch loss 1.5426, batch acc 0.9158
16:39:10.287   Training iter 300, batch loss 1.5449, batch acc 0.9158
16:39:10.791   Training iter 350, batch loss 1.5441, batch acc 0.9130
16:39:11.297   Training iter 400, batch loss 1.5450, batch acc 0.9106
16:39:11.794   Training iter 450, batch loss 1.5413, batch acc 0.9204
16:39:12.305   Training iter 500, batch loss 1.5462, batch acc 0.9128
16:39:12.838   Training iter 550, batch loss 1.5422, batch acc 0.9166
16:39:13.374   Training iter 600, batch loss 1.5365, batch acc 0.9236
16:39:13.376 Testing @ 100 epoch...
16:39:13.425     Testing, total mean loss 1.54563, total acc 0.91450
16:39:13.425 Plot @ 100 epoch...
16:39:13.425 Training @ 101 epoch...
16:39:13.986   Training iter 50, batch loss 1.5391, batch acc 0.9170
16:39:14.549   Training iter 100, batch loss 1.5483, batch acc 0.9112
16:39:15.085   Training iter 150, batch loss 1.5395, batch acc 0.9162
16:39:15.621   Training iter 200, batch loss 1.5386, batch acc 0.9218
16:39:16.186   Training iter 250, batch loss 1.5422, batch acc 0.9224
16:39:16.766   Training iter 300, batch loss 1.5409, batch acc 0.9182
16:39:17.334   Training iter 350, batch loss 1.5418, batch acc 0.9172
16:39:17.924   Training iter 400, batch loss 1.5422, batch acc 0.9158
16:39:18.515   Training iter 450, batch loss 1.5460, batch acc 0.9144
16:39:19.083   Training iter 500, batch loss 1.5452, batch acc 0.9116
16:39:19.646   Training iter 550, batch loss 1.5434, batch acc 0.9142
16:39:20.205   Training iter 600, batch loss 1.5426, batch acc 0.9154
16:39:20.207 Training @ 102 epoch...
16:39:20.778   Training iter 50, batch loss 1.5405, batch acc 0.9172
16:39:21.341   Training iter 100, batch loss 1.5451, batch acc 0.9144
16:39:21.909   Training iter 150, batch loss 1.5429, batch acc 0.9168
16:39:22.488   Training iter 200, batch loss 1.5406, batch acc 0.9176
16:39:23.068   Training iter 250, batch loss 1.5439, batch acc 0.9144
16:39:23.626   Training iter 300, batch loss 1.5476, batch acc 0.9106
16:39:24.186   Training iter 350, batch loss 1.5378, batch acc 0.9218
16:39:24.745   Training iter 400, batch loss 1.5393, batch acc 0.9176
16:39:25.285   Training iter 450, batch loss 1.5443, batch acc 0.9108
16:39:25.822   Training iter 500, batch loss 1.5442, batch acc 0.9170
16:39:26.360   Training iter 550, batch loss 1.5393, batch acc 0.9184
16:39:26.901   Training iter 600, batch loss 1.5416, batch acc 0.9230
16:39:26.903 Training @ 103 epoch...
16:39:27.458   Training iter 50, batch loss 1.5440, batch acc 0.9158
16:39:27.998   Training iter 100, batch loss 1.5443, batch acc 0.9134
16:39:28.535   Training iter 150, batch loss 1.5416, batch acc 0.9168
16:39:29.087   Training iter 200, batch loss 1.5403, batch acc 0.9150
16:39:29.616   Training iter 250, batch loss 1.5398, batch acc 0.9172
16:39:30.132   Training iter 300, batch loss 1.5409, batch acc 0.9168
16:39:30.654   Training iter 350, batch loss 1.5461, batch acc 0.9138
16:39:31.180   Training iter 400, batch loss 1.5373, batch acc 0.9236
16:39:31.707   Training iter 450, batch loss 1.5417, batch acc 0.9164
16:39:32.227   Training iter 500, batch loss 1.5416, batch acc 0.9160
16:39:32.754   Training iter 550, batch loss 1.5478, batch acc 0.9178
16:39:33.271   Training iter 600, batch loss 1.5423, batch acc 0.9190
16:39:33.273 Training @ 104 epoch...
16:39:33.795   Training iter 50, batch loss 1.5476, batch acc 0.9108
16:39:34.325   Training iter 100, batch loss 1.5400, batch acc 0.9176
16:39:34.852   Training iter 150, batch loss 1.5446, batch acc 0.9148
16:39:35.373   Training iter 200, batch loss 1.5403, batch acc 0.9202
16:39:35.912   Training iter 250, batch loss 1.5443, batch acc 0.9130
16:39:36.444   Training iter 300, batch loss 1.5397, batch acc 0.9194
16:39:36.983   Training iter 350, batch loss 1.5410, batch acc 0.9188
16:39:37.524   Training iter 400, batch loss 1.5427, batch acc 0.9150
16:39:38.082   Training iter 450, batch loss 1.5399, batch acc 0.9182
16:39:38.632   Training iter 500, batch loss 1.5399, batch acc 0.9180
16:39:39.179   Training iter 550, batch loss 1.5399, batch acc 0.9194
16:39:39.733   Training iter 600, batch loss 1.5465, batch acc 0.9134
16:39:39.735 Training @ 105 epoch...
16:39:40.293   Training iter 50, batch loss 1.5444, batch acc 0.9122
16:39:40.848   Training iter 100, batch loss 1.5422, batch acc 0.9148
16:39:41.398   Training iter 150, batch loss 1.5393, batch acc 0.9204
16:39:41.956   Training iter 200, batch loss 1.5407, batch acc 0.9176
16:39:42.527   Training iter 250, batch loss 1.5392, batch acc 0.9178
16:39:43.077   Training iter 300, batch loss 1.5444, batch acc 0.9170
16:39:43.606   Training iter 350, batch loss 1.5418, batch acc 0.9170
16:39:44.134   Training iter 400, batch loss 1.5417, batch acc 0.9152
16:39:44.654   Training iter 450, batch loss 1.5414, batch acc 0.9148
16:39:45.184   Training iter 500, batch loss 1.5408, batch acc 0.9158
16:39:45.718   Training iter 550, batch loss 1.5454, batch acc 0.9172
16:39:46.258   Training iter 600, batch loss 1.5429, batch acc 0.9186
16:39:46.260 Testing @ 105 epoch...
16:39:46.307     Testing, total mean loss 1.54484, total acc 0.91580
16:39:46.307 Training @ 106 epoch...
16:39:46.858   Training iter 50, batch loss 1.5388, batch acc 0.9208
16:39:47.391   Training iter 100, batch loss 1.5389, batch acc 0.9162
16:39:47.930   Training iter 150, batch loss 1.5477, batch acc 0.9108
16:39:48.460   Training iter 200, batch loss 1.5432, batch acc 0.9128
16:39:48.998   Training iter 250, batch loss 1.5464, batch acc 0.9138
16:39:49.537   Training iter 300, batch loss 1.5409, batch acc 0.9166
16:39:50.067   Training iter 350, batch loss 1.5400, batch acc 0.9208
16:39:50.598   Training iter 400, batch loss 1.5444, batch acc 0.9190
16:39:51.141   Training iter 450, batch loss 1.5420, batch acc 0.9132
16:39:51.679   Training iter 500, batch loss 1.5380, batch acc 0.9230
16:39:52.232   Training iter 550, batch loss 1.5422, batch acc 0.9156
16:39:52.782   Training iter 600, batch loss 1.5401, batch acc 0.9210
16:39:52.784 Training @ 107 epoch...
16:39:53.321   Training iter 50, batch loss 1.5444, batch acc 0.9156
16:39:53.848   Training iter 100, batch loss 1.5399, batch acc 0.9182
16:39:54.374   Training iter 150, batch loss 1.5420, batch acc 0.9194
16:39:54.923   Training iter 200, batch loss 1.5403, batch acc 0.9216
16:39:55.503   Training iter 250, batch loss 1.5389, batch acc 0.9230
16:39:56.102   Training iter 300, batch loss 1.5326, batch acc 0.9270
16:39:56.696   Training iter 350, batch loss 1.5432, batch acc 0.9144
16:39:57.279   Training iter 400, batch loss 1.5421, batch acc 0.9140
16:39:57.827   Training iter 450, batch loss 1.5427, batch acc 0.9146
16:39:58.362   Training iter 500, batch loss 1.5455, batch acc 0.9092
16:39:58.870   Training iter 550, batch loss 1.5437, batch acc 0.9116
16:39:59.394   Training iter 600, batch loss 1.5440, batch acc 0.9122
16:39:59.396 Training @ 108 epoch...
16:39:59.909   Training iter 50, batch loss 1.5422, batch acc 0.9156
16:40:00.413   Training iter 100, batch loss 1.5403, batch acc 0.9184
16:40:00.926   Training iter 150, batch loss 1.5396, batch acc 0.9204
16:40:01.480   Training iter 200, batch loss 1.5413, batch acc 0.9194
16:40:02.078   Training iter 250, batch loss 1.5418, batch acc 0.9150
16:40:02.641   Training iter 300, batch loss 1.5422, batch acc 0.9150
16:40:03.184   Training iter 350, batch loss 1.5426, batch acc 0.9168
16:40:03.760   Training iter 400, batch loss 1.5426, batch acc 0.9126
16:40:04.325   Training iter 450, batch loss 1.5405, batch acc 0.9232
16:40:04.890   Training iter 500, batch loss 1.5429, batch acc 0.9126
16:40:05.466   Training iter 550, batch loss 1.5400, batch acc 0.9232
16:40:06.029   Training iter 600, batch loss 1.5441, batch acc 0.9136
16:40:06.030 Training @ 109 epoch...
16:40:06.564   Training iter 50, batch loss 1.5404, batch acc 0.9158
16:40:07.069   Training iter 100, batch loss 1.5422, batch acc 0.9158
16:40:07.599   Training iter 150, batch loss 1.5410, batch acc 0.9192
16:40:08.128   Training iter 200, batch loss 1.5419, batch acc 0.9158
16:40:08.676   Training iter 250, batch loss 1.5431, batch acc 0.9154
16:40:09.213   Training iter 300, batch loss 1.5415, batch acc 0.9218
16:40:09.719   Training iter 350, batch loss 1.5419, batch acc 0.9116
16:40:10.220   Training iter 400, batch loss 1.5440, batch acc 0.9148
16:40:10.724   Training iter 450, batch loss 1.5415, batch acc 0.9154
16:40:11.238   Training iter 500, batch loss 1.5394, batch acc 0.9200
16:40:11.749   Training iter 550, batch loss 1.5373, batch acc 0.9208
16:40:12.259   Training iter 600, batch loss 1.5446, batch acc 0.9150
16:40:12.261 Training @ 110 epoch...
16:40:12.780   Training iter 50, batch loss 1.5397, batch acc 0.9158
16:40:13.272   Training iter 100, batch loss 1.5448, batch acc 0.9160
16:40:13.769   Training iter 150, batch loss 1.5464, batch acc 0.9116
16:40:14.279   Training iter 200, batch loss 1.5442, batch acc 0.9118
16:40:14.766   Training iter 250, batch loss 1.5429, batch acc 0.9170
16:40:15.237   Training iter 300, batch loss 1.5380, batch acc 0.9182
16:40:15.724   Training iter 350, batch loss 1.5394, batch acc 0.9188
16:40:16.209   Training iter 400, batch loss 1.5392, batch acc 0.9192
16:40:16.704   Training iter 450, batch loss 1.5401, batch acc 0.9190
16:40:17.220   Training iter 500, batch loss 1.5398, batch acc 0.9208
16:40:17.729   Training iter 550, batch loss 1.5372, batch acc 0.9262
16:40:18.227   Training iter 600, batch loss 1.5448, batch acc 0.9146
16:40:18.229 Testing @ 110 epoch...
16:40:18.279     Testing, total mean loss 1.54431, total acc 0.91740
16:40:18.279 Training @ 111 epoch...
16:40:18.804   Training iter 50, batch loss 1.5397, batch acc 0.9182
16:40:19.342   Training iter 100, batch loss 1.5457, batch acc 0.9116
16:40:19.847   Training iter 150, batch loss 1.5369, batch acc 0.9220
16:40:20.339   Training iter 200, batch loss 1.5366, batch acc 0.9196
16:40:20.839   Training iter 250, batch loss 1.5416, batch acc 0.9116
16:40:21.345   Training iter 300, batch loss 1.5429, batch acc 0.9174
16:40:21.880   Training iter 350, batch loss 1.5429, batch acc 0.9164
16:40:22.411   Training iter 400, batch loss 1.5405, batch acc 0.9210
16:40:22.950   Training iter 450, batch loss 1.5411, batch acc 0.9158
16:40:23.474   Training iter 500, batch loss 1.5407, batch acc 0.9202
16:40:24.004   Training iter 550, batch loss 1.5394, batch acc 0.9196
16:40:24.559   Training iter 600, batch loss 1.5464, batch acc 0.9126
16:40:24.561 Training @ 112 epoch...
16:40:25.182   Training iter 50, batch loss 1.5415, batch acc 0.9190
16:40:25.809   Training iter 100, batch loss 1.5393, batch acc 0.9174
16:40:26.373   Training iter 150, batch loss 1.5391, batch acc 0.9180
16:40:26.932   Training iter 200, batch loss 1.5442, batch acc 0.9128
16:40:27.490   Training iter 250, batch loss 1.5457, batch acc 0.9132
16:40:28.022   Training iter 300, batch loss 1.5380, batch acc 0.9218
16:40:28.543   Training iter 350, batch loss 1.5432, batch acc 0.9162
16:40:29.075   Training iter 400, batch loss 1.5401, batch acc 0.9196
16:40:29.607   Training iter 450, batch loss 1.5377, batch acc 0.9174
16:40:30.150   Training iter 500, batch loss 1.5448, batch acc 0.9136
16:40:30.685   Training iter 550, batch loss 1.5477, batch acc 0.9134
16:40:31.207   Training iter 600, batch loss 1.5335, batch acc 0.9236
16:40:31.208 Training @ 113 epoch...
16:40:31.744   Training iter 50, batch loss 1.5372, batch acc 0.9194
16:40:32.282   Training iter 100, batch loss 1.5467, batch acc 0.9126
16:40:32.822   Training iter 150, batch loss 1.5363, batch acc 0.9254
16:40:33.357   Training iter 200, batch loss 1.5444, batch acc 0.9148
16:40:33.880   Training iter 250, batch loss 1.5411, batch acc 0.9154
16:40:34.407   Training iter 300, batch loss 1.5419, batch acc 0.9152
16:40:34.904   Training iter 350, batch loss 1.5393, batch acc 0.9216
16:40:35.378   Training iter 400, batch loss 1.5402, batch acc 0.9180
16:40:35.847   Training iter 450, batch loss 1.5428, batch acc 0.9172
16:40:36.318   Training iter 500, batch loss 1.5402, batch acc 0.9204
16:40:36.783   Training iter 550, batch loss 1.5422, batch acc 0.9116
16:40:37.263   Training iter 600, batch loss 1.5414, batch acc 0.9174
16:40:37.265 Training @ 114 epoch...
16:40:37.763   Training iter 50, batch loss 1.5371, batch acc 0.9238
16:40:38.259   Training iter 100, batch loss 1.5451, batch acc 0.9124
16:40:38.758   Training iter 150, batch loss 1.5464, batch acc 0.9174
16:40:39.297   Training iter 200, batch loss 1.5395, batch acc 0.9184
16:40:39.792   Training iter 250, batch loss 1.5408, batch acc 0.9146
16:40:40.276   Training iter 300, batch loss 1.5373, batch acc 0.9228
16:40:40.779   Training iter 350, batch loss 1.5405, batch acc 0.9186
16:40:41.281   Training iter 400, batch loss 1.5430, batch acc 0.9140
16:40:41.786   Training iter 450, batch loss 1.5406, batch acc 0.9196
16:40:42.292   Training iter 500, batch loss 1.5432, batch acc 0.9134
16:40:42.795   Training iter 550, batch loss 1.5376, batch acc 0.9170
16:40:43.326   Training iter 600, batch loss 1.5412, batch acc 0.9126
16:40:43.328 Training @ 115 epoch...
16:40:43.888   Training iter 50, batch loss 1.5443, batch acc 0.9088
16:40:44.461   Training iter 100, batch loss 1.5396, batch acc 0.9190
16:40:45.023   Training iter 150, batch loss 1.5383, batch acc 0.9198
16:40:45.588   Training iter 200, batch loss 1.5404, batch acc 0.9198
16:40:46.141   Training iter 250, batch loss 1.5376, batch acc 0.9264
16:40:46.696   Training iter 300, batch loss 1.5384, batch acc 0.9182
16:40:47.238   Training iter 350, batch loss 1.5394, batch acc 0.9212
16:40:47.774   Training iter 400, batch loss 1.5403, batch acc 0.9204
16:40:48.311   Training iter 450, batch loss 1.5416, batch acc 0.9158
16:40:48.858   Training iter 500, batch loss 1.5426, batch acc 0.9142
16:40:49.407   Training iter 550, batch loss 1.5468, batch acc 0.9090
16:40:49.953   Training iter 600, batch loss 1.5420, batch acc 0.9130
16:40:49.955 Testing @ 115 epoch...
16:40:50.003     Testing, total mean loss 1.54456, total acc 0.91540
16:40:50.003 Training @ 116 epoch...
16:40:50.547   Training iter 50, batch loss 1.5475, batch acc 0.9114
16:40:51.113   Training iter 100, batch loss 1.5383, batch acc 0.9180
16:40:51.692   Training iter 150, batch loss 1.5386, batch acc 0.9166
16:40:52.372   Training iter 200, batch loss 1.5370, batch acc 0.9192
16:40:52.982   Training iter 250, batch loss 1.5427, batch acc 0.9166
16:40:53.516   Training iter 300, batch loss 1.5429, batch acc 0.9206
16:40:54.082   Training iter 350, batch loss 1.5349, batch acc 0.9248
16:40:54.660   Training iter 400, batch loss 1.5426, batch acc 0.9178
16:40:55.235   Training iter 450, batch loss 1.5380, batch acc 0.9194
16:40:55.808   Training iter 500, batch loss 1.5405, batch acc 0.9132
16:40:56.384   Training iter 550, batch loss 1.5454, batch acc 0.9112
16:40:56.959   Training iter 600, batch loss 1.5413, batch acc 0.9184
16:40:56.961 Training @ 117 epoch...
16:40:57.543   Training iter 50, batch loss 1.5394, batch acc 0.9184
16:40:58.111   Training iter 100, batch loss 1.5413, batch acc 0.9214
16:40:58.681   Training iter 150, batch loss 1.5393, batch acc 0.9224
16:40:59.258   Training iter 200, batch loss 1.5376, batch acc 0.9190
16:40:59.811   Training iter 250, batch loss 1.5442, batch acc 0.9130
16:41:00.344   Training iter 300, batch loss 1.5444, batch acc 0.9128
16:41:00.866   Training iter 350, batch loss 1.5422, batch acc 0.9170
16:41:01.418   Training iter 400, batch loss 1.5396, batch acc 0.9170
16:41:01.997   Training iter 450, batch loss 1.5410, batch acc 0.9210
16:41:02.559   Training iter 500, batch loss 1.5380, batch acc 0.9204
16:41:03.130   Training iter 550, batch loss 1.5402, batch acc 0.9198
16:41:03.684   Training iter 600, batch loss 1.5411, batch acc 0.9152
16:41:03.686 Training @ 118 epoch...
16:41:04.257   Training iter 50, batch loss 1.5411, batch acc 0.9196
16:41:04.805   Training iter 100, batch loss 1.5409, batch acc 0.9122
16:41:05.352   Training iter 150, batch loss 1.5436, batch acc 0.9142
16:41:05.918   Training iter 200, batch loss 1.5408, batch acc 0.9220
16:41:06.487   Training iter 250, batch loss 1.5404, batch acc 0.9214
16:41:07.028   Training iter 300, batch loss 1.5426, batch acc 0.9166
16:41:07.549   Training iter 350, batch loss 1.5401, batch acc 0.9188
16:41:08.075   Training iter 400, batch loss 1.5379, batch acc 0.9170
16:41:08.594   Training iter 450, batch loss 1.5408, batch acc 0.9164
16:41:09.121   Training iter 500, batch loss 1.5398, batch acc 0.9182
16:41:09.638   Training iter 550, batch loss 1.5360, batch acc 0.9242
16:41:10.156   Training iter 600, batch loss 1.5440, batch acc 0.9142
16:41:10.158 Training @ 119 epoch...
16:41:10.653   Training iter 50, batch loss 1.5329, batch acc 0.9222
16:41:11.151   Training iter 100, batch loss 1.5406, batch acc 0.9130
16:41:11.649   Training iter 150, batch loss 1.5421, batch acc 0.9138
16:41:12.175   Training iter 200, batch loss 1.5417, batch acc 0.9236
16:41:12.684   Training iter 250, batch loss 1.5460, batch acc 0.9126
16:41:13.180   Training iter 300, batch loss 1.5361, batch acc 0.9248
16:41:13.701   Training iter 350, batch loss 1.5402, batch acc 0.9176
16:41:14.251   Training iter 400, batch loss 1.5355, batch acc 0.9238
16:41:14.769   Training iter 450, batch loss 1.5410, batch acc 0.9164
16:41:15.297   Training iter 500, batch loss 1.5360, batch acc 0.9224
16:41:15.823   Training iter 550, batch loss 1.5479, batch acc 0.9138
16:41:16.373   Training iter 600, batch loss 1.5468, batch acc 0.9092
16:41:16.375 Training @ 120 epoch...
16:41:16.973   Training iter 50, batch loss 1.5340, batch acc 0.9270
16:41:17.565   Training iter 100, batch loss 1.5421, batch acc 0.9192
16:41:18.119   Training iter 150, batch loss 1.5452, batch acc 0.9114
16:41:18.630   Training iter 200, batch loss 1.5380, batch acc 0.9208
16:41:19.156   Training iter 250, batch loss 1.5466, batch acc 0.9116
16:41:19.680   Training iter 300, batch loss 1.5393, batch acc 0.9202
16:41:20.213   Training iter 350, batch loss 1.5408, batch acc 0.9156
16:41:20.756   Training iter 400, batch loss 1.5428, batch acc 0.9156
16:41:21.299   Training iter 450, batch loss 1.5429, batch acc 0.9160
16:41:21.828   Training iter 500, batch loss 1.5427, batch acc 0.9174
16:41:22.355   Training iter 550, batch loss 1.5358, batch acc 0.9226
16:41:22.888   Training iter 600, batch loss 1.5350, batch acc 0.9196
16:41:22.889 Testing @ 120 epoch...
16:41:22.936     Testing, total mean loss 1.54437, total acc 0.91510
16:41:22.936 Training @ 121 epoch...
16:41:23.474   Training iter 50, batch loss 1.5402, batch acc 0.9228
16:41:24.020   Training iter 100, batch loss 1.5398, batch acc 0.9160
16:41:24.563   Training iter 150, batch loss 1.5440, batch acc 0.9176
16:41:25.102   Training iter 200, batch loss 1.5428, batch acc 0.9180
16:41:25.648   Training iter 250, batch loss 1.5344, batch acc 0.9284
16:41:26.215   Training iter 300, batch loss 1.5370, batch acc 0.9182
16:41:26.773   Training iter 350, batch loss 1.5400, batch acc 0.9140
16:41:27.319   Training iter 400, batch loss 1.5391, batch acc 0.9184
16:41:27.856   Training iter 450, batch loss 1.5387, batch acc 0.9214
16:41:28.390   Training iter 500, batch loss 1.5418, batch acc 0.9178
16:41:28.938   Training iter 550, batch loss 1.5452, batch acc 0.9096
16:41:29.502   Training iter 600, batch loss 1.5417, batch acc 0.9126
16:41:29.504 Training @ 122 epoch...
16:41:30.083   Training iter 50, batch loss 1.5405, batch acc 0.9190
16:41:30.654   Training iter 100, batch loss 1.5437, batch acc 0.9204
16:41:31.228   Training iter 150, batch loss 1.5364, batch acc 0.9206
16:41:31.777   Training iter 200, batch loss 1.5431, batch acc 0.9132
16:41:32.314   Training iter 250, batch loss 1.5400, batch acc 0.9158
16:41:32.844   Training iter 300, batch loss 1.5434, batch acc 0.9128
16:41:33.391   Training iter 350, batch loss 1.5351, batch acc 0.9228
16:41:33.929   Training iter 400, batch loss 1.5402, batch acc 0.9182
16:41:34.470   Training iter 450, batch loss 1.5414, batch acc 0.9144
16:41:35.016   Training iter 500, batch loss 1.5413, batch acc 0.9206
16:41:35.573   Training iter 550, batch loss 1.5361, batch acc 0.9208
16:41:36.129   Training iter 600, batch loss 1.5420, batch acc 0.9174
16:41:36.130 Training @ 123 epoch...
16:41:36.673   Training iter 50, batch loss 1.5422, batch acc 0.9172
16:41:37.228   Training iter 100, batch loss 1.5425, batch acc 0.9110
16:41:37.780   Training iter 150, batch loss 1.5391, batch acc 0.9178
16:41:38.318   Training iter 200, batch loss 1.5385, batch acc 0.9168
16:41:38.847   Training iter 250, batch loss 1.5429, batch acc 0.9180
16:41:39.404   Training iter 300, batch loss 1.5378, batch acc 0.9212
16:41:39.949   Training iter 350, batch loss 1.5424, batch acc 0.9140
16:41:40.488   Training iter 400, batch loss 1.5422, batch acc 0.9156
16:41:41.021   Training iter 450, batch loss 1.5357, batch acc 0.9204
16:41:41.568   Training iter 500, batch loss 1.5357, batch acc 0.9198
16:41:42.135   Training iter 550, batch loss 1.5412, batch acc 0.9182
16:41:42.702   Training iter 600, batch loss 1.5419, batch acc 0.9206
16:41:42.704 Training @ 124 epoch...
16:41:43.278   Training iter 50, batch loss 1.5397, batch acc 0.9156
16:41:43.843   Training iter 100, batch loss 1.5428, batch acc 0.9118
16:41:44.416   Training iter 150, batch loss 1.5319, batch acc 0.9270
16:41:44.983   Training iter 200, batch loss 1.5379, batch acc 0.9182
16:41:45.546   Training iter 250, batch loss 1.5401, batch acc 0.9190
16:41:46.110   Training iter 300, batch loss 1.5382, batch acc 0.9172
16:41:46.677   Training iter 350, batch loss 1.5453, batch acc 0.9108
16:41:47.239   Training iter 400, batch loss 1.5423, batch acc 0.9182
16:41:47.732   Training iter 450, batch loss 1.5408, batch acc 0.9158
16:41:48.238   Training iter 500, batch loss 1.5431, batch acc 0.9154
16:41:48.744   Training iter 550, batch loss 1.5416, batch acc 0.9224
16:41:49.282   Training iter 600, batch loss 1.5387, batch acc 0.9210
16:41:49.284 Training @ 125 epoch...
16:41:49.832   Training iter 50, batch loss 1.5401, batch acc 0.9140
16:41:50.333   Training iter 100, batch loss 1.5391, batch acc 0.9210
16:41:50.836   Training iter 150, batch loss 1.5362, batch acc 0.9250
16:41:51.348   Training iter 200, batch loss 1.5425, batch acc 0.9148
16:41:51.872   Training iter 250, batch loss 1.5416, batch acc 0.9128
16:41:52.392   Training iter 300, batch loss 1.5413, batch acc 0.9162
16:41:52.921   Training iter 350, batch loss 1.5329, batch acc 0.9278
16:41:53.459   Training iter 400, batch loss 1.5389, batch acc 0.9212
16:41:53.994   Training iter 450, batch loss 1.5381, batch acc 0.9204
16:41:54.533   Training iter 500, batch loss 1.5421, batch acc 0.9132
16:41:55.063   Training iter 550, batch loss 1.5470, batch acc 0.9108
16:41:55.601   Training iter 600, batch loss 1.5400, batch acc 0.9206
16:41:55.603 Testing @ 125 epoch...
16:41:55.650     Testing, total mean loss 1.54408, total acc 0.91750
16:41:55.650 Training @ 126 epoch...
16:41:56.206   Training iter 50, batch loss 1.5409, batch acc 0.9196
16:41:56.763   Training iter 100, batch loss 1.5391, batch acc 0.9190
16:41:57.306   Training iter 150, batch loss 1.5366, batch acc 0.9212
16:41:57.840   Training iter 200, batch loss 1.5371, batch acc 0.9230
16:41:58.380   Training iter 250, batch loss 1.5357, batch acc 0.9212
16:41:58.954   Training iter 300, batch loss 1.5448, batch acc 0.9104
16:41:59.526   Training iter 350, batch loss 1.5412, batch acc 0.9140
16:42:00.105   Training iter 400, batch loss 1.5384, batch acc 0.9176
16:42:00.676   Training iter 450, batch loss 1.5396, batch acc 0.9210
16:42:01.206   Training iter 500, batch loss 1.5428, batch acc 0.9130
16:42:01.776   Training iter 550, batch loss 1.5409, batch acc 0.9194
16:42:02.340   Training iter 600, batch loss 1.5417, batch acc 0.9154
16:42:02.342 Training @ 127 epoch...
16:42:02.892   Training iter 50, batch loss 1.5375, batch acc 0.9250
16:42:03.436   Training iter 100, batch loss 1.5410, batch acc 0.9168
16:42:03.979   Training iter 150, batch loss 1.5349, batch acc 0.9302
16:42:04.527   Training iter 200, batch loss 1.5410, batch acc 0.9154
16:42:05.074   Training iter 250, batch loss 1.5406, batch acc 0.9138
16:42:05.613   Training iter 300, batch loss 1.5431, batch acc 0.9130
16:42:06.153   Training iter 350, batch loss 1.5395, batch acc 0.9166
16:42:06.686   Training iter 400, batch loss 1.5396, batch acc 0.9194
16:42:07.195   Training iter 450, batch loss 1.5352, batch acc 0.9232
16:42:07.706   Training iter 500, batch loss 1.5398, batch acc 0.9180
16:42:08.231   Training iter 550, batch loss 1.5426, batch acc 0.9162
16:42:08.762   Training iter 600, batch loss 1.5434, batch acc 0.9128
16:42:08.764 Training @ 128 epoch...
16:42:09.276   Training iter 50, batch loss 1.5427, batch acc 0.9158
16:42:09.760   Training iter 100, batch loss 1.5387, batch acc 0.9172
16:42:10.221   Training iter 150, batch loss 1.5376, batch acc 0.9208
16:42:10.722   Training iter 200, batch loss 1.5411, batch acc 0.9206
16:42:11.244   Training iter 250, batch loss 1.5404, batch acc 0.9192
16:42:11.752   Training iter 300, batch loss 1.5357, batch acc 0.9240
16:42:12.274   Training iter 350, batch loss 1.5423, batch acc 0.9152
16:42:12.793   Training iter 400, batch loss 1.5401, batch acc 0.9190
16:42:13.364   Training iter 450, batch loss 1.5371, batch acc 0.9184
16:42:13.945   Training iter 500, batch loss 1.5404, batch acc 0.9198
16:42:14.527   Training iter 550, batch loss 1.5416, batch acc 0.9180
16:42:15.093   Training iter 600, batch loss 1.5399, batch acc 0.9122
16:42:15.095 Training @ 129 epoch...
16:42:15.662   Training iter 50, batch loss 1.5397, batch acc 0.9202
16:42:16.229   Training iter 100, batch loss 1.5390, batch acc 0.9204
16:42:16.794   Training iter 150, batch loss 1.5392, batch acc 0.9164
16:42:17.351   Training iter 200, batch loss 1.5421, batch acc 0.9148
16:42:17.921   Training iter 250, batch loss 1.5364, batch acc 0.9234
16:42:18.486   Training iter 300, batch loss 1.5434, batch acc 0.9134
16:42:19.054   Training iter 350, batch loss 1.5387, batch acc 0.9224
16:42:19.611   Training iter 400, batch loss 1.5439, batch acc 0.9128
16:42:20.157   Training iter 450, batch loss 1.5345, batch acc 0.9258
16:42:20.697   Training iter 500, batch loss 1.5438, batch acc 0.9142
16:42:21.246   Training iter 550, batch loss 1.5376, batch acc 0.9182
16:42:21.780   Training iter 600, batch loss 1.5388, batch acc 0.9188
16:42:21.782 Training @ 130 epoch...
16:42:22.322   Training iter 50, batch loss 1.5400, batch acc 0.9166
16:42:22.864   Training iter 100, batch loss 1.5381, batch acc 0.9144
16:42:23.405   Training iter 150, batch loss 1.5385, batch acc 0.9218
16:42:23.945   Training iter 200, batch loss 1.5447, batch acc 0.9114
16:42:24.490   Training iter 250, batch loss 1.5386, batch acc 0.9206
16:42:25.035   Training iter 300, batch loss 1.5384, batch acc 0.9194
16:42:25.579   Training iter 350, batch loss 1.5418, batch acc 0.9164
16:42:26.117   Training iter 400, batch loss 1.5449, batch acc 0.9116
16:42:26.647   Training iter 450, batch loss 1.5406, batch acc 0.9214
16:42:27.179   Training iter 500, batch loss 1.5367, batch acc 0.9178
16:42:27.709   Training iter 550, batch loss 1.5404, batch acc 0.9210
16:42:28.246   Training iter 600, batch loss 1.5336, batch acc 0.9262
16:42:28.248 Testing @ 130 epoch...
16:42:28.296     Testing, total mean loss 1.54372, total acc 0.91530
16:42:28.296 Training @ 131 epoch...
16:42:28.837   Training iter 50, batch loss 1.5395, batch acc 0.9188
16:42:29.395   Training iter 100, batch loss 1.5401, batch acc 0.9204
16:42:29.955   Training iter 150, batch loss 1.5405, batch acc 0.9144
16:42:30.509   Training iter 200, batch loss 1.5388, batch acc 0.9176
16:42:31.065   Training iter 250, batch loss 1.5405, batch acc 0.9164
16:42:31.631   Training iter 300, batch loss 1.5416, batch acc 0.9172
16:42:32.228   Training iter 350, batch loss 1.5365, batch acc 0.9222
16:42:32.822   Training iter 400, batch loss 1.5388, batch acc 0.9176
16:42:33.409   Training iter 450, batch loss 1.5354, batch acc 0.9222
16:42:33.982   Training iter 500, batch loss 1.5401, batch acc 0.9206
16:42:34.554   Training iter 550, batch loss 1.5379, batch acc 0.9186
16:42:35.085   Training iter 600, batch loss 1.5454, batch acc 0.9130
16:42:35.086 Training @ 132 epoch...
16:42:35.562   Training iter 50, batch loss 1.5400, batch acc 0.9198
16:42:36.035   Training iter 100, batch loss 1.5401, batch acc 0.9186
16:42:36.560   Training iter 150, batch loss 1.5417, batch acc 0.9118
16:42:37.114   Training iter 200, batch loss 1.5417, batch acc 0.9170
16:42:37.678   Training iter 250, batch loss 1.5383, batch acc 0.9188
16:42:38.211   Training iter 300, batch loss 1.5392, batch acc 0.9180
16:42:38.742   Training iter 350, batch loss 1.5365, batch acc 0.9196
16:42:39.272   Training iter 400, batch loss 1.5391, batch acc 0.9192
16:42:39.813   Training iter 450, batch loss 1.5362, batch acc 0.9250
16:42:40.353   Training iter 500, batch loss 1.5419, batch acc 0.9158
16:42:40.885   Training iter 550, batch loss 1.5395, batch acc 0.9136
16:42:41.434   Training iter 600, batch loss 1.5401, batch acc 0.9204
16:42:41.436 Training @ 133 epoch...
16:42:41.983   Training iter 50, batch loss 1.5408, batch acc 0.9166
16:42:42.539   Training iter 100, batch loss 1.5343, batch acc 0.9238
16:42:43.084   Training iter 150, batch loss 1.5383, batch acc 0.9192
16:42:43.626   Training iter 200, batch loss 1.5406, batch acc 0.9184
16:42:44.166   Training iter 250, batch loss 1.5350, batch acc 0.9228
16:42:44.714   Training iter 300, batch loss 1.5447, batch acc 0.9134
16:42:45.260   Training iter 350, batch loss 1.5435, batch acc 0.9142
16:42:45.829   Training iter 400, batch loss 1.5430, batch acc 0.9138
16:42:46.398   Training iter 450, batch loss 1.5370, batch acc 0.9238
16:42:46.982   Training iter 500, batch loss 1.5391, batch acc 0.9206
16:42:47.568   Training iter 550, batch loss 1.5392, batch acc 0.9176
16:42:48.143   Training iter 600, batch loss 1.5373, batch acc 0.9210
16:42:48.145 Training @ 134 epoch...
16:42:48.719   Training iter 50, batch loss 1.5360, batch acc 0.9240
16:42:49.284   Training iter 100, batch loss 1.5406, batch acc 0.9186
16:42:49.861   Training iter 150, batch loss 1.5435, batch acc 0.9084
16:42:50.432   Training iter 200, batch loss 1.5333, batch acc 0.9236
16:42:50.994   Training iter 250, batch loss 1.5397, batch acc 0.9182
16:42:51.533   Training iter 300, batch loss 1.5433, batch acc 0.9174
16:42:52.086   Training iter 350, batch loss 1.5389, batch acc 0.9198
16:42:52.645   Training iter 400, batch loss 1.5376, batch acc 0.9170
16:42:53.172   Training iter 450, batch loss 1.5384, batch acc 0.9196
16:42:53.702   Training iter 500, batch loss 1.5413, batch acc 0.9132
16:42:54.233   Training iter 550, batch loss 1.5395, batch acc 0.9212
16:42:54.767   Training iter 600, batch loss 1.5413, batch acc 0.9172
16:42:54.769 Training @ 135 epoch...
16:42:55.301   Training iter 50, batch loss 1.5383, batch acc 0.9220
16:42:55.831   Training iter 100, batch loss 1.5397, batch acc 0.9202
16:42:56.344   Training iter 150, batch loss 1.5385, batch acc 0.9182
16:42:56.858   Training iter 200, batch loss 1.5389, batch acc 0.9208
16:42:57.389   Training iter 250, batch loss 1.5363, batch acc 0.9182
16:42:57.934   Training iter 300, batch loss 1.5388, batch acc 0.9178
16:42:58.465   Training iter 350, batch loss 1.5372, batch acc 0.9256
16:42:58.990   Training iter 400, batch loss 1.5401, batch acc 0.9196
16:42:59.522   Training iter 450, batch loss 1.5364, batch acc 0.9210
16:43:00.059   Training iter 500, batch loss 1.5414, batch acc 0.9178
16:43:00.591   Training iter 550, batch loss 1.5422, batch acc 0.9130
16:43:01.122   Training iter 600, batch loss 1.5433, batch acc 0.9128
16:43:01.124 Testing @ 135 epoch...
16:43:01.171     Testing, total mean loss 1.54314, total acc 0.91620
16:43:01.171 Training @ 136 epoch...
16:43:01.735   Training iter 50, batch loss 1.5363, batch acc 0.9214
16:43:02.315   Training iter 100, batch loss 1.5404, batch acc 0.9170
16:43:02.873   Training iter 150, batch loss 1.5421, batch acc 0.9150
16:43:03.424   Training iter 200, batch loss 1.5368, batch acc 0.9182
16:43:03.949   Training iter 250, batch loss 1.5416, batch acc 0.9176
16:43:04.479   Training iter 300, batch loss 1.5386, batch acc 0.9206
16:43:05.015   Training iter 350, batch loss 1.5387, batch acc 0.9188
16:43:05.544   Training iter 400, batch loss 1.5384, batch acc 0.9188
16:43:06.088   Training iter 450, batch loss 1.5367, batch acc 0.9198
16:43:06.630   Training iter 500, batch loss 1.5390, batch acc 0.9180
16:43:07.169   Training iter 550, batch loss 1.5480, batch acc 0.9098
16:43:07.690   Training iter 600, batch loss 1.5348, batch acc 0.9260
16:43:07.692 Training @ 137 epoch...
16:43:08.225   Training iter 50, batch loss 1.5394, batch acc 0.9190
16:43:08.811   Training iter 100, batch loss 1.5396, batch acc 0.9182
16:43:09.395   Training iter 150, batch loss 1.5396, batch acc 0.9184
16:43:09.987   Training iter 200, batch loss 1.5339, batch acc 0.9236
16:43:10.512   Training iter 250, batch loss 1.5372, batch acc 0.9184
16:43:11.031   Training iter 300, batch loss 1.5392, batch acc 0.9208
16:43:11.559   Training iter 350, batch loss 1.5381, batch acc 0.9260
16:43:12.098   Training iter 400, batch loss 1.5407, batch acc 0.9160
16:43:12.639   Training iter 450, batch loss 1.5436, batch acc 0.9126
16:43:13.154   Training iter 500, batch loss 1.5415, batch acc 0.9172
16:43:13.662   Training iter 550, batch loss 1.5377, batch acc 0.9180
16:43:14.184   Training iter 600, batch loss 1.5398, batch acc 0.9140
16:43:14.186 Training @ 138 epoch...
16:43:14.722   Training iter 50, batch loss 1.5349, batch acc 0.9256
16:43:15.271   Training iter 100, batch loss 1.5374, batch acc 0.9176
16:43:15.814   Training iter 150, batch loss 1.5416, batch acc 0.9154
16:43:16.347   Training iter 200, batch loss 1.5349, batch acc 0.9258
16:43:16.863   Training iter 250, batch loss 1.5409, batch acc 0.9166
16:43:17.391   Training iter 300, batch loss 1.5404, batch acc 0.9170
16:43:17.919   Training iter 350, batch loss 1.5377, batch acc 0.9208
16:43:18.441   Training iter 400, batch loss 1.5371, batch acc 0.9234
16:43:18.952   Training iter 450, batch loss 1.5411, batch acc 0.9174
16:43:19.471   Training iter 500, batch loss 1.5398, batch acc 0.9148
16:43:20.001   Training iter 550, batch loss 1.5408, batch acc 0.9176
16:43:20.517   Training iter 600, batch loss 1.5433, batch acc 0.9130
16:43:20.519 Training @ 139 epoch...
16:43:21.036   Training iter 50, batch loss 1.5431, batch acc 0.9136
16:43:21.553   Training iter 100, batch loss 1.5444, batch acc 0.9162
16:43:22.082   Training iter 150, batch loss 1.5369, batch acc 0.9234
16:43:22.625   Training iter 200, batch loss 1.5396, batch acc 0.9198
16:43:23.143   Training iter 250, batch loss 1.5354, batch acc 0.9224
16:43:23.657   Training iter 300, batch loss 1.5398, batch acc 0.9176
16:43:24.176   Training iter 350, batch loss 1.5393, batch acc 0.9200
16:43:24.713   Training iter 400, batch loss 1.5388, batch acc 0.9194
16:43:25.289   Training iter 450, batch loss 1.5380, batch acc 0.9206
16:43:25.848   Training iter 500, batch loss 1.5387, batch acc 0.9196
16:43:26.425   Training iter 550, batch loss 1.5392, batch acc 0.9172
16:43:26.999   Training iter 600, batch loss 1.5362, batch acc 0.9198
16:43:27.002 Training @ 140 epoch...
16:43:27.561   Training iter 50, batch loss 1.5347, batch acc 0.9230
16:43:28.112   Training iter 100, batch loss 1.5382, batch acc 0.9198
16:43:28.667   Training iter 150, batch loss 1.5408, batch acc 0.9172
16:43:29.207   Training iter 200, batch loss 1.5415, batch acc 0.9172
16:43:29.724   Training iter 250, batch loss 1.5359, batch acc 0.9226
16:43:30.252   Training iter 300, batch loss 1.5413, batch acc 0.9156
16:43:30.799   Training iter 350, batch loss 1.5379, batch acc 0.9218
16:43:31.333   Training iter 400, batch loss 1.5368, batch acc 0.9190
16:43:31.872   Training iter 450, batch loss 1.5420, batch acc 0.9140
16:43:32.430   Training iter 500, batch loss 1.5404, batch acc 0.9188
16:43:32.987   Training iter 550, batch loss 1.5352, batch acc 0.9280
16:43:33.526   Training iter 600, batch loss 1.5433, batch acc 0.9134
16:43:33.528 Testing @ 140 epoch...
16:43:33.575     Testing, total mean loss 1.54268, total acc 0.92030
16:43:33.575 Training @ 141 epoch...
16:43:34.121   Training iter 50, batch loss 1.5421, batch acc 0.9154
16:43:34.667   Training iter 100, batch loss 1.5384, batch acc 0.9228
16:43:35.213   Training iter 150, batch loss 1.5366, batch acc 0.9220
16:43:35.756   Training iter 200, batch loss 1.5362, batch acc 0.9208
16:43:36.318   Training iter 250, batch loss 1.5334, batch acc 0.9230
16:43:36.873   Training iter 300, batch loss 1.5402, batch acc 0.9194
16:43:37.418   Training iter 350, batch loss 1.5395, batch acc 0.9156
16:43:37.971   Training iter 400, batch loss 1.5354, batch acc 0.9250
16:43:38.515   Training iter 450, batch loss 1.5401, batch acc 0.9224
16:43:39.056   Training iter 500, batch loss 1.5425, batch acc 0.9144
16:43:39.599   Training iter 550, batch loss 1.5398, batch acc 0.9150
16:43:40.139   Training iter 600, batch loss 1.5424, batch acc 0.9130
16:43:40.141 Training @ 142 epoch...
16:43:40.683   Training iter 50, batch loss 1.5360, batch acc 0.9204
16:43:41.229   Training iter 100, batch loss 1.5387, batch acc 0.9180
16:43:41.773   Training iter 150, batch loss 1.5386, batch acc 0.9210
16:43:42.325   Training iter 200, batch loss 1.5412, batch acc 0.9156
16:43:42.856   Training iter 250, batch loss 1.5360, batch acc 0.9224
16:43:43.396   Training iter 300, batch loss 1.5387, batch acc 0.9186
16:43:43.941   Training iter 350, batch loss 1.5414, batch acc 0.9184
16:43:44.467   Training iter 400, batch loss 1.5409, batch acc 0.9184
16:43:44.979   Training iter 450, batch loss 1.5416, batch acc 0.9128
16:43:45.483   Training iter 500, batch loss 1.5357, batch acc 0.9224
16:43:45.988   Training iter 550, batch loss 1.5384, batch acc 0.9170
16:43:46.503   Training iter 600, batch loss 1.5396, batch acc 0.9194
16:43:46.505 Training @ 143 epoch...
16:43:47.026   Training iter 50, batch loss 1.5361, batch acc 0.9230
16:43:47.553   Training iter 100, batch loss 1.5409, batch acc 0.9186
16:43:48.044   Training iter 150, batch loss 1.5381, batch acc 0.9208
16:43:48.537   Training iter 200, batch loss 1.5399, batch acc 0.9172
16:43:49.024   Training iter 250, batch loss 1.5354, batch acc 0.9250
16:43:49.523   Training iter 300, batch loss 1.5386, batch acc 0.9230
16:43:50.025   Training iter 350, batch loss 1.5409, batch acc 0.9172
16:43:50.516   Training iter 400, batch loss 1.5399, batch acc 0.9184
16:43:51.048   Training iter 450, batch loss 1.5403, batch acc 0.9162
16:43:51.598   Training iter 500, batch loss 1.5388, batch acc 0.9154
16:43:52.157   Training iter 550, batch loss 1.5393, batch acc 0.9180
16:43:52.709   Training iter 600, batch loss 1.5374, batch acc 0.9204
16:43:52.711 Training @ 144 epoch...
16:43:53.253   Training iter 50, batch loss 1.5349, batch acc 0.9292
16:43:53.782   Training iter 100, batch loss 1.5438, batch acc 0.9130
16:43:54.345   Training iter 150, batch loss 1.5382, batch acc 0.9230
16:43:54.912   Training iter 200, batch loss 1.5444, batch acc 0.9142
16:43:55.426   Training iter 250, batch loss 1.5360, batch acc 0.9238
16:43:55.954   Training iter 300, batch loss 1.5382, batch acc 0.9156
16:43:56.501   Training iter 350, batch loss 1.5406, batch acc 0.9180
16:43:57.024   Training iter 400, batch loss 1.5377, batch acc 0.9214
16:43:57.531   Training iter 450, batch loss 1.5371, batch acc 0.9202
16:43:58.031   Training iter 500, batch loss 1.5364, batch acc 0.9242
16:43:58.512   Training iter 550, batch loss 1.5382, batch acc 0.9194
16:43:58.986   Training iter 600, batch loss 1.5402, batch acc 0.9144
16:43:58.988 Training @ 145 epoch...
16:43:59.470   Training iter 50, batch loss 1.5403, batch acc 0.9230
16:43:59.975   Training iter 100, batch loss 1.5339, batch acc 0.9206
16:44:00.505   Training iter 150, batch loss 1.5391, batch acc 0.9198
16:44:01.030   Training iter 200, batch loss 1.5424, batch acc 0.9126
16:44:01.589   Training iter 250, batch loss 1.5388, batch acc 0.9154
16:44:02.183   Training iter 300, batch loss 1.5421, batch acc 0.9158
16:44:02.730   Training iter 350, batch loss 1.5406, batch acc 0.9202
16:44:03.285   Training iter 400, batch loss 1.5374, batch acc 0.9204
16:44:03.830   Training iter 450, batch loss 1.5377, batch acc 0.9224
16:44:04.383   Training iter 500, batch loss 1.5368, batch acc 0.9198
16:44:04.912   Training iter 550, batch loss 1.5366, batch acc 0.9256
16:44:05.436   Training iter 600, batch loss 1.5388, batch acc 0.9192
16:44:05.438 Testing @ 145 epoch...
16:44:05.486     Testing, total mean loss 1.54385, total acc 0.91830
16:44:05.486 Training @ 146 epoch...
16:44:06.007   Training iter 50, batch loss 1.5420, batch acc 0.9090
16:44:06.514   Training iter 100, batch loss 1.5350, batch acc 0.9272
16:44:07.073   Training iter 150, batch loss 1.5357, batch acc 0.9240
16:44:07.616   Training iter 200, batch loss 1.5363, batch acc 0.9206
16:44:08.145   Training iter 250, batch loss 1.5382, batch acc 0.9180
16:44:08.673   Training iter 300, batch loss 1.5397, batch acc 0.9170
16:44:09.213   Training iter 350, batch loss 1.5437, batch acc 0.9138
16:44:09.742   Training iter 400, batch loss 1.5408, batch acc 0.9180
16:44:10.281   Training iter 450, batch loss 1.5399, batch acc 0.9174
16:44:10.845   Training iter 500, batch loss 1.5353, batch acc 0.9222
16:44:11.385   Training iter 550, batch loss 1.5371, batch acc 0.9224
16:44:11.894   Training iter 600, batch loss 1.5405, batch acc 0.9176
16:44:11.896 Training @ 147 epoch...
16:44:12.426   Training iter 50, batch loss 1.5354, batch acc 0.9236
16:44:12.943   Training iter 100, batch loss 1.5403, batch acc 0.9180
16:44:13.419   Training iter 150, batch loss 1.5396, batch acc 0.9184
16:44:13.911   Training iter 200, batch loss 1.5383, batch acc 0.9232
16:44:14.396   Training iter 250, batch loss 1.5354, batch acc 0.9180
16:44:14.897   Training iter 300, batch loss 1.5380, batch acc 0.9196
16:44:15.433   Training iter 350, batch loss 1.5422, batch acc 0.9152
16:44:15.930   Training iter 400, batch loss 1.5351, batch acc 0.9204
16:44:16.433   Training iter 450, batch loss 1.5391, batch acc 0.9196
16:44:16.951   Training iter 500, batch loss 1.5429, batch acc 0.9122
16:44:17.457   Training iter 550, batch loss 1.5397, batch acc 0.9178
16:44:17.964   Training iter 600, batch loss 1.5373, batch acc 0.9250
16:44:17.966 Training @ 148 epoch...
16:44:18.472   Training iter 50, batch loss 1.5383, batch acc 0.9176
16:44:19.009   Training iter 100, batch loss 1.5364, batch acc 0.9210
16:44:19.549   Training iter 150, batch loss 1.5340, batch acc 0.9208
16:44:20.098   Training iter 200, batch loss 1.5412, batch acc 0.9172
16:44:20.643   Training iter 250, batch loss 1.5404, batch acc 0.9168
16:44:21.203   Training iter 300, batch loss 1.5427, batch acc 0.9128
16:44:21.757   Training iter 350, batch loss 1.5412, batch acc 0.9166
16:44:22.332   Training iter 400, batch loss 1.5384, batch acc 0.9208
16:44:22.895   Training iter 450, batch loss 1.5336, batch acc 0.9258
16:44:23.446   Training iter 500, batch loss 1.5360, batch acc 0.9242
16:44:23.999   Training iter 550, batch loss 1.5429, batch acc 0.9162
16:44:24.545   Training iter 600, batch loss 1.5364, batch acc 0.9238
16:44:24.547 Training @ 149 epoch...
16:44:25.110   Training iter 50, batch loss 1.5338, batch acc 0.9236
16:44:25.654   Training iter 100, batch loss 1.5405, batch acc 0.9144
16:44:26.200   Training iter 150, batch loss 1.5435, batch acc 0.9160
16:44:26.745   Training iter 200, batch loss 1.5421, batch acc 0.9156
16:44:27.290   Training iter 250, batch loss 1.5420, batch acc 0.9106
16:44:27.834   Training iter 300, batch loss 1.5339, batch acc 0.9236
16:44:28.373   Training iter 350, batch loss 1.5397, batch acc 0.9194
16:44:28.917   Training iter 400, batch loss 1.5351, batch acc 0.9218
16:44:29.461   Training iter 450, batch loss 1.5395, batch acc 0.9162
16:44:30.002   Training iter 500, batch loss 1.5323, batch acc 0.9272
16:44:30.533   Training iter 550, batch loss 1.5406, batch acc 0.9236
16:44:31.061   Training iter 600, batch loss 1.5386, batch acc 0.9184
16:44:31.063 Training @ 150 epoch...
16:44:31.576   Training iter 50, batch loss 1.5396, batch acc 0.9214
16:44:32.084   Training iter 100, batch loss 1.5386, batch acc 0.9186
16:44:32.634   Training iter 150, batch loss 1.5414, batch acc 0.9164
16:44:33.189   Training iter 200, batch loss 1.5372, batch acc 0.9178
16:44:33.719   Training iter 250, batch loss 1.5399, batch acc 0.9168
16:44:34.251   Training iter 300, batch loss 1.5375, batch acc 0.9164
16:44:34.782   Training iter 350, batch loss 1.5415, batch acc 0.9158
16:44:35.336   Training iter 400, batch loss 1.5386, batch acc 0.9206
16:44:35.880   Training iter 450, batch loss 1.5367, batch acc 0.9208
16:44:36.449   Training iter 500, batch loss 1.5341, batch acc 0.9272
16:44:37.017   Training iter 550, batch loss 1.5380, batch acc 0.9222
16:44:37.586   Training iter 600, batch loss 1.5386, batch acc 0.9228
16:44:37.588 Testing @ 150 epoch...
16:44:37.639     Testing, total mean loss 1.54231, total acc 0.91930
16:44:37.639 Training @ 151 epoch...
16:44:38.253   Training iter 50, batch loss 1.5395, batch acc 0.9174
16:44:38.851   Training iter 100, batch loss 1.5348, batch acc 0.9238
16:44:39.447   Training iter 150, batch loss 1.5425, batch acc 0.9156
16:44:39.999   Training iter 200, batch loss 1.5406, batch acc 0.9182
16:44:40.544   Training iter 250, batch loss 1.5392, batch acc 0.9198
16:44:41.098   Training iter 300, batch loss 1.5369, batch acc 0.9202
16:44:41.642   Training iter 350, batch loss 1.5365, batch acc 0.9212
16:44:42.182   Training iter 400, batch loss 1.5378, batch acc 0.9180
16:44:42.694   Training iter 450, batch loss 1.5331, batch acc 0.9266
16:44:43.217   Training iter 500, batch loss 1.5406, batch acc 0.9182
16:44:43.731   Training iter 550, batch loss 1.5422, batch acc 0.9150
16:44:44.279   Training iter 600, batch loss 1.5371, batch acc 0.9198
16:44:44.281 Training @ 152 epoch...
16:44:44.854   Training iter 50, batch loss 1.5369, batch acc 0.9192
16:44:45.420   Training iter 100, batch loss 1.5355, batch acc 0.9254
16:44:45.990   Training iter 150, batch loss 1.5415, batch acc 0.9164
16:44:46.550   Training iter 200, batch loss 1.5357, batch acc 0.9224
16:44:47.088   Training iter 250, batch loss 1.5410, batch acc 0.9176
16:44:47.629   Training iter 300, batch loss 1.5390, batch acc 0.9188
16:44:48.132   Training iter 350, batch loss 1.5414, batch acc 0.9182
16:44:48.608   Training iter 400, batch loss 1.5360, batch acc 0.9248
16:44:49.097   Training iter 450, batch loss 1.5358, batch acc 0.9194
16:44:49.582   Training iter 500, batch loss 1.5398, batch acc 0.9186
16:44:50.073   Training iter 550, batch loss 1.5395, batch acc 0.9196
16:44:50.557   Training iter 600, batch loss 1.5374, batch acc 0.9210
16:44:50.559 Training @ 153 epoch...
16:44:51.070   Training iter 50, batch loss 1.5380, batch acc 0.9198
16:44:51.554   Training iter 100, batch loss 1.5369, batch acc 0.9234
16:44:52.047   Training iter 150, batch loss 1.5366, batch acc 0.9208
16:44:52.542   Training iter 200, batch loss 1.5413, batch acc 0.9136
16:44:53.056   Training iter 250, batch loss 1.5387, batch acc 0.9244
16:44:53.580   Training iter 300, batch loss 1.5377, batch acc 0.9206
16:44:54.103   Training iter 350, batch loss 1.5352, batch acc 0.9268
16:44:54.630   Training iter 400, batch loss 1.5366, batch acc 0.9242
16:44:55.186   Training iter 450, batch loss 1.5384, batch acc 0.9156
16:44:55.700   Training iter 500, batch loss 1.5388, batch acc 0.9166
16:44:56.229   Training iter 550, batch loss 1.5400, batch acc 0.9176
16:44:56.737   Training iter 600, batch loss 1.5410, batch acc 0.9140
16:44:56.739 Training @ 154 epoch...
16:44:57.297   Training iter 50, batch loss 1.5396, batch acc 0.9170
16:44:57.855   Training iter 100, batch loss 1.5365, batch acc 0.9206
16:44:58.405   Training iter 150, batch loss 1.5393, batch acc 0.9230
16:44:58.953   Training iter 200, batch loss 1.5408, batch acc 0.9146
16:44:59.510   Training iter 250, batch loss 1.5364, batch acc 0.9212
16:45:00.064   Training iter 300, batch loss 1.5395, batch acc 0.9204
16:45:00.629   Training iter 350, batch loss 1.5424, batch acc 0.9166
16:45:01.206   Training iter 400, batch loss 1.5400, batch acc 0.9214
16:45:01.835   Training iter 450, batch loss 1.5333, batch acc 0.9230
16:45:02.620   Training iter 500, batch loss 1.5350, batch acc 0.9224
16:45:03.331   Training iter 550, batch loss 1.5404, batch acc 0.9158
16:45:03.955   Training iter 600, batch loss 1.5359, batch acc 0.9202
16:45:03.957 Training @ 155 epoch...
16:45:04.577   Training iter 50, batch loss 1.5352, batch acc 0.9232
16:45:05.118   Training iter 100, batch loss 1.5399, batch acc 0.9178
16:45:05.670   Training iter 150, batch loss 1.5398, batch acc 0.9192
16:45:06.177   Training iter 200, batch loss 1.5358, batch acc 0.9230
16:45:06.680   Training iter 250, batch loss 1.5361, batch acc 0.9194
16:45:07.216   Training iter 300, batch loss 1.5365, batch acc 0.9196
16:45:07.758   Training iter 350, batch loss 1.5362, batch acc 0.9250
16:45:08.267   Training iter 400, batch loss 1.5439, batch acc 0.9120
16:45:08.756   Training iter 450, batch loss 1.5405, batch acc 0.9180
16:45:09.260   Training iter 500, batch loss 1.5402, batch acc 0.9186
16:45:09.742   Training iter 550, batch loss 1.5386, batch acc 0.9182
16:45:10.256   Training iter 600, batch loss 1.5358, batch acc 0.9184
16:45:10.259 Testing @ 155 epoch...
16:45:10.307     Testing, total mean loss 1.54181, total acc 0.91970
16:45:10.307 Training @ 156 epoch...
16:45:10.835   Training iter 50, batch loss 1.5351, batch acc 0.9200
16:45:11.364   Training iter 100, batch loss 1.5374, batch acc 0.9218
16:45:11.879   Training iter 150, batch loss 1.5382, batch acc 0.9202
16:45:12.409   Training iter 200, batch loss 1.5381, batch acc 0.9198
16:45:12.938   Training iter 250, batch loss 1.5415, batch acc 0.9150
16:45:13.464   Training iter 300, batch loss 1.5359, batch acc 0.9222
16:45:13.988   Training iter 350, batch loss 1.5453, batch acc 0.9120
16:45:14.526   Training iter 400, batch loss 1.5357, batch acc 0.9208
16:45:15.050   Training iter 450, batch loss 1.5354, batch acc 0.9168
16:45:15.563   Training iter 500, batch loss 1.5389, batch acc 0.9210
16:45:16.072   Training iter 550, batch loss 1.5361, batch acc 0.9228
16:45:16.561   Training iter 600, batch loss 1.5390, batch acc 0.9206
16:45:16.563 Training @ 157 epoch...
16:45:17.050   Training iter 50, batch loss 1.5397, batch acc 0.9208
16:45:17.532   Training iter 100, batch loss 1.5350, batch acc 0.9250
16:45:18.008   Training iter 150, batch loss 1.5395, batch acc 0.9148
16:45:18.483   Training iter 200, batch loss 1.5441, batch acc 0.9138
16:45:18.951   Training iter 250, batch loss 1.5352, batch acc 0.9252
16:45:19.436   Training iter 300, batch loss 1.5361, batch acc 0.9216
16:45:19.941   Training iter 350, batch loss 1.5396, batch acc 0.9166
16:45:20.466   Training iter 400, batch loss 1.5396, batch acc 0.9206
16:45:20.984   Training iter 450, batch loss 1.5382, batch acc 0.9194
16:45:21.493   Training iter 500, batch loss 1.5379, batch acc 0.9216
16:45:22.006   Training iter 550, batch loss 1.5357, batch acc 0.9176
16:45:22.525   Training iter 600, batch loss 1.5363, batch acc 0.9234
16:45:22.527 Training @ 158 epoch...
16:45:23.033   Training iter 50, batch loss 1.5411, batch acc 0.9194
16:45:23.515   Training iter 100, batch loss 1.5402, batch acc 0.9176
16:45:23.995   Training iter 150, batch loss 1.5345, batch acc 0.9238
16:45:24.489   Training iter 200, batch loss 1.5376, batch acc 0.9188
16:45:24.990   Training iter 250, batch loss 1.5332, batch acc 0.9270
16:45:25.507   Training iter 300, batch loss 1.5362, batch acc 0.9220
16:45:26.059   Training iter 350, batch loss 1.5401, batch acc 0.9196
16:45:26.612   Training iter 400, batch loss 1.5382, batch acc 0.9180
16:45:27.168   Training iter 450, batch loss 1.5377, batch acc 0.9182
16:45:27.700   Training iter 500, batch loss 1.5382, batch acc 0.9168
16:45:28.237   Training iter 550, batch loss 1.5369, batch acc 0.9226
16:45:28.752   Training iter 600, batch loss 1.5419, batch acc 0.9174
16:45:28.753 Training @ 159 epoch...
16:45:29.276   Training iter 50, batch loss 1.5376, batch acc 0.9182
16:45:29.802   Training iter 100, batch loss 1.5348, batch acc 0.9242
16:45:30.345   Training iter 150, batch loss 1.5333, batch acc 0.9268
16:45:30.897   Training iter 200, batch loss 1.5417, batch acc 0.9146
16:45:31.443   Training iter 250, batch loss 1.5377, batch acc 0.9174
16:45:31.970   Training iter 300, batch loss 1.5398, batch acc 0.9188
16:45:32.482   Training iter 350, batch loss 1.5411, batch acc 0.9144
16:45:32.969   Training iter 400, batch loss 1.5348, batch acc 0.9248
16:45:33.455   Training iter 450, batch loss 1.5342, batch acc 0.9252
16:45:33.939   Training iter 500, batch loss 1.5378, batch acc 0.9216
16:45:34.437   Training iter 550, batch loss 1.5411, batch acc 0.9162
16:45:34.927   Training iter 600, batch loss 1.5425, batch acc 0.9172
16:45:34.929 Training @ 160 epoch...
16:45:35.409   Training iter 50, batch loss 1.5394, batch acc 0.9180
16:45:35.880   Training iter 100, batch loss 1.5352, batch acc 0.9212
16:45:36.362   Training iter 150, batch loss 1.5406, batch acc 0.9204
16:45:36.865   Training iter 200, batch loss 1.5355, batch acc 0.9234
16:45:37.369   Training iter 250, batch loss 1.5383, batch acc 0.9160
16:45:37.873   Training iter 300, batch loss 1.5391, batch acc 0.9228
16:45:38.395   Training iter 350, batch loss 1.5393, batch acc 0.9184
16:45:38.909   Training iter 400, batch loss 1.5376, batch acc 0.9194
16:45:39.431   Training iter 450, batch loss 1.5389, batch acc 0.9208
16:45:39.944   Training iter 500, batch loss 1.5381, batch acc 0.9184
16:45:40.517   Training iter 550, batch loss 1.5384, batch acc 0.9202
16:45:41.095   Training iter 600, batch loss 1.5352, batch acc 0.9202
16:45:41.096 Testing @ 160 epoch...
16:45:41.144     Testing, total mean loss 1.54203, total acc 0.91790
16:45:41.144 Training @ 161 epoch...
16:45:41.718   Training iter 50, batch loss 1.5418, batch acc 0.9184
16:45:42.263   Training iter 100, batch loss 1.5390, batch acc 0.9152
16:45:42.768   Training iter 150, batch loss 1.5370, batch acc 0.9236
16:45:43.272   Training iter 200, batch loss 1.5356, batch acc 0.9242
16:45:43.767   Training iter 250, batch loss 1.5389, batch acc 0.9188
16:45:44.286   Training iter 300, batch loss 1.5370, batch acc 0.9210
16:45:44.784   Training iter 350, batch loss 1.5357, batch acc 0.9204
16:45:45.282   Training iter 400, batch loss 1.5395, batch acc 0.9194
16:45:45.786   Training iter 450, batch loss 1.5390, batch acc 0.9196
16:45:46.294   Training iter 500, batch loss 1.5383, batch acc 0.9210
16:45:46.783   Training iter 550, batch loss 1.5376, batch acc 0.9176
16:45:47.266   Training iter 600, batch loss 1.5361, batch acc 0.9220
16:45:47.268 Training @ 162 epoch...
16:45:47.787   Training iter 50, batch loss 1.5350, batch acc 0.9238
16:45:48.307   Training iter 100, batch loss 1.5370, batch acc 0.9214
16:45:48.805   Training iter 150, batch loss 1.5364, batch acc 0.9198
16:45:49.293   Training iter 200, batch loss 1.5366, batch acc 0.9222
16:45:49.781   Training iter 250, batch loss 1.5392, batch acc 0.9134
16:45:50.267   Training iter 300, batch loss 1.5348, batch acc 0.9190
16:45:50.755   Training iter 350, batch loss 1.5389, batch acc 0.9192
16:45:51.262   Training iter 400, batch loss 1.5402, batch acc 0.9182
16:45:51.766   Training iter 450, batch loss 1.5391, batch acc 0.9192
16:45:52.279   Training iter 500, batch loss 1.5380, batch acc 0.9202
16:45:52.797   Training iter 550, batch loss 1.5382, batch acc 0.9226
16:45:53.309   Training iter 600, batch loss 1.5407, batch acc 0.9196
16:45:53.311 Training @ 163 epoch...
16:45:53.820   Training iter 50, batch loss 1.5378, batch acc 0.9198
16:45:54.345   Training iter 100, batch loss 1.5379, batch acc 0.9222
16:45:54.867   Training iter 150, batch loss 1.5325, batch acc 0.9248
16:45:55.396   Training iter 200, batch loss 1.5375, batch acc 0.9206
16:45:55.925   Training iter 250, batch loss 1.5383, batch acc 0.9188
16:45:56.467   Training iter 300, batch loss 1.5365, batch acc 0.9204
16:45:56.995   Training iter 350, batch loss 1.5406, batch acc 0.9124
16:45:57.531   Training iter 400, batch loss 1.5376, batch acc 0.9238
16:45:58.068   Training iter 450, batch loss 1.5365, batch acc 0.9186
16:45:58.608   Training iter 500, batch loss 1.5401, batch acc 0.9188
16:45:59.160   Training iter 550, batch loss 1.5395, batch acc 0.9178
16:45:59.677   Training iter 600, batch loss 1.5398, batch acc 0.9166
16:45:59.679 Training @ 164 epoch...
16:46:00.210   Training iter 50, batch loss 1.5356, batch acc 0.9210
16:46:00.733   Training iter 100, batch loss 1.5374, batch acc 0.9200
16:46:01.264   Training iter 150, batch loss 1.5427, batch acc 0.9120
16:46:01.843   Training iter 200, batch loss 1.5339, batch acc 0.9246
16:46:02.396   Training iter 250, batch loss 1.5317, batch acc 0.9242
16:46:02.948   Training iter 300, batch loss 1.5420, batch acc 0.9102
16:46:03.486   Training iter 350, batch loss 1.5382, batch acc 0.9160
16:46:04.042   Training iter 400, batch loss 1.5377, batch acc 0.9210
16:46:04.567   Training iter 450, batch loss 1.5457, batch acc 0.9132
16:46:05.107   Training iter 500, batch loss 1.5370, batch acc 0.9222
16:46:05.644   Training iter 550, batch loss 1.5370, batch acc 0.9218
16:46:06.168   Training iter 600, batch loss 1.5335, batch acc 0.9282
16:46:06.170 Training @ 165 epoch...
16:46:06.691   Training iter 50, batch loss 1.5370, batch acc 0.9228
16:46:07.209   Training iter 100, batch loss 1.5402, batch acc 0.9142
16:46:07.737   Training iter 150, batch loss 1.5388, batch acc 0.9200
16:46:08.282   Training iter 200, batch loss 1.5329, batch acc 0.9292
16:46:08.818   Training iter 250, batch loss 1.5375, batch acc 0.9172
16:46:09.343   Training iter 300, batch loss 1.5381, batch acc 0.9216
16:46:09.870   Training iter 350, batch loss 1.5385, batch acc 0.9178
16:46:10.402   Training iter 400, batch loss 1.5363, batch acc 0.9206
16:46:10.934   Training iter 450, batch loss 1.5363, batch acc 0.9210
16:46:11.459   Training iter 500, batch loss 1.5374, batch acc 0.9208
16:46:11.983   Training iter 550, batch loss 1.5391, batch acc 0.9160
16:46:12.506   Training iter 600, batch loss 1.5403, batch acc 0.9174
16:46:12.508 Testing @ 165 epoch...
16:46:12.554     Testing, total mean loss 1.54189, total acc 0.91870
16:46:12.554 Training @ 166 epoch...
16:46:13.085   Training iter 50, batch loss 1.5397, batch acc 0.9146
16:46:13.612   Training iter 100, batch loss 1.5369, batch acc 0.9220
16:46:14.151   Training iter 150, batch loss 1.5416, batch acc 0.9160
16:46:14.667   Training iter 200, batch loss 1.5358, batch acc 0.9272
16:46:15.216   Training iter 250, batch loss 1.5380, batch acc 0.9206
16:46:15.768   Training iter 300, batch loss 1.5373, batch acc 0.9220
16:46:16.334   Training iter 350, batch loss 1.5358, batch acc 0.9198
16:46:16.906   Training iter 400, batch loss 1.5384, batch acc 0.9194
16:46:17.458   Training iter 450, batch loss 1.5362, batch acc 0.9216
16:46:17.995   Training iter 500, batch loss 1.5363, batch acc 0.9170
16:46:18.541   Training iter 550, batch loss 1.5393, batch acc 0.9186
16:46:19.086   Training iter 600, batch loss 1.5362, batch acc 0.9202
16:46:19.087 Training @ 167 epoch...
16:46:19.641   Training iter 50, batch loss 1.5392, batch acc 0.9164
16:46:20.191   Training iter 100, batch loss 1.5385, batch acc 0.9196
16:46:20.750   Training iter 150, batch loss 1.5374, batch acc 0.9224
16:46:21.276   Training iter 200, batch loss 1.5377, batch acc 0.9202
16:46:21.796   Training iter 250, batch loss 1.5338, batch acc 0.9240
16:46:22.309   Training iter 300, batch loss 1.5373, batch acc 0.9214
16:46:22.828   Training iter 350, batch loss 1.5403, batch acc 0.9162
16:46:23.337   Training iter 400, batch loss 1.5397, batch acc 0.9198
16:46:23.850   Training iter 450, batch loss 1.5407, batch acc 0.9130
16:46:24.360   Training iter 500, batch loss 1.5425, batch acc 0.9160
16:46:24.859   Training iter 550, batch loss 1.5327, batch acc 0.9238
16:46:25.381   Training iter 600, batch loss 1.5318, batch acc 0.9254
16:46:25.383 Training @ 168 epoch...
16:46:25.906   Training iter 50, batch loss 1.5369, batch acc 0.9174
16:46:26.421   Training iter 100, batch loss 1.5352, batch acc 0.9210
16:46:26.936   Training iter 150, batch loss 1.5393, batch acc 0.9162
16:46:27.441   Training iter 200, batch loss 1.5369, batch acc 0.9226
16:46:27.953   Training iter 250, batch loss 1.5336, batch acc 0.9236
16:46:28.473   Training iter 300, batch loss 1.5418, batch acc 0.9174
16:46:28.972   Training iter 350, batch loss 1.5362, batch acc 0.9172
16:46:29.494   Training iter 400, batch loss 1.5345, batch acc 0.9276
16:46:30.024   Training iter 450, batch loss 1.5383, batch acc 0.9202
16:46:30.548   Training iter 500, batch loss 1.5403, batch acc 0.9196
16:46:31.103   Training iter 550, batch loss 1.5396, batch acc 0.9196
16:46:31.675   Training iter 600, batch loss 1.5384, batch acc 0.9222
16:46:31.676 Training @ 169 epoch...
16:46:32.236   Training iter 50, batch loss 1.5417, batch acc 0.9180
16:46:32.781   Training iter 100, batch loss 1.5357, batch acc 0.9234
16:46:33.324   Training iter 150, batch loss 1.5386, batch acc 0.9186
16:46:33.873   Training iter 200, batch loss 1.5390, batch acc 0.9190
16:46:34.423   Training iter 250, batch loss 1.5417, batch acc 0.9144
16:46:34.971   Training iter 300, batch loss 1.5327, batch acc 0.9248
16:46:35.521   Training iter 350, batch loss 1.5351, batch acc 0.9226
16:46:36.082   Training iter 400, batch loss 1.5362, batch acc 0.9212
16:46:36.635   Training iter 450, batch loss 1.5384, batch acc 0.9172
16:46:37.140   Training iter 500, batch loss 1.5360, batch acc 0.9162
16:46:37.650   Training iter 550, batch loss 1.5373, batch acc 0.9224
16:46:38.147   Training iter 600, batch loss 1.5389, batch acc 0.9200
16:46:38.149 Training @ 170 epoch...
16:46:38.651   Training iter 50, batch loss 1.5392, batch acc 0.9168
16:46:39.159   Training iter 100, batch loss 1.5388, batch acc 0.9170
16:46:39.686   Training iter 150, batch loss 1.5354, batch acc 0.9194
16:46:40.223   Training iter 200, batch loss 1.5372, batch acc 0.9222
16:46:40.751   Training iter 250, batch loss 1.5355, batch acc 0.9250
16:46:41.277   Training iter 300, batch loss 1.5354, batch acc 0.9204
16:46:41.800   Training iter 350, batch loss 1.5382, batch acc 0.9228
16:46:42.325   Training iter 400, batch loss 1.5408, batch acc 0.9170
16:46:42.860   Training iter 450, batch loss 1.5348, batch acc 0.9290
16:46:43.372   Training iter 500, batch loss 1.5393, batch acc 0.9166
16:46:43.900   Training iter 550, batch loss 1.5350, batch acc 0.9220
16:46:44.424   Training iter 600, batch loss 1.5403, batch acc 0.9214
16:46:44.426 Testing @ 170 epoch...
16:46:44.473     Testing, total mean loss 1.54172, total acc 0.91920
16:46:44.473 Training @ 171 epoch...
16:46:44.992   Training iter 50, batch loss 1.5370, batch acc 0.9208
16:46:45.506   Training iter 100, batch loss 1.5334, batch acc 0.9290
16:46:46.043   Training iter 150, batch loss 1.5413, batch acc 0.9172
16:46:46.549   Training iter 200, batch loss 1.5410, batch acc 0.9138
16:46:47.076   Training iter 250, batch loss 1.5357, batch acc 0.9212
16:46:47.578   Training iter 300, batch loss 1.5322, batch acc 0.9290
16:46:48.111   Training iter 350, batch loss 1.5420, batch acc 0.9136
16:46:48.614   Training iter 400, batch loss 1.5375, batch acc 0.9164
16:46:49.121   Training iter 450, batch loss 1.5349, batch acc 0.9226
16:46:49.634   Training iter 500, batch loss 1.5383, batch acc 0.9178
16:46:50.143   Training iter 550, batch loss 1.5348, batch acc 0.9268
16:46:50.655   Training iter 600, batch loss 1.5417, batch acc 0.9144
16:46:50.657 Training @ 172 epoch...
16:46:51.198   Training iter 50, batch loss 1.5338, batch acc 0.9274
16:46:51.759   Training iter 100, batch loss 1.5363, batch acc 0.9178
16:46:52.312   Training iter 150, batch loss 1.5406, batch acc 0.9154
16:46:52.848   Training iter 200, batch loss 1.5380, batch acc 0.9176
16:46:53.368   Training iter 250, batch loss 1.5354, batch acc 0.9216
16:46:53.882   Training iter 300, batch loss 1.5424, batch acc 0.9172
16:46:54.396   Training iter 350, batch loss 1.5351, batch acc 0.9222
16:46:54.909   Training iter 400, batch loss 1.5367, batch acc 0.9190
16:46:55.431   Training iter 450, batch loss 1.5361, batch acc 0.9208
16:46:55.951   Training iter 500, batch loss 1.5354, batch acc 0.9232
16:46:56.473   Training iter 550, batch loss 1.5379, batch acc 0.9224
16:46:57.004   Training iter 600, batch loss 1.5420, batch acc 0.9148
16:46:57.006 Training @ 173 epoch...
16:46:57.522   Training iter 50, batch loss 1.5381, batch acc 0.9222
16:46:58.039   Training iter 100, batch loss 1.5399, batch acc 0.9172
16:46:58.552   Training iter 150, batch loss 1.5373, batch acc 0.9206
16:46:59.083   Training iter 200, batch loss 1.5396, batch acc 0.9152
16:46:59.621   Training iter 250, batch loss 1.5335, batch acc 0.9230
16:47:00.157   Training iter 300, batch loss 1.5373, batch acc 0.9226
16:47:00.676   Training iter 350, batch loss 1.5419, batch acc 0.9104
16:47:01.270   Training iter 400, batch loss 1.5377, batch acc 0.9180
16:47:02.017   Training iter 450, batch loss 1.5317, batch acc 0.9286
16:47:02.763   Training iter 500, batch loss 1.5370, batch acc 0.9208
16:47:03.469   Training iter 550, batch loss 1.5356, batch acc 0.9202
16:47:04.104   Training iter 600, batch loss 1.5396, batch acc 0.9212
16:47:04.106 Training @ 174 epoch...
16:47:04.710   Training iter 50, batch loss 1.5370, batch acc 0.9198
16:47:05.274   Training iter 100, batch loss 1.5396, batch acc 0.9174
16:47:05.856   Training iter 150, batch loss 1.5379, batch acc 0.9200
16:47:06.420   Training iter 200, batch loss 1.5363, batch acc 0.9224
16:47:06.983   Training iter 250, batch loss 1.5394, batch acc 0.9184
16:47:07.526   Training iter 300, batch loss 1.5405, batch acc 0.9180
16:47:08.259   Training iter 350, batch loss 1.5386, batch acc 0.9194
16:47:08.813   Training iter 400, batch loss 1.5365, batch acc 0.9196
16:47:09.320   Training iter 450, batch loss 1.5381, batch acc 0.9180
16:47:09.851   Training iter 500, batch loss 1.5329, batch acc 0.9234
16:47:10.336   Training iter 550, batch loss 1.5323, batch acc 0.9278
16:47:10.815   Training iter 600, batch loss 1.5392, batch acc 0.9220
16:47:10.817 Training @ 175 epoch...
16:47:11.322   Training iter 50, batch loss 1.5380, batch acc 0.9168
16:47:11.834   Training iter 100, batch loss 1.5419, batch acc 0.9094
16:47:12.317   Training iter 150, batch loss 1.5369, batch acc 0.9220
16:47:12.778   Training iter 200, batch loss 1.5324, batch acc 0.9288
16:47:13.265   Training iter 250, batch loss 1.5383, batch acc 0.9200
16:47:13.732   Training iter 300, batch loss 1.5328, batch acc 0.9246
16:47:14.226   Training iter 350, batch loss 1.5394, batch acc 0.9168
16:47:14.711   Training iter 400, batch loss 1.5346, batch acc 0.9210
16:47:15.206   Training iter 450, batch loss 1.5353, batch acc 0.9224
16:47:15.705   Training iter 500, batch loss 1.5373, batch acc 0.9248
16:47:16.190   Training iter 550, batch loss 1.5420, batch acc 0.9172
16:47:16.696   Training iter 600, batch loss 1.5380, batch acc 0.9206
16:47:16.697 Testing @ 175 epoch...
16:47:16.743     Testing, total mean loss 1.54205, total acc 0.91840
16:47:16.743 Training @ 176 epoch...
16:47:17.244   Training iter 50, batch loss 1.5347, batch acc 0.9232
16:47:17.738   Training iter 100, batch loss 1.5357, batch acc 0.9242
16:47:18.240   Training iter 150, batch loss 1.5337, batch acc 0.9230
16:47:18.737   Training iter 200, batch loss 1.5376, batch acc 0.9170
16:47:19.265   Training iter 250, batch loss 1.5376, batch acc 0.9182
16:47:19.792   Training iter 300, batch loss 1.5361, batch acc 0.9212
16:47:20.330   Training iter 350, batch loss 1.5336, batch acc 0.9280
16:47:20.866   Training iter 400, batch loss 1.5392, batch acc 0.9178
16:47:21.406   Training iter 450, batch loss 1.5419, batch acc 0.9172
16:47:21.936   Training iter 500, batch loss 1.5404, batch acc 0.9172
16:47:22.477   Training iter 550, batch loss 1.5381, batch acc 0.9204
16:47:23.014   Training iter 600, batch loss 1.5374, batch acc 0.9180
16:47:23.016 Training @ 177 epoch...
16:47:23.551   Training iter 50, batch loss 1.5353, batch acc 0.9258
16:47:24.088   Training iter 100, batch loss 1.5374, batch acc 0.9208
16:47:24.639   Training iter 150, batch loss 1.5386, batch acc 0.9168
16:47:25.191   Training iter 200, batch loss 1.5389, batch acc 0.9180
16:47:25.688   Training iter 250, batch loss 1.5356, batch acc 0.9222
16:47:26.205   Training iter 300, batch loss 1.5396, batch acc 0.9212
16:47:26.725   Training iter 350, batch loss 1.5418, batch acc 0.9164
16:47:27.228   Training iter 400, batch loss 1.5360, batch acc 0.9212
16:47:27.687   Training iter 450, batch loss 1.5386, batch acc 0.9184
16:47:28.149   Training iter 500, batch loss 1.5364, batch acc 0.9196
16:47:28.615   Training iter 550, batch loss 1.5317, batch acc 0.9248
16:47:29.139   Training iter 600, batch loss 1.5361, batch acc 0.9224
16:47:29.141 Training @ 178 epoch...
16:47:29.682   Training iter 50, batch loss 1.5348, batch acc 0.9236
16:47:30.192   Training iter 100, batch loss 1.5331, batch acc 0.9268
16:47:30.695   Training iter 150, batch loss 1.5412, batch acc 0.9178
16:47:31.206   Training iter 200, batch loss 1.5352, batch acc 0.9226
16:47:31.733   Training iter 250, batch loss 1.5422, batch acc 0.9186
16:47:32.251   Training iter 300, batch loss 1.5379, batch acc 0.9208
16:47:32.759   Training iter 350, batch loss 1.5351, batch acc 0.9240
16:47:33.269   Training iter 400, batch loss 1.5353, batch acc 0.9216
16:47:33.809   Training iter 450, batch loss 1.5424, batch acc 0.9154
16:47:34.360   Training iter 500, batch loss 1.5348, batch acc 0.9208
16:47:34.867   Training iter 550, batch loss 1.5353, batch acc 0.9208
16:47:35.345   Training iter 600, batch loss 1.5388, batch acc 0.9142
16:47:35.347 Training @ 179 epoch...
16:47:35.825   Training iter 50, batch loss 1.5341, batch acc 0.9234
16:47:36.299   Training iter 100, batch loss 1.5349, batch acc 0.9248
16:47:36.793   Training iter 150, batch loss 1.5352, batch acc 0.9258
16:47:37.319   Training iter 200, batch loss 1.5402, batch acc 0.9224
16:47:37.843   Training iter 250, batch loss 1.5356, batch acc 0.9222
16:47:38.375   Training iter 300, batch loss 1.5423, batch acc 0.9126
16:47:38.912   Training iter 350, batch loss 1.5388, batch acc 0.9150
16:47:39.458   Training iter 400, batch loss 1.5370, batch acc 0.9220
16:47:40.004   Training iter 450, batch loss 1.5382, batch acc 0.9198
16:47:40.538   Training iter 500, batch loss 1.5396, batch acc 0.9148
16:47:41.063   Training iter 550, batch loss 1.5331, batch acc 0.9278
16:47:41.598   Training iter 600, batch loss 1.5356, batch acc 0.9222
16:47:41.599 Training @ 180 epoch...
16:47:42.149   Training iter 50, batch loss 1.5359, batch acc 0.9216
16:47:42.672   Training iter 100, batch loss 1.5423, batch acc 0.9154
16:47:43.207   Training iter 150, batch loss 1.5371, batch acc 0.9238
16:47:43.736   Training iter 200, batch loss 1.5369, batch acc 0.9176
16:47:44.287   Training iter 250, batch loss 1.5376, batch acc 0.9152
16:47:44.837   Training iter 300, batch loss 1.5416, batch acc 0.9144
16:47:45.392   Training iter 350, batch loss 1.5357, batch acc 0.9274
16:47:45.919   Training iter 400, batch loss 1.5376, batch acc 0.9194
16:47:46.465   Training iter 450, batch loss 1.5356, batch acc 0.9240
16:47:46.986   Training iter 500, batch loss 1.5349, batch acc 0.9240
16:47:47.498   Training iter 550, batch loss 1.5363, batch acc 0.9220
16:47:48.013   Training iter 600, batch loss 1.5335, batch acc 0.9218
16:47:48.015 Testing @ 180 epoch...
16:47:48.060     Testing, total mean loss 1.54128, total acc 0.92110
16:47:48.060 Training @ 181 epoch...
16:47:48.589   Training iter 50, batch loss 1.5348, batch acc 0.9272
16:47:49.123   Training iter 100, batch loss 1.5441, batch acc 0.9118
16:47:49.647   Training iter 150, batch loss 1.5361, batch acc 0.9226
16:47:50.163   Training iter 200, batch loss 1.5361, batch acc 0.9194
16:47:50.675   Training iter 250, batch loss 1.5389, batch acc 0.9214
16:47:51.190   Training iter 300, batch loss 1.5359, batch acc 0.9226
16:47:51.739   Training iter 350, batch loss 1.5324, batch acc 0.9210
16:47:52.313   Training iter 400, batch loss 1.5405, batch acc 0.9182
16:47:52.868   Training iter 450, batch loss 1.5381, batch acc 0.9200
16:47:53.431   Training iter 500, batch loss 1.5335, batch acc 0.9224
16:47:53.992   Training iter 550, batch loss 1.5390, batch acc 0.9180
16:47:54.536   Training iter 600, batch loss 1.5367, batch acc 0.9210
16:47:54.538 Training @ 182 epoch...
16:47:55.089   Training iter 50, batch loss 1.5374, batch acc 0.9236
16:47:55.623   Training iter 100, batch loss 1.5363, batch acc 0.9222
16:47:56.180   Training iter 150, batch loss 1.5386, batch acc 0.9202
16:47:56.726   Training iter 200, batch loss 1.5331, batch acc 0.9244
16:47:57.270   Training iter 250, batch loss 1.5407, batch acc 0.9194
16:47:57.799   Training iter 300, batch loss 1.5315, batch acc 0.9276
16:47:58.337   Training iter 350, batch loss 1.5379, batch acc 0.9176
16:47:58.868   Training iter 400, batch loss 1.5433, batch acc 0.9100
16:47:59.414   Training iter 450, batch loss 1.5369, batch acc 0.9200
16:47:59.957   Training iter 500, batch loss 1.5331, batch acc 0.9248
16:48:00.506   Training iter 550, batch loss 1.5349, batch acc 0.9230
16:48:01.030   Training iter 600, batch loss 1.5399, batch acc 0.9162
16:48:01.031 Training @ 183 epoch...
16:48:01.567   Training iter 50, batch loss 1.5339, batch acc 0.9248
16:48:02.129   Training iter 100, batch loss 1.5402, batch acc 0.9222
16:48:02.649   Training iter 150, batch loss 1.5339, batch acc 0.9240
16:48:03.182   Training iter 200, batch loss 1.5327, batch acc 0.9272
16:48:03.712   Training iter 250, batch loss 1.5365, batch acc 0.9224
16:48:04.244   Training iter 300, batch loss 1.5401, batch acc 0.9162
16:48:04.779   Training iter 350, batch loss 1.5382, batch acc 0.9206
16:48:05.311   Training iter 400, batch loss 1.5368, batch acc 0.9208
16:48:05.867   Training iter 450, batch loss 1.5392, batch acc 0.9134
16:48:06.422   Training iter 500, batch loss 1.5366, batch acc 0.9236
16:48:06.976   Training iter 550, batch loss 1.5348, batch acc 0.9196
16:48:07.544   Training iter 600, batch loss 1.5411, batch acc 0.9148
16:48:07.545 Training @ 184 epoch...
16:48:08.105   Training iter 50, batch loss 1.5393, batch acc 0.9186
16:48:08.652   Training iter 100, batch loss 1.5384, batch acc 0.9224
16:48:09.228   Training iter 150, batch loss 1.5350, batch acc 0.9224
16:48:09.819   Training iter 200, batch loss 1.5347, batch acc 0.9234
16:48:10.430   Training iter 250, batch loss 1.5388, batch acc 0.9174
16:48:11.026   Training iter 300, batch loss 1.5394, batch acc 0.9174
16:48:11.610   Training iter 350, batch loss 1.5387, batch acc 0.9130
16:48:12.229   Training iter 400, batch loss 1.5340, batch acc 0.9220
16:48:12.796   Training iter 450, batch loss 1.5332, batch acc 0.9242
16:48:13.325   Training iter 500, batch loss 1.5358, batch acc 0.9236
16:48:13.838   Training iter 550, batch loss 1.5398, batch acc 0.9154
16:48:14.362   Training iter 600, batch loss 1.5366, batch acc 0.9240
16:48:14.363 Training @ 185 epoch...
16:48:14.899   Training iter 50, batch loss 1.5405, batch acc 0.9164
16:48:15.420   Training iter 100, batch loss 1.5378, batch acc 0.9212
16:48:15.943   Training iter 150, batch loss 1.5376, batch acc 0.9202
16:48:16.443   Training iter 200, batch loss 1.5342, batch acc 0.9216
16:48:16.940   Training iter 250, batch loss 1.5328, batch acc 0.9240
16:48:17.448   Training iter 300, batch loss 1.5376, batch acc 0.9178
16:48:17.953   Training iter 350, batch loss 1.5390, batch acc 0.9182
16:48:18.442   Training iter 400, batch loss 1.5363, batch acc 0.9210
16:48:18.929   Training iter 450, batch loss 1.5389, batch acc 0.9210
16:48:19.448   Training iter 500, batch loss 1.5339, batch acc 0.9236
16:48:19.987   Training iter 550, batch loss 1.5387, batch acc 0.9250
16:48:20.515   Training iter 600, batch loss 1.5355, batch acc 0.9218
16:48:20.517 Testing @ 185 epoch...
16:48:20.562     Testing, total mean loss 1.54168, total acc 0.91780
16:48:20.562 Training @ 186 epoch...
16:48:21.103   Training iter 50, batch loss 1.5379, batch acc 0.9178
16:48:21.639   Training iter 100, batch loss 1.5383, batch acc 0.9178
16:48:22.183   Training iter 150, batch loss 1.5347, batch acc 0.9188
16:48:22.726   Training iter 200, batch loss 1.5394, batch acc 0.9174
16:48:23.285   Training iter 250, batch loss 1.5362, batch acc 0.9230
16:48:23.844   Training iter 300, batch loss 1.5330, batch acc 0.9226
16:48:24.417   Training iter 350, batch loss 1.5374, batch acc 0.9212
16:48:24.982   Training iter 400, batch loss 1.5372, batch acc 0.9214
16:48:25.541   Training iter 450, batch loss 1.5364, batch acc 0.9228
16:48:26.102   Training iter 500, batch loss 1.5389, batch acc 0.9210
16:48:26.656   Training iter 550, batch loss 1.5366, batch acc 0.9194
16:48:27.208   Training iter 600, batch loss 1.5362, batch acc 0.9236
16:48:27.210 Training @ 187 epoch...
16:48:27.771   Training iter 50, batch loss 1.5369, batch acc 0.9222
16:48:28.308   Training iter 100, batch loss 1.5338, batch acc 0.9194
16:48:28.852   Training iter 150, batch loss 1.5363, batch acc 0.9234
16:48:29.389   Training iter 200, batch loss 1.5359, batch acc 0.9216
16:48:29.919   Training iter 250, batch loss 1.5380, batch acc 0.9236
16:48:30.445   Training iter 300, batch loss 1.5341, batch acc 0.9264
16:48:30.957   Training iter 350, batch loss 1.5433, batch acc 0.9138
16:48:31.446   Training iter 400, batch loss 1.5350, batch acc 0.9216
16:48:31.945   Training iter 450, batch loss 1.5362, batch acc 0.9202
16:48:32.469   Training iter 500, batch loss 1.5381, batch acc 0.9150
16:48:32.999   Training iter 550, batch loss 1.5356, batch acc 0.9208
16:48:33.508   Training iter 600, batch loss 1.5397, batch acc 0.9154
16:48:33.510 Training @ 188 epoch...
16:48:34.036   Training iter 50, batch loss 1.5316, batch acc 0.9218
16:48:34.551   Training iter 100, batch loss 1.5367, batch acc 0.9186
16:48:35.073   Training iter 150, batch loss 1.5326, batch acc 0.9256
16:48:35.578   Training iter 200, batch loss 1.5363, batch acc 0.9224
16:48:36.087   Training iter 250, batch loss 1.5430, batch acc 0.9116
16:48:36.590   Training iter 300, batch loss 1.5358, batch acc 0.9198
16:48:37.113   Training iter 350, batch loss 1.5359, batch acc 0.9224
16:48:37.623   Training iter 400, batch loss 1.5361, batch acc 0.9212
16:48:38.138   Training iter 450, batch loss 1.5430, batch acc 0.9138
16:48:38.664   Training iter 500, batch loss 1.5374, batch acc 0.9196
16:48:39.199   Training iter 550, batch loss 1.5383, batch acc 0.9252
16:48:39.751   Training iter 600, batch loss 1.5355, batch acc 0.9212
16:48:39.753 Training @ 189 epoch...
16:48:40.321   Training iter 50, batch loss 1.5318, batch acc 0.9268
16:48:40.872   Training iter 100, batch loss 1.5336, batch acc 0.9228
16:48:41.424   Training iter 150, batch loss 1.5358, batch acc 0.9230
16:48:41.985   Training iter 200, batch loss 1.5362, batch acc 0.9246
16:48:42.546   Training iter 250, batch loss 1.5440, batch acc 0.9118
16:48:43.089   Training iter 300, batch loss 1.5374, batch acc 0.9242
16:48:43.635   Training iter 350, batch loss 1.5346, batch acc 0.9206
16:48:44.178   Training iter 400, batch loss 1.5327, batch acc 0.9276
16:48:44.761   Training iter 450, batch loss 1.5397, batch acc 0.9202
16:48:45.344   Training iter 500, batch loss 1.5370, batch acc 0.9184
16:48:45.920   Training iter 550, batch loss 1.5408, batch acc 0.9136
16:48:46.430   Training iter 600, batch loss 1.5395, batch acc 0.9192
16:48:46.432 Training @ 190 epoch...
16:48:46.925   Training iter 50, batch loss 1.5384, batch acc 0.9180
16:48:47.427   Training iter 100, batch loss 1.5342, batch acc 0.9244
16:48:47.933   Training iter 150, batch loss 1.5329, batch acc 0.9290
16:48:48.440   Training iter 200, batch loss 1.5348, batch acc 0.9256
16:48:48.956   Training iter 250, batch loss 1.5358, batch acc 0.9214
16:48:49.453   Training iter 300, batch loss 1.5378, batch acc 0.9168
16:48:49.967   Training iter 350, batch loss 1.5388, batch acc 0.9174
16:48:50.479   Training iter 400, batch loss 1.5389, batch acc 0.9196
16:48:50.972   Training iter 450, batch loss 1.5397, batch acc 0.9158
16:48:51.457   Training iter 500, batch loss 1.5369, batch acc 0.9206
16:48:51.952   Training iter 550, batch loss 1.5369, batch acc 0.9178
16:48:52.457   Training iter 600, batch loss 1.5363, batch acc 0.9198
16:48:52.459 Testing @ 190 epoch...
16:48:52.504     Testing, total mean loss 1.54146, total acc 0.91990
16:48:52.504 Training @ 191 epoch...
16:48:53.018   Training iter 50, batch loss 1.5401, batch acc 0.9160
16:48:53.501   Training iter 100, batch loss 1.5332, batch acc 0.9244
16:48:54.004   Training iter 150, batch loss 1.5331, batch acc 0.9258
16:48:54.528   Training iter 200, batch loss 1.5389, batch acc 0.9178
16:48:55.048   Training iter 250, batch loss 1.5395, batch acc 0.9174
16:48:55.578   Training iter 300, batch loss 1.5359, batch acc 0.9198
16:48:56.144   Training iter 350, batch loss 1.5380, batch acc 0.9218
16:48:56.704   Training iter 400, batch loss 1.5388, batch acc 0.9164
16:48:57.278   Training iter 450, batch loss 1.5354, batch acc 0.9220
16:48:57.852   Training iter 500, batch loss 1.5319, batch acc 0.9262
16:48:58.437   Training iter 550, batch loss 1.5403, batch acc 0.9184
16:48:58.976   Training iter 600, batch loss 1.5352, batch acc 0.9214
16:48:58.977 Training @ 192 epoch...
16:48:59.526   Training iter 50, batch loss 1.5367, batch acc 0.9202
16:49:00.066   Training iter 100, batch loss 1.5365, batch acc 0.9156
16:49:00.605   Training iter 150, batch loss 1.5320, batch acc 0.9256
16:49:01.138   Training iter 200, batch loss 1.5337, batch acc 0.9212
16:49:01.695   Training iter 250, batch loss 1.5429, batch acc 0.9180
16:49:02.266   Training iter 300, batch loss 1.5418, batch acc 0.9204
16:49:02.812   Training iter 350, batch loss 1.5351, batch acc 0.9206
16:49:03.360   Training iter 400, batch loss 1.5366, batch acc 0.9246
16:49:03.909   Training iter 450, batch loss 1.5403, batch acc 0.9182
16:49:04.469   Training iter 500, batch loss 1.5351, batch acc 0.9214
16:49:05.025   Training iter 550, batch loss 1.5341, batch acc 0.9260
16:49:05.575   Training iter 600, batch loss 1.5352, batch acc 0.9188
16:49:05.577 Training @ 193 epoch...
16:49:06.091   Training iter 50, batch loss 1.5410, batch acc 0.9152
16:49:06.621   Training iter 100, batch loss 1.5349, batch acc 0.9234
16:49:07.161   Training iter 150, batch loss 1.5394, batch acc 0.9166
16:49:07.689   Training iter 200, batch loss 1.5416, batch acc 0.9160
16:49:08.203   Training iter 250, batch loss 1.5357, batch acc 0.9172
16:49:08.733   Training iter 300, batch loss 1.5337, batch acc 0.9240
16:49:09.244   Training iter 350, batch loss 1.5301, batch acc 0.9286
16:49:09.757   Training iter 400, batch loss 1.5370, batch acc 0.9186
16:49:10.267   Training iter 450, batch loss 1.5359, batch acc 0.9194
16:49:10.765   Training iter 500, batch loss 1.5413, batch acc 0.9198
16:49:11.248   Training iter 550, batch loss 1.5346, batch acc 0.9194
16:49:11.735   Training iter 600, batch loss 1.5349, batch acc 0.9262
16:49:11.737 Training @ 194 epoch...
16:49:12.255   Training iter 50, batch loss 1.5398, batch acc 0.9168
16:49:12.757   Training iter 100, batch loss 1.5379, batch acc 0.9168
16:49:13.220   Training iter 150, batch loss 1.5292, batch acc 0.9296
16:49:13.691   Training iter 200, batch loss 1.5375, batch acc 0.9202
16:49:14.183   Training iter 250, batch loss 1.5333, batch acc 0.9262
16:49:14.667   Training iter 300, batch loss 1.5397, batch acc 0.9178
16:49:15.150   Training iter 350, batch loss 1.5346, batch acc 0.9214
16:49:15.647   Training iter 400, batch loss 1.5374, batch acc 0.9188
16:49:16.136   Training iter 450, batch loss 1.5375, batch acc 0.9198
16:49:16.609   Training iter 500, batch loss 1.5363, batch acc 0.9246
16:49:17.084   Training iter 550, batch loss 1.5360, batch acc 0.9208
16:49:17.554   Training iter 600, batch loss 1.5401, batch acc 0.9170
16:49:17.555 Training @ 195 epoch...
16:49:18.018   Training iter 50, batch loss 1.5408, batch acc 0.9156
16:49:18.488   Training iter 100, batch loss 1.5327, batch acc 0.9268
16:49:18.952   Training iter 150, batch loss 1.5370, batch acc 0.9224
16:49:19.429   Training iter 200, batch loss 1.5372, batch acc 0.9212
16:49:19.904   Training iter 250, batch loss 1.5364, batch acc 0.9188
16:49:20.374   Training iter 300, batch loss 1.5355, batch acc 0.9196
16:49:20.858   Training iter 350, batch loss 1.5347, batch acc 0.9232
16:49:21.357   Training iter 400, batch loss 1.5353, batch acc 0.9216
16:49:21.864   Training iter 450, batch loss 1.5384, batch acc 0.9218
16:49:22.370   Training iter 500, batch loss 1.5353, batch acc 0.9216
16:49:22.877   Training iter 550, batch loss 1.5383, batch acc 0.9170
16:49:23.369   Training iter 600, batch loss 1.5379, batch acc 0.9200
16:49:23.371 Testing @ 195 epoch...
16:49:23.417     Testing, total mean loss 1.54107, total acc 0.91970
16:49:23.417 Training @ 196 epoch...
16:49:23.907   Training iter 50, batch loss 1.5375, batch acc 0.9202
16:49:24.451   Training iter 100, batch loss 1.5347, batch acc 0.9216
16:49:24.960   Training iter 150, batch loss 1.5374, batch acc 0.9212
16:49:25.505   Training iter 200, batch loss 1.5344, batch acc 0.9230
16:49:26.012   Training iter 250, batch loss 1.5344, batch acc 0.9252
16:49:26.562   Training iter 300, batch loss 1.5357, batch acc 0.9212
16:49:27.102   Training iter 350, batch loss 1.5342, batch acc 0.9200
16:49:27.682   Training iter 400, batch loss 1.5356, batch acc 0.9208
16:49:28.289   Training iter 450, batch loss 1.5388, batch acc 0.9184
16:49:28.881   Training iter 500, batch loss 1.5402, batch acc 0.9182
16:49:29.471   Training iter 550, batch loss 1.5359, batch acc 0.9208
16:49:30.064   Training iter 600, batch loss 1.5401, batch acc 0.9160
16:49:30.065 Training @ 197 epoch...
16:49:30.640   Training iter 50, batch loss 1.5383, batch acc 0.9212
16:49:31.219   Training iter 100, batch loss 1.5347, batch acc 0.9208
16:49:31.777   Training iter 150, batch loss 1.5364, batch acc 0.9214
16:49:32.331   Training iter 200, batch loss 1.5377, batch acc 0.9174
16:49:32.890   Training iter 250, batch loss 1.5335, batch acc 0.9242
16:49:33.462   Training iter 300, batch loss 1.5387, batch acc 0.9208
16:49:34.013   Training iter 350, batch loss 1.5370, batch acc 0.9196
16:49:34.528   Training iter 400, batch loss 1.5365, batch acc 0.9220
16:49:35.031   Training iter 450, batch loss 1.5381, batch acc 0.9196
16:49:35.503   Training iter 500, batch loss 1.5335, batch acc 0.9288
16:49:35.984   Training iter 550, batch loss 1.5349, batch acc 0.9216
16:49:36.463   Training iter 600, batch loss 1.5399, batch acc 0.9166
16:49:36.465 Training @ 198 epoch...
16:49:36.962   Training iter 50, batch loss 1.5351, batch acc 0.9228
16:49:37.470   Training iter 100, batch loss 1.5388, batch acc 0.9210
16:49:37.950   Training iter 150, batch loss 1.5365, batch acc 0.9186
16:49:38.455   Training iter 200, batch loss 1.5399, batch acc 0.9146
16:49:38.979   Training iter 250, batch loss 1.5299, batch acc 0.9300
16:49:39.502   Training iter 300, batch loss 1.5368, batch acc 0.9192
16:49:40.021   Training iter 350, batch loss 1.5379, batch acc 0.9202
16:49:40.551   Training iter 400, batch loss 1.5359, batch acc 0.9196
16:49:41.078   Training iter 450, batch loss 1.5331, batch acc 0.9236
16:49:41.579   Training iter 500, batch loss 1.5353, batch acc 0.9260
16:49:42.098   Training iter 550, batch loss 1.5377, batch acc 0.9200
16:49:42.620   Training iter 600, batch loss 1.5414, batch acc 0.9162
16:49:42.621 Training @ 199 epoch...
16:49:43.135   Training iter 50, batch loss 1.5371, batch acc 0.9176
16:49:43.632   Training iter 100, batch loss 1.5347, batch acc 0.9200
16:49:44.148   Training iter 150, batch loss 1.5387, batch acc 0.9178
16:49:44.659   Training iter 200, batch loss 1.5406, batch acc 0.9178
16:49:45.169   Training iter 250, batch loss 1.5339, batch acc 0.9256
16:49:45.684   Training iter 300, batch loss 1.5339, batch acc 0.9260
16:49:46.207   Training iter 350, batch loss 1.5287, batch acc 0.9300
16:49:46.734   Training iter 400, batch loss 1.5360, batch acc 0.9186
16:49:47.250   Training iter 450, batch loss 1.5354, batch acc 0.9194
16:49:47.776   Training iter 500, batch loss 1.5392, batch acc 0.9182
16:49:48.291   Training iter 550, batch loss 1.5362, batch acc 0.9240
16:49:48.815   Training iter 600, batch loss 1.5420, batch acc 0.9172
16:49:48.817 Training @ 200 epoch...
16:49:49.342   Training iter 50, batch loss 1.5393, batch acc 0.9228
16:49:49.861   Training iter 100, batch loss 1.5370, batch acc 0.9192
16:49:50.329   Training iter 150, batch loss 1.5335, batch acc 0.9242
16:49:50.827   Training iter 200, batch loss 1.5301, batch acc 0.9272
16:49:51.322   Training iter 250, batch loss 1.5353, batch acc 0.9210
16:49:51.809   Training iter 300, batch loss 1.5277, batch acc 0.9332
16:49:52.301   Training iter 350, batch loss 1.5412, batch acc 0.9140
16:49:52.786   Training iter 400, batch loss 1.5360, batch acc 0.9222
16:49:53.278   Training iter 450, batch loss 1.5355, batch acc 0.9236
16:49:53.768   Training iter 500, batch loss 1.5405, batch acc 0.9154
16:49:54.256   Training iter 550, batch loss 1.5392, batch acc 0.9162
16:49:54.740   Training iter 600, batch loss 1.5419, batch acc 0.9126
16:49:54.742 Testing @ 200 epoch...
16:49:54.787     Testing, total mean loss 1.54226, total acc 0.91850
16:49:54.787 Plot @ 200 epoch...
16:49:54.787 Training @ 201 epoch...
16:49:55.293   Training iter 50, batch loss 1.5332, batch acc 0.9254
16:49:55.792   Training iter 100, batch loss 1.5387, batch acc 0.9230
16:49:56.297   Training iter 150, batch loss 1.5352, batch acc 0.9208
16:49:56.792   Training iter 200, batch loss 1.5347, batch acc 0.9194
16:49:57.303   Training iter 250, batch loss 1.5368, batch acc 0.9248
16:49:57.805   Training iter 300, batch loss 1.5367, batch acc 0.9220
16:49:58.310   Training iter 350, batch loss 1.5363, batch acc 0.9232
16:49:58.804   Training iter 400, batch loss 1.5339, batch acc 0.9230
16:49:59.315   Training iter 450, batch loss 1.5348, batch acc 0.9176
16:49:59.818   Training iter 500, batch loss 1.5385, batch acc 0.9166
16:50:00.344   Training iter 550, batch loss 1.5377, batch acc 0.9190
16:50:00.875   Training iter 600, batch loss 1.5404, batch acc 0.9174
16:50:00.876 Training @ 202 epoch...
16:50:01.420   Training iter 50, batch loss 1.5379, batch acc 0.9236
16:50:02.011   Training iter 100, batch loss 1.5368, batch acc 0.9210
16:50:02.572   Training iter 150, batch loss 1.5345, batch acc 0.9244
16:50:03.135   Training iter 200, batch loss 1.5386, batch acc 0.9188
16:50:03.695   Training iter 250, batch loss 1.5381, batch acc 0.9204
16:50:04.263   Training iter 300, batch loss 1.5403, batch acc 0.9140
16:50:04.823   Training iter 350, batch loss 1.5351, batch acc 0.9200
16:50:05.365   Training iter 400, batch loss 1.5401, batch acc 0.9194
16:50:05.911   Training iter 450, batch loss 1.5323, batch acc 0.9266
16:50:06.461   Training iter 500, batch loss 1.5358, batch acc 0.9174
16:50:07.011   Training iter 550, batch loss 1.5353, batch acc 0.9248
16:50:07.544   Training iter 600, batch loss 1.5321, batch acc 0.9256
16:50:07.546 Training @ 203 epoch...
16:50:08.083   Training iter 50, batch loss 1.5318, batch acc 0.9266
16:50:08.645   Training iter 100, batch loss 1.5347, batch acc 0.9246
16:50:09.209   Training iter 150, batch loss 1.5356, batch acc 0.9240
16:50:09.757   Training iter 200, batch loss 1.5350, batch acc 0.9276
16:50:10.311   Training iter 250, batch loss 1.5353, batch acc 0.9206
16:50:10.864   Training iter 300, batch loss 1.5389, batch acc 0.9166
16:50:11.410   Training iter 350, batch loss 1.5401, batch acc 0.9196
16:50:11.950   Training iter 400, batch loss 1.5383, batch acc 0.9192
16:50:12.482   Training iter 450, batch loss 1.5306, batch acc 0.9288
16:50:12.998   Training iter 500, batch loss 1.5376, batch acc 0.9170
16:50:13.510   Training iter 550, batch loss 1.5372, batch acc 0.9180
16:50:14.049   Training iter 600, batch loss 1.5406, batch acc 0.9082
16:50:14.051 Training @ 204 epoch...
16:50:14.562   Training iter 50, batch loss 1.5330, batch acc 0.9276
16:50:15.050   Training iter 100, batch loss 1.5363, batch acc 0.9206
16:50:15.551   Training iter 150, batch loss 1.5361, batch acc 0.9202
16:50:16.050   Training iter 200, batch loss 1.5352, batch acc 0.9196
16:50:16.551   Training iter 250, batch loss 1.5377, batch acc 0.9228
16:50:17.070   Training iter 300, batch loss 1.5325, batch acc 0.9224
16:50:17.570   Training iter 350, batch loss 1.5357, batch acc 0.9212
16:50:18.077   Training iter 400, batch loss 1.5351, batch acc 0.9232
16:50:18.576   Training iter 450, batch loss 1.5365, batch acc 0.9234
16:50:19.074   Training iter 500, batch loss 1.5398, batch acc 0.9200
16:50:19.579   Training iter 550, batch loss 1.5381, batch acc 0.9196
16:50:20.072   Training iter 600, batch loss 1.5394, batch acc 0.9146
16:50:20.073 Training @ 205 epoch...
16:50:20.599   Training iter 50, batch loss 1.5376, batch acc 0.9192
16:50:21.118   Training iter 100, batch loss 1.5356, batch acc 0.9184
16:50:21.642   Training iter 150, batch loss 1.5368, batch acc 0.9228
16:50:22.154   Training iter 200, batch loss 1.5352, batch acc 0.9228
16:50:22.663   Training iter 250, batch loss 1.5360, batch acc 0.9216
16:50:23.169   Training iter 300, batch loss 1.5364, batch acc 0.9212
16:50:23.678   Training iter 350, batch loss 1.5361, batch acc 0.9236
16:50:24.190   Training iter 400, batch loss 1.5333, batch acc 0.9224
16:50:24.688   Training iter 450, batch loss 1.5358, batch acc 0.9208
16:50:25.191   Training iter 500, batch loss 1.5374, batch acc 0.9170
16:50:25.692   Training iter 550, batch loss 1.5372, batch acc 0.9228
16:50:26.197   Training iter 600, batch loss 1.5389, batch acc 0.9156
16:50:26.199 Testing @ 205 epoch...
16:50:26.244     Testing, total mean loss 1.54085, total acc 0.92000
16:50:26.244 Training @ 206 epoch...
16:50:26.752   Training iter 50, batch loss 1.5359, batch acc 0.9240
16:50:27.268   Training iter 100, batch loss 1.5314, batch acc 0.9248
16:50:27.771   Training iter 150, batch loss 1.5355, batch acc 0.9206
16:50:28.281   Training iter 200, batch loss 1.5331, batch acc 0.9254
16:50:28.785   Training iter 250, batch loss 1.5377, batch acc 0.9186
16:50:29.347   Training iter 300, batch loss 1.5356, batch acc 0.9222
16:50:29.932   Training iter 350, batch loss 1.5378, batch acc 0.9212
16:50:30.505   Training iter 400, batch loss 1.5390, batch acc 0.9176
16:50:31.076   Training iter 450, batch loss 1.5407, batch acc 0.9160
16:50:31.601   Training iter 500, batch loss 1.5338, batch acc 0.9260
16:50:32.145   Training iter 550, batch loss 1.5373, batch acc 0.9216
16:50:32.691   Training iter 600, batch loss 1.5367, batch acc 0.9196
16:50:32.693 Training @ 207 epoch...
16:50:33.249   Training iter 50, batch loss 1.5325, batch acc 0.9236
16:50:33.793   Training iter 100, batch loss 1.5329, batch acc 0.9234
16:50:34.338   Training iter 150, batch loss 1.5405, batch acc 0.9184
16:50:34.875   Training iter 200, batch loss 1.5390, batch acc 0.9192
16:50:35.420   Training iter 250, batch loss 1.5355, batch acc 0.9222
16:50:35.962   Training iter 300, batch loss 1.5339, batch acc 0.9260
16:50:36.507   Training iter 350, batch loss 1.5338, batch acc 0.9218
16:50:37.057   Training iter 400, batch loss 1.5365, batch acc 0.9186
16:50:37.553   Training iter 450, batch loss 1.5371, batch acc 0.9256
16:50:38.047   Training iter 500, batch loss 1.5402, batch acc 0.9158
16:50:38.571   Training iter 550, batch loss 1.5349, batch acc 0.9188
16:50:39.110   Training iter 600, batch loss 1.5383, batch acc 0.9232
16:50:39.112 Training @ 208 epoch...
16:50:39.647   Training iter 50, batch loss 1.5325, batch acc 0.9266
16:50:40.181   Training iter 100, batch loss 1.5356, batch acc 0.9218
16:50:40.714   Training iter 150, batch loss 1.5352, batch acc 0.9226
16:50:41.239   Training iter 200, batch loss 1.5356, batch acc 0.9176
16:50:41.783   Training iter 250, batch loss 1.5356, batch acc 0.9230
16:50:42.332   Training iter 300, batch loss 1.5401, batch acc 0.9194
16:50:42.871   Training iter 350, batch loss 1.5355, batch acc 0.9190
16:50:43.394   Training iter 400, batch loss 1.5363, batch acc 0.9154
16:50:43.923   Training iter 450, batch loss 1.5373, batch acc 0.9218
16:50:44.458   Training iter 500, batch loss 1.5394, batch acc 0.9184
16:50:44.984   Training iter 550, batch loss 1.5367, batch acc 0.9266
16:50:45.507   Training iter 600, batch loss 1.5353, batch acc 0.9216
16:50:45.509 Training @ 209 epoch...
16:50:46.037   Training iter 50, batch loss 1.5368, batch acc 0.9248
16:50:46.562   Training iter 100, batch loss 1.5349, batch acc 0.9248
16:50:47.103   Training iter 150, batch loss 1.5380, batch acc 0.9192
16:50:47.635   Training iter 200, batch loss 1.5356, batch acc 0.9212
16:50:48.163   Training iter 250, batch loss 1.5337, batch acc 0.9208
16:50:48.711   Training iter 300, batch loss 1.5403, batch acc 0.9136
16:50:49.263   Training iter 350, batch loss 1.5349, batch acc 0.9272
16:50:49.834   Training iter 400, batch loss 1.5343, batch acc 0.9226
16:50:50.411   Training iter 450, batch loss 1.5402, batch acc 0.9188
16:50:50.965   Training iter 500, batch loss 1.5340, batch acc 0.9244
16:50:51.523   Training iter 550, batch loss 1.5375, batch acc 0.9206
16:50:52.087   Training iter 600, batch loss 1.5336, batch acc 0.9226
16:50:52.089 Training @ 210 epoch...
16:50:52.653   Training iter 50, batch loss 1.5383, batch acc 0.9202
16:50:53.211   Training iter 100, batch loss 1.5363, batch acc 0.9232
16:50:53.774   Training iter 150, batch loss 1.5338, batch acc 0.9238
16:50:54.348   Training iter 200, batch loss 1.5329, batch acc 0.9194
16:50:54.891   Training iter 250, batch loss 1.5334, batch acc 0.9240
16:50:55.417   Training iter 300, batch loss 1.5410, batch acc 0.9140
16:50:55.930   Training iter 350, batch loss 1.5360, batch acc 0.9222
16:50:56.457   Training iter 400, batch loss 1.5363, batch acc 0.9208
16:50:56.992   Training iter 450, batch loss 1.5391, batch acc 0.9194
16:50:57.511   Training iter 500, batch loss 1.5379, batch acc 0.9210
16:50:58.014   Training iter 550, batch loss 1.5369, batch acc 0.9196
16:50:58.531   Training iter 600, batch loss 1.5323, batch acc 0.9238
16:50:58.532 Testing @ 210 epoch...
16:50:58.579     Testing, total mean loss 1.54118, total acc 0.92010
16:50:58.579 Training @ 211 epoch...
16:50:59.122   Training iter 50, batch loss 1.5346, batch acc 0.9214
16:50:59.650   Training iter 100, batch loss 1.5327, batch acc 0.9236
16:51:00.181   Training iter 150, batch loss 1.5388, batch acc 0.9212
16:51:00.708   Training iter 200, batch loss 1.5360, batch acc 0.9224
16:51:01.270   Training iter 250, batch loss 1.5345, batch acc 0.9278
16:51:01.879   Training iter 300, batch loss 1.5402, batch acc 0.9220
16:51:02.421   Training iter 350, batch loss 1.5377, batch acc 0.9166
16:51:02.997   Training iter 400, batch loss 1.5327, batch acc 0.9234
16:51:03.606   Training iter 450, batch loss 1.5336, batch acc 0.9236
16:51:04.197   Training iter 500, batch loss 1.5340, batch acc 0.9244
16:51:04.772   Training iter 550, batch loss 1.5408, batch acc 0.9120
16:51:05.356   Training iter 600, batch loss 1.5367, batch acc 0.9166
16:51:05.357 Training @ 212 epoch...
16:51:05.935   Training iter 50, batch loss 1.5377, batch acc 0.9198
16:51:06.539   Training iter 100, batch loss 1.5366, batch acc 0.9250
16:51:07.128   Training iter 150, batch loss 1.5336, batch acc 0.9264
16:51:07.675   Training iter 200, batch loss 1.5346, batch acc 0.9244
16:51:08.199   Training iter 250, batch loss 1.5335, batch acc 0.9246
16:51:08.729   Training iter 300, batch loss 1.5376, batch acc 0.9172
16:51:09.251   Training iter 350, batch loss 1.5330, batch acc 0.9226
16:51:09.796   Training iter 400, batch loss 1.5383, batch acc 0.9188
16:51:10.318   Training iter 450, batch loss 1.5422, batch acc 0.9118
16:51:10.834   Training iter 500, batch loss 1.5339, batch acc 0.9242
16:51:11.339   Training iter 550, batch loss 1.5373, batch acc 0.9208
16:51:11.842   Training iter 600, batch loss 1.5354, batch acc 0.9212
16:51:11.844 Training @ 213 epoch...
16:51:12.386   Training iter 50, batch loss 1.5386, batch acc 0.9178
16:51:12.914   Training iter 100, batch loss 1.5392, batch acc 0.9142
16:51:13.446   Training iter 150, batch loss 1.5329, batch acc 0.9238
16:51:13.978   Training iter 200, batch loss 1.5394, batch acc 0.9172
16:51:14.525   Training iter 250, batch loss 1.5350, batch acc 0.9210
16:51:15.058   Training iter 300, batch loss 1.5363, batch acc 0.9190
16:51:15.592   Training iter 350, batch loss 1.5371, batch acc 0.9276
16:51:16.121   Training iter 400, batch loss 1.5348, batch acc 0.9210
16:51:16.634   Training iter 450, batch loss 1.5314, batch acc 0.9266
16:51:17.142   Training iter 500, batch loss 1.5327, batch acc 0.9250
16:51:17.648   Training iter 550, batch loss 1.5406, batch acc 0.9196
16:51:18.137   Training iter 600, batch loss 1.5353, batch acc 0.9216
16:51:18.139 Training @ 214 epoch...
16:51:18.644   Training iter 50, batch loss 1.5392, batch acc 0.9162
16:51:19.156   Training iter 100, batch loss 1.5320, batch acc 0.9244
16:51:19.675   Training iter 150, batch loss 1.5393, batch acc 0.9214
16:51:20.192   Training iter 200, batch loss 1.5314, batch acc 0.9272
16:51:20.671   Training iter 250, batch loss 1.5325, batch acc 0.9258
16:51:21.140   Training iter 300, batch loss 1.5383, batch acc 0.9152
16:51:21.619   Training iter 350, batch loss 1.5379, batch acc 0.9196
16:51:22.100   Training iter 400, batch loss 1.5330, batch acc 0.9244
16:51:22.586   Training iter 450, batch loss 1.5376, batch acc 0.9226
16:51:23.076   Training iter 500, batch loss 1.5426, batch acc 0.9124
16:51:23.573   Training iter 550, batch loss 1.5363, batch acc 0.9198
16:51:24.067   Training iter 600, batch loss 1.5334, batch acc 0.9240
16:51:24.069 Training @ 215 epoch...
16:51:24.564   Training iter 50, batch loss 1.5401, batch acc 0.9174
16:51:25.108   Training iter 100, batch loss 1.5353, batch acc 0.9228
16:51:25.681   Training iter 150, batch loss 1.5384, batch acc 0.9164
16:51:26.260   Training iter 200, batch loss 1.5360, batch acc 0.9210
16:51:26.819   Training iter 250, batch loss 1.5380, batch acc 0.9200
16:51:27.347   Training iter 300, batch loss 1.5379, batch acc 0.9198
16:51:27.879   Training iter 350, batch loss 1.5305, batch acc 0.9238
16:51:28.406   Training iter 400, batch loss 1.5339, batch acc 0.9238
16:51:28.922   Training iter 450, batch loss 1.5347, batch acc 0.9224
16:51:29.453   Training iter 500, batch loss 1.5311, batch acc 0.9270
16:51:29.983   Training iter 550, batch loss 1.5382, batch acc 0.9192
16:51:30.504   Training iter 600, batch loss 1.5372, batch acc 0.9212
16:51:30.505 Testing @ 215 epoch...
16:51:30.551     Testing, total mean loss 1.54141, total acc 0.91950
16:51:30.551 Training @ 216 epoch...
16:51:31.087   Training iter 50, batch loss 1.5355, batch acc 0.9186
16:51:31.644   Training iter 100, batch loss 1.5384, batch acc 0.9218
16:51:32.163   Training iter 150, batch loss 1.5368, batch acc 0.9182
16:51:32.674   Training iter 200, batch loss 1.5358, batch acc 0.9238
16:51:33.185   Training iter 250, batch loss 1.5293, batch acc 0.9310
16:51:33.650   Training iter 300, batch loss 1.5306, batch acc 0.9298
16:51:34.150   Training iter 350, batch loss 1.5408, batch acc 0.9182
16:51:34.679   Training iter 400, batch loss 1.5319, batch acc 0.9270
16:51:35.235   Training iter 450, batch loss 1.5352, batch acc 0.9270
16:51:35.810   Training iter 500, batch loss 1.5378, batch acc 0.9182
16:51:36.364   Training iter 550, batch loss 1.5384, batch acc 0.9172
16:51:36.947   Training iter 600, batch loss 1.5413, batch acc 0.9128
16:51:36.948 Training @ 217 epoch...
16:51:37.535   Training iter 50, batch loss 1.5411, batch acc 0.9168
16:51:38.099   Training iter 100, batch loss 1.5328, batch acc 0.9246
16:51:38.678   Training iter 150, batch loss 1.5337, batch acc 0.9264
16:51:39.236   Training iter 200, batch loss 1.5348, batch acc 0.9238
16:51:39.759   Training iter 250, batch loss 1.5344, batch acc 0.9222
16:51:40.275   Training iter 300, batch loss 1.5326, batch acc 0.9234
16:51:40.759   Training iter 350, batch loss 1.5353, batch acc 0.9210
16:51:41.255   Training iter 400, batch loss 1.5366, batch acc 0.9228
16:51:41.743   Training iter 450, batch loss 1.5420, batch acc 0.9118
16:51:42.238   Training iter 500, batch loss 1.5382, batch acc 0.9226
16:51:42.741   Training iter 550, batch loss 1.5331, batch acc 0.9214
16:51:43.254   Training iter 600, batch loss 1.5375, batch acc 0.9210
16:51:43.256 Training @ 218 epoch...
16:51:43.773   Training iter 50, batch loss 1.5357, batch acc 0.9158
16:51:44.285   Training iter 100, batch loss 1.5331, batch acc 0.9240
16:51:44.788   Training iter 150, batch loss 1.5342, batch acc 0.9224
16:51:45.278   Training iter 200, batch loss 1.5371, batch acc 0.9214
16:51:45.757   Training iter 250, batch loss 1.5381, batch acc 0.9216
16:51:46.246   Training iter 300, batch loss 1.5364, batch acc 0.9210
16:51:46.727   Training iter 350, batch loss 1.5387, batch acc 0.9198
16:51:47.225   Training iter 400, batch loss 1.5353, batch acc 0.9226
16:51:47.733   Training iter 450, batch loss 1.5381, batch acc 0.9200
16:51:48.249   Training iter 500, batch loss 1.5342, batch acc 0.9214
16:51:48.744   Training iter 550, batch loss 1.5391, batch acc 0.9210
16:51:49.223   Training iter 600, batch loss 1.5302, batch acc 0.9280
16:51:49.225 Training @ 219 epoch...
16:51:49.764   Training iter 50, batch loss 1.5342, batch acc 0.9232
16:51:50.282   Training iter 100, batch loss 1.5369, batch acc 0.9202
16:51:50.816   Training iter 150, batch loss 1.5368, batch acc 0.9222
16:51:51.357   Training iter 200, batch loss 1.5359, batch acc 0.9212
16:51:51.901   Training iter 250, batch loss 1.5390, batch acc 0.9172
16:51:52.455   Training iter 300, batch loss 1.5387, batch acc 0.9220
16:51:53.012   Training iter 350, batch loss 1.5375, batch acc 0.9138
16:51:53.569   Training iter 400, batch loss 1.5315, batch acc 0.9234
16:51:54.134   Training iter 450, batch loss 1.5376, batch acc 0.9184
16:51:54.698   Training iter 500, batch loss 1.5363, batch acc 0.9218
16:51:55.265   Training iter 550, batch loss 1.5336, batch acc 0.9262
16:51:55.823   Training iter 600, batch loss 1.5350, batch acc 0.9224
16:51:55.824 Training @ 220 epoch...
16:51:56.385   Training iter 50, batch loss 1.5375, batch acc 0.9204
16:51:56.919   Training iter 100, batch loss 1.5321, batch acc 0.9258
16:51:57.461   Training iter 150, batch loss 1.5361, batch acc 0.9192
16:51:58.059   Training iter 200, batch loss 1.5337, batch acc 0.9216
16:51:58.766   Training iter 250, batch loss 1.5316, batch acc 0.9318
16:51:59.532   Training iter 300, batch loss 1.5405, batch acc 0.9184
16:52:00.260   Training iter 350, batch loss 1.5376, batch acc 0.9198
16:52:00.786   Training iter 400, batch loss 1.5380, batch acc 0.9228
16:52:01.317   Training iter 450, batch loss 1.5380, batch acc 0.9170
16:52:01.887   Training iter 500, batch loss 1.5357, batch acc 0.9242
16:52:02.444   Training iter 550, batch loss 1.5356, batch acc 0.9208
16:52:02.988   Training iter 600, batch loss 1.5340, batch acc 0.9226
16:52:02.990 Testing @ 220 epoch...
16:52:03.037     Testing, total mean loss 1.54174, total acc 0.91980
16:52:03.037 Training @ 221 epoch...
16:52:03.584   Training iter 50, batch loss 1.5361, batch acc 0.9242
16:52:04.132   Training iter 100, batch loss 1.5326, batch acc 0.9220
16:52:04.663   Training iter 150, batch loss 1.5361, batch acc 0.9186
16:52:05.193   Training iter 200, batch loss 1.5358, batch acc 0.9176
16:52:05.734   Training iter 250, batch loss 1.5396, batch acc 0.9220
16:52:06.285   Training iter 300, batch loss 1.5350, batch acc 0.9200
16:52:06.814   Training iter 350, batch loss 1.5382, batch acc 0.9170
16:52:07.365   Training iter 400, batch loss 1.5370, batch acc 0.9194
16:52:07.898   Training iter 450, batch loss 1.5299, batch acc 0.9302
16:52:08.415   Training iter 500, batch loss 1.5408, batch acc 0.9202
16:52:08.919   Training iter 550, batch loss 1.5333, batch acc 0.9208
16:52:09.448   Training iter 600, batch loss 1.5362, batch acc 0.9222
16:52:09.449 Training @ 222 epoch...
16:52:09.994   Training iter 50, batch loss 1.5342, batch acc 0.9220
16:52:10.554   Training iter 100, batch loss 1.5395, batch acc 0.9204
16:52:11.108   Training iter 150, batch loss 1.5353, batch acc 0.9240
16:52:11.663   Training iter 200, batch loss 1.5347, batch acc 0.9232
16:52:12.224   Training iter 250, batch loss 1.5379, batch acc 0.9184
16:52:12.787   Training iter 300, batch loss 1.5370, batch acc 0.9216
16:52:13.330   Training iter 350, batch loss 1.5358, batch acc 0.9236
16:52:13.853   Training iter 400, batch loss 1.5332, batch acc 0.9262
16:52:14.380   Training iter 450, batch loss 1.5335, batch acc 0.9216
16:52:14.898   Training iter 500, batch loss 1.5333, batch acc 0.9232
16:52:15.432   Training iter 550, batch loss 1.5387, batch acc 0.9160
16:52:15.966   Training iter 600, batch loss 1.5380, batch acc 0.9232
16:52:15.967 Training @ 223 epoch...
16:52:16.500   Training iter 50, batch loss 1.5323, batch acc 0.9288
16:52:17.032   Training iter 100, batch loss 1.5366, batch acc 0.9234
16:52:17.562   Training iter 150, batch loss 1.5373, batch acc 0.9214
16:52:18.086   Training iter 200, batch loss 1.5411, batch acc 0.9098
16:52:18.605   Training iter 250, batch loss 1.5339, batch acc 0.9220
16:52:19.135   Training iter 300, batch loss 1.5347, batch acc 0.9200
16:52:19.671   Training iter 350, batch loss 1.5407, batch acc 0.9222
16:52:20.194   Training iter 400, batch loss 1.5338, batch acc 0.9200
16:52:20.723   Training iter 450, batch loss 1.5377, batch acc 0.9200
16:52:21.242   Training iter 500, batch loss 1.5320, batch acc 0.9246
16:52:21.742   Training iter 550, batch loss 1.5356, batch acc 0.9184
16:52:22.252   Training iter 600, batch loss 1.5350, batch acc 0.9266
16:52:22.254 Training @ 224 epoch...
16:52:22.776   Training iter 50, batch loss 1.5320, batch acc 0.9240
16:52:23.322   Training iter 100, batch loss 1.5349, batch acc 0.9190
16:52:23.852   Training iter 150, batch loss 1.5348, batch acc 0.9192
16:52:24.422   Training iter 200, batch loss 1.5335, batch acc 0.9248
16:52:25.002   Training iter 250, batch loss 1.5378, batch acc 0.9204
16:52:25.575   Training iter 300, batch loss 1.5337, batch acc 0.9210
16:52:26.127   Training iter 350, batch loss 1.5398, batch acc 0.9168
16:52:26.667   Training iter 400, batch loss 1.5370, batch acc 0.9226
16:52:27.219   Training iter 450, batch loss 1.5384, batch acc 0.9192
16:52:27.775   Training iter 500, batch loss 1.5355, batch acc 0.9232
16:52:28.321   Training iter 550, batch loss 1.5382, batch acc 0.9230
16:52:28.849   Training iter 600, batch loss 1.5344, batch acc 0.9230
16:52:28.850 Training @ 225 epoch...
16:52:29.385   Training iter 50, batch loss 1.5380, batch acc 0.9208
16:52:29.902   Training iter 100, batch loss 1.5344, batch acc 0.9230
16:52:30.414   Training iter 150, batch loss 1.5339, batch acc 0.9228
16:52:30.915   Training iter 200, batch loss 1.5324, batch acc 0.9242
16:52:31.424   Training iter 250, batch loss 1.5364, batch acc 0.9218
16:52:31.927   Training iter 300, batch loss 1.5399, batch acc 0.9170
16:52:32.395   Training iter 350, batch loss 1.5367, batch acc 0.9174
16:52:32.858   Training iter 400, batch loss 1.5340, batch acc 0.9234
16:52:33.327   Training iter 450, batch loss 1.5359, batch acc 0.9224
16:52:33.822   Training iter 500, batch loss 1.5320, batch acc 0.9256
16:52:34.337   Training iter 550, batch loss 1.5397, batch acc 0.9200
16:52:34.860   Training iter 600, batch loss 1.5366, batch acc 0.9214
16:52:34.861 Testing @ 225 epoch...
16:52:34.907     Testing, total mean loss 1.54100, total acc 0.91880
16:52:34.907 Training @ 226 epoch...
16:52:35.426   Training iter 50, batch loss 1.5329, batch acc 0.9242
16:52:35.930   Training iter 100, batch loss 1.5374, batch acc 0.9198
16:52:36.429   Training iter 150, batch loss 1.5417, batch acc 0.9182
16:52:36.936   Training iter 200, batch loss 1.5346, batch acc 0.9216
16:52:37.451   Training iter 250, batch loss 1.5373, batch acc 0.9196
16:52:37.964   Training iter 300, batch loss 1.5365, batch acc 0.9200
16:52:38.490   Training iter 350, batch loss 1.5292, batch acc 0.9282
16:52:39.018   Training iter 400, batch loss 1.5372, batch acc 0.9204
16:52:39.552   Training iter 450, batch loss 1.5297, batch acc 0.9282
16:52:40.101   Training iter 500, batch loss 1.5344, batch acc 0.9200
16:52:40.631   Training iter 550, batch loss 1.5407, batch acc 0.9196
16:52:41.170   Training iter 600, batch loss 1.5366, batch acc 0.9184
16:52:41.172 Training @ 227 epoch...
16:52:41.715   Training iter 50, batch loss 1.5327, batch acc 0.9226
16:52:42.267   Training iter 100, batch loss 1.5297, batch acc 0.9300
16:52:42.812   Training iter 150, batch loss 1.5432, batch acc 0.9132
16:52:43.356   Training iter 200, batch loss 1.5334, batch acc 0.9260
16:52:43.899   Training iter 250, batch loss 1.5333, batch acc 0.9192
16:52:44.423   Training iter 300, batch loss 1.5366, batch acc 0.9228
16:52:44.942   Training iter 350, batch loss 1.5415, batch acc 0.9134
16:52:45.459   Training iter 400, batch loss 1.5325, batch acc 0.9270
16:52:45.992   Training iter 450, batch loss 1.5338, batch acc 0.9268
16:52:46.535   Training iter 500, batch loss 1.5322, batch acc 0.9288
16:52:47.069   Training iter 550, batch loss 1.5354, batch acc 0.9214
16:52:47.593   Training iter 600, batch loss 1.5448, batch acc 0.9110
16:52:47.594 Training @ 228 epoch...
16:52:48.160   Training iter 50, batch loss 1.5310, batch acc 0.9286
16:52:48.682   Training iter 100, batch loss 1.5351, batch acc 0.9212
16:52:49.214   Training iter 150, batch loss 1.5357, batch acc 0.9198
16:52:49.738   Training iter 200, batch loss 1.5373, batch acc 0.9176
16:52:50.270   Training iter 250, batch loss 1.5385, batch acc 0.9164
16:52:50.802   Training iter 300, batch loss 1.5327, batch acc 0.9246
16:52:51.346   Training iter 350, batch loss 1.5348, batch acc 0.9234
16:52:51.885   Training iter 400, batch loss 1.5372, batch acc 0.9244
16:52:52.430   Training iter 450, batch loss 1.5351, batch acc 0.9262
16:52:52.977   Training iter 500, batch loss 1.5360, batch acc 0.9200
16:52:53.507   Training iter 550, batch loss 1.5387, batch acc 0.9168
16:52:54.046   Training iter 600, batch loss 1.5377, batch acc 0.9222
16:52:54.048 Training @ 229 epoch...
16:52:54.597   Training iter 50, batch loss 1.5366, batch acc 0.9216
16:52:55.195   Training iter 100, batch loss 1.5358, batch acc 0.9216
16:52:55.756   Training iter 150, batch loss 1.5370, batch acc 0.9174
16:52:56.324   Training iter 200, batch loss 1.5338, batch acc 0.9228
16:52:56.888   Training iter 250, batch loss 1.5412, batch acc 0.9136
16:52:57.447   Training iter 300, batch loss 1.5362, batch acc 0.9232
16:52:57.997   Training iter 350, batch loss 1.5376, batch acc 0.9194
16:52:58.537   Training iter 400, batch loss 1.5358, batch acc 0.9280
16:52:59.070   Training iter 450, batch loss 1.5284, batch acc 0.9314
16:52:59.602   Training iter 500, batch loss 1.5330, batch acc 0.9204
16:53:00.147   Training iter 550, batch loss 1.5385, batch acc 0.9176
16:53:00.668   Training iter 600, batch loss 1.5342, batch acc 0.9248
16:53:00.670 Training @ 230 epoch...
16:53:01.184   Training iter 50, batch loss 1.5369, batch acc 0.9208
16:53:01.709   Training iter 100, batch loss 1.5348, batch acc 0.9238
16:53:02.272   Training iter 150, batch loss 1.5357, batch acc 0.9202
16:53:02.837   Training iter 200, batch loss 1.5357, batch acc 0.9222
16:53:03.372   Training iter 250, batch loss 1.5388, batch acc 0.9170
16:53:03.897   Training iter 300, batch loss 1.5350, batch acc 0.9184
16:53:04.437   Training iter 350, batch loss 1.5340, batch acc 0.9242
16:53:04.980   Training iter 400, batch loss 1.5341, batch acc 0.9244
16:53:05.520   Training iter 450, batch loss 1.5327, batch acc 0.9220
16:53:06.049   Training iter 500, batch loss 1.5355, batch acc 0.9254
16:53:06.569   Training iter 550, batch loss 1.5387, batch acc 0.9218
16:53:07.079   Training iter 600, batch loss 1.5376, batch acc 0.9192
16:53:07.081 Testing @ 230 epoch...
16:53:07.127     Testing, total mean loss 1.54032, total acc 0.92000
16:53:07.127 Training @ 231 epoch...
16:53:07.649   Training iter 50, batch loss 1.5335, batch acc 0.9240
16:53:08.182   Training iter 100, batch loss 1.5369, batch acc 0.9188
16:53:08.685   Training iter 150, batch loss 1.5371, batch acc 0.9214
16:53:09.207   Training iter 200, batch loss 1.5345, batch acc 0.9214
16:53:09.733   Training iter 250, batch loss 1.5371, batch acc 0.9200
16:53:10.269   Training iter 300, batch loss 1.5358, batch acc 0.9238
16:53:10.771   Training iter 350, batch loss 1.5326, batch acc 0.9244
16:53:11.266   Training iter 400, batch loss 1.5330, batch acc 0.9276
16:53:11.743   Training iter 450, batch loss 1.5378, batch acc 0.9174
16:53:12.214   Training iter 500, batch loss 1.5376, batch acc 0.9210
16:53:12.686   Training iter 550, batch loss 1.5360, batch acc 0.9218
16:53:13.178   Training iter 600, batch loss 1.5359, batch acc 0.9218
16:53:13.179 Training @ 232 epoch...
16:53:13.685   Training iter 50, batch loss 1.5301, batch acc 0.9288
16:53:14.210   Training iter 100, batch loss 1.5343, batch acc 0.9236
16:53:14.738   Training iter 150, batch loss 1.5369, batch acc 0.9228
16:53:15.266   Training iter 200, batch loss 1.5334, batch acc 0.9238
16:53:15.798   Training iter 250, batch loss 1.5395, batch acc 0.9196
16:53:16.345   Training iter 300, batch loss 1.5399, batch acc 0.9128
16:53:16.882   Training iter 350, batch loss 1.5344, batch acc 0.9274
16:53:17.417   Training iter 400, batch loss 1.5353, batch acc 0.9202
16:53:17.955   Training iter 450, batch loss 1.5344, batch acc 0.9208
16:53:18.521   Training iter 500, batch loss 1.5404, batch acc 0.9130
16:53:19.057   Training iter 550, batch loss 1.5356, batch acc 0.9210
16:53:19.590   Training iter 600, batch loss 1.5335, batch acc 0.9256
16:53:19.592 Training @ 233 epoch...
16:53:20.121   Training iter 50, batch loss 1.5328, batch acc 0.9272
16:53:20.629   Training iter 100, batch loss 1.5355, batch acc 0.9176
16:53:21.143   Training iter 150, batch loss 1.5394, batch acc 0.9158
16:53:21.658   Training iter 200, batch loss 1.5370, batch acc 0.9230
16:53:22.163   Training iter 250, batch loss 1.5377, batch acc 0.9206
16:53:22.666   Training iter 300, batch loss 1.5342, batch acc 0.9226
16:53:23.182   Training iter 350, batch loss 1.5333, batch acc 0.9230
16:53:23.662   Training iter 400, batch loss 1.5353, batch acc 0.9202
16:53:24.141   Training iter 450, batch loss 1.5366, batch acc 0.9208
16:53:24.615   Training iter 500, batch loss 1.5348, batch acc 0.9256
16:53:25.090   Training iter 550, batch loss 1.5323, batch acc 0.9266
16:53:25.586   Training iter 600, batch loss 1.5392, batch acc 0.9166
16:53:25.587 Training @ 234 epoch...
16:53:26.089   Training iter 50, batch loss 1.5346, batch acc 0.9216
16:53:26.618   Training iter 100, batch loss 1.5392, batch acc 0.9172
16:53:27.168   Training iter 150, batch loss 1.5369, batch acc 0.9212
16:53:27.704   Training iter 200, batch loss 1.5355, batch acc 0.9204
16:53:28.241   Training iter 250, batch loss 1.5301, batch acc 0.9276
16:53:28.764   Training iter 300, batch loss 1.5334, batch acc 0.9268
16:53:29.322   Training iter 350, batch loss 1.5382, batch acc 0.9188
16:53:29.876   Training iter 400, batch loss 1.5367, batch acc 0.9276
16:53:30.602   Training iter 450, batch loss 1.5334, batch acc 0.9216
16:53:31.380   Training iter 500, batch loss 1.5372, batch acc 0.9178
16:53:31.999   Training iter 550, batch loss 1.5355, batch acc 0.9220
16:53:32.541   Training iter 600, batch loss 1.5361, batch acc 0.9202
16:53:32.543 Training @ 235 epoch...
16:53:33.088   Training iter 50, batch loss 1.5325, batch acc 0.9238
16:53:33.613   Training iter 100, batch loss 1.5354, batch acc 0.9224
16:53:34.132   Training iter 150, batch loss 1.5395, batch acc 0.9174
16:53:34.656   Training iter 200, batch loss 1.5356, batch acc 0.9250
16:53:35.184   Training iter 250, batch loss 1.5350, batch acc 0.9236
16:53:35.710   Training iter 300, batch loss 1.5358, batch acc 0.9204
16:53:36.231   Training iter 350, batch loss 1.5312, batch acc 0.9284
16:53:36.750   Training iter 400, batch loss 1.5358, batch acc 0.9248
16:53:37.291   Training iter 450, batch loss 1.5397, batch acc 0.9168
16:53:37.834   Training iter 500, batch loss 1.5382, batch acc 0.9182
16:53:38.373   Training iter 550, batch loss 1.5353, batch acc 0.9214
16:53:38.912   Training iter 600, batch loss 1.5330, batch acc 0.9216
16:53:38.913 Testing @ 235 epoch...
16:53:38.959     Testing, total mean loss 1.54139, total acc 0.91890
16:53:38.959 Training @ 236 epoch...
16:53:39.470   Training iter 50, batch loss 1.5375, batch acc 0.9186
16:53:39.969   Training iter 100, batch loss 1.5384, batch acc 0.9194
16:53:40.488   Training iter 150, batch loss 1.5377, batch acc 0.9178
16:53:40.984   Training iter 200, batch loss 1.5347, batch acc 0.9210
16:53:41.469   Training iter 250, batch loss 1.5351, batch acc 0.9204
16:53:41.962   Training iter 300, batch loss 1.5357, batch acc 0.9224
16:53:42.447   Training iter 350, batch loss 1.5369, batch acc 0.9214
16:53:42.930   Training iter 400, batch loss 1.5309, batch acc 0.9276
16:53:43.420   Training iter 450, batch loss 1.5339, batch acc 0.9198
16:53:43.902   Training iter 500, batch loss 1.5355, batch acc 0.9240
16:53:44.392   Training iter 550, batch loss 1.5396, batch acc 0.9166
16:53:44.878   Training iter 600, batch loss 1.5304, batch acc 0.9322
16:53:44.880 Training @ 237 epoch...
16:53:45.379   Training iter 50, batch loss 1.5373, batch acc 0.9210
16:53:45.921   Training iter 100, batch loss 1.5343, batch acc 0.9208
16:53:46.481   Training iter 150, batch loss 1.5340, batch acc 0.9226
16:53:47.024   Training iter 200, batch loss 1.5355, batch acc 0.9240
16:53:47.543   Training iter 250, batch loss 1.5348, batch acc 0.9230
16:53:48.069   Training iter 300, batch loss 1.5355, batch acc 0.9230
16:53:48.611   Training iter 350, batch loss 1.5365, batch acc 0.9222
16:53:49.167   Training iter 400, batch loss 1.5377, batch acc 0.9206
16:53:49.717   Training iter 450, batch loss 1.5320, batch acc 0.9260
16:53:50.292   Training iter 500, batch loss 1.5309, batch acc 0.9290
16:53:50.868   Training iter 550, batch loss 1.5374, batch acc 0.9158
16:53:51.430   Training iter 600, batch loss 1.5404, batch acc 0.9134
16:53:51.431 Training @ 238 epoch...
16:53:51.944   Training iter 50, batch loss 1.5381, batch acc 0.9196
16:53:52.435   Training iter 100, batch loss 1.5367, batch acc 0.9192
16:53:52.916   Training iter 150, batch loss 1.5365, batch acc 0.9228
16:53:53.439   Training iter 200, batch loss 1.5351, batch acc 0.9242
16:53:53.957   Training iter 250, batch loss 1.5357, batch acc 0.9206
16:53:54.476   Training iter 300, batch loss 1.5345, batch acc 0.9202
16:53:54.990   Training iter 350, batch loss 1.5338, batch acc 0.9252
16:53:55.519   Training iter 400, batch loss 1.5328, batch acc 0.9240
16:53:56.035   Training iter 450, batch loss 1.5371, batch acc 0.9246
16:53:56.544   Training iter 500, batch loss 1.5350, batch acc 0.9214
16:53:57.056   Training iter 550, batch loss 1.5397, batch acc 0.9162
16:53:57.569   Training iter 600, batch loss 1.5309, batch acc 0.9260
16:53:57.571 Training @ 239 epoch...
16:53:58.082   Training iter 50, batch loss 1.5375, batch acc 0.9194
16:53:58.587   Training iter 100, batch loss 1.5356, batch acc 0.9206
16:53:59.109   Training iter 150, batch loss 1.5356, batch acc 0.9222
16:53:59.649   Training iter 200, batch loss 1.5354, batch acc 0.9238
16:54:00.191   Training iter 250, batch loss 1.5359, batch acc 0.9180
16:54:00.723   Training iter 300, batch loss 1.5346, batch acc 0.9252
16:54:01.211   Training iter 350, batch loss 1.5336, batch acc 0.9210
16:54:01.713   Training iter 400, batch loss 1.5330, batch acc 0.9272
16:54:02.274   Training iter 450, batch loss 1.5373, batch acc 0.9194
16:54:02.801   Training iter 500, batch loss 1.5360, batch acc 0.9228
16:54:03.317   Training iter 550, batch loss 1.5310, batch acc 0.9308
16:54:03.810   Training iter 600, batch loss 1.5406, batch acc 0.9154
16:54:03.811 Training @ 240 epoch...
16:54:04.342   Training iter 50, batch loss 1.5332, batch acc 0.9254
16:54:04.875   Training iter 100, batch loss 1.5334, batch acc 0.9244
16:54:05.423   Training iter 150, batch loss 1.5351, batch acc 0.9198
16:54:05.947   Training iter 200, batch loss 1.5400, batch acc 0.9184
16:54:06.462   Training iter 250, batch loss 1.5333, batch acc 0.9266
16:54:06.996   Training iter 300, batch loss 1.5328, batch acc 0.9244
16:54:07.532   Training iter 350, batch loss 1.5343, batch acc 0.9228
16:54:08.014   Training iter 400, batch loss 1.5335, batch acc 0.9258
16:54:08.505   Training iter 450, batch loss 1.5356, batch acc 0.9190
16:54:09.023   Training iter 500, batch loss 1.5400, batch acc 0.9186
16:54:09.522   Training iter 550, batch loss 1.5368, batch acc 0.9204
16:54:10.013   Training iter 600, batch loss 1.5379, batch acc 0.9208
16:54:10.015 Testing @ 240 epoch...
16:54:10.062     Testing, total mean loss 1.54058, total acc 0.92110
16:54:10.062 Training @ 241 epoch...
16:54:10.576   Training iter 50, batch loss 1.5376, batch acc 0.9182
16:54:11.047   Training iter 100, batch loss 1.5359, batch acc 0.9238
16:54:11.538   Training iter 150, batch loss 1.5370, batch acc 0.9210
16:54:12.029   Training iter 200, batch loss 1.5397, batch acc 0.9190
16:54:12.569   Training iter 250, batch loss 1.5343, batch acc 0.9200
16:54:13.096   Training iter 300, batch loss 1.5364, batch acc 0.9252
16:54:13.611   Training iter 350, batch loss 1.5330, batch acc 0.9312
16:54:14.115   Training iter 400, batch loss 1.5367, batch acc 0.9150
16:54:14.610   Training iter 450, batch loss 1.5323, batch acc 0.9246
16:54:15.128   Training iter 500, batch loss 1.5331, batch acc 0.9226
16:54:15.647   Training iter 550, batch loss 1.5343, batch acc 0.9240
16:54:16.156   Training iter 600, batch loss 1.5353, batch acc 0.9208
16:54:16.157 Training @ 242 epoch...
16:54:16.676   Training iter 50, batch loss 1.5326, batch acc 0.9230
16:54:17.197   Training iter 100, batch loss 1.5349, batch acc 0.9184
16:54:17.723   Training iter 150, batch loss 1.5418, batch acc 0.9154
16:54:18.236   Training iter 200, batch loss 1.5309, batch acc 0.9270
16:54:18.743   Training iter 250, batch loss 1.5371, batch acc 0.9202
16:54:19.251   Training iter 300, batch loss 1.5334, batch acc 0.9250
16:54:19.775   Training iter 350, batch loss 1.5354, batch acc 0.9216
16:54:20.295   Training iter 400, batch loss 1.5367, batch acc 0.9186
16:54:20.814   Training iter 450, batch loss 1.5396, batch acc 0.9162
16:54:21.332   Training iter 500, batch loss 1.5331, batch acc 0.9246
16:54:21.851   Training iter 550, batch loss 1.5360, batch acc 0.9262
16:54:22.370   Training iter 600, batch loss 1.5333, batch acc 0.9232
16:54:22.372 Training @ 243 epoch...
16:54:22.913   Training iter 50, batch loss 1.5361, batch acc 0.9188
16:54:23.425   Training iter 100, batch loss 1.5317, batch acc 0.9278
16:54:23.937   Training iter 150, batch loss 1.5334, batch acc 0.9266
16:54:24.463   Training iter 200, batch loss 1.5385, batch acc 0.9184
16:54:24.981   Training iter 250, batch loss 1.5381, batch acc 0.9174
16:54:25.524   Training iter 300, batch loss 1.5395, batch acc 0.9198
16:54:26.050   Training iter 350, batch loss 1.5351, batch acc 0.9224
16:54:26.585   Training iter 400, batch loss 1.5310, batch acc 0.9230
16:54:27.114   Training iter 450, batch loss 1.5346, batch acc 0.9228
16:54:27.644   Training iter 500, batch loss 1.5349, batch acc 0.9230
16:54:28.159   Training iter 550, batch loss 1.5337, batch acc 0.9194
16:54:28.658   Training iter 600, batch loss 1.5375, batch acc 0.9228
16:54:28.660 Training @ 244 epoch...
16:54:29.203   Training iter 50, batch loss 1.5329, batch acc 0.9218
16:54:29.745   Training iter 100, batch loss 1.5348, batch acc 0.9190
16:54:30.296   Training iter 150, batch loss 1.5354, batch acc 0.9224
16:54:30.836   Training iter 200, batch loss 1.5353, batch acc 0.9224
16:54:31.366   Training iter 250, batch loss 1.5381, batch acc 0.9164
16:54:31.898   Training iter 300, batch loss 1.5326, batch acc 0.9250
16:54:32.448   Training iter 350, batch loss 1.5399, batch acc 0.9204
16:54:32.993   Training iter 400, batch loss 1.5270, batch acc 0.9302
16:54:33.520   Training iter 450, batch loss 1.5362, batch acc 0.9242
16:54:34.069   Training iter 500, batch loss 1.5340, batch acc 0.9234
16:54:34.583   Training iter 550, batch loss 1.5432, batch acc 0.9120
16:54:35.129   Training iter 600, batch loss 1.5356, batch acc 0.9230
16:54:35.131 Training @ 245 epoch...
16:54:35.689   Training iter 50, batch loss 1.5397, batch acc 0.9160
16:54:36.243   Training iter 100, batch loss 1.5357, batch acc 0.9190
16:54:36.779   Training iter 150, batch loss 1.5369, batch acc 0.9180
16:54:37.309   Training iter 200, batch loss 1.5377, batch acc 0.9220
16:54:37.833   Training iter 250, batch loss 1.5355, batch acc 0.9214
16:54:38.366   Training iter 300, batch loss 1.5353, batch acc 0.9228
16:54:38.889   Training iter 350, batch loss 1.5363, batch acc 0.9224
16:54:39.425   Training iter 400, batch loss 1.5339, batch acc 0.9210
16:54:39.930   Training iter 450, batch loss 1.5316, batch acc 0.9300
16:54:40.449   Training iter 500, batch loss 1.5356, batch acc 0.9228
16:54:41.023   Training iter 550, batch loss 1.5324, batch acc 0.9226
16:54:41.602   Training iter 600, batch loss 1.5345, batch acc 0.9232
16:54:41.604 Testing @ 245 epoch...
16:54:41.653     Testing, total mean loss 1.54019, total acc 0.92110
16:54:41.653 Training @ 246 epoch...
16:54:42.235   Training iter 50, batch loss 1.5383, batch acc 0.9170
16:54:42.772   Training iter 100, batch loss 1.5381, batch acc 0.9174
16:54:43.322   Training iter 150, batch loss 1.5345, batch acc 0.9246
16:54:43.850   Training iter 200, batch loss 1.5320, batch acc 0.9240
16:54:44.385   Training iter 250, batch loss 1.5353, batch acc 0.9224
16:54:44.909   Training iter 300, batch loss 1.5364, batch acc 0.9206
16:54:45.458   Training iter 350, batch loss 1.5358, batch acc 0.9210
16:54:45.990   Training iter 400, batch loss 1.5332, batch acc 0.9232
16:54:46.551   Training iter 450, batch loss 1.5313, batch acc 0.9288
16:54:47.102   Training iter 500, batch loss 1.5358, batch acc 0.9246
16:54:47.660   Training iter 550, batch loss 1.5389, batch acc 0.9194
16:54:48.207   Training iter 600, batch loss 1.5345, batch acc 0.9214
16:54:48.208 Training @ 247 epoch...
16:54:48.760   Training iter 50, batch loss 1.5341, batch acc 0.9256
16:54:49.306   Training iter 100, batch loss 1.5359, batch acc 0.9206
16:54:49.864   Training iter 150, batch loss 1.5356, batch acc 0.9182
16:54:50.419   Training iter 200, batch loss 1.5315, batch acc 0.9252
16:54:50.968   Training iter 250, batch loss 1.5356, batch acc 0.9194
16:54:51.507   Training iter 300, batch loss 1.5353, batch acc 0.9244
16:54:52.035   Training iter 350, batch loss 1.5366, batch acc 0.9230
16:54:52.590   Training iter 400, batch loss 1.5319, batch acc 0.9300
16:54:53.112   Training iter 450, batch loss 1.5385, batch acc 0.9146
16:54:53.607   Training iter 500, batch loss 1.5399, batch acc 0.9172
16:54:54.118   Training iter 550, batch loss 1.5346, batch acc 0.9208
16:54:54.626   Training iter 600, batch loss 1.5342, batch acc 0.9238
16:54:54.628 Training @ 248 epoch...
16:54:55.163   Training iter 50, batch loss 1.5315, batch acc 0.9272
16:54:55.668   Training iter 100, batch loss 1.5395, batch acc 0.9214
16:54:56.158   Training iter 150, batch loss 1.5334, batch acc 0.9238
16:54:56.638   Training iter 200, batch loss 1.5343, batch acc 0.9246
16:54:57.111   Training iter 250, batch loss 1.5348, batch acc 0.9202
16:54:57.577   Training iter 300, batch loss 1.5364, batch acc 0.9264
16:54:58.050   Training iter 350, batch loss 1.5374, batch acc 0.9184
16:54:58.527   Training iter 400, batch loss 1.5309, batch acc 0.9256
16:54:59.008   Training iter 450, batch loss 1.5372, batch acc 0.9202
16:54:59.485   Training iter 500, batch loss 1.5366, batch acc 0.9230
16:54:59.957   Training iter 550, batch loss 1.5356, batch acc 0.9188
16:55:00.438   Training iter 600, batch loss 1.5367, batch acc 0.9204
16:55:00.440 Training @ 249 epoch...
16:55:00.933   Training iter 50, batch loss 1.5396, batch acc 0.9186
16:55:01.442   Training iter 100, batch loss 1.5331, batch acc 0.9234
16:55:01.972   Training iter 150, batch loss 1.5329, batch acc 0.9246
16:55:02.506   Training iter 200, batch loss 1.5344, batch acc 0.9192
16:55:03.049   Training iter 250, batch loss 1.5301, batch acc 0.9328
16:55:03.588   Training iter 300, batch loss 1.5358, batch acc 0.9192
16:55:04.116   Training iter 350, batch loss 1.5376, batch acc 0.9214
16:55:04.636   Training iter 400, batch loss 1.5395, batch acc 0.9198
16:55:05.170   Training iter 450, batch loss 1.5344, batch acc 0.9248
16:55:05.689   Training iter 500, batch loss 1.5349, batch acc 0.9218
16:55:06.216   Training iter 550, batch loss 1.5393, batch acc 0.9164
16:55:06.751   Training iter 600, batch loss 1.5328, batch acc 0.9258
16:55:06.752 Training @ 250 epoch...
16:55:07.285   Training iter 50, batch loss 1.5342, batch acc 0.9234
16:55:07.823   Training iter 100, batch loss 1.5332, batch acc 0.9244
16:55:08.339   Training iter 150, batch loss 1.5389, batch acc 0.9170
16:55:08.820   Training iter 200, batch loss 1.5365, batch acc 0.9210
16:55:09.305   Training iter 250, batch loss 1.5361, batch acc 0.9222
16:55:09.784   Training iter 300, batch loss 1.5356, batch acc 0.9224
16:55:10.264   Training iter 350, batch loss 1.5372, batch acc 0.9202
16:55:10.752   Training iter 400, batch loss 1.5324, batch acc 0.9260
16:55:11.233   Training iter 450, batch loss 1.5405, batch acc 0.9154
16:55:11.708   Training iter 500, batch loss 1.5291, batch acc 0.9270
16:55:12.193   Training iter 550, batch loss 1.5345, batch acc 0.9236
16:55:12.665   Training iter 600, batch loss 1.5342, batch acc 0.9218
16:55:12.666 Testing @ 250 epoch...
16:55:12.712     Testing, total mean loss 1.54089, total acc 0.92040
16:55:12.713 Training @ 251 epoch...
16:55:13.211   Training iter 50, batch loss 1.5330, batch acc 0.9232
16:55:13.718   Training iter 100, batch loss 1.5394, batch acc 0.9172
16:55:14.235   Training iter 150, batch loss 1.5310, batch acc 0.9258
16:55:14.751   Training iter 200, batch loss 1.5382, batch acc 0.9196
16:55:15.273   Training iter 250, batch loss 1.5309, batch acc 0.9246
16:55:15.813   Training iter 300, batch loss 1.5376, batch acc 0.9214
16:55:16.358   Training iter 350, batch loss 1.5377, batch acc 0.9166
16:55:16.898   Training iter 400, batch loss 1.5329, batch acc 0.9224
16:55:17.439   Training iter 450, batch loss 1.5305, batch acc 0.9280
16:55:17.980   Training iter 500, batch loss 1.5320, batch acc 0.9274
16:55:18.528   Training iter 550, batch loss 1.5401, batch acc 0.9170
16:55:19.090   Training iter 600, batch loss 1.5406, batch acc 0.9172
16:55:19.092 Training @ 252 epoch...
16:55:19.660   Training iter 50, batch loss 1.5341, batch acc 0.9252
16:55:20.239   Training iter 100, batch loss 1.5335, batch acc 0.9254
16:55:20.833   Training iter 150, batch loss 1.5370, batch acc 0.9162
16:55:21.426   Training iter 200, batch loss 1.5362, batch acc 0.9216
16:55:22.010   Training iter 250, batch loss 1.5336, batch acc 0.9222
16:55:22.586   Training iter 300, batch loss 1.5376, batch acc 0.9196
16:55:23.157   Training iter 350, batch loss 1.5329, batch acc 0.9254
16:55:23.710   Training iter 400, batch loss 1.5319, batch acc 0.9266
16:55:24.257   Training iter 450, batch loss 1.5369, batch acc 0.9186
16:55:24.751   Training iter 500, batch loss 1.5377, batch acc 0.9164
16:55:25.254   Training iter 550, batch loss 1.5336, batch acc 0.9290
16:55:25.755   Training iter 600, batch loss 1.5383, batch acc 0.9214
16:55:25.757 Training @ 253 epoch...
16:55:26.279   Training iter 50, batch loss 1.5328, batch acc 0.9228
16:55:26.795   Training iter 100, batch loss 1.5345, batch acc 0.9216
16:55:27.351   Training iter 150, batch loss 1.5334, batch acc 0.9224
16:55:27.898   Training iter 200, batch loss 1.5339, batch acc 0.9232
16:55:28.431   Training iter 250, batch loss 1.5339, batch acc 0.9236
16:55:28.961   Training iter 300, batch loss 1.5353, batch acc 0.9234
16:55:29.496   Training iter 350, batch loss 1.5346, batch acc 0.9222
16:55:30.026   Training iter 400, batch loss 1.5388, batch acc 0.9204
16:55:30.555   Training iter 450, batch loss 1.5416, batch acc 0.9160
16:55:31.095   Training iter 500, batch loss 1.5348, batch acc 0.9246
16:55:31.639   Training iter 550, batch loss 1.5333, batch acc 0.9260
16:55:32.221   Training iter 600, batch loss 1.5377, batch acc 0.9172
16:55:32.222 Training @ 254 epoch...
16:55:32.812   Training iter 50, batch loss 1.5351, batch acc 0.9206
16:55:33.399   Training iter 100, batch loss 1.5348, batch acc 0.9224
16:55:33.911   Training iter 150, batch loss 1.5290, batch acc 0.9286
16:55:34.446   Training iter 200, batch loss 1.5393, batch acc 0.9176
16:55:35.009   Training iter 250, batch loss 1.5342, batch acc 0.9248
16:55:35.577   Training iter 300, batch loss 1.5393, batch acc 0.9156
16:55:36.144   Training iter 350, batch loss 1.5334, batch acc 0.9274
16:55:36.709   Training iter 400, batch loss 1.5360, batch acc 0.9218
16:55:37.266   Training iter 450, batch loss 1.5372, batch acc 0.9194
16:55:37.824   Training iter 500, batch loss 1.5350, batch acc 0.9226
16:55:38.384   Training iter 550, batch loss 1.5354, batch acc 0.9234
16:55:38.950   Training iter 600, batch loss 1.5343, batch acc 0.9228
16:55:38.951 Training @ 255 epoch...
16:55:39.514   Training iter 50, batch loss 1.5304, batch acc 0.9260
16:55:40.064   Training iter 100, batch loss 1.5348, batch acc 0.9214
16:55:40.600   Training iter 150, batch loss 1.5357, batch acc 0.9228
16:55:41.144   Training iter 200, batch loss 1.5346, batch acc 0.9226
16:55:41.684   Training iter 250, batch loss 1.5404, batch acc 0.9158
16:55:42.222   Training iter 300, batch loss 1.5360, batch acc 0.9210
16:55:42.766   Training iter 350, batch loss 1.5323, batch acc 0.9204
16:55:43.292   Training iter 400, batch loss 1.5354, batch acc 0.9248
16:55:43.822   Training iter 450, batch loss 1.5373, batch acc 0.9194
16:55:44.352   Training iter 500, batch loss 1.5363, batch acc 0.9236
16:55:44.876   Training iter 550, batch loss 1.5348, batch acc 0.9228
16:55:45.405   Training iter 600, batch loss 1.5344, batch acc 0.9248
16:55:45.407 Testing @ 255 epoch...
16:55:45.453     Testing, total mean loss 1.54018, total acc 0.92140
16:55:45.453 Training @ 256 epoch...
16:55:45.992   Training iter 50, batch loss 1.5371, batch acc 0.9182
16:55:46.532   Training iter 100, batch loss 1.5345, batch acc 0.9212
16:55:47.065   Training iter 150, batch loss 1.5389, batch acc 0.9168
16:55:47.590   Training iter 200, batch loss 1.5350, batch acc 0.9238
16:55:48.131   Training iter 250, batch loss 1.5352, batch acc 0.9214
16:55:48.678   Training iter 300, batch loss 1.5366, batch acc 0.9170
16:55:49.232   Training iter 350, batch loss 1.5309, batch acc 0.9276
16:55:49.789   Training iter 400, batch loss 1.5337, batch acc 0.9236
16:55:50.339   Training iter 450, batch loss 1.5346, batch acc 0.9240
16:55:50.913   Training iter 500, batch loss 1.5330, batch acc 0.9272
16:55:51.493   Training iter 550, batch loss 1.5374, batch acc 0.9214
16:55:52.089   Training iter 600, batch loss 1.5346, batch acc 0.9216
16:55:52.091 Training @ 257 epoch...
16:55:52.688   Training iter 50, batch loss 1.5331, batch acc 0.9210
16:55:53.287   Training iter 100, batch loss 1.5367, batch acc 0.9210
16:55:53.865   Training iter 150, batch loss 1.5386, batch acc 0.9204
16:55:54.450   Training iter 200, batch loss 1.5347, batch acc 0.9260
16:55:55.004   Training iter 250, batch loss 1.5339, batch acc 0.9248
16:55:55.555   Training iter 300, batch loss 1.5314, batch acc 0.9246
16:55:56.112   Training iter 350, batch loss 1.5363, batch acc 0.9194
16:55:56.627   Training iter 400, batch loss 1.5353, batch acc 0.9220
16:55:57.133   Training iter 450, batch loss 1.5333, batch acc 0.9256
16:55:57.649   Training iter 500, batch loss 1.5353, batch acc 0.9236
16:55:58.163   Training iter 550, batch loss 1.5370, batch acc 0.9192
16:55:58.676   Training iter 600, batch loss 1.5364, batch acc 0.9226
16:55:58.678 Training @ 258 epoch...
16:55:59.215   Training iter 50, batch loss 1.5340, batch acc 0.9204
16:55:59.724   Training iter 100, batch loss 1.5337, batch acc 0.9210
16:56:00.247   Training iter 150, batch loss 1.5338, batch acc 0.9240
16:56:00.788   Training iter 200, batch loss 1.5375, batch acc 0.9224
16:56:01.325   Training iter 250, batch loss 1.5311, batch acc 0.9274
16:56:01.895   Training iter 300, batch loss 1.5393, batch acc 0.9168
16:56:02.472   Training iter 350, batch loss 1.5371, batch acc 0.9176
16:56:03.040   Training iter 400, batch loss 1.5384, batch acc 0.9200
16:56:03.628   Training iter 450, batch loss 1.5366, batch acc 0.9222
16:56:04.210   Training iter 500, batch loss 1.5341, batch acc 0.9240
16:56:04.789   Training iter 550, batch loss 1.5313, batch acc 0.9216
16:56:05.360   Training iter 600, batch loss 1.5357, batch acc 0.9256
16:56:05.362 Training @ 259 epoch...
16:56:05.960   Training iter 50, batch loss 1.5342, batch acc 0.9220
16:56:06.546   Training iter 100, batch loss 1.5318, batch acc 0.9270
16:56:07.112   Training iter 150, batch loss 1.5341, batch acc 0.9222
16:56:07.656   Training iter 200, batch loss 1.5399, batch acc 0.9186
16:56:08.227   Training iter 250, batch loss 1.5365, batch acc 0.9222
16:56:08.788   Training iter 300, batch loss 1.5409, batch acc 0.9132
16:56:09.345   Training iter 350, batch loss 1.5312, batch acc 0.9274
16:56:09.912   Training iter 400, batch loss 1.5311, batch acc 0.9264
16:56:10.464   Training iter 450, batch loss 1.5394, batch acc 0.9154
16:56:11.009   Training iter 500, batch loss 1.5333, batch acc 0.9222
16:56:11.547   Training iter 550, batch loss 1.5377, batch acc 0.9222
16:56:12.097   Training iter 600, batch loss 1.5318, batch acc 0.9272
16:56:12.098 Training @ 260 epoch...
16:56:12.647   Training iter 50, batch loss 1.5315, batch acc 0.9284
16:56:13.188   Training iter 100, batch loss 1.5306, batch acc 0.9272
16:56:13.720   Training iter 150, batch loss 1.5367, batch acc 0.9210
16:56:14.268   Training iter 200, batch loss 1.5343, batch acc 0.9252
16:56:14.794   Training iter 250, batch loss 1.5373, batch acc 0.9142
16:56:15.329   Training iter 300, batch loss 1.5328, batch acc 0.9256
16:56:15.865   Training iter 350, batch loss 1.5381, batch acc 0.9192
16:56:16.408   Training iter 400, batch loss 1.5318, batch acc 0.9246
16:56:16.946   Training iter 450, batch loss 1.5383, batch acc 0.9178
16:56:17.477   Training iter 500, batch loss 1.5333, batch acc 0.9236
16:56:18.018   Training iter 550, batch loss 1.5389, batch acc 0.9216
16:56:18.525   Training iter 600, batch loss 1.5375, batch acc 0.9202
16:56:18.527 Testing @ 260 epoch...
16:56:18.573     Testing, total mean loss 1.54001, total acc 0.92090
16:56:18.573 Training @ 261 epoch...
16:56:19.087   Training iter 50, batch loss 1.5333, batch acc 0.9254
16:56:19.579   Training iter 100, batch loss 1.5300, batch acc 0.9296
16:56:20.071   Training iter 150, batch loss 1.5357, batch acc 0.9204
16:56:20.577   Training iter 200, batch loss 1.5349, batch acc 0.9212
16:56:21.072   Training iter 250, batch loss 1.5317, batch acc 0.9208
16:56:21.580   Training iter 300, batch loss 1.5407, batch acc 0.9152
16:56:22.100   Training iter 350, batch loss 1.5318, batch acc 0.9302
16:56:22.649   Training iter 400, batch loss 1.5343, batch acc 0.9248
16:56:23.182   Training iter 450, batch loss 1.5392, batch acc 0.9166
16:56:23.704   Training iter 500, batch loss 1.5367, batch acc 0.9256
16:56:24.240   Training iter 550, batch loss 1.5360, batch acc 0.9176
16:56:24.805   Training iter 600, batch loss 1.5360, batch acc 0.9196
16:56:24.807 Training @ 262 epoch...
16:56:25.369   Training iter 50, batch loss 1.5379, batch acc 0.9208
16:56:25.936   Training iter 100, batch loss 1.5317, batch acc 0.9226
16:56:26.512   Training iter 150, batch loss 1.5376, batch acc 0.9184
16:56:27.083   Training iter 200, batch loss 1.5373, batch acc 0.9216
16:56:27.644   Training iter 250, batch loss 1.5381, batch acc 0.9188
16:56:28.190   Training iter 300, batch loss 1.5337, batch acc 0.9278
16:56:28.712   Training iter 350, batch loss 1.5336, batch acc 0.9228
16:56:29.268   Training iter 400, batch loss 1.5338, batch acc 0.9240
16:56:29.820   Training iter 450, batch loss 1.5348, batch acc 0.9198
16:56:30.329   Training iter 500, batch loss 1.5328, batch acc 0.9220
16:56:30.816   Training iter 550, batch loss 1.5396, batch acc 0.9196
16:56:31.340   Training iter 600, batch loss 1.5302, batch acc 0.9252
16:56:31.341 Training @ 263 epoch...
16:56:31.881   Training iter 50, batch loss 1.5309, batch acc 0.9252
16:56:32.412   Training iter 100, batch loss 1.5359, batch acc 0.9238
16:56:32.943   Training iter 150, batch loss 1.5352, batch acc 0.9228
16:56:33.421   Training iter 200, batch loss 1.5328, batch acc 0.9242
16:56:33.893   Training iter 250, batch loss 1.5317, batch acc 0.9268
16:56:34.384   Training iter 300, batch loss 1.5371, batch acc 0.9194
16:56:34.890   Training iter 350, batch loss 1.5333, batch acc 0.9258
16:56:35.400   Training iter 400, batch loss 1.5351, batch acc 0.9220
16:56:35.920   Training iter 450, batch loss 1.5368, batch acc 0.9188
16:56:36.433   Training iter 500, batch loss 1.5364, batch acc 0.9196
16:56:36.944   Training iter 550, batch loss 1.5384, batch acc 0.9154
16:56:37.451   Training iter 600, batch loss 1.5375, batch acc 0.9188
16:56:37.452 Training @ 264 epoch...
16:56:37.962   Training iter 50, batch loss 1.5373, batch acc 0.9192
16:56:38.490   Training iter 100, batch loss 1.5386, batch acc 0.9180
16:56:39.011   Training iter 150, batch loss 1.5363, batch acc 0.9222
16:56:39.528   Training iter 200, batch loss 1.5319, batch acc 0.9278
16:56:40.037   Training iter 250, batch loss 1.5333, batch acc 0.9236
16:56:40.566   Training iter 300, batch loss 1.5325, batch acc 0.9278
16:56:41.094   Training iter 350, batch loss 1.5364, batch acc 0.9230
16:56:41.606   Training iter 400, batch loss 1.5372, batch acc 0.9204
16:56:42.107   Training iter 450, batch loss 1.5302, batch acc 0.9240
16:56:42.613   Training iter 500, batch loss 1.5360, batch acc 0.9198
16:56:43.125   Training iter 550, batch loss 1.5350, batch acc 0.9220
16:56:43.643   Training iter 600, batch loss 1.5370, batch acc 0.9204
16:56:43.645 Training @ 265 epoch...
16:56:44.166   Training iter 50, batch loss 1.5326, batch acc 0.9222
16:56:44.701   Training iter 100, batch loss 1.5373, batch acc 0.9166
16:56:45.236   Training iter 150, batch loss 1.5363, batch acc 0.9190
16:56:45.751   Training iter 200, batch loss 1.5326, batch acc 0.9250
16:56:46.270   Training iter 250, batch loss 1.5353, batch acc 0.9204
16:56:46.807   Training iter 300, batch loss 1.5328, batch acc 0.9242
16:56:47.335   Training iter 350, batch loss 1.5360, batch acc 0.9204
16:56:47.881   Training iter 400, batch loss 1.5363, batch acc 0.9222
16:56:48.403   Training iter 450, batch loss 1.5364, batch acc 0.9212
16:56:48.918   Training iter 500, batch loss 1.5360, batch acc 0.9240
16:56:49.464   Training iter 550, batch loss 1.5352, batch acc 0.9210
16:56:49.967   Training iter 600, batch loss 1.5334, batch acc 0.9260
16:56:49.969 Testing @ 265 epoch...
16:56:50.016     Testing, total mean loss 1.54014, total acc 0.92230
16:56:50.016 Training @ 266 epoch...
16:56:50.545   Training iter 50, batch loss 1.5344, batch acc 0.9240
16:56:51.068   Training iter 100, batch loss 1.5366, batch acc 0.9196
16:56:51.562   Training iter 150, batch loss 1.5340, batch acc 0.9182
16:56:52.053   Training iter 200, batch loss 1.5396, batch acc 0.9150
16:56:52.538   Training iter 250, batch loss 1.5348, batch acc 0.9192
16:56:53.019   Training iter 300, batch loss 1.5339, batch acc 0.9284
16:56:53.512   Training iter 350, batch loss 1.5371, batch acc 0.9222
16:56:54.008   Training iter 400, batch loss 1.5349, batch acc 0.9226
16:56:54.543   Training iter 450, batch loss 1.5317, batch acc 0.9282
16:56:55.076   Training iter 500, batch loss 1.5336, batch acc 0.9226
16:56:55.598   Training iter 550, batch loss 1.5334, batch acc 0.9234
16:56:56.130   Training iter 600, batch loss 1.5361, batch acc 0.9236
16:56:56.132 Training @ 267 epoch...
16:56:56.660   Training iter 50, batch loss 1.5374, batch acc 0.9190
16:56:57.175   Training iter 100, batch loss 1.5331, batch acc 0.9250
16:56:57.688   Training iter 150, batch loss 1.5343, batch acc 0.9216
16:56:58.192   Training iter 200, batch loss 1.5366, batch acc 0.9188
16:56:58.704   Training iter 250, batch loss 1.5341, batch acc 0.9214
16:56:59.229   Training iter 300, batch loss 1.5374, batch acc 0.9214
16:56:59.745   Training iter 350, batch loss 1.5378, batch acc 0.9140
16:57:00.247   Training iter 400, batch loss 1.5312, batch acc 0.9268
16:57:00.744   Training iter 450, batch loss 1.5321, batch acc 0.9290
16:57:01.230   Training iter 500, batch loss 1.5363, batch acc 0.9208
16:57:01.728   Training iter 550, batch loss 1.5354, batch acc 0.9166
16:57:02.298   Training iter 600, batch loss 1.5340, batch acc 0.9274
16:57:02.300 Training @ 268 epoch...
16:57:02.871   Training iter 50, batch loss 1.5349, batch acc 0.9222
16:57:03.435   Training iter 100, batch loss 1.5348, batch acc 0.9218
16:57:03.987   Training iter 150, batch loss 1.5337, batch acc 0.9244
16:57:04.570   Training iter 200, batch loss 1.5380, batch acc 0.9184
16:57:05.133   Training iter 250, batch loss 1.5324, batch acc 0.9224
16:57:05.730   Training iter 300, batch loss 1.5352, batch acc 0.9202
16:57:06.328   Training iter 350, batch loss 1.5331, batch acc 0.9264
16:57:06.949   Training iter 400, batch loss 1.5371, batch acc 0.9188
16:57:07.544   Training iter 450, batch loss 1.5369, batch acc 0.9170
16:57:08.110   Training iter 500, batch loss 1.5351, batch acc 0.9240
16:57:08.671   Training iter 550, batch loss 1.5319, batch acc 0.9270
16:57:09.258   Training iter 600, batch loss 1.5372, batch acc 0.9226
16:57:09.260 Training @ 269 epoch...
16:57:09.837   Training iter 50, batch loss 1.5356, batch acc 0.9198
16:57:10.409   Training iter 100, batch loss 1.5371, batch acc 0.9212
16:57:10.996   Training iter 150, batch loss 1.5319, batch acc 0.9272
16:57:11.564   Training iter 200, batch loss 1.5360, batch acc 0.9244
16:57:12.117   Training iter 250, batch loss 1.5329, batch acc 0.9226
16:57:12.686   Training iter 300, batch loss 1.5321, batch acc 0.9238
16:57:13.246   Training iter 350, batch loss 1.5325, batch acc 0.9252
16:57:13.803   Training iter 400, batch loss 1.5338, batch acc 0.9236
16:57:14.356   Training iter 450, batch loss 1.5389, batch acc 0.9172
16:57:14.894   Training iter 500, batch loss 1.5388, batch acc 0.9142
16:57:15.443   Training iter 550, batch loss 1.5364, batch acc 0.9232
16:57:16.013   Training iter 600, batch loss 1.5340, batch acc 0.9228
16:57:16.015 Training @ 270 epoch...
16:57:16.551   Training iter 50, batch loss 1.5302, batch acc 0.9252
16:57:17.080   Training iter 100, batch loss 1.5321, batch acc 0.9282
16:57:17.601   Training iter 150, batch loss 1.5414, batch acc 0.9150
16:57:18.108   Training iter 200, batch loss 1.5303, batch acc 0.9234
16:57:18.633   Training iter 250, batch loss 1.5327, batch acc 0.9246
16:57:19.177   Training iter 300, batch loss 1.5371, batch acc 0.9196
16:57:19.706   Training iter 350, batch loss 1.5342, batch acc 0.9274
16:57:20.233   Training iter 400, batch loss 1.5367, batch acc 0.9170
16:57:20.767   Training iter 450, batch loss 1.5322, batch acc 0.9242
16:57:21.305   Training iter 500, batch loss 1.5436, batch acc 0.9144
16:57:21.841   Training iter 550, batch loss 1.5353, batch acc 0.9232
16:57:22.384   Training iter 600, batch loss 1.5340, batch acc 0.9232
16:57:22.386 Testing @ 270 epoch...
16:57:22.432     Testing, total mean loss 1.53974, total acc 0.92190
16:57:22.432 Training @ 271 epoch...
16:57:22.970   Training iter 50, batch loss 1.5380, batch acc 0.9210
16:57:23.540   Training iter 100, batch loss 1.5351, batch acc 0.9198
16:57:24.103   Training iter 150, batch loss 1.5396, batch acc 0.9168
16:57:24.666   Training iter 200, batch loss 1.5360, batch acc 0.9190
16:57:25.225   Training iter 250, batch loss 1.5366, batch acc 0.9172
16:57:25.762   Training iter 300, batch loss 1.5290, batch acc 0.9318
16:57:26.317   Training iter 350, batch loss 1.5339, batch acc 0.9240
16:57:26.865   Training iter 400, batch loss 1.5307, batch acc 0.9282
16:57:27.421   Training iter 450, batch loss 1.5343, batch acc 0.9200
16:57:27.973   Training iter 500, batch loss 1.5369, batch acc 0.9188
16:57:28.518   Training iter 550, batch loss 1.5350, batch acc 0.9224
16:57:29.065   Training iter 600, batch loss 1.5346, batch acc 0.9236
16:57:29.067 Training @ 272 epoch...
16:57:29.617   Training iter 50, batch loss 1.5358, batch acc 0.9242
16:57:30.149   Training iter 100, batch loss 1.5387, batch acc 0.9188
16:57:30.667   Training iter 150, batch loss 1.5396, batch acc 0.9164
16:57:31.193   Training iter 200, batch loss 1.5360, batch acc 0.9182
16:57:31.716   Training iter 250, batch loss 1.5280, batch acc 0.9274
16:57:32.230   Training iter 300, batch loss 1.5340, batch acc 0.9268
16:57:32.724   Training iter 350, batch loss 1.5322, batch acc 0.9282
16:57:33.208   Training iter 400, batch loss 1.5343, batch acc 0.9224
16:57:33.686   Training iter 450, batch loss 1.5381, batch acc 0.9172
16:57:34.174   Training iter 500, batch loss 1.5360, batch acc 0.9240
16:57:34.654   Training iter 550, batch loss 1.5326, batch acc 0.9260
16:57:35.142   Training iter 600, batch loss 1.5345, batch acc 0.9210
16:57:35.143 Training @ 273 epoch...
16:57:35.639   Training iter 50, batch loss 1.5357, batch acc 0.9204
16:57:36.137   Training iter 100, batch loss 1.5310, batch acc 0.9244
16:57:36.633   Training iter 150, batch loss 1.5343, batch acc 0.9208
16:57:37.145   Training iter 200, batch loss 1.5355, batch acc 0.9232
16:57:37.650   Training iter 250, batch loss 1.5353, batch acc 0.9254
16:57:38.129   Training iter 300, batch loss 1.5340, batch acc 0.9262
16:57:38.652   Training iter 350, batch loss 1.5378, batch acc 0.9194
16:57:39.183   Training iter 400, batch loss 1.5330, batch acc 0.9246
16:57:39.706   Training iter 450, batch loss 1.5380, batch acc 0.9162
16:57:40.210   Training iter 500, batch loss 1.5375, batch acc 0.9196
16:57:40.716   Training iter 550, batch loss 1.5334, batch acc 0.9258
16:57:41.219   Training iter 600, batch loss 1.5345, batch acc 0.9256
16:57:41.221 Training @ 274 epoch...
16:57:41.717   Training iter 50, batch loss 1.5358, batch acc 0.9210
16:57:42.227   Training iter 100, batch loss 1.5337, batch acc 0.9218
16:57:42.744   Training iter 150, batch loss 1.5343, batch acc 0.9234
16:57:43.259   Training iter 200, batch loss 1.5345, batch acc 0.9230
16:57:43.761   Training iter 250, batch loss 1.5339, batch acc 0.9228
16:57:44.270   Training iter 300, batch loss 1.5375, batch acc 0.9202
16:57:44.766   Training iter 350, batch loss 1.5330, batch acc 0.9276
16:57:45.243   Training iter 400, batch loss 1.5325, batch acc 0.9244
16:57:45.727   Training iter 450, batch loss 1.5328, batch acc 0.9250
16:57:46.212   Training iter 500, batch loss 1.5370, batch acc 0.9230
16:57:46.700   Training iter 550, batch loss 1.5375, batch acc 0.9186
16:57:47.188   Training iter 600, batch loss 1.5361, batch acc 0.9236
16:57:47.190 Training @ 275 epoch...
16:57:47.670   Training iter 50, batch loss 1.5322, batch acc 0.9264
16:57:48.159   Training iter 100, batch loss 1.5335, batch acc 0.9258
16:57:48.643   Training iter 150, batch loss 1.5324, batch acc 0.9270
16:57:49.133   Training iter 200, batch loss 1.5390, batch acc 0.9152
16:57:49.616   Training iter 250, batch loss 1.5373, batch acc 0.9222
16:57:50.086   Training iter 300, batch loss 1.5307, batch acc 0.9270
16:57:50.554   Training iter 350, batch loss 1.5384, batch acc 0.9212
16:57:51.016   Training iter 400, batch loss 1.5290, batch acc 0.9274
16:57:51.497   Training iter 450, batch loss 1.5358, batch acc 0.9190
16:57:51.982   Training iter 500, batch loss 1.5359, batch acc 0.9208
16:57:52.466   Training iter 550, batch loss 1.5402, batch acc 0.9168
16:57:52.930   Training iter 600, batch loss 1.5353, batch acc 0.9188
16:57:52.932 Testing @ 275 epoch...
16:57:52.978     Testing, total mean loss 1.53967, total acc 0.92140
16:57:52.978 Training @ 276 epoch...
16:57:53.443   Training iter 50, batch loss 1.5346, batch acc 0.9224
16:57:53.913   Training iter 100, batch loss 1.5371, batch acc 0.9206
16:57:54.397   Training iter 150, batch loss 1.5363, batch acc 0.9218
16:57:54.902   Training iter 200, batch loss 1.5363, batch acc 0.9206
16:57:55.483   Training iter 250, batch loss 1.5328, batch acc 0.9242
16:57:56.020   Training iter 300, batch loss 1.5327, batch acc 0.9244
16:57:56.757   Training iter 350, batch loss 1.5358, batch acc 0.9246
16:57:57.517   Training iter 400, batch loss 1.5336, batch acc 0.9256
16:57:58.116   Training iter 450, batch loss 1.5333, batch acc 0.9242
16:57:58.591   Training iter 500, batch loss 1.5333, batch acc 0.9222
16:57:59.082   Training iter 550, batch loss 1.5346, batch acc 0.9226
16:57:59.568   Training iter 600, batch loss 1.5377, batch acc 0.9188
16:57:59.570 Training @ 277 epoch...
16:58:00.082   Training iter 50, batch loss 1.5376, batch acc 0.9208
16:58:00.589   Training iter 100, batch loss 1.5327, batch acc 0.9254
16:58:01.097   Training iter 150, batch loss 1.5331, batch acc 0.9244
16:58:01.671   Training iter 200, batch loss 1.5355, batch acc 0.9224
16:58:02.242   Training iter 250, batch loss 1.5373, batch acc 0.9192
16:58:02.789   Training iter 300, batch loss 1.5331, batch acc 0.9222
16:58:03.348   Training iter 350, batch loss 1.5370, batch acc 0.9212
16:58:03.910   Training iter 400, batch loss 1.5357, batch acc 0.9206
16:58:04.494   Training iter 450, batch loss 1.5360, batch acc 0.9208
16:58:05.077   Training iter 500, batch loss 1.5336, batch acc 0.9208
16:58:05.664   Training iter 550, batch loss 1.5326, batch acc 0.9278
16:58:06.237   Training iter 600, batch loss 1.5350, batch acc 0.9204
16:58:06.238 Training @ 278 epoch...
16:58:06.800   Training iter 50, batch loss 1.5366, batch acc 0.9220
16:58:07.338   Training iter 100, batch loss 1.5328, batch acc 0.9240
16:58:07.866   Training iter 150, batch loss 1.5365, batch acc 0.9174
16:58:08.383   Training iter 200, batch loss 1.5352, batch acc 0.9214
16:58:08.899   Training iter 250, batch loss 1.5369, batch acc 0.9180
16:58:09.428   Training iter 300, batch loss 1.5357, batch acc 0.9272
16:58:09.931   Training iter 350, batch loss 1.5362, batch acc 0.9238
16:58:10.429   Training iter 400, batch loss 1.5291, batch acc 0.9260
16:58:10.913   Training iter 450, batch loss 1.5380, batch acc 0.9206
16:58:11.433   Training iter 500, batch loss 1.5370, batch acc 0.9184
16:58:11.955   Training iter 550, batch loss 1.5293, batch acc 0.9288
16:58:12.488   Training iter 600, batch loss 1.5344, batch acc 0.9240
16:58:12.490 Training @ 279 epoch...
16:58:13.040   Training iter 50, batch loss 1.5348, batch acc 0.9230
16:58:13.562   Training iter 100, batch loss 1.5398, batch acc 0.9150
16:58:14.098   Training iter 150, batch loss 1.5363, batch acc 0.9210
16:58:14.615   Training iter 200, batch loss 1.5330, batch acc 0.9260
16:58:15.133   Training iter 250, batch loss 1.5291, batch acc 0.9300
16:58:15.644   Training iter 300, batch loss 1.5396, batch acc 0.9176
16:58:16.189   Training iter 350, batch loss 1.5369, batch acc 0.9210
16:58:16.714   Training iter 400, batch loss 1.5314, batch acc 0.9232
16:58:17.207   Training iter 450, batch loss 1.5346, batch acc 0.9232
16:58:17.692   Training iter 500, batch loss 1.5330, batch acc 0.9254
16:58:18.174   Training iter 550, batch loss 1.5301, batch acc 0.9316
16:58:18.645   Training iter 600, batch loss 1.5398, batch acc 0.9180
16:58:18.647 Training @ 280 epoch...
16:58:19.131   Training iter 50, batch loss 1.5339, batch acc 0.9236
16:58:19.607   Training iter 100, batch loss 1.5398, batch acc 0.9118
16:58:20.083   Training iter 150, batch loss 1.5361, batch acc 0.9232
16:58:20.571   Training iter 200, batch loss 1.5367, batch acc 0.9218
16:58:21.073   Training iter 250, batch loss 1.5391, batch acc 0.9198
16:58:21.567   Training iter 300, batch loss 1.5349, batch acc 0.9230
16:58:22.042   Training iter 350, batch loss 1.5335, batch acc 0.9248
16:58:22.542   Training iter 400, batch loss 1.5318, batch acc 0.9268
16:58:23.033   Training iter 450, batch loss 1.5334, batch acc 0.9248
16:58:23.557   Training iter 500, batch loss 1.5346, batch acc 0.9236
16:58:24.092   Training iter 550, batch loss 1.5329, batch acc 0.9226
16:58:24.639   Training iter 600, batch loss 1.5321, batch acc 0.9260
16:58:24.641 Testing @ 280 epoch...
16:58:24.690     Testing, total mean loss 1.53965, total acc 0.92160
16:58:24.690 Training @ 281 epoch...
16:58:25.237   Training iter 50, batch loss 1.5364, batch acc 0.9186
16:58:25.788   Training iter 100, batch loss 1.5322, batch acc 0.9288
16:58:26.312   Training iter 150, batch loss 1.5373, batch acc 0.9222
16:58:26.822   Training iter 200, batch loss 1.5349, batch acc 0.9244
16:58:27.323   Training iter 250, batch loss 1.5386, batch acc 0.9164
16:58:27.821   Training iter 300, batch loss 1.5364, batch acc 0.9214
16:58:28.317   Training iter 350, batch loss 1.5346, batch acc 0.9210
16:58:28.821   Training iter 400, batch loss 1.5357, batch acc 0.9204
16:58:29.360   Training iter 450, batch loss 1.5313, batch acc 0.9284
16:58:29.875   Training iter 500, batch loss 1.5343, batch acc 0.9234
16:58:30.405   Training iter 550, batch loss 1.5317, batch acc 0.9250
16:58:30.933   Training iter 600, batch loss 1.5346, batch acc 0.9236
16:58:30.935 Training @ 282 epoch...
16:58:31.476   Training iter 50, batch loss 1.5365, batch acc 0.9156
16:58:32.009   Training iter 100, batch loss 1.5349, batch acc 0.9218
16:58:32.540   Training iter 150, batch loss 1.5322, batch acc 0.9272
16:58:33.090   Training iter 200, batch loss 1.5367, batch acc 0.9200
16:58:33.607   Training iter 250, batch loss 1.5376, batch acc 0.9242
16:58:34.132   Training iter 300, batch loss 1.5356, batch acc 0.9252
16:58:34.640   Training iter 350, batch loss 1.5390, batch acc 0.9192
16:58:35.137   Training iter 400, batch loss 1.5305, batch acc 0.9274
16:58:35.645   Training iter 450, batch loss 1.5338, batch acc 0.9232
16:58:36.148   Training iter 500, batch loss 1.5329, batch acc 0.9262
16:58:36.647   Training iter 550, batch loss 1.5372, batch acc 0.9158
16:58:37.161   Training iter 600, batch loss 1.5299, batch acc 0.9246
16:58:37.162 Training @ 283 epoch...
16:58:37.684   Training iter 50, batch loss 1.5337, batch acc 0.9202
16:58:38.206   Training iter 100, batch loss 1.5335, batch acc 0.9222
16:58:38.726   Training iter 150, batch loss 1.5330, batch acc 0.9222
16:58:39.248   Training iter 200, batch loss 1.5318, batch acc 0.9248
16:58:39.778   Training iter 250, batch loss 1.5362, batch acc 0.9242
16:58:40.263   Training iter 300, batch loss 1.5347, batch acc 0.9180
16:58:40.756   Training iter 350, batch loss 1.5346, batch acc 0.9180
16:58:41.251   Training iter 400, batch loss 1.5334, batch acc 0.9224
16:58:41.748   Training iter 450, batch loss 1.5397, batch acc 0.9240
16:58:42.259   Training iter 500, batch loss 1.5392, batch acc 0.9214
16:58:42.753   Training iter 550, batch loss 1.5328, batch acc 0.9296
16:58:43.256   Training iter 600, batch loss 1.5341, batch acc 0.9232
16:58:43.257 Training @ 284 epoch...
16:58:43.758   Training iter 50, batch loss 1.5354, batch acc 0.9226
16:58:44.288   Training iter 100, batch loss 1.5335, batch acc 0.9246
16:58:44.798   Training iter 150, batch loss 1.5345, batch acc 0.9208
16:58:45.310   Training iter 200, batch loss 1.5348, batch acc 0.9220
16:58:45.813   Training iter 250, batch loss 1.5367, batch acc 0.9184
16:58:46.339   Training iter 300, batch loss 1.5337, batch acc 0.9258
16:58:46.908   Training iter 350, batch loss 1.5388, batch acc 0.9168
16:58:47.476   Training iter 400, batch loss 1.5335, batch acc 0.9192
16:58:48.045   Training iter 450, batch loss 1.5375, batch acc 0.9204
16:58:48.535   Training iter 500, batch loss 1.5351, batch acc 0.9202
16:58:49.033   Training iter 550, batch loss 1.5327, batch acc 0.9254
16:58:49.535   Training iter 600, batch loss 1.5304, batch acc 0.9282
16:58:49.536 Training @ 285 epoch...
16:58:50.044   Training iter 50, batch loss 1.5357, batch acc 0.9218
16:58:50.555   Training iter 100, batch loss 1.5353, batch acc 0.9232
16:58:51.094   Training iter 150, batch loss 1.5317, batch acc 0.9264
16:58:51.631   Training iter 200, batch loss 1.5358, batch acc 0.9240
16:58:52.129   Training iter 250, batch loss 1.5341, batch acc 0.9188
16:58:52.629   Training iter 300, batch loss 1.5356, batch acc 0.9196
16:58:53.128   Training iter 350, batch loss 1.5345, batch acc 0.9182
16:58:53.645   Training iter 400, batch loss 1.5320, batch acc 0.9288
16:58:54.160   Training iter 450, batch loss 1.5313, batch acc 0.9286
16:58:54.673   Training iter 500, batch loss 1.5407, batch acc 0.9132
16:58:55.180   Training iter 550, batch loss 1.5364, batch acc 0.9202
16:58:55.691   Training iter 600, batch loss 1.5334, batch acc 0.9228
16:58:55.692 Testing @ 285 epoch...
16:58:55.739     Testing, total mean loss 1.53978, total acc 0.92130
16:58:55.739 Training @ 286 epoch...
16:58:56.274   Training iter 50, batch loss 1.5332, batch acc 0.9240
16:58:56.792   Training iter 100, batch loss 1.5330, batch acc 0.9244
16:58:57.311   Training iter 150, batch loss 1.5334, batch acc 0.9264
16:58:57.837   Training iter 200, batch loss 1.5341, batch acc 0.9184
16:58:58.332   Training iter 250, batch loss 1.5359, batch acc 0.9220
16:58:58.807   Training iter 300, batch loss 1.5301, batch acc 0.9252
16:58:59.305   Training iter 350, batch loss 1.5352, batch acc 0.9240
16:58:59.793   Training iter 400, batch loss 1.5324, batch acc 0.9282
16:59:00.290   Training iter 450, batch loss 1.5388, batch acc 0.9164
16:59:00.799   Training iter 500, batch loss 1.5349, batch acc 0.9190
16:59:01.304   Training iter 550, batch loss 1.5376, batch acc 0.9190
16:59:01.831   Training iter 600, batch loss 1.5388, batch acc 0.9186
16:59:01.833 Training @ 287 epoch...
16:59:02.414   Training iter 50, batch loss 1.5353, batch acc 0.9240
16:59:02.977   Training iter 100, batch loss 1.5348, batch acc 0.9198
16:59:03.529   Training iter 150, batch loss 1.5339, batch acc 0.9240
16:59:04.095   Training iter 200, batch loss 1.5334, batch acc 0.9284
16:59:04.665   Training iter 250, batch loss 1.5326, batch acc 0.9194
16:59:05.251   Training iter 300, batch loss 1.5362, batch acc 0.9268
16:59:05.835   Training iter 350, batch loss 1.5359, batch acc 0.9200
16:59:06.428   Training iter 400, batch loss 1.5394, batch acc 0.9166
16:59:06.989   Training iter 450, batch loss 1.5374, batch acc 0.9182
16:59:07.562   Training iter 500, batch loss 1.5345, batch acc 0.9192
16:59:08.089   Training iter 550, batch loss 1.5334, batch acc 0.9264
16:59:08.579   Training iter 600, batch loss 1.5300, batch acc 0.9284
16:59:08.581 Training @ 288 epoch...
16:59:09.085   Training iter 50, batch loss 1.5331, batch acc 0.9242
16:59:09.588   Training iter 100, batch loss 1.5357, batch acc 0.9208
16:59:10.113   Training iter 150, batch loss 1.5382, batch acc 0.9190
16:59:10.647   Training iter 200, batch loss 1.5354, batch acc 0.9206
16:59:11.170   Training iter 250, batch loss 1.5304, batch acc 0.9306
16:59:11.662   Training iter 300, batch loss 1.5334, batch acc 0.9212
16:59:12.170   Training iter 350, batch loss 1.5368, batch acc 0.9202
16:59:12.686   Training iter 400, batch loss 1.5350, batch acc 0.9244
16:59:13.214   Training iter 450, batch loss 1.5339, batch acc 0.9212
16:59:13.758   Training iter 500, batch loss 1.5376, batch acc 0.9190
16:59:14.320   Training iter 550, batch loss 1.5381, batch acc 0.9202
16:59:14.870   Training iter 600, batch loss 1.5278, batch acc 0.9288
16:59:14.872 Training @ 289 epoch...
16:59:15.450   Training iter 50, batch loss 1.5328, batch acc 0.9254
16:59:16.039   Training iter 100, batch loss 1.5378, batch acc 0.9190
16:59:16.607   Training iter 150, batch loss 1.5284, batch acc 0.9310
16:59:17.178   Training iter 200, batch loss 1.5330, batch acc 0.9216
16:59:17.749   Training iter 250, batch loss 1.5330, batch acc 0.9234
16:59:18.364   Training iter 300, batch loss 1.5328, batch acc 0.9222
16:59:18.960   Training iter 350, batch loss 1.5388, batch acc 0.9184
16:59:19.529   Training iter 400, batch loss 1.5397, batch acc 0.9176
16:59:20.100   Training iter 450, batch loss 1.5342, batch acc 0.9234
16:59:20.661   Training iter 500, batch loss 1.5339, batch acc 0.9254
16:59:21.219   Training iter 550, batch loss 1.5349, batch acc 0.9196
16:59:21.780   Training iter 600, batch loss 1.5371, batch acc 0.9214
16:59:21.781 Training @ 290 epoch...
16:59:22.351   Training iter 50, batch loss 1.5406, batch acc 0.9150
16:59:22.912   Training iter 100, batch loss 1.5355, batch acc 0.9222
16:59:23.479   Training iter 150, batch loss 1.5324, batch acc 0.9274
16:59:24.039   Training iter 200, batch loss 1.5336, batch acc 0.9228
16:59:24.579   Training iter 250, batch loss 1.5343, batch acc 0.9214
16:59:25.126   Training iter 300, batch loss 1.5304, batch acc 0.9298
16:59:25.636   Training iter 350, batch loss 1.5325, batch acc 0.9226
16:59:26.168   Training iter 400, batch loss 1.5376, batch acc 0.9180
16:59:26.717   Training iter 450, batch loss 1.5337, batch acc 0.9240
16:59:27.270   Training iter 500, batch loss 1.5344, batch acc 0.9178
16:59:27.805   Training iter 550, batch loss 1.5342, batch acc 0.9242
16:59:28.337   Training iter 600, batch loss 1.5360, batch acc 0.9232
16:59:28.339 Testing @ 290 epoch...
16:59:28.385     Testing, total mean loss 1.53979, total acc 0.92100
16:59:28.385 Training @ 291 epoch...
16:59:28.929   Training iter 50, batch loss 1.5362, batch acc 0.9244
16:59:29.479   Training iter 100, batch loss 1.5306, batch acc 0.9252
16:59:29.996   Training iter 150, batch loss 1.5398, batch acc 0.9130
16:59:30.508   Training iter 200, batch loss 1.5325, batch acc 0.9254
16:59:31.013   Training iter 250, batch loss 1.5313, batch acc 0.9298
16:59:31.526   Training iter 300, batch loss 1.5368, batch acc 0.9194
16:59:32.029   Training iter 350, batch loss 1.5402, batch acc 0.9188
16:59:32.535   Training iter 400, batch loss 1.5359, batch acc 0.9174
16:59:33.045   Training iter 450, batch loss 1.5323, batch acc 0.9254
16:59:33.562   Training iter 500, batch loss 1.5324, batch acc 0.9254
16:59:34.082   Training iter 550, batch loss 1.5334, batch acc 0.9246
16:59:34.631   Training iter 600, batch loss 1.5343, batch acc 0.9216
16:59:34.633 Training @ 292 epoch...
16:59:35.175   Training iter 50, batch loss 1.5325, batch acc 0.9256
16:59:35.715   Training iter 100, batch loss 1.5324, batch acc 0.9232
16:59:36.254   Training iter 150, batch loss 1.5348, batch acc 0.9226
16:59:36.775   Training iter 200, batch loss 1.5345, batch acc 0.9206
16:59:37.308   Training iter 250, batch loss 1.5347, batch acc 0.9246
16:59:37.854   Training iter 300, batch loss 1.5333, batch acc 0.9242
16:59:38.389   Training iter 350, batch loss 1.5362, batch acc 0.9198
16:59:38.926   Training iter 400, batch loss 1.5397, batch acc 0.9170
16:59:39.487   Training iter 450, batch loss 1.5361, batch acc 0.9240
16:59:40.046   Training iter 500, batch loss 1.5300, batch acc 0.9290
16:59:40.566   Training iter 550, batch loss 1.5379, batch acc 0.9146
16:59:41.066   Training iter 600, batch loss 1.5318, batch acc 0.9240
16:59:41.068 Training @ 293 epoch...
16:59:41.575   Training iter 50, batch loss 1.5346, batch acc 0.9234
16:59:42.063   Training iter 100, batch loss 1.5339, batch acc 0.9240
16:59:42.559   Training iter 150, batch loss 1.5301, batch acc 0.9284
16:59:43.080   Training iter 200, batch loss 1.5292, batch acc 0.9278
16:59:43.625   Training iter 250, batch loss 1.5346, batch acc 0.9206
16:59:44.190   Training iter 300, batch loss 1.5339, batch acc 0.9244
16:59:44.736   Training iter 350, batch loss 1.5365, batch acc 0.9236
16:59:45.243   Training iter 400, batch loss 1.5387, batch acc 0.9180
16:59:45.778   Training iter 450, batch loss 1.5358, batch acc 0.9224
16:59:46.302   Training iter 500, batch loss 1.5329, batch acc 0.9222
16:59:46.823   Training iter 550, batch loss 1.5409, batch acc 0.9174
16:59:47.340   Training iter 600, batch loss 1.5347, batch acc 0.9234
16:59:47.342 Training @ 294 epoch...
16:59:47.842   Training iter 50, batch loss 1.5337, batch acc 0.9202
16:59:48.336   Training iter 100, batch loss 1.5331, batch acc 0.9232
16:59:48.839   Training iter 150, batch loss 1.5344, batch acc 0.9244
16:59:49.316   Training iter 200, batch loss 1.5352, batch acc 0.9228
16:59:49.814   Training iter 250, batch loss 1.5384, batch acc 0.9170
16:59:50.301   Training iter 300, batch loss 1.5314, batch acc 0.9254
16:59:50.784   Training iter 350, batch loss 1.5348, batch acc 0.9218
16:59:51.272   Training iter 400, batch loss 1.5387, batch acc 0.9198
16:59:51.760   Training iter 450, batch loss 1.5301, batch acc 0.9264
16:59:52.262   Training iter 500, batch loss 1.5332, batch acc 0.9246
16:59:52.758   Training iter 550, batch loss 1.5344, batch acc 0.9264
16:59:53.254   Training iter 600, batch loss 1.5371, batch acc 0.9184
16:59:53.256 Training @ 295 epoch...
16:59:53.737   Training iter 50, batch loss 1.5307, batch acc 0.9230
16:59:54.264   Training iter 100, batch loss 1.5313, batch acc 0.9246
16:59:54.762   Training iter 150, batch loss 1.5339, batch acc 0.9242
16:59:55.263   Training iter 200, batch loss 1.5344, batch acc 0.9208
16:59:55.746   Training iter 250, batch loss 1.5416, batch acc 0.9148
16:59:56.232   Training iter 300, batch loss 1.5338, batch acc 0.9252
16:59:56.752   Training iter 350, batch loss 1.5309, batch acc 0.9248
16:59:57.276   Training iter 400, batch loss 1.5412, batch acc 0.9128
16:59:57.787   Training iter 450, batch loss 1.5347, batch acc 0.9246
16:59:58.307   Training iter 500, batch loss 1.5316, batch acc 0.9260
16:59:58.820   Training iter 550, batch loss 1.5357, batch acc 0.9236
16:59:59.352   Training iter 600, batch loss 1.5340, batch acc 0.9232
16:59:59.354 Testing @ 295 epoch...
16:59:59.400     Testing, total mean loss 1.53944, total acc 0.92250
16:59:59.400 Training @ 296 epoch...
16:59:59.912   Training iter 50, batch loss 1.5374, batch acc 0.9188
17:00:00.440   Training iter 100, batch loss 1.5311, batch acc 0.9292
17:00:00.931   Training iter 150, batch loss 1.5359, batch acc 0.9178
17:00:01.471   Training iter 200, batch loss 1.5352, batch acc 0.9238
17:00:02.046   Training iter 250, batch loss 1.5377, batch acc 0.9192
17:00:02.613   Training iter 300, batch loss 1.5317, batch acc 0.9240
17:00:03.163   Training iter 350, batch loss 1.5336, batch acc 0.9242
17:00:03.709   Training iter 400, batch loss 1.5379, batch acc 0.9178
17:00:04.251   Training iter 450, batch loss 1.5328, batch acc 0.9220
17:00:04.805   Training iter 500, batch loss 1.5340, batch acc 0.9240
17:00:05.362   Training iter 550, batch loss 1.5358, batch acc 0.9246
17:00:05.912   Training iter 600, batch loss 1.5296, batch acc 0.9274
17:00:05.913 Training @ 297 epoch...
17:00:06.457   Training iter 50, batch loss 1.5353, batch acc 0.9214
17:00:06.972   Training iter 100, batch loss 1.5348, batch acc 0.9216
17:00:07.505   Training iter 150, batch loss 1.5320, batch acc 0.9250
17:00:08.028   Training iter 200, batch loss 1.5385, batch acc 0.9184
17:00:08.553   Training iter 250, batch loss 1.5354, batch acc 0.9190
17:00:09.087   Training iter 300, batch loss 1.5337, batch acc 0.9196
17:00:09.638   Training iter 350, batch loss 1.5361, batch acc 0.9204
17:00:10.175   Training iter 400, batch loss 1.5316, batch acc 0.9236
17:00:10.691   Training iter 450, batch loss 1.5326, batch acc 0.9272
17:00:11.217   Training iter 500, batch loss 1.5336, batch acc 0.9242
17:00:11.726   Training iter 550, batch loss 1.5370, batch acc 0.9196
17:00:12.248   Training iter 600, batch loss 1.5323, batch acc 0.9260
17:00:12.250 Training @ 298 epoch...
17:00:12.770   Training iter 50, batch loss 1.5363, batch acc 0.9220
17:00:13.286   Training iter 100, batch loss 1.5322, batch acc 0.9204
17:00:13.769   Training iter 150, batch loss 1.5338, batch acc 0.9248
17:00:14.244   Training iter 200, batch loss 1.5359, batch acc 0.9256
17:00:14.707   Training iter 250, batch loss 1.5358, batch acc 0.9172
17:00:15.168   Training iter 300, batch loss 1.5332, batch acc 0.9236
17:00:15.633   Training iter 350, batch loss 1.5355, batch acc 0.9206
17:00:16.129   Training iter 400, batch loss 1.5326, batch acc 0.9260
17:00:16.626   Training iter 450, batch loss 1.5321, batch acc 0.9266
17:00:17.123   Training iter 500, batch loss 1.5356, batch acc 0.9212
17:00:17.643   Training iter 550, batch loss 1.5390, batch acc 0.9178
17:00:18.164   Training iter 600, batch loss 1.5297, batch acc 0.9276
17:00:18.165 Training @ 299 epoch...
17:00:18.680   Training iter 50, batch loss 1.5387, batch acc 0.9170
17:00:19.196   Training iter 100, batch loss 1.5320, batch acc 0.9248
17:00:19.717   Training iter 150, batch loss 1.5286, batch acc 0.9296
17:00:20.244   Training iter 200, batch loss 1.5349, batch acc 0.9232
17:00:20.767   Training iter 250, batch loss 1.5307, batch acc 0.9260
17:00:21.287   Training iter 300, batch loss 1.5367, batch acc 0.9210
17:00:21.816   Training iter 350, batch loss 1.5353, batch acc 0.9234
17:00:22.346   Training iter 400, batch loss 1.5350, batch acc 0.9224
17:00:22.874   Training iter 450, batch loss 1.5352, batch acc 0.9164
17:00:23.408   Training iter 500, batch loss 1.5339, batch acc 0.9266
17:00:23.949   Training iter 550, batch loss 1.5368, batch acc 0.9166
17:00:24.487   Training iter 600, batch loss 1.5343, batch acc 0.9226
17:00:24.489 Training @ 300 epoch...
17:00:25.035   Training iter 50, batch loss 1.5329, batch acc 0.9236
17:00:25.550   Training iter 100, batch loss 1.5363, batch acc 0.9174
17:00:26.058   Training iter 150, batch loss 1.5357, batch acc 0.9212
17:00:26.584   Training iter 200, batch loss 1.5339, batch acc 0.9218
17:00:27.117   Training iter 250, batch loss 1.5341, batch acc 0.9260
17:00:27.695   Training iter 300, batch loss 1.5345, batch acc 0.9222
17:00:28.257   Training iter 350, batch loss 1.5360, batch acc 0.9228
17:00:28.793   Training iter 400, batch loss 1.5332, batch acc 0.9252
17:00:29.328   Training iter 450, batch loss 1.5338, batch acc 0.9242
17:00:29.859   Training iter 500, batch loss 1.5339, batch acc 0.9200
17:00:30.376   Training iter 550, batch loss 1.5346, batch acc 0.9214
17:00:30.896   Training iter 600, batch loss 1.5318, batch acc 0.9290
17:00:30.898 Testing @ 300 epoch...
17:00:30.944     Testing, total mean loss 1.54037, total acc 0.92070
17:00:30.944 Plot @ 300 epoch...
17:00:30.944 Training @ 301 epoch...
17:00:31.484   Training iter 50, batch loss 1.5369, batch acc 0.9214
17:00:32.024   Training iter 100, batch loss 1.5334, batch acc 0.9216
17:00:32.559   Training iter 150, batch loss 1.5342, batch acc 0.9252
17:00:33.091   Training iter 200, batch loss 1.5366, batch acc 0.9188
17:00:33.630   Training iter 250, batch loss 1.5359, batch acc 0.9190
17:00:34.223   Training iter 300, batch loss 1.5346, batch acc 0.9250
17:00:34.803   Training iter 350, batch loss 1.5357, batch acc 0.9224
17:00:35.383   Training iter 400, batch loss 1.5279, batch acc 0.9332
17:00:35.897   Training iter 450, batch loss 1.5357, batch acc 0.9208
17:00:36.412   Training iter 500, batch loss 1.5318, batch acc 0.9242
17:00:36.933   Training iter 550, batch loss 1.5351, batch acc 0.9236
17:00:37.468   Training iter 600, batch loss 1.5325, batch acc 0.9234
17:00:37.470 Training @ 302 epoch...
17:00:38.008   Training iter 50, batch loss 1.5356, batch acc 0.9238
17:00:38.542   Training iter 100, batch loss 1.5388, batch acc 0.9156
17:00:39.069   Training iter 150, batch loss 1.5307, batch acc 0.9272
17:00:39.582   Training iter 200, batch loss 1.5326, batch acc 0.9218
17:00:40.107   Training iter 250, batch loss 1.5365, batch acc 0.9214
17:00:40.629   Training iter 300, batch loss 1.5314, batch acc 0.9238
17:00:41.138   Training iter 350, batch loss 1.5323, batch acc 0.9272
17:00:41.657   Training iter 400, batch loss 1.5355, batch acc 0.9196
17:00:42.180   Training iter 450, batch loss 1.5312, batch acc 0.9246
17:00:42.719   Training iter 500, batch loss 1.5353, batch acc 0.9256
17:00:43.253   Training iter 550, batch loss 1.5345, batch acc 0.9210
17:00:43.774   Training iter 600, batch loss 1.5342, batch acc 0.9232
17:00:43.776 Training @ 303 epoch...
17:00:44.336   Training iter 50, batch loss 1.5318, batch acc 0.9242
17:00:44.883   Training iter 100, batch loss 1.5382, batch acc 0.9210
17:00:45.429   Training iter 150, batch loss 1.5357, batch acc 0.9192
17:00:45.954   Training iter 200, batch loss 1.5325, batch acc 0.9294
17:00:46.470   Training iter 250, batch loss 1.5364, batch acc 0.9194
17:00:46.960   Training iter 300, batch loss 1.5380, batch acc 0.9214
17:00:47.472   Training iter 350, batch loss 1.5324, batch acc 0.9202
17:00:47.988   Training iter 400, batch loss 1.5359, batch acc 0.9200
17:00:48.524   Training iter 450, batch loss 1.5272, batch acc 0.9316
17:00:49.067   Training iter 500, batch loss 1.5330, batch acc 0.9264
17:00:49.592   Training iter 550, batch loss 1.5347, batch acc 0.9226
17:00:50.135   Training iter 600, batch loss 1.5349, batch acc 0.9184
17:00:50.136 Training @ 304 epoch...
17:00:50.651   Training iter 50, batch loss 1.5326, batch acc 0.9258
17:00:51.157   Training iter 100, batch loss 1.5316, batch acc 0.9266
17:00:51.659   Training iter 150, batch loss 1.5311, batch acc 0.9264
17:00:52.164   Training iter 200, batch loss 1.5332, batch acc 0.9238
17:00:52.699   Training iter 250, batch loss 1.5312, batch acc 0.9236
17:00:53.235   Training iter 300, batch loss 1.5348, batch acc 0.9202
17:00:53.757   Training iter 350, batch loss 1.5390, batch acc 0.9204
17:00:54.298   Training iter 400, batch loss 1.5330, batch acc 0.9200
17:00:54.814   Training iter 450, batch loss 1.5334, batch acc 0.9216
17:00:55.328   Training iter 500, batch loss 1.5320, batch acc 0.9244
17:00:55.858   Training iter 550, batch loss 1.5401, batch acc 0.9170
17:00:56.407   Training iter 600, batch loss 1.5364, batch acc 0.9206
17:00:56.409 Training @ 305 epoch...
17:00:56.931   Training iter 50, batch loss 1.5328, batch acc 0.9246
17:00:57.447   Training iter 100, batch loss 1.5288, batch acc 0.9288
17:00:57.974   Training iter 150, batch loss 1.5331, batch acc 0.9282
17:00:58.474   Training iter 200, batch loss 1.5301, batch acc 0.9278
17:00:58.975   Training iter 250, batch loss 1.5340, batch acc 0.9242
17:00:59.497   Training iter 300, batch loss 1.5321, batch acc 0.9250
17:00:59.994   Training iter 350, batch loss 1.5364, batch acc 0.9200
17:01:00.482   Training iter 400, batch loss 1.5353, batch acc 0.9204
17:01:00.966   Training iter 450, batch loss 1.5357, batch acc 0.9200
17:01:01.498   Training iter 500, batch loss 1.5346, batch acc 0.9236
17:01:02.084   Training iter 550, batch loss 1.5356, batch acc 0.9182
17:01:02.659   Training iter 600, batch loss 1.5394, batch acc 0.9168
17:01:02.660 Testing @ 305 epoch...
17:01:02.710     Testing, total mean loss 1.53987, total acc 0.92110
17:01:02.710 Training @ 306 epoch...
17:01:03.283   Training iter 50, batch loss 1.5323, batch acc 0.9254
17:01:03.851   Training iter 100, batch loss 1.5358, batch acc 0.9222
17:01:04.431   Training iter 150, batch loss 1.5309, batch acc 0.9248
17:01:04.996   Training iter 200, batch loss 1.5327, batch acc 0.9232
17:01:05.555   Training iter 250, batch loss 1.5331, batch acc 0.9214
17:01:06.101   Training iter 300, batch loss 1.5313, batch acc 0.9278
17:01:06.657   Training iter 350, batch loss 1.5309, batch acc 0.9292
17:01:07.181   Training iter 400, batch loss 1.5344, batch acc 0.9222
17:01:07.692   Training iter 450, batch loss 1.5382, batch acc 0.9182
17:01:08.188   Training iter 500, batch loss 1.5344, batch acc 0.9252
17:01:08.682   Training iter 550, batch loss 1.5368, batch acc 0.9208
17:01:09.193   Training iter 600, batch loss 1.5372, batch acc 0.9164
17:01:09.195 Training @ 307 epoch...
17:01:09.720   Training iter 50, batch loss 1.5325, batch acc 0.9254
17:01:10.237   Training iter 100, batch loss 1.5355, batch acc 0.9248
17:01:10.753   Training iter 150, batch loss 1.5364, batch acc 0.9210
17:01:11.279   Training iter 200, batch loss 1.5300, batch acc 0.9276
17:01:11.795   Training iter 250, batch loss 1.5324, batch acc 0.9244
17:01:12.305   Training iter 300, batch loss 1.5365, batch acc 0.9218
17:01:12.780   Training iter 350, batch loss 1.5338, batch acc 0.9216
17:01:13.260   Training iter 400, batch loss 1.5333, batch acc 0.9248
17:01:13.725   Training iter 450, batch loss 1.5307, batch acc 0.9242
17:01:14.218   Training iter 500, batch loss 1.5291, batch acc 0.9262
17:01:14.717   Training iter 550, batch loss 1.5380, batch acc 0.9158
17:01:15.231   Training iter 600, batch loss 1.5378, batch acc 0.9184
17:01:15.232 Training @ 308 epoch...
17:01:15.743   Training iter 50, batch loss 1.5303, batch acc 0.9264
17:01:16.258   Training iter 100, batch loss 1.5371, batch acc 0.9184
17:01:16.774   Training iter 150, batch loss 1.5316, batch acc 0.9290
17:01:17.290   Training iter 200, batch loss 1.5291, batch acc 0.9262
17:01:17.818   Training iter 250, batch loss 1.5327, batch acc 0.9246
17:01:18.349   Training iter 300, batch loss 1.5380, batch acc 0.9190
17:01:18.882   Training iter 350, batch loss 1.5326, batch acc 0.9266
17:01:19.428   Training iter 400, batch loss 1.5361, batch acc 0.9220
17:01:20.005   Training iter 450, batch loss 1.5373, batch acc 0.9188
17:01:20.549   Training iter 500, batch loss 1.5330, batch acc 0.9228
17:01:21.094   Training iter 550, batch loss 1.5335, batch acc 0.9230
17:01:21.611   Training iter 600, batch loss 1.5343, batch acc 0.9238
17:01:21.612 Training @ 309 epoch...
17:01:22.129   Training iter 50, batch loss 1.5270, batch acc 0.9314
17:01:22.653   Training iter 100, batch loss 1.5357, batch acc 0.9226
17:01:23.177   Training iter 150, batch loss 1.5309, batch acc 0.9288
17:01:23.704   Training iter 200, batch loss 1.5334, batch acc 0.9242
17:01:24.235   Training iter 250, batch loss 1.5337, batch acc 0.9238
17:01:24.774   Training iter 300, batch loss 1.5357, batch acc 0.9210
17:01:25.302   Training iter 350, batch loss 1.5361, batch acc 0.9194
17:01:25.832   Training iter 400, batch loss 1.5366, batch acc 0.9208
17:01:26.371   Training iter 450, batch loss 1.5332, batch acc 0.9248
17:01:26.915   Training iter 500, batch loss 1.5423, batch acc 0.9120
17:01:27.448   Training iter 550, batch loss 1.5275, batch acc 0.9310
17:01:28.012   Training iter 600, batch loss 1.5335, batch acc 0.9224
17:01:28.014 Training @ 310 epoch...
17:01:28.680   Training iter 50, batch loss 1.5368, batch acc 0.9222
17:01:29.288   Training iter 100, batch loss 1.5302, batch acc 0.9232
17:01:29.838   Training iter 150, batch loss 1.5331, batch acc 0.9296
17:01:30.372   Training iter 200, batch loss 1.5336, batch acc 0.9232
17:01:30.902   Training iter 250, batch loss 1.5309, batch acc 0.9234
17:01:31.435   Training iter 300, batch loss 1.5335, batch acc 0.9242
17:01:31.928   Training iter 350, batch loss 1.5350, batch acc 0.9198
17:01:32.416   Training iter 400, batch loss 1.5338, batch acc 0.9230
17:01:32.891   Training iter 450, batch loss 1.5341, batch acc 0.9222
17:01:33.375   Training iter 500, batch loss 1.5357, batch acc 0.9198
17:01:33.873   Training iter 550, batch loss 1.5346, batch acc 0.9240
17:01:34.416   Training iter 600, batch loss 1.5324, batch acc 0.9258
17:01:34.417 Testing @ 310 epoch...
17:01:34.464     Testing, total mean loss 1.53874, total acc 0.92270
17:01:34.464 Training @ 311 epoch...
17:01:34.986   Training iter 50, batch loss 1.5379, batch acc 0.9212
17:01:35.510   Training iter 100, batch loss 1.5356, batch acc 0.9158
17:01:36.039   Training iter 150, batch loss 1.5303, batch acc 0.9286
17:01:36.582   Training iter 200, batch loss 1.5325, batch acc 0.9256
17:01:37.118   Training iter 250, batch loss 1.5341, batch acc 0.9204
17:01:37.677   Training iter 300, batch loss 1.5289, batch acc 0.9258
17:01:38.229   Training iter 350, batch loss 1.5324, batch acc 0.9270
17:01:38.765   Training iter 400, batch loss 1.5299, batch acc 0.9312
17:01:39.308   Training iter 450, batch loss 1.5352, batch acc 0.9194
17:01:39.829   Training iter 500, batch loss 1.5321, batch acc 0.9264
17:01:40.373   Training iter 550, batch loss 1.5383, batch acc 0.9154
17:01:40.906   Training iter 600, batch loss 1.5347, batch acc 0.9246
17:01:40.908 Training @ 312 epoch...
17:01:41.459   Training iter 50, batch loss 1.5314, batch acc 0.9258
17:01:42.016   Training iter 100, batch loss 1.5323, batch acc 0.9226
17:01:42.585   Training iter 150, batch loss 1.5291, batch acc 0.9306
17:01:43.148   Training iter 200, batch loss 1.5326, batch acc 0.9232
17:01:43.722   Training iter 250, batch loss 1.5381, batch acc 0.9178
17:01:44.298   Training iter 300, batch loss 1.5391, batch acc 0.9154
17:01:44.843   Training iter 350, batch loss 1.5333, batch acc 0.9230
17:01:45.389   Training iter 400, batch loss 1.5299, batch acc 0.9308
17:01:45.928   Training iter 450, batch loss 1.5341, batch acc 0.9204
17:01:46.476   Training iter 500, batch loss 1.5317, batch acc 0.9276
17:01:47.013   Training iter 550, batch loss 1.5352, batch acc 0.9224
17:01:47.547   Training iter 600, batch loss 1.5365, batch acc 0.9198
17:01:47.549 Training @ 313 epoch...
17:01:48.069   Training iter 50, batch loss 1.5369, batch acc 0.9188
17:01:48.613   Training iter 100, batch loss 1.5339, batch acc 0.9210
17:01:49.145   Training iter 150, batch loss 1.5341, batch acc 0.9220
17:01:49.640   Training iter 200, batch loss 1.5326, batch acc 0.9206
17:01:50.140   Training iter 250, batch loss 1.5332, batch acc 0.9234
17:01:50.631   Training iter 300, batch loss 1.5281, batch acc 0.9332
17:01:51.134   Training iter 350, batch loss 1.5314, batch acc 0.9282
17:01:51.639   Training iter 400, batch loss 1.5303, batch acc 0.9258
17:01:52.144   Training iter 450, batch loss 1.5353, batch acc 0.9216
17:01:52.646   Training iter 500, batch loss 1.5386, batch acc 0.9162
17:01:53.155   Training iter 550, batch loss 1.5327, batch acc 0.9262
17:01:53.660   Training iter 600, batch loss 1.5351, batch acc 0.9228
17:01:53.662 Training @ 314 epoch...
17:01:54.176   Training iter 50, batch loss 1.5339, batch acc 0.9184
17:01:54.682   Training iter 100, batch loss 1.5332, batch acc 0.9248
17:01:55.201   Training iter 150, batch loss 1.5361, batch acc 0.9236
17:01:55.712   Training iter 200, batch loss 1.5358, batch acc 0.9194
17:01:56.234   Training iter 250, batch loss 1.5310, batch acc 0.9296
17:01:56.757   Training iter 300, batch loss 1.5331, batch acc 0.9236
17:01:57.290   Training iter 350, batch loss 1.5302, batch acc 0.9256
17:01:57.845   Training iter 400, batch loss 1.5313, batch acc 0.9300
17:01:58.414   Training iter 450, batch loss 1.5403, batch acc 0.9174
17:01:58.989   Training iter 500, batch loss 1.5329, batch acc 0.9212
17:01:59.567   Training iter 550, batch loss 1.5326, batch acc 0.9228
17:02:00.144   Training iter 600, batch loss 1.5304, batch acc 0.9288
17:02:00.145 Training @ 315 epoch...
17:02:00.728   Training iter 50, batch loss 1.5287, batch acc 0.9324
17:02:01.310   Training iter 100, batch loss 1.5320, batch acc 0.9260
17:02:01.904   Training iter 150, batch loss 1.5299, batch acc 0.9274
17:02:02.487   Training iter 200, batch loss 1.5354, batch acc 0.9228
17:02:03.062   Training iter 250, batch loss 1.5317, batch acc 0.9254
17:02:03.643   Training iter 300, batch loss 1.5376, batch acc 0.9144
17:02:04.209   Training iter 350, batch loss 1.5355, batch acc 0.9236
17:02:04.775   Training iter 400, batch loss 1.5299, batch acc 0.9278
17:02:05.317   Training iter 450, batch loss 1.5307, batch acc 0.9240
17:02:05.883   Training iter 500, batch loss 1.5391, batch acc 0.9170
17:02:06.450   Training iter 550, batch loss 1.5355, batch acc 0.9210
17:02:06.988   Training iter 600, batch loss 1.5332, batch acc 0.9234
17:02:06.989 Testing @ 315 epoch...
17:02:07.038     Testing, total mean loss 1.53959, total acc 0.92190
17:02:07.038 Training @ 316 epoch...
17:02:07.590   Training iter 50, batch loss 1.5361, batch acc 0.9192
17:02:08.135   Training iter 100, batch loss 1.5302, batch acc 0.9320
17:02:08.667   Training iter 150, batch loss 1.5332, batch acc 0.9224
17:02:09.202   Training iter 200, batch loss 1.5326, batch acc 0.9208
17:02:09.724   Training iter 250, batch loss 1.5346, batch acc 0.9238
17:02:10.256   Training iter 300, batch loss 1.5343, batch acc 0.9234
17:02:10.807   Training iter 350, batch loss 1.5343, batch acc 0.9242
17:02:11.345   Training iter 400, batch loss 1.5314, batch acc 0.9258
17:02:11.859   Training iter 450, batch loss 1.5333, batch acc 0.9246
17:02:12.375   Training iter 500, batch loss 1.5344, batch acc 0.9270
17:02:12.899   Training iter 550, batch loss 1.5342, batch acc 0.9204
17:02:13.412   Training iter 600, batch loss 1.5306, batch acc 0.9238
17:02:13.414 Training @ 317 epoch...
17:02:13.949   Training iter 50, batch loss 1.5368, batch acc 0.9218
17:02:14.536   Training iter 100, batch loss 1.5322, batch acc 0.9254
17:02:15.129   Training iter 150, batch loss 1.5326, batch acc 0.9192
17:02:15.720   Training iter 200, batch loss 1.5323, batch acc 0.9256
17:02:16.306   Training iter 250, batch loss 1.5359, batch acc 0.9208
17:02:16.868   Training iter 300, batch loss 1.5336, batch acc 0.9212
17:02:17.440   Training iter 350, batch loss 1.5329, batch acc 0.9248
17:02:18.026   Training iter 400, batch loss 1.5332, batch acc 0.9260
17:02:18.587   Training iter 450, batch loss 1.5301, batch acc 0.9282
17:02:19.148   Training iter 500, batch loss 1.5322, batch acc 0.9258
17:02:19.703   Training iter 550, batch loss 1.5376, batch acc 0.9188
17:02:20.216   Training iter 600, batch loss 1.5293, batch acc 0.9282
17:02:20.217 Training @ 318 epoch...
17:02:20.725   Training iter 50, batch loss 1.5350, batch acc 0.9186
17:02:21.219   Training iter 100, batch loss 1.5362, batch acc 0.9234
17:02:21.693   Training iter 150, batch loss 1.5318, batch acc 0.9256
17:02:22.174   Training iter 200, batch loss 1.5352, batch acc 0.9180
17:02:22.657   Training iter 250, batch loss 1.5335, batch acc 0.9204
17:02:23.140   Training iter 300, batch loss 1.5329, batch acc 0.9266
17:02:23.662   Training iter 350, batch loss 1.5343, batch acc 0.9212
17:02:24.164   Training iter 400, batch loss 1.5310, batch acc 0.9244
17:02:24.674   Training iter 450, batch loss 1.5310, batch acc 0.9278
17:02:25.183   Training iter 500, batch loss 1.5302, batch acc 0.9288
17:02:25.687   Training iter 550, batch loss 1.5358, batch acc 0.9190
17:02:26.193   Training iter 600, batch loss 1.5307, batch acc 0.9314
17:02:26.195 Training @ 319 epoch...
17:02:26.694   Training iter 50, batch loss 1.5370, batch acc 0.9190
17:02:27.186   Training iter 100, batch loss 1.5348, batch acc 0.9218
17:02:27.676   Training iter 150, batch loss 1.5375, batch acc 0.9158
17:02:28.158   Training iter 200, batch loss 1.5327, batch acc 0.9254
17:02:28.649   Training iter 250, batch loss 1.5247, batch acc 0.9350
17:02:29.156   Training iter 300, batch loss 1.5350, batch acc 0.9214
17:02:29.642   Training iter 350, batch loss 1.5297, batch acc 0.9256
17:02:30.142   Training iter 400, batch loss 1.5330, batch acc 0.9242
17:02:30.636   Training iter 450, batch loss 1.5326, batch acc 0.9230
17:02:31.139   Training iter 500, batch loss 1.5347, batch acc 0.9204
17:02:31.632   Training iter 550, batch loss 1.5365, batch acc 0.9204
17:02:32.128   Training iter 600, batch loss 1.5285, batch acc 0.9288
17:02:32.130 Training @ 320 epoch...
17:02:32.643   Training iter 50, batch loss 1.5319, batch acc 0.9220
17:02:33.147   Training iter 100, batch loss 1.5303, batch acc 0.9318
17:02:33.637   Training iter 150, batch loss 1.5295, batch acc 0.9290
17:02:34.129   Training iter 200, batch loss 1.5342, batch acc 0.9244
17:02:34.629   Training iter 250, batch loss 1.5291, batch acc 0.9306
17:02:35.110   Training iter 300, batch loss 1.5370, batch acc 0.9172
17:02:35.595   Training iter 350, batch loss 1.5354, batch acc 0.9200
17:02:36.080   Training iter 400, batch loss 1.5296, batch acc 0.9256
17:02:36.569   Training iter 450, batch loss 1.5334, batch acc 0.9218
17:02:37.037   Training iter 500, batch loss 1.5342, batch acc 0.9186
17:02:37.515   Training iter 550, batch loss 1.5330, batch acc 0.9232
17:02:38.014   Training iter 600, batch loss 1.5375, batch acc 0.9226
17:02:38.016 Testing @ 320 epoch...
17:02:38.062     Testing, total mean loss 1.53861, total acc 0.92350
17:02:38.062 Training @ 321 epoch...
17:02:38.567   Training iter 50, batch loss 1.5327, batch acc 0.9226
17:02:39.073   Training iter 100, batch loss 1.5373, batch acc 0.9188
17:02:39.602   Training iter 150, batch loss 1.5356, batch acc 0.9204
17:02:40.078   Training iter 200, batch loss 1.5293, batch acc 0.9286
17:02:40.558   Training iter 250, batch loss 1.5359, batch acc 0.9224
17:02:41.007   Training iter 300, batch loss 1.5318, batch acc 0.9226
17:02:41.489   Training iter 350, batch loss 1.5298, batch acc 0.9264
17:02:41.979   Training iter 400, batch loss 1.5370, batch acc 0.9198
17:02:42.487   Training iter 450, batch loss 1.5292, batch acc 0.9298
17:02:42.992   Training iter 500, batch loss 1.5359, batch acc 0.9224
17:02:43.515   Training iter 550, batch loss 1.5336, batch acc 0.9234
17:02:44.040   Training iter 600, batch loss 1.5269, batch acc 0.9344
17:02:44.041 Training @ 322 epoch...
17:02:44.573   Training iter 50, batch loss 1.5367, batch acc 0.9174
17:02:45.087   Training iter 100, batch loss 1.5343, batch acc 0.9200
17:02:45.613   Training iter 150, batch loss 1.5348, batch acc 0.9222
17:02:46.133   Training iter 200, batch loss 1.5368, batch acc 0.9186
17:02:46.649   Training iter 250, batch loss 1.5344, batch acc 0.9198
17:02:47.159   Training iter 300, batch loss 1.5322, batch acc 0.9254
17:02:47.668   Training iter 350, batch loss 1.5310, batch acc 0.9274
17:02:48.191   Training iter 400, batch loss 1.5272, batch acc 0.9352
17:02:48.710   Training iter 450, batch loss 1.5339, batch acc 0.9216
17:02:49.229   Training iter 500, batch loss 1.5313, batch acc 0.9254
17:02:49.746   Training iter 550, batch loss 1.5331, batch acc 0.9274
17:02:50.276   Training iter 600, batch loss 1.5290, batch acc 0.9294
17:02:50.278 Training @ 323 epoch...
17:02:50.800   Training iter 50, batch loss 1.5347, batch acc 0.9214
17:02:51.326   Training iter 100, batch loss 1.5306, batch acc 0.9282
17:02:51.855   Training iter 150, batch loss 1.5291, batch acc 0.9292
17:02:52.373   Training iter 200, batch loss 1.5335, batch acc 0.9220
17:02:52.885   Training iter 250, batch loss 1.5358, batch acc 0.9210
17:02:53.397   Training iter 300, batch loss 1.5250, batch acc 0.9330
17:02:53.898   Training iter 350, batch loss 1.5324, batch acc 0.9264
17:02:54.448   Training iter 400, batch loss 1.5334, batch acc 0.9248
17:02:54.985   Training iter 450, batch loss 1.5369, batch acc 0.9176
17:02:55.539   Training iter 500, batch loss 1.5371, batch acc 0.9200
17:02:56.074   Training iter 550, batch loss 1.5348, batch acc 0.9250
17:02:56.633   Training iter 600, batch loss 1.5302, batch acc 0.9270
17:02:56.635 Training @ 324 epoch...
17:02:57.160   Training iter 50, batch loss 1.5381, batch acc 0.9154
17:02:57.725   Training iter 100, batch loss 1.5283, batch acc 0.9284
17:02:58.307   Training iter 150, batch loss 1.5341, batch acc 0.9220
17:02:58.910   Training iter 200, batch loss 1.5277, batch acc 0.9334
17:02:59.516   Training iter 250, batch loss 1.5322, batch acc 0.9268
17:03:00.032   Training iter 300, batch loss 1.5311, batch acc 0.9262
17:03:00.545   Training iter 350, batch loss 1.5330, batch acc 0.9250
17:03:01.081   Training iter 400, batch loss 1.5373, batch acc 0.9200
17:03:01.642   Training iter 450, batch loss 1.5333, batch acc 0.9284
17:03:02.226   Training iter 500, batch loss 1.5355, batch acc 0.9194
17:03:02.757   Training iter 550, batch loss 1.5328, batch acc 0.9232
17:03:03.275   Training iter 600, batch loss 1.5302, batch acc 0.9278
17:03:03.276 Training @ 325 epoch...
17:03:03.802   Training iter 50, batch loss 1.5367, batch acc 0.9182
17:03:04.350   Training iter 100, batch loss 1.5307, batch acc 0.9296
17:03:04.955   Training iter 150, batch loss 1.5314, batch acc 0.9250
17:03:05.497   Training iter 200, batch loss 1.5353, batch acc 0.9202
17:03:06.050   Training iter 250, batch loss 1.5327, batch acc 0.9210
17:03:06.598   Training iter 300, batch loss 1.5358, batch acc 0.9232
17:03:07.163   Training iter 350, batch loss 1.5286, batch acc 0.9296
17:03:07.758   Training iter 400, batch loss 1.5321, batch acc 0.9244
17:03:08.330   Training iter 450, batch loss 1.5338, batch acc 0.9244
17:03:08.855   Training iter 500, batch loss 1.5319, batch acc 0.9258
17:03:09.386   Training iter 550, batch loss 1.5303, batch acc 0.9260
17:03:09.904   Training iter 600, batch loss 1.5334, batch acc 0.9260
17:03:09.906 Testing @ 325 epoch...
17:03:09.954     Testing, total mean loss 1.53855, total acc 0.92280
17:03:09.954 Training @ 326 epoch...
17:03:10.486   Training iter 50, batch loss 1.5321, batch acc 0.9212
17:03:11.004   Training iter 100, batch loss 1.5305, batch acc 0.9242
17:03:11.563   Training iter 150, batch loss 1.5320, batch acc 0.9210
17:03:12.110   Training iter 200, batch loss 1.5318, batch acc 0.9260
17:03:12.701   Training iter 250, batch loss 1.5350, batch acc 0.9194
17:03:13.244   Training iter 300, batch loss 1.5311, batch acc 0.9268
17:03:13.785   Training iter 350, batch loss 1.5313, batch acc 0.9264
17:03:14.347   Training iter 400, batch loss 1.5309, batch acc 0.9294
17:03:14.877   Training iter 450, batch loss 1.5325, batch acc 0.9264
17:03:15.430   Training iter 500, batch loss 1.5363, batch acc 0.9226
17:03:15.993   Training iter 550, batch loss 1.5350, batch acc 0.9242
17:03:16.561   Training iter 600, batch loss 1.5344, batch acc 0.9198
17:03:16.563 Training @ 327 epoch...
17:03:17.117   Training iter 50, batch loss 1.5338, batch acc 0.9240
17:03:17.658   Training iter 100, batch loss 1.5292, batch acc 0.9274
17:03:18.250   Training iter 150, batch loss 1.5283, batch acc 0.9320
17:03:18.812   Training iter 200, batch loss 1.5330, batch acc 0.9252
17:03:19.387   Training iter 250, batch loss 1.5338, batch acc 0.9254
17:03:19.966   Training iter 300, batch loss 1.5295, batch acc 0.9296
17:03:20.529   Training iter 350, batch loss 1.5339, batch acc 0.9200
17:03:21.095   Training iter 400, batch loss 1.5356, batch acc 0.9240
17:03:21.675   Training iter 450, batch loss 1.5364, batch acc 0.9154
17:03:22.245   Training iter 500, batch loss 1.5336, batch acc 0.9194
17:03:22.818   Training iter 550, batch loss 1.5338, batch acc 0.9250
17:03:23.395   Training iter 600, batch loss 1.5317, batch acc 0.9250
17:03:23.397 Training @ 328 epoch...
17:03:23.973   Training iter 50, batch loss 1.5307, batch acc 0.9250
17:03:24.544   Training iter 100, batch loss 1.5358, batch acc 0.9228
17:03:25.073   Training iter 150, batch loss 1.5373, batch acc 0.9216
17:03:25.612   Training iter 200, batch loss 1.5321, batch acc 0.9238
17:03:26.164   Training iter 250, batch loss 1.5285, batch acc 0.9262
17:03:26.737   Training iter 300, batch loss 1.5350, batch acc 0.9238
17:03:27.290   Training iter 350, batch loss 1.5308, batch acc 0.9256
17:03:27.825   Training iter 400, batch loss 1.5345, batch acc 0.9230
17:03:28.345   Training iter 450, batch loss 1.5283, batch acc 0.9296
17:03:28.873   Training iter 500, batch loss 1.5325, batch acc 0.9190
17:03:29.416   Training iter 550, batch loss 1.5321, batch acc 0.9246
17:03:29.949   Training iter 600, batch loss 1.5330, batch acc 0.9234
17:03:29.950 Training @ 329 epoch...
17:03:30.485   Training iter 50, batch loss 1.5302, batch acc 0.9270
17:03:31.006   Training iter 100, batch loss 1.5329, batch acc 0.9240
17:03:31.540   Training iter 150, batch loss 1.5325, batch acc 0.9238
17:03:32.072   Training iter 200, batch loss 1.5294, batch acc 0.9274
17:03:32.596   Training iter 250, batch loss 1.5355, batch acc 0.9268
17:03:33.150   Training iter 300, batch loss 1.5379, batch acc 0.9134
17:03:33.707   Training iter 350, batch loss 1.5312, batch acc 0.9224
17:03:34.262   Training iter 400, batch loss 1.5351, batch acc 0.9214
17:03:34.826   Training iter 450, batch loss 1.5270, batch acc 0.9306
17:03:35.388   Training iter 500, batch loss 1.5299, batch acc 0.9282
17:03:35.923   Training iter 550, batch loss 1.5396, batch acc 0.9192
17:03:36.472   Training iter 600, batch loss 1.5300, batch acc 0.9300
17:03:36.474 Training @ 330 epoch...
17:03:37.022   Training iter 50, batch loss 1.5335, batch acc 0.9208
17:03:37.589   Training iter 100, batch loss 1.5344, batch acc 0.9280
17:03:38.119   Training iter 150, batch loss 1.5318, batch acc 0.9210
17:03:38.641   Training iter 200, batch loss 1.5372, batch acc 0.9226
17:03:39.170   Training iter 250, batch loss 1.5279, batch acc 0.9264
17:03:39.677   Training iter 300, batch loss 1.5360, batch acc 0.9206
17:03:40.155   Training iter 350, batch loss 1.5293, batch acc 0.9272
17:03:40.637   Training iter 400, batch loss 1.5366, batch acc 0.9220
17:03:41.133   Training iter 450, batch loss 1.5301, batch acc 0.9280
17:03:41.652   Training iter 500, batch loss 1.5297, batch acc 0.9252
17:03:42.185   Training iter 550, batch loss 1.5312, batch acc 0.9252
17:03:42.767   Training iter 600, batch loss 1.5321, batch acc 0.9264
17:03:42.769 Testing @ 330 epoch...
17:03:42.815     Testing, total mean loss 1.53927, total acc 0.92160
17:03:42.815 Training @ 331 epoch...
17:03:43.404   Training iter 50, batch loss 1.5335, batch acc 0.9242
17:03:43.983   Training iter 100, batch loss 1.5312, batch acc 0.9264
17:03:44.528   Training iter 150, batch loss 1.5331, batch acc 0.9260
17:03:45.027   Training iter 200, batch loss 1.5320, batch acc 0.9254
17:03:45.544   Training iter 250, batch loss 1.5339, batch acc 0.9236
17:03:46.061   Training iter 300, batch loss 1.5291, batch acc 0.9272
17:03:46.571   Training iter 350, batch loss 1.5322, batch acc 0.9272
17:03:47.081   Training iter 400, batch loss 1.5311, batch acc 0.9254
17:03:47.604   Training iter 450, batch loss 1.5316, batch acc 0.9218
17:03:48.134   Training iter 500, batch loss 1.5322, batch acc 0.9238
17:03:48.654   Training iter 550, batch loss 1.5332, batch acc 0.9240
17:03:49.180   Training iter 600, batch loss 1.5364, batch acc 0.9172
17:03:49.182 Training @ 332 epoch...
17:03:49.717   Training iter 50, batch loss 1.5322, batch acc 0.9252
17:03:50.249   Training iter 100, batch loss 1.5283, batch acc 0.9286
17:03:50.788   Training iter 150, batch loss 1.5287, batch acc 0.9314
17:03:51.325   Training iter 200, batch loss 1.5318, batch acc 0.9252
17:03:51.878   Training iter 250, batch loss 1.5326, batch acc 0.9232
17:03:52.418   Training iter 300, batch loss 1.5320, batch acc 0.9204
17:03:52.953   Training iter 350, batch loss 1.5300, batch acc 0.9298
17:03:53.463   Training iter 400, batch loss 1.5361, batch acc 0.9230
17:03:53.972   Training iter 450, batch loss 1.5338, batch acc 0.9190
17:03:54.491   Training iter 500, batch loss 1.5369, batch acc 0.9202
17:03:55.003   Training iter 550, batch loss 1.5315, batch acc 0.9260
17:03:55.513   Training iter 600, batch loss 1.5344, batch acc 0.9218
17:03:55.515 Training @ 333 epoch...
17:03:56.028   Training iter 50, batch loss 1.5331, batch acc 0.9260
17:03:56.555   Training iter 100, batch loss 1.5328, batch acc 0.9240
17:03:57.104   Training iter 150, batch loss 1.5296, batch acc 0.9320
17:03:57.655   Training iter 200, batch loss 1.5324, batch acc 0.9282
17:03:58.218   Training iter 250, batch loss 1.5337, batch acc 0.9234
17:03:58.761   Training iter 300, batch loss 1.5296, batch acc 0.9272
17:03:59.299   Training iter 350, batch loss 1.5320, batch acc 0.9248
17:03:59.817   Training iter 400, batch loss 1.5353, batch acc 0.9182
17:04:00.337   Training iter 450, batch loss 1.5315, batch acc 0.9248
17:04:00.869   Training iter 500, batch loss 1.5314, batch acc 0.9274
17:04:01.405   Training iter 550, batch loss 1.5330, batch acc 0.9224
17:04:01.994   Training iter 600, batch loss 1.5334, batch acc 0.9218
17:04:01.995 Training @ 334 epoch...
17:04:02.543   Training iter 50, batch loss 1.5326, batch acc 0.9296
17:04:03.085   Training iter 100, batch loss 1.5318, batch acc 0.9276
17:04:03.641   Training iter 150, batch loss 1.5330, batch acc 0.9230
17:04:04.170   Training iter 200, batch loss 1.5295, batch acc 0.9276
17:04:04.699   Training iter 250, batch loss 1.5345, batch acc 0.9164
17:04:05.218   Training iter 300, batch loss 1.5353, batch acc 0.9216
17:04:05.750   Training iter 350, batch loss 1.5309, batch acc 0.9258
17:04:06.323   Training iter 400, batch loss 1.5349, batch acc 0.9182
17:04:06.886   Training iter 450, batch loss 1.5313, batch acc 0.9288
17:04:07.428   Training iter 500, batch loss 1.5311, batch acc 0.9252
17:04:07.951   Training iter 550, batch loss 1.5298, batch acc 0.9294
17:04:08.471   Training iter 600, batch loss 1.5322, batch acc 0.9248
17:04:08.473 Training @ 335 epoch...
17:04:08.991   Training iter 50, batch loss 1.5306, batch acc 0.9250
17:04:09.515   Training iter 100, batch loss 1.5317, batch acc 0.9232
17:04:10.037   Training iter 150, batch loss 1.5331, batch acc 0.9260
17:04:10.595   Training iter 200, batch loss 1.5297, batch acc 0.9282
17:04:11.149   Training iter 250, batch loss 1.5285, batch acc 0.9292
17:04:11.708   Training iter 300, batch loss 1.5323, batch acc 0.9220
17:04:12.230   Training iter 350, batch loss 1.5358, batch acc 0.9158
17:04:12.784   Training iter 400, batch loss 1.5315, batch acc 0.9266
17:04:13.295   Training iter 450, batch loss 1.5355, batch acc 0.9232
17:04:13.818   Training iter 500, batch loss 1.5321, batch acc 0.9294
17:04:14.345   Training iter 550, batch loss 1.5349, batch acc 0.9182
17:04:14.866   Training iter 600, batch loss 1.5305, batch acc 0.9274
17:04:14.868 Testing @ 335 epoch...
17:04:14.916     Testing, total mean loss 1.53823, total acc 0.92250
17:04:14.916 Training @ 336 epoch...
17:04:15.451   Training iter 50, batch loss 1.5269, batch acc 0.9290
17:04:15.991   Training iter 100, batch loss 1.5325, batch acc 0.9196
17:04:16.532   Training iter 150, batch loss 1.5306, batch acc 0.9264
17:04:17.061   Training iter 200, batch loss 1.5356, batch acc 0.9244
17:04:17.590   Training iter 250, batch loss 1.5358, batch acc 0.9190
17:04:18.125   Training iter 300, batch loss 1.5329, batch acc 0.9286
17:04:18.648   Training iter 350, batch loss 1.5293, batch acc 0.9276
17:04:19.174   Training iter 400, batch loss 1.5299, batch acc 0.9288
17:04:19.690   Training iter 450, batch loss 1.5313, batch acc 0.9236
17:04:20.207   Training iter 500, batch loss 1.5368, batch acc 0.9180
17:04:20.746   Training iter 550, batch loss 1.5319, batch acc 0.9246
17:04:21.277   Training iter 600, batch loss 1.5334, batch acc 0.9230
17:04:21.279 Training @ 337 epoch...
17:04:21.815   Training iter 50, batch loss 1.5311, batch acc 0.9262
17:04:22.381   Training iter 100, batch loss 1.5350, batch acc 0.9224
17:04:22.967   Training iter 150, batch loss 1.5295, batch acc 0.9292
17:04:23.544   Training iter 200, batch loss 1.5309, batch acc 0.9220
17:04:24.133   Training iter 250, batch loss 1.5299, batch acc 0.9260
17:04:24.705   Training iter 300, batch loss 1.5346, batch acc 0.9238
17:04:25.268   Training iter 350, batch loss 1.5354, batch acc 0.9192
17:04:25.828   Training iter 400, batch loss 1.5289, batch acc 0.9304
17:04:26.395   Training iter 450, batch loss 1.5292, batch acc 0.9260
17:04:26.956   Training iter 500, batch loss 1.5355, batch acc 0.9248
17:04:27.568   Training iter 550, batch loss 1.5337, batch acc 0.9230
17:04:28.122   Training iter 600, batch loss 1.5313, batch acc 0.9288
17:04:28.123 Training @ 338 epoch...
17:04:28.631   Training iter 50, batch loss 1.5305, batch acc 0.9268
17:04:29.179   Training iter 100, batch loss 1.5331, batch acc 0.9242
17:04:29.704   Training iter 150, batch loss 1.5283, batch acc 0.9320
17:04:30.182   Training iter 200, batch loss 1.5318, batch acc 0.9258
17:04:30.686   Training iter 250, batch loss 1.5344, batch acc 0.9218
17:04:31.225   Training iter 300, batch loss 1.5357, batch acc 0.9182
17:04:31.769   Training iter 350, batch loss 1.5300, batch acc 0.9254
17:04:32.295   Training iter 400, batch loss 1.5320, batch acc 0.9206
17:04:32.823   Training iter 450, batch loss 1.5298, batch acc 0.9262
17:04:33.345   Training iter 500, batch loss 1.5302, batch acc 0.9294
17:04:33.857   Training iter 550, batch loss 1.5372, batch acc 0.9136
17:04:34.367   Training iter 600, batch loss 1.5322, batch acc 0.9242
17:04:34.369 Training @ 339 epoch...
17:04:34.893   Training iter 50, batch loss 1.5322, batch acc 0.9232
17:04:35.416   Training iter 100, batch loss 1.5333, batch acc 0.9238
17:04:35.915   Training iter 150, batch loss 1.5302, batch acc 0.9294
17:04:36.412   Training iter 200, batch loss 1.5360, batch acc 0.9204
17:04:36.915   Training iter 250, batch loss 1.5341, batch acc 0.9232
17:04:37.400   Training iter 300, batch loss 1.5350, batch acc 0.9252
17:04:37.891   Training iter 350, batch loss 1.5306, batch acc 0.9280
17:04:38.378   Training iter 400, batch loss 1.5264, batch acc 0.9294
17:04:38.903   Training iter 450, batch loss 1.5305, batch acc 0.9266
17:04:39.449   Training iter 500, batch loss 1.5333, batch acc 0.9192
17:04:39.993   Training iter 550, batch loss 1.5351, batch acc 0.9240
17:04:40.518   Training iter 600, batch loss 1.5282, batch acc 0.9282
17:04:40.520 Training @ 340 epoch...
17:04:41.042   Training iter 50, batch loss 1.5348, batch acc 0.9200
17:04:41.541   Training iter 100, batch loss 1.5269, batch acc 0.9292
17:04:42.035   Training iter 150, batch loss 1.5300, batch acc 0.9242
17:04:42.543   Training iter 200, batch loss 1.5347, batch acc 0.9214
17:04:43.075   Training iter 250, batch loss 1.5344, batch acc 0.9184
17:04:43.586   Training iter 300, batch loss 1.5274, batch acc 0.9294
17:04:44.095   Training iter 350, batch loss 1.5329, batch acc 0.9212
17:04:44.601   Training iter 400, batch loss 1.5307, batch acc 0.9286
17:04:45.094   Training iter 450, batch loss 1.5355, batch acc 0.9214
17:04:45.600   Training iter 500, batch loss 1.5315, batch acc 0.9282
17:04:46.108   Training iter 550, batch loss 1.5285, batch acc 0.9298
17:04:46.634   Training iter 600, batch loss 1.5363, batch acc 0.9214
17:04:46.635 Testing @ 340 epoch...
17:04:46.683     Testing, total mean loss 1.53796, total acc 0.92260
17:04:46.683 Training @ 341 epoch...
17:04:47.186   Training iter 50, batch loss 1.5328, batch acc 0.9218
17:04:47.703   Training iter 100, batch loss 1.5341, batch acc 0.9236
17:04:48.254   Training iter 150, batch loss 1.5305, batch acc 0.9232
17:04:48.777   Training iter 200, batch loss 1.5275, batch acc 0.9304
17:04:49.282   Training iter 250, batch loss 1.5351, batch acc 0.9190
17:04:49.787   Training iter 300, batch loss 1.5304, batch acc 0.9300
17:04:50.299   Training iter 350, batch loss 1.5310, batch acc 0.9248
17:04:50.810   Training iter 400, batch loss 1.5372, batch acc 0.9178
17:04:51.328   Training iter 450, batch loss 1.5272, batch acc 0.9294
17:04:51.838   Training iter 500, batch loss 1.5374, batch acc 0.9160
17:04:52.357   Training iter 550, batch loss 1.5310, batch acc 0.9288
17:04:52.875   Training iter 600, batch loss 1.5294, batch acc 0.9290
17:04:52.877 Training @ 342 epoch...
17:04:53.400   Training iter 50, batch loss 1.5289, batch acc 0.9260
17:04:53.913   Training iter 100, batch loss 1.5292, batch acc 0.9252
17:04:54.446   Training iter 150, batch loss 1.5338, batch acc 0.9194
17:04:54.995   Training iter 200, batch loss 1.5331, batch acc 0.9232
17:04:55.548   Training iter 250, batch loss 1.5322, batch acc 0.9320
17:04:56.094   Training iter 300, batch loss 1.5292, batch acc 0.9284
17:04:56.643   Training iter 350, batch loss 1.5335, batch acc 0.9202
17:04:57.199   Training iter 400, batch loss 1.5296, batch acc 0.9300
17:04:57.760   Training iter 450, batch loss 1.5315, batch acc 0.9250
17:04:58.317   Training iter 500, batch loss 1.5328, batch acc 0.9212
17:04:58.874   Training iter 550, batch loss 1.5356, batch acc 0.9236
17:04:59.442   Training iter 600, batch loss 1.5334, batch acc 0.9274
17:04:59.444 Training @ 343 epoch...
17:05:00.005   Training iter 50, batch loss 1.5265, batch acc 0.9326
17:05:00.572   Training iter 100, batch loss 1.5300, batch acc 0.9270
17:05:01.121   Training iter 150, batch loss 1.5328, batch acc 0.9248
17:05:01.699   Training iter 200, batch loss 1.5309, batch acc 0.9252
17:05:02.265   Training iter 250, batch loss 1.5308, batch acc 0.9268
17:05:02.810   Training iter 300, batch loss 1.5327, batch acc 0.9240
17:05:03.347   Training iter 350, batch loss 1.5383, batch acc 0.9186
17:05:03.873   Training iter 400, batch loss 1.5341, batch acc 0.9234
17:05:04.424   Training iter 450, batch loss 1.5349, batch acc 0.9208
17:05:04.964   Training iter 500, batch loss 1.5326, batch acc 0.9244
17:05:05.512   Training iter 550, batch loss 1.5304, batch acc 0.9282
17:05:06.057   Training iter 600, batch loss 1.5282, batch acc 0.9248
17:05:06.059 Training @ 344 epoch...
17:05:06.588   Training iter 50, batch loss 1.5340, batch acc 0.9234
17:05:07.097   Training iter 100, batch loss 1.5348, batch acc 0.9202
17:05:07.619   Training iter 150, batch loss 1.5334, batch acc 0.9198
17:05:08.138   Training iter 200, batch loss 1.5307, batch acc 0.9248
17:05:08.658   Training iter 250, batch loss 1.5309, batch acc 0.9310
17:05:09.182   Training iter 300, batch loss 1.5308, batch acc 0.9292
17:05:09.673   Training iter 350, batch loss 1.5319, batch acc 0.9264
17:05:10.133   Training iter 400, batch loss 1.5290, batch acc 0.9284
17:05:10.605   Training iter 450, batch loss 1.5300, batch acc 0.9298
17:05:11.092   Training iter 500, batch loss 1.5320, batch acc 0.9228
17:05:11.577   Training iter 550, batch loss 1.5314, batch acc 0.9228
17:05:12.059   Training iter 600, batch loss 1.5339, batch acc 0.9210
17:05:12.061 Training @ 345 epoch...
17:05:12.572   Training iter 50, batch loss 1.5322, batch acc 0.9222
17:05:13.051   Training iter 100, batch loss 1.5333, batch acc 0.9248
17:05:13.529   Training iter 150, batch loss 1.5378, batch acc 0.9192
17:05:14.004   Training iter 200, batch loss 1.5311, batch acc 0.9238
17:05:14.497   Training iter 250, batch loss 1.5315, batch acc 0.9258
17:05:14.981   Training iter 300, batch loss 1.5276, batch acc 0.9306
17:05:15.469   Training iter 350, batch loss 1.5299, batch acc 0.9266
17:05:15.954   Training iter 400, batch loss 1.5286, batch acc 0.9304
17:05:16.436   Training iter 450, batch loss 1.5276, batch acc 0.9290
17:05:16.923   Training iter 500, batch loss 1.5320, batch acc 0.9288
17:05:17.440   Training iter 550, batch loss 1.5331, batch acc 0.9250
17:05:17.948   Training iter 600, batch loss 1.5366, batch acc 0.9146
17:05:17.950 Testing @ 345 epoch...
17:05:17.996     Testing, total mean loss 1.53799, total acc 0.92290
17:05:17.997 Training @ 346 epoch...
17:05:18.486   Training iter 50, batch loss 1.5358, batch acc 0.9204
17:05:18.959   Training iter 100, batch loss 1.5314, batch acc 0.9264
17:05:19.444   Training iter 150, batch loss 1.5346, batch acc 0.9188
17:05:19.926   Training iter 200, batch loss 1.5342, batch acc 0.9248
17:05:20.420   Training iter 250, batch loss 1.5343, batch acc 0.9220
17:05:20.914   Training iter 300, batch loss 1.5313, batch acc 0.9296
17:05:21.416   Training iter 350, batch loss 1.5286, batch acc 0.9280
17:05:21.930   Training iter 400, batch loss 1.5265, batch acc 0.9290
17:05:22.439   Training iter 450, batch loss 1.5320, batch acc 0.9248
17:05:22.957   Training iter 500, batch loss 1.5281, batch acc 0.9288
17:05:23.464   Training iter 550, batch loss 1.5305, batch acc 0.9304
17:05:23.967   Training iter 600, batch loss 1.5337, batch acc 0.9206
17:05:23.969 Training @ 347 epoch...
17:05:24.503   Training iter 50, batch loss 1.5331, batch acc 0.9212
17:05:25.033   Training iter 100, batch loss 1.5321, batch acc 0.9242
17:05:25.571   Training iter 150, batch loss 1.5290, batch acc 0.9350
17:05:26.127   Training iter 200, batch loss 1.5337, batch acc 0.9208
17:05:26.678   Training iter 250, batch loss 1.5344, batch acc 0.9216
17:05:27.251   Training iter 300, batch loss 1.5356, batch acc 0.9186
17:05:27.818   Training iter 350, batch loss 1.5296, batch acc 0.9244
17:05:28.411   Training iter 400, batch loss 1.5322, batch acc 0.9262
17:05:29.010   Training iter 450, batch loss 1.5299, batch acc 0.9304
17:05:29.606   Training iter 500, batch loss 1.5282, batch acc 0.9278
17:05:30.167   Training iter 550, batch loss 1.5340, batch acc 0.9260
17:05:30.737   Training iter 600, batch loss 1.5302, batch acc 0.9284
17:05:30.739 Training @ 348 epoch...
17:05:31.314   Training iter 50, batch loss 1.5306, batch acc 0.9258
17:05:31.871   Training iter 100, batch loss 1.5306, batch acc 0.9264
17:05:32.432   Training iter 150, batch loss 1.5327, batch acc 0.9202
17:05:32.966   Training iter 200, batch loss 1.5304, batch acc 0.9256
17:05:33.496   Training iter 250, batch loss 1.5345, batch acc 0.9226
17:05:34.012   Training iter 300, batch loss 1.5327, batch acc 0.9234
17:05:34.524   Training iter 350, batch loss 1.5314, batch acc 0.9246
17:05:35.039   Training iter 400, batch loss 1.5275, batch acc 0.9294
17:05:35.565   Training iter 450, batch loss 1.5309, batch acc 0.9270
17:05:36.081   Training iter 500, batch loss 1.5327, batch acc 0.9282
17:05:36.584   Training iter 550, batch loss 1.5347, batch acc 0.9208
17:05:37.094   Training iter 600, batch loss 1.5313, batch acc 0.9250
17:05:37.096 Training @ 349 epoch...
17:05:37.622   Training iter 50, batch loss 1.5301, batch acc 0.9258
17:05:38.150   Training iter 100, batch loss 1.5330, batch acc 0.9254
17:05:38.671   Training iter 150, batch loss 1.5345, batch acc 0.9240
17:05:39.197   Training iter 200, batch loss 1.5347, batch acc 0.9214
17:05:39.729   Training iter 250, batch loss 1.5307, batch acc 0.9246
17:05:40.259   Training iter 300, batch loss 1.5258, batch acc 0.9312
17:05:40.781   Training iter 350, batch loss 1.5305, batch acc 0.9294
17:05:41.275   Training iter 400, batch loss 1.5314, batch acc 0.9226
17:05:41.780   Training iter 450, batch loss 1.5290, batch acc 0.9270
17:05:42.290   Training iter 500, batch loss 1.5331, batch acc 0.9256
17:05:42.813   Training iter 550, batch loss 1.5338, batch acc 0.9198
17:05:43.337   Training iter 600, batch loss 1.5340, batch acc 0.9272
17:05:43.338 Training @ 350 epoch...
17:05:43.853   Training iter 50, batch loss 1.5310, batch acc 0.9290
17:05:44.380   Training iter 100, batch loss 1.5310, batch acc 0.9246
17:05:44.907   Training iter 150, batch loss 1.5301, batch acc 0.9290
17:05:45.431   Training iter 200, batch loss 1.5329, batch acc 0.9254
17:05:45.951   Training iter 250, batch loss 1.5315, batch acc 0.9220
17:05:46.488   Training iter 300, batch loss 1.5337, batch acc 0.9234
17:05:47.012   Training iter 350, batch loss 1.5331, batch acc 0.9262
17:05:47.545   Training iter 400, batch loss 1.5346, batch acc 0.9204
17:05:48.073   Training iter 450, batch loss 1.5307, batch acc 0.9254
17:05:48.610   Training iter 500, batch loss 1.5331, batch acc 0.9238
17:05:49.143   Training iter 550, batch loss 1.5304, batch acc 0.9266
17:05:49.642   Training iter 600, batch loss 1.5277, batch acc 0.9298
17:05:49.644 Testing @ 350 epoch...
17:05:49.690     Testing, total mean loss 1.53797, total acc 0.92270
17:05:49.690 Training @ 351 epoch...
17:05:50.181   Training iter 50, batch loss 1.5319, batch acc 0.9226
17:05:50.671   Training iter 100, batch loss 1.5325, batch acc 0.9264
17:05:51.170   Training iter 150, batch loss 1.5314, batch acc 0.9246
17:05:51.673   Training iter 200, batch loss 1.5325, batch acc 0.9258
17:05:52.178   Training iter 250, batch loss 1.5340, batch acc 0.9264
17:05:52.680   Training iter 300, batch loss 1.5330, batch acc 0.9260
17:05:53.182   Training iter 350, batch loss 1.5252, batch acc 0.9326
17:05:53.700   Training iter 400, batch loss 1.5288, batch acc 0.9286
17:05:54.231   Training iter 450, batch loss 1.5318, batch acc 0.9226
17:05:54.762   Training iter 500, batch loss 1.5319, batch acc 0.9272
17:05:55.298   Training iter 550, batch loss 1.5345, batch acc 0.9252
17:05:55.843   Training iter 600, batch loss 1.5322, batch acc 0.9246
17:05:55.845 Training @ 352 epoch...
17:05:56.382   Training iter 50, batch loss 1.5346, batch acc 0.9224
17:05:56.912   Training iter 100, batch loss 1.5294, batch acc 0.9260
17:05:57.447   Training iter 150, batch loss 1.5379, batch acc 0.9204
17:05:57.990   Training iter 200, batch loss 1.5288, batch acc 0.9292
17:05:58.522   Training iter 250, batch loss 1.5293, batch acc 0.9256
17:05:59.056   Training iter 300, batch loss 1.5287, batch acc 0.9304
17:05:59.615   Training iter 350, batch loss 1.5308, batch acc 0.9274
17:06:00.185   Training iter 400, batch loss 1.5302, batch acc 0.9294
17:06:00.752   Training iter 450, batch loss 1.5353, batch acc 0.9220
17:06:01.305   Training iter 500, batch loss 1.5350, batch acc 0.9176
17:06:01.880   Training iter 550, batch loss 1.5287, batch acc 0.9286
17:06:02.470   Training iter 600, batch loss 1.5301, batch acc 0.9274
17:06:02.471 Training @ 353 epoch...
17:06:03.034   Training iter 50, batch loss 1.5343, batch acc 0.9196
17:06:03.584   Training iter 100, batch loss 1.5321, batch acc 0.9230
17:06:04.158   Training iter 150, batch loss 1.5312, batch acc 0.9238
17:06:04.732   Training iter 200, batch loss 1.5318, batch acc 0.9246
17:06:05.305   Training iter 250, batch loss 1.5291, batch acc 0.9276
17:06:05.851   Training iter 300, batch loss 1.5291, batch acc 0.9276
17:06:06.417   Training iter 350, batch loss 1.5268, batch acc 0.9342
17:06:06.962   Training iter 400, batch loss 1.5375, batch acc 0.9168
17:06:07.496   Training iter 450, batch loss 1.5289, batch acc 0.9290
17:06:08.001   Training iter 500, batch loss 1.5343, batch acc 0.9244
17:06:08.521   Training iter 550, batch loss 1.5340, batch acc 0.9240
17:06:09.037   Training iter 600, batch loss 1.5302, batch acc 0.9312
17:06:09.039 Training @ 354 epoch...
17:06:09.549   Training iter 50, batch loss 1.5300, batch acc 0.9278
17:06:10.064   Training iter 100, batch loss 1.5338, batch acc 0.9194
17:06:10.578   Training iter 150, batch loss 1.5324, batch acc 0.9262
17:06:11.098   Training iter 200, batch loss 1.5275, batch acc 0.9242
17:06:11.617   Training iter 250, batch loss 1.5291, batch acc 0.9300
17:06:12.141   Training iter 300, batch loss 1.5294, batch acc 0.9290
17:06:12.667   Training iter 350, batch loss 1.5287, batch acc 0.9302
17:06:13.194   Training iter 400, batch loss 1.5338, batch acc 0.9226
17:06:13.713   Training iter 450, batch loss 1.5373, batch acc 0.9206
17:06:14.246   Training iter 500, batch loss 1.5314, batch acc 0.9260
17:06:14.770   Training iter 550, batch loss 1.5345, batch acc 0.9238
17:06:15.305   Training iter 600, batch loss 1.5302, batch acc 0.9260
17:06:15.307 Training @ 355 epoch...
17:06:15.859   Training iter 50, batch loss 1.5328, batch acc 0.9280
17:06:16.405   Training iter 100, batch loss 1.5303, batch acc 0.9256
17:06:16.969   Training iter 150, batch loss 1.5382, batch acc 0.9180
17:06:17.518   Training iter 200, batch loss 1.5312, batch acc 0.9238
17:06:18.080   Training iter 250, batch loss 1.5303, batch acc 0.9298
17:06:18.627   Training iter 300, batch loss 1.5317, batch acc 0.9256
17:06:19.184   Training iter 350, batch loss 1.5346, batch acc 0.9202
17:06:19.734   Training iter 400, batch loss 1.5284, batch acc 0.9286
17:06:20.293   Training iter 450, batch loss 1.5309, batch acc 0.9258
17:06:20.843   Training iter 500, batch loss 1.5284, batch acc 0.9278
17:06:21.382   Training iter 550, batch loss 1.5294, batch acc 0.9270
17:06:21.895   Training iter 600, batch loss 1.5325, batch acc 0.9226
17:06:21.897 Testing @ 355 epoch...
17:06:21.943     Testing, total mean loss 1.53765, total acc 0.92280
17:06:21.943 Training @ 356 epoch...
17:06:22.478   Training iter 50, batch loss 1.5304, batch acc 0.9242
17:06:23.001   Training iter 100, batch loss 1.5326, batch acc 0.9188
17:06:23.560   Training iter 150, batch loss 1.5315, batch acc 0.9260
17:06:24.143   Training iter 200, batch loss 1.5308, batch acc 0.9256
17:06:24.717   Training iter 250, batch loss 1.5254, batch acc 0.9328
17:06:25.254   Training iter 300, batch loss 1.5317, batch acc 0.9244
17:06:25.796   Training iter 350, batch loss 1.5315, batch acc 0.9302
17:06:26.361   Training iter 400, batch loss 1.5319, batch acc 0.9280
17:06:26.905   Training iter 450, batch loss 1.5322, batch acc 0.9234
17:06:27.476   Training iter 500, batch loss 1.5352, batch acc 0.9220
17:06:28.018   Training iter 550, batch loss 1.5334, batch acc 0.9228
17:06:28.532   Training iter 600, batch loss 1.5310, batch acc 0.9250
17:06:28.534 Training @ 357 epoch...
17:06:29.036   Training iter 50, batch loss 1.5307, batch acc 0.9288
17:06:29.612   Training iter 100, batch loss 1.5301, batch acc 0.9260
17:06:30.149   Training iter 150, batch loss 1.5309, batch acc 0.9256
17:06:30.769   Training iter 200, batch loss 1.5292, batch acc 0.9300
17:06:31.313   Training iter 250, batch loss 1.5326, batch acc 0.9218
17:06:31.863   Training iter 300, batch loss 1.5323, batch acc 0.9238
17:06:32.424   Training iter 350, batch loss 1.5344, batch acc 0.9190
17:06:32.970   Training iter 400, batch loss 1.5312, batch acc 0.9270
17:06:33.496   Training iter 450, batch loss 1.5285, batch acc 0.9292
17:06:33.993   Training iter 500, batch loss 1.5337, batch acc 0.9258
17:06:34.485   Training iter 550, batch loss 1.5322, batch acc 0.9246
17:06:34.956   Training iter 600, batch loss 1.5305, batch acc 0.9220
17:06:34.958 Training @ 358 epoch...
17:06:35.462   Training iter 50, batch loss 1.5302, batch acc 0.9254
17:06:35.959   Training iter 100, batch loss 1.5298, batch acc 0.9272
17:06:36.479   Training iter 150, batch loss 1.5286, batch acc 0.9296
17:06:37.025   Training iter 200, batch loss 1.5332, batch acc 0.9224
17:06:37.587   Training iter 250, batch loss 1.5290, batch acc 0.9262
17:06:38.142   Training iter 300, batch loss 1.5347, batch acc 0.9220
17:06:38.676   Training iter 350, batch loss 1.5297, batch acc 0.9300
17:06:39.193   Training iter 400, batch loss 1.5283, batch acc 0.9288
17:06:39.703   Training iter 450, batch loss 1.5333, batch acc 0.9246
17:06:40.222   Training iter 500, batch loss 1.5370, batch acc 0.9182
17:06:40.732   Training iter 550, batch loss 1.5345, batch acc 0.9224
17:06:41.249   Training iter 600, batch loss 1.5286, batch acc 0.9282
17:06:41.251 Training @ 359 epoch...
17:06:41.768   Training iter 50, batch loss 1.5304, batch acc 0.9268
17:06:42.273   Training iter 100, batch loss 1.5328, batch acc 0.9240
17:06:42.770   Training iter 150, batch loss 1.5307, batch acc 0.9270
17:06:43.261   Training iter 200, batch loss 1.5314, batch acc 0.9262
17:06:43.755   Training iter 250, batch loss 1.5301, batch acc 0.9252
17:06:44.303   Training iter 300, batch loss 1.5331, batch acc 0.9220
17:06:44.802   Training iter 350, batch loss 1.5323, batch acc 0.9286
17:06:45.291   Training iter 400, batch loss 1.5357, batch acc 0.9192
17:06:45.803   Training iter 450, batch loss 1.5317, batch acc 0.9256
17:06:46.304   Training iter 500, batch loss 1.5293, batch acc 0.9314
17:06:46.790   Training iter 550, batch loss 1.5290, batch acc 0.9266
17:06:47.292   Training iter 600, batch loss 1.5295, batch acc 0.9270
17:06:47.294 Training @ 360 epoch...
17:06:47.809   Training iter 50, batch loss 1.5335, batch acc 0.9226
17:06:48.325   Training iter 100, batch loss 1.5271, batch acc 0.9306
17:06:48.824   Training iter 150, batch loss 1.5267, batch acc 0.9308
17:06:49.328   Training iter 200, batch loss 1.5333, batch acc 0.9208
17:06:49.830   Training iter 250, batch loss 1.5300, batch acc 0.9278
17:06:50.327   Training iter 300, batch loss 1.5337, batch acc 0.9246
17:06:50.810   Training iter 350, batch loss 1.5300, batch acc 0.9276
17:06:51.295   Training iter 400, batch loss 1.5298, batch acc 0.9316
17:06:51.778   Training iter 450, batch loss 1.5305, batch acc 0.9240
17:06:52.311   Training iter 500, batch loss 1.5346, batch acc 0.9258
17:06:52.833   Training iter 550, batch loss 1.5365, batch acc 0.9150
17:06:53.358   Training iter 600, batch loss 1.5298, batch acc 0.9276
17:06:53.360 Testing @ 360 epoch...
17:06:53.407     Testing, total mean loss 1.53872, total acc 0.92220
17:06:53.407 Training @ 361 epoch...
17:06:53.917   Training iter 50, batch loss 1.5322, batch acc 0.9236
17:06:54.435   Training iter 100, batch loss 1.5307, batch acc 0.9250
17:06:54.937   Training iter 150, batch loss 1.5305, batch acc 0.9230
17:06:55.446   Training iter 200, batch loss 1.5309, batch acc 0.9246
17:06:55.950   Training iter 250, batch loss 1.5299, batch acc 0.9250
17:06:56.468   Training iter 300, batch loss 1.5348, batch acc 0.9224
17:06:56.971   Training iter 350, batch loss 1.5323, batch acc 0.9274
17:06:57.481   Training iter 400, batch loss 1.5270, batch acc 0.9322
17:06:57.985   Training iter 450, batch loss 1.5344, batch acc 0.9200
17:06:58.493   Training iter 500, batch loss 1.5282, batch acc 0.9296
17:06:59.008   Training iter 550, batch loss 1.5314, batch acc 0.9240
17:06:59.518   Training iter 600, batch loss 1.5329, batch acc 0.9250
17:06:59.520 Training @ 362 epoch...
17:07:00.035   Training iter 50, batch loss 1.5259, batch acc 0.9330
17:07:00.545   Training iter 100, batch loss 1.5302, batch acc 0.9286
17:07:01.045   Training iter 150, batch loss 1.5303, batch acc 0.9258
17:07:01.563   Training iter 200, batch loss 1.5301, batch acc 0.9286
17:07:02.083   Training iter 250, batch loss 1.5279, batch acc 0.9264
17:07:02.596   Training iter 300, batch loss 1.5325, batch acc 0.9246
17:07:03.102   Training iter 350, batch loss 1.5352, batch acc 0.9188
17:07:03.637   Training iter 400, batch loss 1.5308, batch acc 0.9290
17:07:04.178   Training iter 450, batch loss 1.5271, batch acc 0.9292
17:07:04.734   Training iter 500, batch loss 1.5350, batch acc 0.9196
17:07:05.279   Training iter 550, batch loss 1.5347, batch acc 0.9266
17:07:05.847   Training iter 600, batch loss 1.5354, batch acc 0.9206
17:07:05.849 Training @ 363 epoch...
17:07:06.420   Training iter 50, batch loss 1.5267, batch acc 0.9332
17:07:06.964   Training iter 100, batch loss 1.5344, batch acc 0.9226
17:07:07.503   Training iter 150, batch loss 1.5334, batch acc 0.9228
17:07:08.049   Training iter 200, batch loss 1.5312, batch acc 0.9278
17:07:08.599   Training iter 250, batch loss 1.5314, batch acc 0.9226
17:07:09.148   Training iter 300, batch loss 1.5325, batch acc 0.9252
17:07:09.710   Training iter 350, batch loss 1.5340, batch acc 0.9234
17:07:10.242   Training iter 400, batch loss 1.5332, batch acc 0.9258
17:07:10.769   Training iter 450, batch loss 1.5253, batch acc 0.9292
17:07:11.309   Training iter 500, batch loss 1.5326, batch acc 0.9210
17:07:11.852   Training iter 550, batch loss 1.5324, batch acc 0.9236
17:07:12.391   Training iter 600, batch loss 1.5282, batch acc 0.9306
17:07:12.393 Training @ 364 epoch...
17:07:12.947   Training iter 50, batch loss 1.5328, batch acc 0.9270
17:07:13.468   Training iter 100, batch loss 1.5308, batch acc 0.9222
17:07:13.996   Training iter 150, batch loss 1.5277, batch acc 0.9310
17:07:14.580   Training iter 200, batch loss 1.5293, batch acc 0.9242
17:07:15.151   Training iter 250, batch loss 1.5317, batch acc 0.9272
17:07:15.713   Training iter 300, batch loss 1.5326, batch acc 0.9250
17:07:16.215   Training iter 350, batch loss 1.5323, batch acc 0.9258
17:07:16.711   Training iter 400, batch loss 1.5322, batch acc 0.9264
17:07:17.222   Training iter 450, batch loss 1.5303, batch acc 0.9278
17:07:17.736   Training iter 500, batch loss 1.5280, batch acc 0.9284
17:07:18.251   Training iter 550, batch loss 1.5330, batch acc 0.9218
17:07:18.758   Training iter 600, batch loss 1.5338, batch acc 0.9276
17:07:18.760 Training @ 365 epoch...
17:07:19.265   Training iter 50, batch loss 1.5279, batch acc 0.9304
17:07:19.803   Training iter 100, batch loss 1.5283, batch acc 0.9288
17:07:20.346   Training iter 150, batch loss 1.5268, batch acc 0.9262
17:07:20.893   Training iter 200, batch loss 1.5318, batch acc 0.9272
17:07:21.441   Training iter 250, batch loss 1.5309, batch acc 0.9248
17:07:21.981   Training iter 300, batch loss 1.5295, batch acc 0.9284
17:07:22.525   Training iter 350, batch loss 1.5309, batch acc 0.9282
17:07:23.020   Training iter 400, batch loss 1.5344, batch acc 0.9220
17:07:23.519   Training iter 450, batch loss 1.5337, batch acc 0.9260
17:07:24.014   Training iter 500, batch loss 1.5319, batch acc 0.9258
17:07:24.548   Training iter 550, batch loss 1.5370, batch acc 0.9186
17:07:25.092   Training iter 600, batch loss 1.5303, batch acc 0.9250
17:07:25.093 Testing @ 365 epoch...
17:07:25.139     Testing, total mean loss 1.53750, total acc 0.92290
17:07:25.139 Training @ 366 epoch...
17:07:25.671   Training iter 50, batch loss 1.5279, batch acc 0.9290
17:07:26.177   Training iter 100, batch loss 1.5279, batch acc 0.9292
17:07:26.668   Training iter 150, batch loss 1.5320, batch acc 0.9242
17:07:27.190   Training iter 200, batch loss 1.5334, batch acc 0.9224
17:07:27.721   Training iter 250, batch loss 1.5267, batch acc 0.9332
17:07:28.249   Training iter 300, batch loss 1.5297, batch acc 0.9272
17:07:28.776   Training iter 350, batch loss 1.5350, batch acc 0.9226
17:07:29.320   Training iter 400, batch loss 1.5320, batch acc 0.9242
17:07:29.917   Training iter 450, batch loss 1.5337, batch acc 0.9236
17:07:30.577   Training iter 500, batch loss 1.5344, batch acc 0.9196
17:07:31.251   Training iter 550, batch loss 1.5316, batch acc 0.9258
17:07:31.891   Training iter 600, batch loss 1.5299, batch acc 0.9294
17:07:31.894 Training @ 367 epoch...
17:07:32.567   Training iter 50, batch loss 1.5273, batch acc 0.9322
17:07:33.235   Training iter 100, batch loss 1.5343, batch acc 0.9258
17:07:33.896   Training iter 150, batch loss 1.5315, batch acc 0.9278
17:07:34.457   Training iter 200, batch loss 1.5335, batch acc 0.9234
17:07:35.010   Training iter 250, batch loss 1.5327, batch acc 0.9230
17:07:35.540   Training iter 300, batch loss 1.5333, batch acc 0.9200
17:07:36.102   Training iter 350, batch loss 1.5315, batch acc 0.9228
17:07:36.664   Training iter 400, batch loss 1.5299, batch acc 0.9290
17:07:37.248   Training iter 450, batch loss 1.5299, batch acc 0.9258
17:07:37.823   Training iter 500, batch loss 1.5306, batch acc 0.9288
17:07:38.388   Training iter 550, batch loss 1.5321, batch acc 0.9248
17:07:38.952   Training iter 600, batch loss 1.5273, batch acc 0.9278
17:07:38.954 Training @ 368 epoch...
17:07:39.533   Training iter 50, batch loss 1.5313, batch acc 0.9232
17:07:40.099   Training iter 100, batch loss 1.5265, batch acc 0.9334
17:07:40.662   Training iter 150, batch loss 1.5342, batch acc 0.9260
17:07:41.229   Training iter 200, batch loss 1.5301, batch acc 0.9252
17:07:41.784   Training iter 250, batch loss 1.5325, batch acc 0.9244
17:07:42.330   Training iter 300, batch loss 1.5316, batch acc 0.9238
17:07:42.861   Training iter 350, batch loss 1.5333, batch acc 0.9222
17:07:43.399   Training iter 400, batch loss 1.5307, batch acc 0.9284
17:07:43.932   Training iter 450, batch loss 1.5315, batch acc 0.9256
17:07:44.476   Training iter 500, batch loss 1.5332, batch acc 0.9214
17:07:45.029   Training iter 550, batch loss 1.5301, batch acc 0.9262
17:07:45.566   Training iter 600, batch loss 1.5283, batch acc 0.9336
17:07:45.567 Training @ 369 epoch...
17:07:46.117   Training iter 50, batch loss 1.5330, batch acc 0.9212
17:07:46.685   Training iter 100, batch loss 1.5336, batch acc 0.9254
17:07:47.250   Training iter 150, batch loss 1.5314, batch acc 0.9246
17:07:47.807   Training iter 200, batch loss 1.5325, batch acc 0.9234
17:07:48.330   Training iter 250, batch loss 1.5341, batch acc 0.9196
17:07:48.845   Training iter 300, batch loss 1.5272, batch acc 0.9328
17:07:49.377   Training iter 350, batch loss 1.5290, batch acc 0.9320
17:07:49.895   Training iter 400, batch loss 1.5327, batch acc 0.9226
17:07:50.424   Training iter 450, batch loss 1.5268, batch acc 0.9250
17:07:50.959   Training iter 500, batch loss 1.5282, batch acc 0.9278
17:07:51.478   Training iter 550, batch loss 1.5312, batch acc 0.9266
17:07:51.996   Training iter 600, batch loss 1.5346, batch acc 0.9252
17:07:51.998 Training @ 370 epoch...
17:07:52.515   Training iter 50, batch loss 1.5340, batch acc 0.9248
17:07:53.034   Training iter 100, batch loss 1.5308, batch acc 0.9228
17:07:53.550   Training iter 150, batch loss 1.5313, batch acc 0.9214
17:07:54.067   Training iter 200, batch loss 1.5327, batch acc 0.9210
17:07:54.582   Training iter 250, batch loss 1.5269, batch acc 0.9274
17:07:55.092   Training iter 300, batch loss 1.5309, batch acc 0.9242
17:07:55.585   Training iter 350, batch loss 1.5319, batch acc 0.9284
17:07:56.106   Training iter 400, batch loss 1.5318, batch acc 0.9242
17:07:56.624   Training iter 450, batch loss 1.5311, batch acc 0.9264
17:07:57.152   Training iter 500, batch loss 1.5272, batch acc 0.9296
17:07:57.673   Training iter 550, batch loss 1.5309, batch acc 0.9296
17:07:58.202   Training iter 600, batch loss 1.5338, batch acc 0.9238
17:07:58.204 Testing @ 370 epoch...
17:07:58.249     Testing, total mean loss 1.53695, total acc 0.92280
17:07:58.250 Training @ 371 epoch...
17:07:58.763   Training iter 50, batch loss 1.5283, batch acc 0.9272
17:07:59.282   Training iter 100, batch loss 1.5357, batch acc 0.9186
17:07:59.805   Training iter 150, batch loss 1.5273, batch acc 0.9288
17:08:00.325   Training iter 200, batch loss 1.5281, batch acc 0.9294
17:08:00.799   Training iter 250, batch loss 1.5319, batch acc 0.9242
17:08:01.292   Training iter 300, batch loss 1.5318, batch acc 0.9274
17:08:01.802   Training iter 350, batch loss 1.5359, batch acc 0.9200
17:08:02.355   Training iter 400, batch loss 1.5318, batch acc 0.9250
17:08:02.843   Training iter 450, batch loss 1.5337, batch acc 0.9256
17:08:03.346   Training iter 500, batch loss 1.5306, batch acc 0.9260
17:08:03.831   Training iter 550, batch loss 1.5291, batch acc 0.9258
17:08:04.342   Training iter 600, batch loss 1.5289, batch acc 0.9258
17:08:04.344 Training @ 372 epoch...
17:08:04.871   Training iter 50, batch loss 1.5294, batch acc 0.9298
17:08:05.398   Training iter 100, batch loss 1.5314, batch acc 0.9248
17:08:05.918   Training iter 150, batch loss 1.5299, batch acc 0.9268
17:08:06.444   Training iter 200, batch loss 1.5301, batch acc 0.9306
17:08:06.972   Training iter 250, batch loss 1.5336, batch acc 0.9162
17:08:07.506   Training iter 300, batch loss 1.5321, batch acc 0.9234
17:08:08.041   Training iter 350, batch loss 1.5339, batch acc 0.9218
17:08:08.568   Training iter 400, batch loss 1.5291, batch acc 0.9280
17:08:09.101   Training iter 450, batch loss 1.5297, batch acc 0.9268
17:08:09.635   Training iter 500, batch loss 1.5293, batch acc 0.9308
17:08:10.161   Training iter 550, batch loss 1.5319, batch acc 0.9232
17:08:10.692   Training iter 600, batch loss 1.5312, batch acc 0.9264
17:08:10.693 Training @ 373 epoch...
17:08:11.222   Training iter 50, batch loss 1.5279, batch acc 0.9328
17:08:11.733   Training iter 100, batch loss 1.5269, batch acc 0.9338
17:08:12.225   Training iter 150, batch loss 1.5302, batch acc 0.9238
17:08:12.724   Training iter 200, batch loss 1.5266, batch acc 0.9284
17:08:13.241   Training iter 250, batch loss 1.5340, batch acc 0.9210
17:08:13.733   Training iter 300, batch loss 1.5332, batch acc 0.9258
17:08:14.251   Training iter 350, batch loss 1.5299, batch acc 0.9246
17:08:14.751   Training iter 400, batch loss 1.5301, batch acc 0.9256
17:08:15.256   Training iter 450, batch loss 1.5290, batch acc 0.9284
17:08:15.775   Training iter 500, batch loss 1.5349, batch acc 0.9250
17:08:16.286   Training iter 550, batch loss 1.5315, batch acc 0.9238
17:08:16.805   Training iter 600, batch loss 1.5380, batch acc 0.9192
17:08:16.807 Training @ 374 epoch...
17:08:17.323   Training iter 50, batch loss 1.5319, batch acc 0.9302
17:08:17.834   Training iter 100, batch loss 1.5285, batch acc 0.9306
17:08:18.328   Training iter 150, batch loss 1.5332, batch acc 0.9220
17:08:18.833   Training iter 200, batch loss 1.5339, batch acc 0.9230
17:08:19.733   Training iter 250, batch loss 1.5311, batch acc 0.9264
17:08:20.434   Training iter 300, batch loss 1.5301, batch acc 0.9278
17:08:20.953   Training iter 350, batch loss 1.5250, batch acc 0.9304
17:08:21.462   Training iter 400, batch loss 1.5307, batch acc 0.9292
17:08:21.942   Training iter 450, batch loss 1.5332, batch acc 0.9252
17:08:22.406   Training iter 500, batch loss 1.5303, batch acc 0.9282
17:08:22.889   Training iter 550, batch loss 1.5332, batch acc 0.9200
17:08:23.365   Training iter 600, batch loss 1.5307, batch acc 0.9244
17:08:23.367 Training @ 375 epoch...
17:08:23.868   Training iter 50, batch loss 1.5294, batch acc 0.9268
17:08:24.379   Training iter 100, batch loss 1.5343, batch acc 0.9230
17:08:24.892   Training iter 150, batch loss 1.5327, batch acc 0.9282
17:08:25.407   Training iter 200, batch loss 1.5299, batch acc 0.9274
17:08:25.926   Training iter 250, batch loss 1.5292, batch acc 0.9310
17:08:26.450   Training iter 300, batch loss 1.5311, batch acc 0.9206
17:08:26.969   Training iter 350, batch loss 1.5291, batch acc 0.9280
17:08:27.474   Training iter 400, batch loss 1.5257, batch acc 0.9332
17:08:27.969   Training iter 450, batch loss 1.5377, batch acc 0.9158
17:08:28.454   Training iter 500, batch loss 1.5286, batch acc 0.9282
17:08:28.951   Training iter 550, batch loss 1.5316, batch acc 0.9276
17:08:29.460   Training iter 600, batch loss 1.5309, batch acc 0.9248
17:08:29.462 Testing @ 375 epoch...
17:08:29.508     Testing, total mean loss 1.53683, total acc 0.92410
17:08:29.508 Training @ 376 epoch...
17:08:29.989   Training iter 50, batch loss 1.5314, batch acc 0.9256
17:08:30.497   Training iter 100, batch loss 1.5266, batch acc 0.9292
17:08:31.013   Training iter 150, batch loss 1.5291, batch acc 0.9288
17:08:31.519   Training iter 200, batch loss 1.5316, batch acc 0.9222
17:08:32.017   Training iter 250, batch loss 1.5314, batch acc 0.9288
17:08:32.522   Training iter 300, batch loss 1.5327, batch acc 0.9222
17:08:33.026   Training iter 350, batch loss 1.5314, batch acc 0.9300
17:08:33.529   Training iter 400, batch loss 1.5273, batch acc 0.9336
17:08:34.041   Training iter 450, batch loss 1.5285, batch acc 0.9306
17:08:34.563   Training iter 500, batch loss 1.5337, batch acc 0.9222
17:08:35.073   Training iter 550, batch loss 1.5360, batch acc 0.9222
17:08:35.580   Training iter 600, batch loss 1.5310, batch acc 0.9196
17:08:35.582 Training @ 377 epoch...
17:08:36.101   Training iter 50, batch loss 1.5292, batch acc 0.9246
17:08:36.621   Training iter 100, batch loss 1.5293, batch acc 0.9286
17:08:37.128   Training iter 150, batch loss 1.5292, batch acc 0.9294
17:08:37.630   Training iter 200, batch loss 1.5345, batch acc 0.9200
17:08:38.116   Training iter 250, batch loss 1.5294, batch acc 0.9250
17:08:38.620   Training iter 300, batch loss 1.5327, batch acc 0.9252
17:08:39.123   Training iter 350, batch loss 1.5335, batch acc 0.9276
17:08:39.632   Training iter 400, batch loss 1.5358, batch acc 0.9214
17:08:40.138   Training iter 450, batch loss 1.5271, batch acc 0.9360
17:08:40.655   Training iter 500, batch loss 1.5303, batch acc 0.9268
17:08:41.175   Training iter 550, batch loss 1.5281, batch acc 0.9268
17:08:41.688   Training iter 600, batch loss 1.5320, batch acc 0.9238
17:08:41.689 Training @ 378 epoch...
17:08:42.206   Training iter 50, batch loss 1.5277, batch acc 0.9298
17:08:42.740   Training iter 100, batch loss 1.5293, batch acc 0.9286
17:08:43.258   Training iter 150, batch loss 1.5325, batch acc 0.9262
17:08:43.796   Training iter 200, batch loss 1.5329, batch acc 0.9250
17:08:44.339   Training iter 250, batch loss 1.5279, batch acc 0.9274
17:08:44.870   Training iter 300, batch loss 1.5330, batch acc 0.9184
17:08:45.405   Training iter 350, batch loss 1.5356, batch acc 0.9218
17:08:45.944   Training iter 400, batch loss 1.5310, batch acc 0.9232
17:08:46.492   Training iter 450, batch loss 1.5331, batch acc 0.9246
17:08:47.031   Training iter 500, batch loss 1.5305, batch acc 0.9240
17:08:47.548   Training iter 550, batch loss 1.5281, batch acc 0.9326
17:08:48.075   Training iter 600, batch loss 1.5287, batch acc 0.9274
17:08:48.077 Training @ 379 epoch...
17:08:48.598   Training iter 50, batch loss 1.5299, batch acc 0.9270
17:08:49.125   Training iter 100, batch loss 1.5300, batch acc 0.9286
17:08:49.666   Training iter 150, batch loss 1.5289, batch acc 0.9238
17:08:50.197   Training iter 200, batch loss 1.5315, batch acc 0.9242
17:08:50.748   Training iter 250, batch loss 1.5317, batch acc 0.9260
17:08:51.317   Training iter 300, batch loss 1.5298, batch acc 0.9312
17:08:51.878   Training iter 350, batch loss 1.5334, batch acc 0.9212
17:08:52.462   Training iter 400, batch loss 1.5302, batch acc 0.9250
17:08:52.980   Training iter 450, batch loss 1.5292, batch acc 0.9274
17:08:53.523   Training iter 500, batch loss 1.5336, batch acc 0.9250
17:08:54.039   Training iter 550, batch loss 1.5303, batch acc 0.9288
17:08:54.517   Training iter 600, batch loss 1.5325, batch acc 0.9226
17:08:54.518 Training @ 380 epoch...
17:08:54.997   Training iter 50, batch loss 1.5278, batch acc 0.9270
17:08:55.483   Training iter 100, batch loss 1.5312, batch acc 0.9272
17:08:55.980   Training iter 150, batch loss 1.5276, batch acc 0.9298
17:08:56.496   Training iter 200, batch loss 1.5349, batch acc 0.9248
17:08:57.017   Training iter 250, batch loss 1.5311, batch acc 0.9260
17:08:57.544   Training iter 300, batch loss 1.5325, batch acc 0.9240
17:08:58.064   Training iter 350, batch loss 1.5331, batch acc 0.9150
17:08:58.592   Training iter 400, batch loss 1.5314, batch acc 0.9298
17:08:59.115   Training iter 450, batch loss 1.5325, batch acc 0.9276
17:08:59.643   Training iter 500, batch loss 1.5337, batch acc 0.9200
17:09:00.149   Training iter 550, batch loss 1.5282, batch acc 0.9298
17:09:00.638   Training iter 600, batch loss 1.5262, batch acc 0.9320
17:09:00.639 Testing @ 380 epoch...
17:09:00.685     Testing, total mean loss 1.53658, total acc 0.92390
17:09:00.685 Training @ 381 epoch...
17:09:01.208   Training iter 50, batch loss 1.5271, batch acc 0.9310
17:09:01.750   Training iter 100, batch loss 1.5315, batch acc 0.9264
17:09:02.285   Training iter 150, batch loss 1.5325, batch acc 0.9292
17:09:02.790   Training iter 200, batch loss 1.5345, batch acc 0.9200
17:09:03.313   Training iter 250, batch loss 1.5291, batch acc 0.9262
17:09:03.859   Training iter 300, batch loss 1.5308, batch acc 0.9254
17:09:04.429   Training iter 350, batch loss 1.5300, batch acc 0.9230
17:09:04.973   Training iter 400, batch loss 1.5267, batch acc 0.9278
17:09:05.524   Training iter 450, batch loss 1.5303, batch acc 0.9296
17:09:06.064   Training iter 500, batch loss 1.5293, batch acc 0.9270
17:09:06.587   Training iter 550, batch loss 1.5327, batch acc 0.9232
17:09:07.132   Training iter 600, batch loss 1.5356, batch acc 0.9232
17:09:07.134 Training @ 382 epoch...
17:09:07.670   Training iter 50, batch loss 1.5335, batch acc 0.9226
17:09:08.204   Training iter 100, batch loss 1.5318, batch acc 0.9256
17:09:08.729   Training iter 150, batch loss 1.5303, batch acc 0.9264
17:09:09.261   Training iter 200, batch loss 1.5271, batch acc 0.9296
17:09:09.769   Training iter 250, batch loss 1.5338, batch acc 0.9224
17:09:10.295   Training iter 300, batch loss 1.5258, batch acc 0.9326
17:09:10.822   Training iter 350, batch loss 1.5333, batch acc 0.9228
17:09:11.348   Training iter 400, batch loss 1.5344, batch acc 0.9230
17:09:11.912   Training iter 450, batch loss 1.5291, batch acc 0.9264
17:09:12.469   Training iter 500, batch loss 1.5266, batch acc 0.9314
17:09:13.018   Training iter 550, batch loss 1.5335, batch acc 0.9226
17:09:13.564   Training iter 600, batch loss 1.5309, batch acc 0.9252
17:09:13.566 Training @ 383 epoch...
17:09:14.136   Training iter 50, batch loss 1.5320, batch acc 0.9206
17:09:14.692   Training iter 100, batch loss 1.5321, batch acc 0.9262
17:09:15.246   Training iter 150, batch loss 1.5310, batch acc 0.9260
17:09:15.810   Training iter 200, batch loss 1.5350, batch acc 0.9236
17:09:16.370   Training iter 250, batch loss 1.5290, batch acc 0.9270
17:09:16.930   Training iter 300, batch loss 1.5280, batch acc 0.9302
17:09:17.469   Training iter 350, batch loss 1.5308, batch acc 0.9222
17:09:17.982   Training iter 400, batch loss 1.5320, batch acc 0.9242
17:09:18.502   Training iter 450, batch loss 1.5285, batch acc 0.9298
17:09:19.014   Training iter 500, batch loss 1.5293, batch acc 0.9292
17:09:19.560   Training iter 550, batch loss 1.5306, batch acc 0.9300
17:09:20.111   Training iter 600, batch loss 1.5319, batch acc 0.9266
17:09:20.113 Training @ 384 epoch...
17:09:20.657   Training iter 50, batch loss 1.5339, batch acc 0.9186
17:09:21.205   Training iter 100, batch loss 1.5322, batch acc 0.9266
17:09:21.749   Training iter 150, batch loss 1.5241, batch acc 0.9346
17:09:22.290   Training iter 200, batch loss 1.5289, batch acc 0.9306
17:09:22.812   Training iter 250, batch loss 1.5304, batch acc 0.9258
17:09:23.356   Training iter 300, batch loss 1.5335, batch acc 0.9230
17:09:23.869   Training iter 350, batch loss 1.5288, batch acc 0.9268
17:09:24.392   Training iter 400, batch loss 1.5311, batch acc 0.9256
17:09:24.910   Training iter 450, batch loss 1.5311, batch acc 0.9262
17:09:25.424   Training iter 500, batch loss 1.5295, batch acc 0.9302
17:09:25.942   Training iter 550, batch loss 1.5350, batch acc 0.9222
17:09:26.503   Training iter 600, batch loss 1.5297, batch acc 0.9264
17:09:26.505 Training @ 385 epoch...
17:09:27.072   Training iter 50, batch loss 1.5333, batch acc 0.9210
17:09:27.657   Training iter 100, batch loss 1.5307, batch acc 0.9248
17:09:28.256   Training iter 150, batch loss 1.5260, batch acc 0.9286
17:09:28.851   Training iter 200, batch loss 1.5311, batch acc 0.9246
17:09:29.436   Training iter 250, batch loss 1.5251, batch acc 0.9314
17:09:29.983   Training iter 300, batch loss 1.5342, batch acc 0.9186
17:09:30.531   Training iter 350, batch loss 1.5323, batch acc 0.9238
17:09:31.089   Training iter 400, batch loss 1.5321, batch acc 0.9230
17:09:31.633   Training iter 450, batch loss 1.5356, batch acc 0.9208
17:09:32.175   Training iter 500, batch loss 1.5294, batch acc 0.9284
17:09:32.693   Training iter 550, batch loss 1.5293, batch acc 0.9298
17:09:33.220   Training iter 600, batch loss 1.5296, batch acc 0.9312
17:09:33.222 Testing @ 385 epoch...
17:09:33.267     Testing, total mean loss 1.53831, total acc 0.92370
17:09:33.267 Training @ 386 epoch...
17:09:33.795   Training iter 50, batch loss 1.5276, batch acc 0.9274
17:09:34.290   Training iter 100, batch loss 1.5275, batch acc 0.9318
17:09:34.788   Training iter 150, batch loss 1.5311, batch acc 0.9198
17:09:35.285   Training iter 200, batch loss 1.5299, batch acc 0.9280
17:09:35.790   Training iter 250, batch loss 1.5281, batch acc 0.9304
17:09:36.303   Training iter 300, batch loss 1.5302, batch acc 0.9278
17:09:36.813   Training iter 350, batch loss 1.5330, batch acc 0.9262
17:09:37.312   Training iter 400, batch loss 1.5333, batch acc 0.9234
17:09:37.806   Training iter 450, batch loss 1.5312, batch acc 0.9274
17:09:38.301   Training iter 500, batch loss 1.5343, batch acc 0.9226
17:09:38.839   Training iter 550, batch loss 1.5317, batch acc 0.9220
17:09:39.386   Training iter 600, batch loss 1.5310, batch acc 0.9248
17:09:39.388 Training @ 387 epoch...
17:09:39.935   Training iter 50, batch loss 1.5326, batch acc 0.9246
17:09:40.459   Training iter 100, batch loss 1.5321, batch acc 0.9240
17:09:40.984   Training iter 150, batch loss 1.5281, batch acc 0.9308
17:09:41.514   Training iter 200, batch loss 1.5314, batch acc 0.9244
17:09:42.031   Training iter 250, batch loss 1.5357, batch acc 0.9200
17:09:42.559   Training iter 300, batch loss 1.5306, batch acc 0.9254
17:09:43.051   Training iter 350, batch loss 1.5302, batch acc 0.9288
17:09:43.561   Training iter 400, batch loss 1.5314, batch acc 0.9248
17:09:44.108   Training iter 450, batch loss 1.5280, batch acc 0.9278
17:09:44.674   Training iter 500, batch loss 1.5299, batch acc 0.9298
17:09:45.235   Training iter 550, batch loss 1.5281, batch acc 0.9260
17:09:45.803   Training iter 600, batch loss 1.5300, batch acc 0.9274
17:09:45.805 Training @ 388 epoch...
17:09:46.396   Training iter 50, batch loss 1.5352, batch acc 0.9226
17:09:46.969   Training iter 100, batch loss 1.5262, batch acc 0.9316
17:09:47.524   Training iter 150, batch loss 1.5321, batch acc 0.9214
17:09:48.072   Training iter 200, batch loss 1.5311, batch acc 0.9250
17:09:48.601   Training iter 250, batch loss 1.5259, batch acc 0.9296
17:09:49.110   Training iter 300, batch loss 1.5313, batch acc 0.9278
17:09:49.617   Training iter 350, batch loss 1.5278, batch acc 0.9292
17:09:50.088   Training iter 400, batch loss 1.5329, batch acc 0.9224
17:09:50.554   Training iter 450, batch loss 1.5339, batch acc 0.9248
17:09:51.023   Training iter 500, batch loss 1.5323, batch acc 0.9252
17:09:51.514   Training iter 550, batch loss 1.5250, batch acc 0.9354
17:09:51.990   Training iter 600, batch loss 1.5337, batch acc 0.9248
17:09:51.991 Training @ 389 epoch...
17:09:52.476   Training iter 50, batch loss 1.5303, batch acc 0.9278
17:09:52.957   Training iter 100, batch loss 1.5327, batch acc 0.9248
17:09:53.445   Training iter 150, batch loss 1.5308, batch acc 0.9226
17:09:53.949   Training iter 200, batch loss 1.5271, batch acc 0.9326
17:09:54.459   Training iter 250, batch loss 1.5337, batch acc 0.9228
17:09:54.949   Training iter 300, batch loss 1.5296, batch acc 0.9294
17:09:55.391   Training iter 350, batch loss 1.5300, batch acc 0.9288
17:09:55.834   Training iter 400, batch loss 1.5299, batch acc 0.9248
17:09:56.298   Training iter 450, batch loss 1.5275, batch acc 0.9230
17:09:56.760   Training iter 500, batch loss 1.5335, batch acc 0.9240
17:09:57.235   Training iter 550, batch loss 1.5291, batch acc 0.9290
17:09:57.700   Training iter 600, batch loss 1.5343, batch acc 0.9232
17:09:57.702 Training @ 390 epoch...
17:09:58.173   Training iter 50, batch loss 1.5357, batch acc 0.9200
17:09:58.633   Training iter 100, batch loss 1.5264, batch acc 0.9314
17:09:59.114   Training iter 150, batch loss 1.5294, batch acc 0.9288
17:09:59.598   Training iter 200, batch loss 1.5295, batch acc 0.9274
17:10:00.104   Training iter 250, batch loss 1.5308, batch acc 0.9262
17:10:00.624   Training iter 300, batch loss 1.5342, batch acc 0.9218
17:10:01.166   Training iter 350, batch loss 1.5279, batch acc 0.9284
17:10:01.761   Training iter 400, batch loss 1.5307, batch acc 0.9270
17:10:02.333   Training iter 450, batch loss 1.5284, batch acc 0.9274
17:10:02.884   Training iter 500, batch loss 1.5296, batch acc 0.9302
17:10:03.431   Training iter 550, batch loss 1.5350, batch acc 0.9240
17:10:03.990   Training iter 600, batch loss 1.5297, batch acc 0.9244
17:10:03.992 Testing @ 390 epoch...
17:10:04.041     Testing, total mean loss 1.53804, total acc 0.92350
17:10:04.041 Training @ 391 epoch...
17:10:04.605   Training iter 50, batch loss 1.5309, batch acc 0.9280
17:10:05.170   Training iter 100, batch loss 1.5264, batch acc 0.9312
17:10:05.728   Training iter 150, batch loss 1.5303, batch acc 0.9252
17:10:06.277   Training iter 200, batch loss 1.5330, batch acc 0.9218
17:10:06.793   Training iter 250, batch loss 1.5310, batch acc 0.9220
17:10:07.303   Training iter 300, batch loss 1.5337, batch acc 0.9260
17:10:07.874   Training iter 350, batch loss 1.5273, batch acc 0.9278
17:10:08.427   Training iter 400, batch loss 1.5308, batch acc 0.9282
17:10:08.955   Training iter 450, batch loss 1.5313, batch acc 0.9252
17:10:09.491   Training iter 500, batch loss 1.5299, batch acc 0.9250
17:10:10.036   Training iter 550, batch loss 1.5288, batch acc 0.9278
17:10:10.597   Training iter 600, batch loss 1.5339, batch acc 0.9230
17:10:10.599 Training @ 392 epoch...
17:10:11.174   Training iter 50, batch loss 1.5324, batch acc 0.9232
17:10:11.695   Training iter 100, batch loss 1.5318, batch acc 0.9236
17:10:12.221   Training iter 150, batch loss 1.5311, batch acc 0.9286
17:10:12.760   Training iter 200, batch loss 1.5291, batch acc 0.9258
17:10:13.299   Training iter 250, batch loss 1.5329, batch acc 0.9224
17:10:13.825   Training iter 300, batch loss 1.5286, batch acc 0.9282
17:10:14.362   Training iter 350, batch loss 1.5270, batch acc 0.9330
17:10:14.882   Training iter 400, batch loss 1.5279, batch acc 0.9316
17:10:15.413   Training iter 450, batch loss 1.5281, batch acc 0.9310
17:10:15.960   Training iter 500, batch loss 1.5275, batch acc 0.9288
17:10:16.536   Training iter 550, batch loss 1.5360, batch acc 0.9218
17:10:17.108   Training iter 600, batch loss 1.5335, batch acc 0.9218
17:10:17.110 Training @ 393 epoch...
17:10:17.664   Training iter 50, batch loss 1.5339, batch acc 0.9218
17:10:18.214   Training iter 100, batch loss 1.5267, batch acc 0.9314
17:10:18.771   Training iter 150, batch loss 1.5335, batch acc 0.9240
17:10:19.334   Training iter 200, batch loss 1.5289, batch acc 0.9242
17:10:19.898   Training iter 250, batch loss 1.5249, batch acc 0.9310
17:10:20.472   Training iter 300, batch loss 1.5328, batch acc 0.9234
17:10:21.045   Training iter 350, batch loss 1.5309, batch acc 0.9286
17:10:21.608   Training iter 400, batch loss 1.5314, batch acc 0.9270
17:10:22.169   Training iter 450, batch loss 1.5302, batch acc 0.9284
17:10:22.723   Training iter 500, batch loss 1.5301, batch acc 0.9270
17:10:23.257   Training iter 550, batch loss 1.5348, batch acc 0.9242
17:10:23.784   Training iter 600, batch loss 1.5287, batch acc 0.9254
17:10:23.786 Training @ 394 epoch...
17:10:24.330   Training iter 50, batch loss 1.5307, batch acc 0.9282
17:10:24.875   Training iter 100, batch loss 1.5300, batch acc 0.9230
17:10:25.418   Training iter 150, batch loss 1.5297, batch acc 0.9270
17:10:25.929   Training iter 200, batch loss 1.5312, batch acc 0.9228
17:10:26.451   Training iter 250, batch loss 1.5302, batch acc 0.9246
17:10:26.966   Training iter 300, batch loss 1.5317, batch acc 0.9238
17:10:27.512   Training iter 350, batch loss 1.5327, batch acc 0.9238
17:10:28.094   Training iter 400, batch loss 1.5299, batch acc 0.9306
17:10:28.678   Training iter 450, batch loss 1.5271, batch acc 0.9292
17:10:29.262   Training iter 500, batch loss 1.5295, batch acc 0.9266
17:10:29.781   Training iter 550, batch loss 1.5322, batch acc 0.9226
17:10:30.298   Training iter 600, batch loss 1.5311, batch acc 0.9292
17:10:30.300 Training @ 395 epoch...
17:10:30.818   Training iter 50, batch loss 1.5325, batch acc 0.9220
17:10:31.340   Training iter 100, batch loss 1.5281, batch acc 0.9292
17:10:31.867   Training iter 150, batch loss 1.5333, batch acc 0.9222
17:10:32.387   Training iter 200, batch loss 1.5253, batch acc 0.9326
17:10:32.894   Training iter 250, batch loss 1.5304, batch acc 0.9244
17:10:33.408   Training iter 300, batch loss 1.5290, batch acc 0.9258
17:10:33.915   Training iter 350, batch loss 1.5324, batch acc 0.9266
17:10:34.424   Training iter 400, batch loss 1.5296, batch acc 0.9266
17:10:34.941   Training iter 450, batch loss 1.5302, batch acc 0.9298
17:10:35.456   Training iter 500, batch loss 1.5327, batch acc 0.9244
17:10:35.978   Training iter 550, batch loss 1.5283, batch acc 0.9298
17:10:36.500   Training iter 600, batch loss 1.5340, batch acc 0.9222
17:10:36.502 Testing @ 395 epoch...
17:10:36.547     Testing, total mean loss 1.53660, total acc 0.92450
17:10:36.547 Training @ 396 epoch...
17:10:37.075   Training iter 50, batch loss 1.5317, batch acc 0.9252
17:10:37.593   Training iter 100, batch loss 1.5284, batch acc 0.9284
17:10:38.101   Training iter 150, batch loss 1.5324, batch acc 0.9232
17:10:38.613   Training iter 200, batch loss 1.5335, batch acc 0.9224
17:10:39.127   Training iter 250, batch loss 1.5315, batch acc 0.9248
17:10:39.653   Training iter 300, batch loss 1.5263, batch acc 0.9332
17:10:40.166   Training iter 350, batch loss 1.5248, batch acc 0.9322
17:10:40.684   Training iter 400, batch loss 1.5345, batch acc 0.9216
17:10:41.194   Training iter 450, batch loss 1.5302, batch acc 0.9270
17:10:41.707   Training iter 500, batch loss 1.5334, batch acc 0.9226
17:10:42.183   Training iter 550, batch loss 1.5281, batch acc 0.9294
17:10:42.659   Training iter 600, batch loss 1.5316, batch acc 0.9236
17:10:42.661 Training @ 397 epoch...
17:10:43.154   Training iter 50, batch loss 1.5300, batch acc 0.9280
17:10:43.616   Training iter 100, batch loss 1.5306, batch acc 0.9246
17:10:44.065   Training iter 150, batch loss 1.5327, batch acc 0.9224
17:10:44.520   Training iter 200, batch loss 1.5258, batch acc 0.9328
17:10:44.962   Training iter 250, batch loss 1.5336, batch acc 0.9248
17:10:45.417   Training iter 300, batch loss 1.5304, batch acc 0.9282
17:10:45.861   Training iter 350, batch loss 1.5287, batch acc 0.9286
17:10:46.316   Training iter 400, batch loss 1.5291, batch acc 0.9288
17:10:46.788   Training iter 450, batch loss 1.5343, batch acc 0.9206
17:10:47.242   Training iter 500, batch loss 1.5310, batch acc 0.9268
17:10:47.742   Training iter 550, batch loss 1.5318, batch acc 0.9254
17:10:48.247   Training iter 600, batch loss 1.5276, batch acc 0.9296
17:10:48.249 Training @ 398 epoch...
17:10:48.761   Training iter 50, batch loss 1.5298, batch acc 0.9278
17:10:49.271   Training iter 100, batch loss 1.5312, batch acc 0.9272
17:10:49.806   Training iter 150, batch loss 1.5322, batch acc 0.9244
17:10:50.340   Training iter 200, batch loss 1.5338, batch acc 0.9266
17:10:50.872   Training iter 250, batch loss 1.5288, batch acc 0.9280
17:10:51.422   Training iter 300, batch loss 1.5328, batch acc 0.9252
17:10:51.965   Training iter 350, batch loss 1.5307, batch acc 0.9256
17:10:52.509   Training iter 400, batch loss 1.5279, batch acc 0.9308
17:10:53.054   Training iter 450, batch loss 1.5285, batch acc 0.9260
17:10:53.598   Training iter 500, batch loss 1.5324, batch acc 0.9216
17:10:54.131   Training iter 550, batch loss 1.5262, batch acc 0.9322
17:10:54.647   Training iter 600, batch loss 1.5317, batch acc 0.9230
17:10:54.649 Training @ 399 epoch...
17:10:55.149   Training iter 50, batch loss 1.5250, batch acc 0.9324
17:10:55.648   Training iter 100, batch loss 1.5301, batch acc 0.9256
17:10:56.161   Training iter 150, batch loss 1.5324, batch acc 0.9246
17:10:56.665   Training iter 200, batch loss 1.5255, batch acc 0.9344
17:10:57.177   Training iter 250, batch loss 1.5296, batch acc 0.9278
17:10:57.697   Training iter 300, batch loss 1.5350, batch acc 0.9226
17:10:58.251   Training iter 350, batch loss 1.5336, batch acc 0.9224
17:10:58.785   Training iter 400, batch loss 1.5308, batch acc 0.9284
17:10:59.326   Training iter 450, batch loss 1.5290, batch acc 0.9256
17:10:59.848   Training iter 500, batch loss 1.5291, batch acc 0.9298
17:11:00.373   Training iter 550, batch loss 1.5287, batch acc 0.9280
17:11:00.908   Training iter 600, batch loss 1.5357, batch acc 0.9190
17:11:00.909 Training @ 400 epoch...
17:11:01.482   Training iter 50, batch loss 1.5308, batch acc 0.9216
17:11:02.047   Training iter 100, batch loss 1.5278, batch acc 0.9268
17:11:02.592   Training iter 150, batch loss 1.5282, batch acc 0.9242
17:11:03.139   Training iter 200, batch loss 1.5286, batch acc 0.9330
17:11:03.683   Training iter 250, batch loss 1.5335, batch acc 0.9252
17:11:04.244   Training iter 300, batch loss 1.5351, batch acc 0.9196
17:11:04.788   Training iter 350, batch loss 1.5328, batch acc 0.9252
17:11:05.314   Training iter 400, batch loss 1.5277, batch acc 0.9296
17:11:05.822   Training iter 450, batch loss 1.5296, batch acc 0.9300
17:11:06.355   Training iter 500, batch loss 1.5324, batch acc 0.9244
17:11:06.874   Training iter 550, batch loss 1.5312, batch acc 0.9252
17:11:07.360   Training iter 600, batch loss 1.5281, batch acc 0.9324
17:11:07.362 Testing @ 400 epoch...
17:11:07.407     Testing, total mean loss 1.53632, total acc 0.92530
17:11:07.407 Plot @ 400 epoch...
17:11:07.407 Training @ 401 epoch...
17:11:07.879   Training iter 50, batch loss 1.5292, batch acc 0.9282
17:11:08.342   Training iter 100, batch loss 1.5344, batch acc 0.9246
17:11:08.811   Training iter 150, batch loss 1.5320, batch acc 0.9246
17:11:09.304   Training iter 200, batch loss 1.5269, batch acc 0.9300
17:11:09.796   Training iter 250, batch loss 1.5291, batch acc 0.9278
17:11:10.322   Training iter 300, batch loss 1.5288, batch acc 0.9282
17:11:10.823   Training iter 350, batch loss 1.5335, batch acc 0.9206
17:11:11.363   Training iter 400, batch loss 1.5286, batch acc 0.9276
17:11:11.866   Training iter 450, batch loss 1.5281, batch acc 0.9288
17:11:12.359   Training iter 500, batch loss 1.5304, batch acc 0.9276
17:11:12.854   Training iter 550, batch loss 1.5335, batch acc 0.9240
17:11:13.342   Training iter 600, batch loss 1.5318, batch acc 0.9240
17:11:13.343 Training @ 402 epoch...
17:11:13.823   Training iter 50, batch loss 1.5332, batch acc 0.9244
17:11:14.297   Training iter 100, batch loss 1.5276, batch acc 0.9328
17:11:14.780   Training iter 150, batch loss 1.5273, batch acc 0.9266
17:11:15.268   Training iter 200, batch loss 1.5297, batch acc 0.9288
17:11:15.745   Training iter 250, batch loss 1.5282, batch acc 0.9290
17:11:16.226   Training iter 300, batch loss 1.5282, batch acc 0.9274
17:11:16.726   Training iter 350, batch loss 1.5344, batch acc 0.9202
17:11:17.221   Training iter 400, batch loss 1.5291, batch acc 0.9250
17:11:17.704   Training iter 450, batch loss 1.5308, batch acc 0.9256
17:11:18.217   Training iter 500, batch loss 1.5287, batch acc 0.9294
17:11:18.706   Training iter 550, batch loss 1.5375, batch acc 0.9202
17:11:19.187   Training iter 600, batch loss 1.5298, batch acc 0.9254
17:11:19.189 Training @ 403 epoch...
17:11:19.678   Training iter 50, batch loss 1.5278, batch acc 0.9320
17:11:20.170   Training iter 100, batch loss 1.5299, batch acc 0.9260
17:11:20.672   Training iter 150, batch loss 1.5302, batch acc 0.9224
17:11:21.186   Training iter 200, batch loss 1.5328, batch acc 0.9210
17:11:21.683   Training iter 250, batch loss 1.5272, batch acc 0.9308
17:11:22.255   Training iter 300, batch loss 1.5291, batch acc 0.9282
17:11:22.851   Training iter 350, batch loss 1.5266, batch acc 0.9352
17:11:23.431   Training iter 400, batch loss 1.5319, batch acc 0.9274
17:11:23.976   Training iter 450, batch loss 1.5345, batch acc 0.9212
17:11:24.498   Training iter 500, batch loss 1.5356, batch acc 0.9210
17:11:25.022   Training iter 550, batch loss 1.5301, batch acc 0.9238
17:11:25.580   Training iter 600, batch loss 1.5292, batch acc 0.9282
17:11:25.582 Training @ 404 epoch...
17:11:26.154   Training iter 50, batch loss 1.5241, batch acc 0.9374
17:11:26.743   Training iter 100, batch loss 1.5296, batch acc 0.9274
17:11:27.306   Training iter 150, batch loss 1.5306, batch acc 0.9234
17:11:27.865   Training iter 200, batch loss 1.5302, batch acc 0.9254
17:11:28.389   Training iter 250, batch loss 1.5376, batch acc 0.9164
17:11:28.918   Training iter 300, batch loss 1.5246, batch acc 0.9326
17:11:29.441   Training iter 350, batch loss 1.5325, batch acc 0.9256
17:11:29.961   Training iter 400, batch loss 1.5298, batch acc 0.9300
17:11:30.482   Training iter 450, batch loss 1.5336, batch acc 0.9242
17:11:31.015   Training iter 500, batch loss 1.5335, batch acc 0.9216
17:11:31.539   Training iter 550, batch loss 1.5314, batch acc 0.9230
17:11:32.061   Training iter 600, batch loss 1.5270, batch acc 0.9280
17:11:32.063 Training @ 405 epoch...
17:11:32.597   Training iter 50, batch loss 1.5334, batch acc 0.9182
17:11:33.107   Training iter 100, batch loss 1.5288, batch acc 0.9288
17:11:33.620   Training iter 150, batch loss 1.5316, batch acc 0.9278
17:11:34.116   Training iter 200, batch loss 1.5259, batch acc 0.9302
17:11:34.614   Training iter 250, batch loss 1.5377, batch acc 0.9184
17:11:35.115   Training iter 300, batch loss 1.5325, batch acc 0.9244
17:11:35.645   Training iter 350, batch loss 1.5288, batch acc 0.9264
17:11:36.148   Training iter 400, batch loss 1.5305, batch acc 0.9280
17:11:36.627   Training iter 450, batch loss 1.5278, batch acc 0.9278
17:11:37.128   Training iter 500, batch loss 1.5280, batch acc 0.9288
17:11:37.667   Training iter 550, batch loss 1.5320, batch acc 0.9254
17:11:38.207   Training iter 600, batch loss 1.5263, batch acc 0.9336
17:11:38.209 Testing @ 405 epoch...
17:11:38.257     Testing, total mean loss 1.53653, total acc 0.92480
17:11:38.257 Training @ 406 epoch...
17:11:38.795   Training iter 50, batch loss 1.5295, batch acc 0.9276
17:11:39.337   Training iter 100, batch loss 1.5297, batch acc 0.9262
17:11:39.874   Training iter 150, batch loss 1.5300, batch acc 0.9286
17:11:40.425   Training iter 200, batch loss 1.5324, batch acc 0.9216
17:11:40.977   Training iter 250, batch loss 1.5282, batch acc 0.9280
17:11:41.535   Training iter 300, batch loss 1.5304, batch acc 0.9282
17:11:42.077   Training iter 350, batch loss 1.5275, batch acc 0.9324
17:11:42.628   Training iter 400, batch loss 1.5298, batch acc 0.9268
17:11:43.186   Training iter 450, batch loss 1.5306, batch acc 0.9246
17:11:43.730   Training iter 500, batch loss 1.5320, batch acc 0.9260
17:11:44.309   Training iter 550, batch loss 1.5338, batch acc 0.9234
17:11:44.847   Training iter 600, batch loss 1.5299, batch acc 0.9280
17:11:44.849 Training @ 407 epoch...
17:11:45.380   Training iter 50, batch loss 1.5357, batch acc 0.9188
17:11:45.909   Training iter 100, batch loss 1.5268, batch acc 0.9296
17:11:46.431   Training iter 150, batch loss 1.5284, batch acc 0.9318
17:11:46.939   Training iter 200, batch loss 1.5317, batch acc 0.9238
17:11:47.454   Training iter 250, batch loss 1.5320, batch acc 0.9236
17:11:47.962   Training iter 300, batch loss 1.5281, batch acc 0.9298
17:11:48.491   Training iter 350, batch loss 1.5308, batch acc 0.9250
17:11:49.004   Training iter 400, batch loss 1.5304, batch acc 0.9244
17:11:49.515   Training iter 450, batch loss 1.5310, batch acc 0.9254
17:11:50.011   Training iter 500, batch loss 1.5269, batch acc 0.9292
17:11:50.505   Training iter 550, batch loss 1.5325, batch acc 0.9218
17:11:51.003   Training iter 600, batch loss 1.5285, batch acc 0.9304
17:11:51.005 Training @ 408 epoch...
17:11:51.513   Training iter 50, batch loss 1.5332, batch acc 0.9250
17:11:52.001   Training iter 100, batch loss 1.5223, batch acc 0.9354
17:11:52.536   Training iter 150, batch loss 1.5343, batch acc 0.9172
17:11:53.058   Training iter 200, batch loss 1.5266, batch acc 0.9310
17:11:53.588   Training iter 250, batch loss 1.5240, batch acc 0.9336
17:11:54.107   Training iter 300, batch loss 1.5296, batch acc 0.9280
17:11:54.630   Training iter 350, batch loss 1.5323, batch acc 0.9266
17:11:55.154   Training iter 400, batch loss 1.5340, batch acc 0.9216
17:11:55.689   Training iter 450, batch loss 1.5312, batch acc 0.9218
17:11:56.229   Training iter 500, batch loss 1.5311, batch acc 0.9236
17:11:56.752   Training iter 550, batch loss 1.5278, batch acc 0.9328
17:11:57.276   Training iter 600, batch loss 1.5363, batch acc 0.9190
17:11:57.278 Training @ 409 epoch...
17:11:57.806   Training iter 50, batch loss 1.5304, batch acc 0.9276
17:11:58.321   Training iter 100, batch loss 1.5281, batch acc 0.9296
17:11:58.815   Training iter 150, batch loss 1.5280, batch acc 0.9300
17:11:59.344   Training iter 200, batch loss 1.5311, batch acc 0.9258
17:11:59.842   Training iter 250, batch loss 1.5258, batch acc 0.9290
17:12:00.352   Training iter 300, batch loss 1.5315, batch acc 0.9278
17:12:00.862   Training iter 350, batch loss 1.5289, batch acc 0.9272
17:12:01.400   Training iter 400, batch loss 1.5287, batch acc 0.9322
17:12:01.968   Training iter 450, batch loss 1.5285, batch acc 0.9234
17:12:02.533   Training iter 500, batch loss 1.5366, batch acc 0.9202
17:12:03.096   Training iter 550, batch loss 1.5323, batch acc 0.9222
17:12:03.652   Training iter 600, batch loss 1.5332, batch acc 0.9262
17:12:03.654 Training @ 410 epoch...
17:12:04.202   Training iter 50, batch loss 1.5315, batch acc 0.9246
17:12:04.753   Training iter 100, batch loss 1.5313, batch acc 0.9260
17:12:05.298   Training iter 150, batch loss 1.5264, batch acc 0.9330
17:12:05.830   Training iter 200, batch loss 1.5314, batch acc 0.9224
17:12:06.394   Training iter 250, batch loss 1.5295, batch acc 0.9300
17:12:06.923   Training iter 300, batch loss 1.5327, batch acc 0.9242
17:12:07.442   Training iter 350, batch loss 1.5302, batch acc 0.9220
17:12:07.955   Training iter 400, batch loss 1.5268, batch acc 0.9314
17:12:08.471   Training iter 450, batch loss 1.5317, batch acc 0.9262
17:12:08.961   Training iter 500, batch loss 1.5271, batch acc 0.9302
17:12:09.478   Training iter 550, batch loss 1.5310, batch acc 0.9246
17:12:10.009   Training iter 600, batch loss 1.5334, batch acc 0.9236
17:12:10.010 Testing @ 410 epoch...
17:12:10.056     Testing, total mean loss 1.53620, total acc 0.92520
17:12:10.056 Training @ 411 epoch...
17:12:10.595   Training iter 50, batch loss 1.5293, batch acc 0.9272
17:12:11.133   Training iter 100, batch loss 1.5317, batch acc 0.9248
17:12:11.652   Training iter 150, batch loss 1.5292, batch acc 0.9254
17:12:12.184   Training iter 200, batch loss 1.5277, batch acc 0.9266
17:12:12.714   Training iter 250, batch loss 1.5270, batch acc 0.9302
17:12:13.260   Training iter 300, batch loss 1.5251, batch acc 0.9290
17:12:13.805   Training iter 350, batch loss 1.5362, batch acc 0.9200
17:12:14.352   Training iter 400, batch loss 1.5337, batch acc 0.9270
17:12:14.889   Training iter 450, batch loss 1.5311, batch acc 0.9236
17:12:15.403   Training iter 500, batch loss 1.5306, batch acc 0.9300
17:12:15.898   Training iter 550, batch loss 1.5322, batch acc 0.9232
17:12:16.400   Training iter 600, batch loss 1.5295, batch acc 0.9256
17:12:16.402 Training @ 412 epoch...
17:12:16.918   Training iter 50, batch loss 1.5266, batch acc 0.9294
17:12:17.461   Training iter 100, batch loss 1.5274, batch acc 0.9336
17:12:18.021   Training iter 150, batch loss 1.5305, batch acc 0.9252
17:12:18.554   Training iter 200, batch loss 1.5260, batch acc 0.9290
17:12:19.099   Training iter 250, batch loss 1.5266, batch acc 0.9296
17:12:19.634   Training iter 300, batch loss 1.5357, batch acc 0.9190
17:12:20.162   Training iter 350, batch loss 1.5347, batch acc 0.9226
17:12:20.694   Training iter 400, batch loss 1.5304, batch acc 0.9248
17:12:21.251   Training iter 450, batch loss 1.5304, batch acc 0.9260
17:12:21.787   Training iter 500, batch loss 1.5330, batch acc 0.9264
17:12:22.315   Training iter 550, batch loss 1.5328, batch acc 0.9250
17:12:22.854   Training iter 600, batch loss 1.5295, batch acc 0.9290
17:12:22.856 Training @ 413 epoch...
17:12:23.384   Training iter 50, batch loss 1.5322, batch acc 0.9256
17:12:23.909   Training iter 100, batch loss 1.5277, batch acc 0.9270
17:12:24.427   Training iter 150, batch loss 1.5312, batch acc 0.9280
17:12:24.936   Training iter 200, batch loss 1.5325, batch acc 0.9236
17:12:25.462   Training iter 250, batch loss 1.5358, batch acc 0.9188
17:12:26.009   Training iter 300, batch loss 1.5298, batch acc 0.9282
17:12:26.561   Training iter 350, batch loss 1.5289, batch acc 0.9282
17:12:27.112   Training iter 400, batch loss 1.5312, batch acc 0.9266
17:12:27.673   Training iter 450, batch loss 1.5271, batch acc 0.9294
17:12:28.228   Training iter 500, batch loss 1.5282, batch acc 0.9260
17:12:28.771   Training iter 550, batch loss 1.5295, batch acc 0.9252
17:12:29.319   Training iter 600, batch loss 1.5277, batch acc 0.9292
17:12:29.320 Training @ 414 epoch...
17:12:29.870   Training iter 50, batch loss 1.5306, batch acc 0.9252
17:12:30.411   Training iter 100, batch loss 1.5344, batch acc 0.9176
17:12:30.960   Training iter 150, batch loss 1.5323, batch acc 0.9240
17:12:31.518   Training iter 200, batch loss 1.5259, batch acc 0.9306
17:12:32.059   Training iter 250, batch loss 1.5302, batch acc 0.9320
17:12:32.602   Training iter 300, batch loss 1.5293, batch acc 0.9262
17:12:33.126   Training iter 350, batch loss 1.5331, batch acc 0.9280
17:12:33.639   Training iter 400, batch loss 1.5277, batch acc 0.9358
17:12:34.154   Training iter 450, batch loss 1.5261, batch acc 0.9298
17:12:34.679   Training iter 500, batch loss 1.5306, batch acc 0.9252
17:12:35.203   Training iter 550, batch loss 1.5317, batch acc 0.9218
17:12:35.722   Training iter 600, batch loss 1.5304, batch acc 0.9260
17:12:35.724 Training @ 415 epoch...
17:12:36.256   Training iter 50, batch loss 1.5321, batch acc 0.9244
17:12:36.777   Training iter 100, batch loss 1.5307, batch acc 0.9292
17:12:37.278   Training iter 150, batch loss 1.5293, batch acc 0.9280
17:12:37.816   Training iter 200, batch loss 1.5308, batch acc 0.9276
17:12:38.355   Training iter 250, batch loss 1.5315, batch acc 0.9260
17:12:38.860   Training iter 300, batch loss 1.5314, batch acc 0.9232
17:12:39.386   Training iter 350, batch loss 1.5283, batch acc 0.9282
17:12:39.915   Training iter 400, batch loss 1.5288, batch acc 0.9296
17:12:40.451   Training iter 450, batch loss 1.5269, batch acc 0.9300
17:12:40.993   Training iter 500, batch loss 1.5347, batch acc 0.9198
17:12:41.500   Training iter 550, batch loss 1.5278, batch acc 0.9306
17:12:41.959   Training iter 600, batch loss 1.5290, batch acc 0.9262
17:12:41.960 Testing @ 415 epoch...
17:12:42.009     Testing, total mean loss 1.53607, total acc 0.92530
17:12:42.009 Training @ 416 epoch...
17:12:42.479   Training iter 50, batch loss 1.5312, batch acc 0.9240
17:12:42.947   Training iter 100, batch loss 1.5266, batch acc 0.9344
17:12:43.413   Training iter 150, batch loss 1.5287, batch acc 0.9290
17:12:43.898   Training iter 200, batch loss 1.5263, batch acc 0.9312
17:12:44.363   Training iter 250, batch loss 1.5300, batch acc 0.9310
17:12:44.822   Training iter 300, batch loss 1.5336, batch acc 0.9212
17:12:45.283   Training iter 350, batch loss 1.5284, batch acc 0.9276
17:12:45.753   Training iter 400, batch loss 1.5308, batch acc 0.9280
17:12:46.225   Training iter 450, batch loss 1.5304, batch acc 0.9296
17:12:46.690   Training iter 500, batch loss 1.5336, batch acc 0.9224
17:12:47.156   Training iter 550, batch loss 1.5297, batch acc 0.9294
17:12:47.615   Training iter 600, batch loss 1.5328, batch acc 0.9210
17:12:47.617 Training @ 417 epoch...
17:12:48.092   Training iter 50, batch loss 1.5295, batch acc 0.9280
17:12:48.564   Training iter 100, batch loss 1.5320, batch acc 0.9220
17:12:49.026   Training iter 150, batch loss 1.5291, batch acc 0.9270
17:12:49.499   Training iter 200, batch loss 1.5256, batch acc 0.9340
17:12:49.970   Training iter 250, batch loss 1.5270, batch acc 0.9302
17:12:50.447   Training iter 300, batch loss 1.5387, batch acc 0.9188
17:12:50.933   Training iter 350, batch loss 1.5284, batch acc 0.9316
17:12:51.416   Training iter 400, batch loss 1.5277, batch acc 0.9314
17:12:51.916   Training iter 450, batch loss 1.5313, batch acc 0.9266
17:12:52.406   Training iter 500, batch loss 1.5318, batch acc 0.9244
17:12:52.910   Training iter 550, batch loss 1.5282, batch acc 0.9270
17:12:53.424   Training iter 600, batch loss 1.5312, batch acc 0.9234
17:12:53.426 Training @ 418 epoch...
17:12:53.947   Training iter 50, batch loss 1.5264, batch acc 0.9296
17:12:54.474   Training iter 100, batch loss 1.5343, batch acc 0.9224
17:12:54.974   Training iter 150, batch loss 1.5336, batch acc 0.9242
17:12:55.488   Training iter 200, batch loss 1.5305, batch acc 0.9268
17:12:56.003   Training iter 250, batch loss 1.5285, batch acc 0.9286
17:12:56.530   Training iter 300, batch loss 1.5301, batch acc 0.9286
17:12:57.042   Training iter 350, batch loss 1.5247, batch acc 0.9300
17:12:57.573   Training iter 400, batch loss 1.5333, batch acc 0.9200
17:12:58.147   Training iter 450, batch loss 1.5343, batch acc 0.9212
17:12:58.725   Training iter 500, batch loss 1.5288, batch acc 0.9322
17:12:59.306   Training iter 550, batch loss 1.5249, batch acc 0.9348
17:12:59.839   Training iter 600, batch loss 1.5319, batch acc 0.9266
17:12:59.840 Training @ 419 epoch...
17:13:00.391   Training iter 50, batch loss 1.5287, batch acc 0.9292
17:13:00.960   Training iter 100, batch loss 1.5330, batch acc 0.9246
17:13:01.539   Training iter 150, batch loss 1.5247, batch acc 0.9300
17:13:02.118   Training iter 200, batch loss 1.5323, batch acc 0.9264
17:13:02.671   Training iter 250, batch loss 1.5314, batch acc 0.9218
17:13:03.168   Training iter 300, batch loss 1.5328, batch acc 0.9256
17:13:03.675   Training iter 350, batch loss 1.5235, batch acc 0.9342
17:13:04.177   Training iter 400, batch loss 1.5261, batch acc 0.9324
17:13:04.664   Training iter 450, batch loss 1.5268, batch acc 0.9298
17:13:05.173   Training iter 500, batch loss 1.5330, batch acc 0.9220
17:13:05.675   Training iter 550, batch loss 1.5316, batch acc 0.9240
17:13:06.178   Training iter 600, batch loss 1.5367, batch acc 0.9190
17:13:06.180 Training @ 420 epoch...
17:13:06.663   Training iter 50, batch loss 1.5281, batch acc 0.9264
17:13:07.137   Training iter 100, batch loss 1.5295, batch acc 0.9278
17:13:07.607   Training iter 150, batch loss 1.5248, batch acc 0.9316
17:13:08.088   Training iter 200, batch loss 1.5306, batch acc 0.9242
17:13:08.571   Training iter 250, batch loss 1.5324, batch acc 0.9254
17:13:09.036   Training iter 300, batch loss 1.5292, batch acc 0.9290
17:13:09.502   Training iter 350, batch loss 1.5336, batch acc 0.9210
17:13:09.971   Training iter 400, batch loss 1.5317, batch acc 0.9272
17:13:10.441   Training iter 450, batch loss 1.5306, batch acc 0.9244
17:13:10.913   Training iter 500, batch loss 1.5288, batch acc 0.9262
17:13:11.404   Training iter 550, batch loss 1.5296, batch acc 0.9284
17:13:11.921   Training iter 600, batch loss 1.5309, batch acc 0.9292
17:13:11.923 Testing @ 420 epoch...
17:13:11.968     Testing, total mean loss 1.53603, total acc 0.92540
17:13:11.968 Training @ 421 epoch...
17:13:12.504   Training iter 50, batch loss 1.5334, batch acc 0.9242
17:13:13.032   Training iter 100, batch loss 1.5317, batch acc 0.9248
17:13:13.547   Training iter 150, batch loss 1.5267, batch acc 0.9320
17:13:14.056   Training iter 200, batch loss 1.5280, batch acc 0.9300
17:13:14.546   Training iter 250, batch loss 1.5257, batch acc 0.9310
17:13:15.026   Training iter 300, batch loss 1.5339, batch acc 0.9234
17:13:15.519   Training iter 350, batch loss 1.5312, batch acc 0.9258
17:13:16.043   Training iter 400, batch loss 1.5284, batch acc 0.9276
17:13:16.557   Training iter 450, batch loss 1.5288, batch acc 0.9322
17:13:17.063   Training iter 500, batch loss 1.5300, batch acc 0.9280
17:13:17.565   Training iter 550, batch loss 1.5347, batch acc 0.9192
17:13:18.067   Training iter 600, batch loss 1.5273, batch acc 0.9290
17:13:18.069 Training @ 422 epoch...
17:13:18.575   Training iter 50, batch loss 1.5278, batch acc 0.9278
17:13:19.100   Training iter 100, batch loss 1.5317, batch acc 0.9248
17:13:19.619   Training iter 150, batch loss 1.5320, batch acc 0.9276
17:13:20.134   Training iter 200, batch loss 1.5330, batch acc 0.9208
17:13:20.633   Training iter 250, batch loss 1.5272, batch acc 0.9262
17:13:21.136   Training iter 300, batch loss 1.5294, batch acc 0.9302
17:13:21.633   Training iter 350, batch loss 1.5337, batch acc 0.9192
17:13:22.127   Training iter 400, batch loss 1.5304, batch acc 0.9282
17:13:22.645   Training iter 450, batch loss 1.5316, batch acc 0.9278
17:13:23.146   Training iter 500, batch loss 1.5268, batch acc 0.9274
17:13:23.662   Training iter 550, batch loss 1.5274, batch acc 0.9280
17:13:24.178   Training iter 600, batch loss 1.5299, batch acc 0.9298
17:13:24.180 Training @ 423 epoch...
17:13:24.693   Training iter 50, batch loss 1.5322, batch acc 0.9242
17:13:25.208   Training iter 100, batch loss 1.5268, batch acc 0.9300
17:13:25.731   Training iter 150, batch loss 1.5346, batch acc 0.9208
17:13:26.265   Training iter 200, batch loss 1.5307, batch acc 0.9248
17:13:26.790   Training iter 250, batch loss 1.5286, batch acc 0.9288
17:13:27.321   Training iter 300, batch loss 1.5294, batch acc 0.9256
17:13:27.818   Training iter 350, batch loss 1.5278, batch acc 0.9308
17:13:28.314   Training iter 400, batch loss 1.5334, batch acc 0.9246
17:13:28.800   Training iter 450, batch loss 1.5316, batch acc 0.9250
17:13:29.287   Training iter 500, batch loss 1.5297, batch acc 0.9266
17:13:29.772   Training iter 550, batch loss 1.5276, batch acc 0.9336
17:13:30.307   Training iter 600, batch loss 1.5275, batch acc 0.9288
17:13:30.308 Training @ 424 epoch...
17:13:30.864   Training iter 50, batch loss 1.5250, batch acc 0.9292
17:13:31.412   Training iter 100, batch loss 1.5293, batch acc 0.9270
17:13:31.957   Training iter 150, batch loss 1.5285, batch acc 0.9298
17:13:32.473   Training iter 200, batch loss 1.5276, batch acc 0.9294
17:13:32.988   Training iter 250, batch loss 1.5365, batch acc 0.9216
17:13:33.535   Training iter 300, batch loss 1.5316, batch acc 0.9324
17:13:34.091   Training iter 350, batch loss 1.5302, batch acc 0.9266
17:13:34.645   Training iter 400, batch loss 1.5319, batch acc 0.9236
17:13:35.199   Training iter 450, batch loss 1.5286, batch acc 0.9262
17:13:35.744   Training iter 500, batch loss 1.5304, batch acc 0.9262
17:13:36.285   Training iter 550, batch loss 1.5287, batch acc 0.9302
17:13:36.801   Training iter 600, batch loss 1.5318, batch acc 0.9264
17:13:36.803 Training @ 425 epoch...
17:13:37.330   Training iter 50, batch loss 1.5283, batch acc 0.9288
17:13:37.871   Training iter 100, batch loss 1.5275, batch acc 0.9316
17:13:38.421   Training iter 150, batch loss 1.5248, batch acc 0.9330
17:13:38.982   Training iter 200, batch loss 1.5296, batch acc 0.9272
17:13:39.527   Training iter 250, batch loss 1.5314, batch acc 0.9246
17:13:40.055   Training iter 300, batch loss 1.5304, batch acc 0.9258
17:13:40.584   Training iter 350, batch loss 1.5274, batch acc 0.9294
17:13:41.120   Training iter 400, batch loss 1.5345, batch acc 0.9258
17:13:41.645   Training iter 450, batch loss 1.5327, batch acc 0.9250
17:13:42.174   Training iter 500, batch loss 1.5298, batch acc 0.9256
17:13:42.703   Training iter 550, batch loss 1.5318, batch acc 0.9244
17:13:43.247   Training iter 600, batch loss 1.5316, batch acc 0.9274
17:13:43.249 Testing @ 425 epoch...
17:13:43.295     Testing, total mean loss 1.53606, total acc 0.92470
17:13:43.295 Training @ 426 epoch...
17:13:43.837   Training iter 50, batch loss 1.5308, batch acc 0.9246
17:13:44.425   Training iter 100, batch loss 1.5312, batch acc 0.9240
17:13:44.974   Training iter 150, batch loss 1.5311, batch acc 0.9254
17:13:45.502   Training iter 200, batch loss 1.5277, batch acc 0.9276
17:13:46.040   Training iter 250, batch loss 1.5309, batch acc 0.9248
17:13:46.568   Training iter 300, batch loss 1.5276, batch acc 0.9352
17:13:47.094   Training iter 350, batch loss 1.5289, batch acc 0.9282
17:13:47.613   Training iter 400, batch loss 1.5311, batch acc 0.9250
17:13:48.141   Training iter 450, batch loss 1.5270, batch acc 0.9294
17:13:48.682   Training iter 500, batch loss 1.5351, batch acc 0.9224
17:13:49.231   Training iter 550, batch loss 1.5307, batch acc 0.9282
17:13:49.754   Training iter 600, batch loss 1.5281, batch acc 0.9312
17:13:49.756 Training @ 427 epoch...
17:13:50.291   Training iter 50, batch loss 1.5282, batch acc 0.9272
17:13:50.796   Training iter 100, batch loss 1.5293, batch acc 0.9278
17:13:51.317   Training iter 150, batch loss 1.5309, batch acc 0.9238
17:13:51.838   Training iter 200, batch loss 1.5295, batch acc 0.9280
17:13:52.376   Training iter 250, batch loss 1.5313, batch acc 0.9256
17:13:52.918   Training iter 300, batch loss 1.5295, batch acc 0.9300
17:13:53.454   Training iter 350, batch loss 1.5276, batch acc 0.9284
17:13:53.988   Training iter 400, batch loss 1.5313, batch acc 0.9256
17:13:54.552   Training iter 450, batch loss 1.5280, batch acc 0.9310
17:13:55.090   Training iter 500, batch loss 1.5368, batch acc 0.9188
17:13:55.626   Training iter 550, batch loss 1.5285, batch acc 0.9276
17:13:56.168   Training iter 600, batch loss 1.5292, batch acc 0.9276
17:13:56.170 Training @ 428 epoch...
17:13:56.705   Training iter 50, batch loss 1.5288, batch acc 0.9286
17:13:57.235   Training iter 100, batch loss 1.5331, batch acc 0.9244
17:13:57.773   Training iter 150, batch loss 1.5296, batch acc 0.9306
17:13:58.311   Training iter 200, batch loss 1.5252, batch acc 0.9300
17:13:58.844   Training iter 250, batch loss 1.5295, batch acc 0.9290
17:13:59.380   Training iter 300, batch loss 1.5297, batch acc 0.9248
17:13:59.903   Training iter 350, batch loss 1.5339, batch acc 0.9194
17:14:00.442   Training iter 400, batch loss 1.5338, batch acc 0.9252
17:14:00.979   Training iter 450, batch loss 1.5254, batch acc 0.9336
17:14:01.545   Training iter 500, batch loss 1.5303, batch acc 0.9254
17:14:02.122   Training iter 550, batch loss 1.5293, batch acc 0.9296
17:14:02.680   Training iter 600, batch loss 1.5298, batch acc 0.9294
17:14:02.681 Training @ 429 epoch...
17:14:03.243   Training iter 50, batch loss 1.5307, batch acc 0.9234
17:14:03.780   Training iter 100, batch loss 1.5275, batch acc 0.9288
17:14:04.332   Training iter 150, batch loss 1.5311, batch acc 0.9234
17:14:04.891   Training iter 200, batch loss 1.5294, batch acc 0.9270
17:14:05.446   Training iter 250, batch loss 1.5314, batch acc 0.9246
17:14:06.002   Training iter 300, batch loss 1.5282, batch acc 0.9322
17:14:06.581   Training iter 350, batch loss 1.5318, batch acc 0.9260
17:14:07.148   Training iter 400, batch loss 1.5305, batch acc 0.9258
17:14:07.731   Training iter 450, batch loss 1.5288, batch acc 0.9282
17:14:08.279   Training iter 500, batch loss 1.5285, batch acc 0.9280
17:14:08.817   Training iter 550, batch loss 1.5332, batch acc 0.9282
17:14:09.371   Training iter 600, batch loss 1.5275, batch acc 0.9328
17:14:09.372 Training @ 430 epoch...
17:14:09.911   Training iter 50, batch loss 1.5271, batch acc 0.9300
17:14:10.463   Training iter 100, batch loss 1.5332, batch acc 0.9242
17:14:11.016   Training iter 150, batch loss 1.5279, batch acc 0.9266
17:14:11.565   Training iter 200, batch loss 1.5286, batch acc 0.9272
17:14:12.103   Training iter 250, batch loss 1.5293, batch acc 0.9280
17:14:12.641   Training iter 300, batch loss 1.5320, batch acc 0.9200
17:14:13.194   Training iter 350, batch loss 1.5267, batch acc 0.9286
17:14:13.723   Training iter 400, batch loss 1.5323, batch acc 0.9244
17:14:14.244   Training iter 450, batch loss 1.5344, batch acc 0.9230
17:14:14.753   Training iter 500, batch loss 1.5241, batch acc 0.9376
17:14:15.266   Training iter 550, batch loss 1.5332, batch acc 0.9230
17:14:15.783   Training iter 600, batch loss 1.5300, batch acc 0.9250
17:14:15.785 Testing @ 430 epoch...
17:14:15.829     Testing, total mean loss 1.53556, total acc 0.92630
17:14:15.829 Training @ 431 epoch...
17:14:16.364   Training iter 50, batch loss 1.5290, batch acc 0.9292
17:14:16.890   Training iter 100, batch loss 1.5251, batch acc 0.9332
17:14:17.404   Training iter 150, batch loss 1.5316, batch acc 0.9274
17:14:17.919   Training iter 200, batch loss 1.5350, batch acc 0.9166
17:14:18.468   Training iter 250, batch loss 1.5293, batch acc 0.9294
17:14:19.031   Training iter 300, batch loss 1.5327, batch acc 0.9202
17:14:19.581   Training iter 350, batch loss 1.5300, batch acc 0.9284
17:14:20.131   Training iter 400, batch loss 1.5275, batch acc 0.9320
17:14:20.685   Training iter 450, batch loss 1.5281, batch acc 0.9298
17:14:21.255   Training iter 500, batch loss 1.5322, batch acc 0.9264
17:14:21.818   Training iter 550, batch loss 1.5258, batch acc 0.9300
17:14:22.373   Training iter 600, batch loss 1.5319, batch acc 0.9276
17:14:22.374 Training @ 432 epoch...
17:14:22.939   Training iter 50, batch loss 1.5310, batch acc 0.9262
17:14:23.503   Training iter 100, batch loss 1.5317, batch acc 0.9262
17:14:24.034   Training iter 150, batch loss 1.5340, batch acc 0.9224
17:14:24.537   Training iter 200, batch loss 1.5279, batch acc 0.9266
17:14:25.050   Training iter 250, batch loss 1.5265, batch acc 0.9312
17:14:25.567   Training iter 300, batch loss 1.5302, batch acc 0.9286
17:14:26.045   Training iter 350, batch loss 1.5250, batch acc 0.9304
17:14:26.539   Training iter 400, batch loss 1.5287, batch acc 0.9272
17:14:27.016   Training iter 450, batch loss 1.5325, batch acc 0.9250
17:14:27.513   Training iter 500, batch loss 1.5290, batch acc 0.9286
17:14:28.033   Training iter 550, batch loss 1.5312, batch acc 0.9290
17:14:28.584   Training iter 600, batch loss 1.5302, batch acc 0.9270
17:14:28.585 Training @ 433 epoch...
17:14:29.141   Training iter 50, batch loss 1.5298, batch acc 0.9252
17:14:29.689   Training iter 100, batch loss 1.5338, batch acc 0.9236
17:14:30.204   Training iter 150, batch loss 1.5305, batch acc 0.9280
17:14:30.737   Training iter 200, batch loss 1.5274, batch acc 0.9278
17:14:31.269   Training iter 250, batch loss 1.5282, batch acc 0.9290
17:14:31.810   Training iter 300, batch loss 1.5282, batch acc 0.9298
17:14:32.332   Training iter 350, batch loss 1.5282, batch acc 0.9284
17:14:32.833   Training iter 400, batch loss 1.5294, batch acc 0.9266
17:14:33.335   Training iter 450, batch loss 1.5292, batch acc 0.9290
17:14:33.834   Training iter 500, batch loss 1.5245, batch acc 0.9322
17:14:34.347   Training iter 550, batch loss 1.5336, batch acc 0.9248
17:14:34.859   Training iter 600, batch loss 1.5348, batch acc 0.9252
17:14:34.861 Training @ 434 epoch...
17:14:35.373   Training iter 50, batch loss 1.5311, batch acc 0.9246
17:14:35.870   Training iter 100, batch loss 1.5285, batch acc 0.9284
17:14:36.466   Training iter 150, batch loss 1.5323, batch acc 0.9258
17:14:37.046   Training iter 200, batch loss 1.5258, batch acc 0.9314
17:14:37.625   Training iter 250, batch loss 1.5276, batch acc 0.9298
17:14:38.134   Training iter 300, batch loss 1.5323, batch acc 0.9252
17:14:38.668   Training iter 350, batch loss 1.5318, batch acc 0.9240
17:14:39.210   Training iter 400, batch loss 1.5304, batch acc 0.9262
17:14:39.740   Training iter 450, batch loss 1.5258, batch acc 0.9280
17:14:40.271   Training iter 500, batch loss 1.5293, batch acc 0.9294
17:14:40.812   Training iter 550, batch loss 1.5296, batch acc 0.9280
17:14:41.336   Training iter 600, batch loss 1.5333, batch acc 0.9228
17:14:41.337 Training @ 435 epoch...
17:14:41.832   Training iter 50, batch loss 1.5281, batch acc 0.9284
17:14:42.358   Training iter 100, batch loss 1.5296, batch acc 0.9216
17:14:42.862   Training iter 150, batch loss 1.5291, batch acc 0.9306
17:14:43.385   Training iter 200, batch loss 1.5330, batch acc 0.9242
17:14:43.889   Training iter 250, batch loss 1.5323, batch acc 0.9252
17:14:44.414   Training iter 300, batch loss 1.5269, batch acc 0.9354
17:14:44.926   Training iter 350, batch loss 1.5258, batch acc 0.9260
17:14:45.449   Training iter 400, batch loss 1.5302, batch acc 0.9306
17:14:45.941   Training iter 450, batch loss 1.5311, batch acc 0.9244
17:14:46.425   Training iter 500, batch loss 1.5300, batch acc 0.9316
17:14:46.909   Training iter 550, batch loss 1.5323, batch acc 0.9228
17:14:47.391   Training iter 600, batch loss 1.5291, batch acc 0.9276
17:14:47.392 Testing @ 435 epoch...
17:14:47.441     Testing, total mean loss 1.53545, total acc 0.92560
17:14:47.441 Training @ 436 epoch...
17:14:47.941   Training iter 50, batch loss 1.5330, batch acc 0.9236
17:14:48.475   Training iter 100, batch loss 1.5288, batch acc 0.9298
17:14:48.980   Training iter 150, batch loss 1.5319, batch acc 0.9280
17:14:49.466   Training iter 200, batch loss 1.5331, batch acc 0.9236
17:14:49.945   Training iter 250, batch loss 1.5327, batch acc 0.9268
17:14:50.427   Training iter 300, batch loss 1.5239, batch acc 0.9346
17:14:50.902   Training iter 350, batch loss 1.5304, batch acc 0.9280
17:14:51.403   Training iter 400, batch loss 1.5256, batch acc 0.9344
17:14:51.911   Training iter 450, batch loss 1.5304, batch acc 0.9274
17:14:52.444   Training iter 500, batch loss 1.5322, batch acc 0.9204
17:14:52.984   Training iter 550, batch loss 1.5314, batch acc 0.9228
17:14:53.526   Training iter 600, batch loss 1.5241, batch acc 0.9324
17:14:53.527 Training @ 437 epoch...
17:14:54.097   Training iter 50, batch loss 1.5305, batch acc 0.9260
17:14:54.674   Training iter 100, batch loss 1.5269, batch acc 0.9338
17:14:55.227   Training iter 150, batch loss 1.5283, batch acc 0.9258
17:14:55.746   Training iter 200, batch loss 1.5333, batch acc 0.9208
17:14:56.265   Training iter 250, batch loss 1.5298, batch acc 0.9308
17:14:56.789   Training iter 300, batch loss 1.5263, batch acc 0.9300
17:14:57.320   Training iter 350, batch loss 1.5314, batch acc 0.9296
17:14:57.848   Training iter 400, batch loss 1.5357, batch acc 0.9180
17:14:58.370   Training iter 450, batch loss 1.5293, batch acc 0.9314
17:14:58.888   Training iter 500, batch loss 1.5320, batch acc 0.9200
17:14:59.399   Training iter 550, batch loss 1.5247, batch acc 0.9312
17:14:59.897   Training iter 600, batch loss 1.5281, batch acc 0.9318
17:14:59.899 Training @ 438 epoch...
17:15:00.418   Training iter 50, batch loss 1.5256, batch acc 0.9316
17:15:00.931   Training iter 100, batch loss 1.5296, batch acc 0.9254
17:15:01.447   Training iter 150, batch loss 1.5340, batch acc 0.9250
17:15:02.032   Training iter 200, batch loss 1.5254, batch acc 0.9334
17:15:02.598   Training iter 250, batch loss 1.5256, batch acc 0.9272
17:15:03.167   Training iter 300, batch loss 1.5277, batch acc 0.9316
17:15:03.720   Training iter 350, batch loss 1.5331, batch acc 0.9236
17:15:04.280   Training iter 400, batch loss 1.5283, batch acc 0.9290
17:15:04.824   Training iter 450, batch loss 1.5308, batch acc 0.9272
17:15:05.374   Training iter 500, batch loss 1.5351, batch acc 0.9248
17:15:05.928   Training iter 550, batch loss 1.5301, batch acc 0.9284
17:15:06.457   Training iter 600, batch loss 1.5328, batch acc 0.9208
17:15:06.459 Training @ 439 epoch...
17:15:07.037   Training iter 50, batch loss 1.5275, batch acc 0.9294
17:15:07.580   Training iter 100, batch loss 1.5239, batch acc 0.9308
17:15:08.091   Training iter 150, batch loss 1.5297, batch acc 0.9290
17:15:08.608   Training iter 200, batch loss 1.5267, batch acc 0.9326
17:15:09.126   Training iter 250, batch loss 1.5325, batch acc 0.9226
17:15:09.647   Training iter 300, batch loss 1.5280, batch acc 0.9300
17:15:10.172   Training iter 350, batch loss 1.5336, batch acc 0.9208
17:15:10.701   Training iter 400, batch loss 1.5353, batch acc 0.9154
17:15:11.224   Training iter 450, batch loss 1.5336, batch acc 0.9252
17:15:11.739   Training iter 500, batch loss 1.5291, batch acc 0.9308
17:15:12.251   Training iter 550, batch loss 1.5272, batch acc 0.9324
17:15:12.770   Training iter 600, batch loss 1.5303, batch acc 0.9256
17:15:12.772 Training @ 440 epoch...
17:15:13.322   Training iter 50, batch loss 1.5269, batch acc 0.9310
17:15:13.870   Training iter 100, batch loss 1.5308, batch acc 0.9234
17:15:14.444   Training iter 150, batch loss 1.5310, batch acc 0.9248
17:15:14.985   Training iter 200, batch loss 1.5276, batch acc 0.9364
17:15:15.560   Training iter 250, batch loss 1.5254, batch acc 0.9304
17:15:16.096   Training iter 300, batch loss 1.5310, batch acc 0.9256
17:15:16.597   Training iter 350, batch loss 1.5292, batch acc 0.9274
17:15:17.086   Training iter 400, batch loss 1.5284, batch acc 0.9294
17:15:17.577   Training iter 450, batch loss 1.5311, batch acc 0.9248
17:15:18.080   Training iter 500, batch loss 1.5338, batch acc 0.9218
17:15:18.554   Training iter 550, batch loss 1.5300, batch acc 0.9312
17:15:19.019   Training iter 600, batch loss 1.5321, batch acc 0.9238
17:15:19.020 Testing @ 440 epoch...
17:15:19.066     Testing, total mean loss 1.53539, total acc 0.92580
17:15:19.066 Training @ 441 epoch...
17:15:19.542   Training iter 50, batch loss 1.5279, batch acc 0.9262
17:15:20.014   Training iter 100, batch loss 1.5345, batch acc 0.9218
17:15:20.490   Training iter 150, batch loss 1.5282, batch acc 0.9308
17:15:20.974   Training iter 200, batch loss 1.5298, batch acc 0.9250
17:15:21.464   Training iter 250, batch loss 1.5273, batch acc 0.9328
17:15:21.947   Training iter 300, batch loss 1.5335, batch acc 0.9262
17:15:22.446   Training iter 350, batch loss 1.5267, batch acc 0.9292
17:15:22.943   Training iter 400, batch loss 1.5296, batch acc 0.9266
17:15:23.455   Training iter 450, batch loss 1.5283, batch acc 0.9306
17:15:23.959   Training iter 500, batch loss 1.5306, batch acc 0.9280
17:15:24.500   Training iter 550, batch loss 1.5288, batch acc 0.9258
17:15:25.015   Training iter 600, batch loss 1.5307, batch acc 0.9276
17:15:25.017 Training @ 442 epoch...
17:15:25.566   Training iter 50, batch loss 1.5316, batch acc 0.9252
17:15:26.105   Training iter 100, batch loss 1.5303, batch acc 0.9294
17:15:26.645   Training iter 150, batch loss 1.5272, batch acc 0.9258
17:15:27.179   Training iter 200, batch loss 1.5268, batch acc 0.9326
17:15:27.716   Training iter 250, batch loss 1.5304, batch acc 0.9286
17:15:28.262   Training iter 300, batch loss 1.5321, batch acc 0.9276
17:15:28.811   Training iter 350, batch loss 1.5258, batch acc 0.9306
17:15:29.369   Training iter 400, batch loss 1.5261, batch acc 0.9272
17:15:29.909   Training iter 450, batch loss 1.5311, batch acc 0.9274
17:15:30.439   Training iter 500, batch loss 1.5322, batch acc 0.9248
17:15:31.069   Training iter 550, batch loss 1.5300, batch acc 0.9260
17:15:31.759   Training iter 600, batch loss 1.5336, batch acc 0.9234
17:15:31.760 Training @ 443 epoch...
17:15:32.331   Training iter 50, batch loss 1.5266, batch acc 0.9298
17:15:32.836   Training iter 100, batch loss 1.5278, batch acc 0.9296
17:15:33.347   Training iter 150, batch loss 1.5305, batch acc 0.9256
17:15:33.872   Training iter 200, batch loss 1.5345, batch acc 0.9208
17:15:34.391   Training iter 250, batch loss 1.5292, batch acc 0.9284
17:15:34.902   Training iter 300, batch loss 1.5280, batch acc 0.9292
17:15:35.425   Training iter 350, batch loss 1.5263, batch acc 0.9340
17:15:35.925   Training iter 400, batch loss 1.5296, batch acc 0.9278
17:15:36.429   Training iter 450, batch loss 1.5316, batch acc 0.9222
17:15:36.927   Training iter 500, batch loss 1.5330, batch acc 0.9244
17:15:37.430   Training iter 550, batch loss 1.5326, batch acc 0.9246
17:15:37.943   Training iter 600, batch loss 1.5269, batch acc 0.9340
17:15:37.945 Training @ 444 epoch...
17:15:38.473   Training iter 50, batch loss 1.5310, batch acc 0.9256
17:15:38.986   Training iter 100, batch loss 1.5272, batch acc 0.9306
17:15:39.524   Training iter 150, batch loss 1.5297, batch acc 0.9322
17:15:40.040   Training iter 200, batch loss 1.5246, batch acc 0.9312
17:15:40.565   Training iter 250, batch loss 1.5290, batch acc 0.9274
17:15:41.078   Training iter 300, batch loss 1.5298, batch acc 0.9290
17:15:41.599   Training iter 350, batch loss 1.5302, batch acc 0.9244
17:15:42.138   Training iter 400, batch loss 1.5306, batch acc 0.9268
17:15:42.673   Training iter 450, batch loss 1.5329, batch acc 0.9256
17:15:43.207   Training iter 500, batch loss 1.5264, batch acc 0.9304
17:15:43.727   Training iter 550, batch loss 1.5315, batch acc 0.9272
17:15:44.261   Training iter 600, batch loss 1.5334, batch acc 0.9194
17:15:44.263 Training @ 445 epoch...
17:15:44.787   Training iter 50, batch loss 1.5267, batch acc 0.9268
17:15:45.311   Training iter 100, batch loss 1.5341, batch acc 0.9202
17:15:45.838   Training iter 150, batch loss 1.5271, batch acc 0.9292
17:15:46.369   Training iter 200, batch loss 1.5326, batch acc 0.9234
17:15:46.902   Training iter 250, batch loss 1.5293, batch acc 0.9280
17:15:47.441   Training iter 300, batch loss 1.5294, batch acc 0.9276
17:15:48.026   Training iter 350, batch loss 1.5225, batch acc 0.9344
17:15:48.603   Training iter 400, batch loss 1.5333, batch acc 0.9262
17:15:49.191   Training iter 450, batch loss 1.5324, batch acc 0.9234
17:15:49.756   Training iter 500, batch loss 1.5298, batch acc 0.9256
17:15:50.327   Training iter 550, batch loss 1.5284, batch acc 0.9294
17:15:50.874   Training iter 600, batch loss 1.5302, batch acc 0.9260
17:15:50.876 Testing @ 445 epoch...
17:15:50.924     Testing, total mean loss 1.53599, total acc 0.92550
17:15:50.924 Training @ 446 epoch...
17:15:51.448   Training iter 50, batch loss 1.5302, batch acc 0.9244
17:15:51.939   Training iter 100, batch loss 1.5289, batch acc 0.9288
17:15:52.443   Training iter 150, batch loss 1.5325, batch acc 0.9212
17:15:52.936   Training iter 200, batch loss 1.5320, batch acc 0.9274
17:15:53.429   Training iter 250, batch loss 1.5260, batch acc 0.9294
17:15:53.928   Training iter 300, batch loss 1.5266, batch acc 0.9294
17:15:54.443   Training iter 350, batch loss 1.5264, batch acc 0.9326
17:15:54.947   Training iter 400, batch loss 1.5304, batch acc 0.9278
17:15:55.466   Training iter 450, batch loss 1.5306, batch acc 0.9266
17:15:55.988   Training iter 500, batch loss 1.5315, batch acc 0.9270
17:15:56.523   Training iter 550, batch loss 1.5289, batch acc 0.9308
17:15:57.062   Training iter 600, batch loss 1.5313, batch acc 0.9278
17:15:57.064 Training @ 447 epoch...
17:15:57.616   Training iter 50, batch loss 1.5312, batch acc 0.9228
17:15:58.178   Training iter 100, batch loss 1.5285, batch acc 0.9286
17:15:58.715   Training iter 150, batch loss 1.5302, batch acc 0.9240
17:15:59.286   Training iter 200, batch loss 1.5286, batch acc 0.9296
17:15:59.825   Training iter 250, batch loss 1.5330, batch acc 0.9210
17:16:00.384   Training iter 300, batch loss 1.5337, batch acc 0.9252
17:16:00.901   Training iter 350, batch loss 1.5276, batch acc 0.9344
17:16:01.425   Training iter 400, batch loss 1.5287, batch acc 0.9288
17:16:01.977   Training iter 450, batch loss 1.5325, batch acc 0.9258
17:16:02.520   Training iter 500, batch loss 1.5287, batch acc 0.9264
17:16:03.064   Training iter 550, batch loss 1.5252, batch acc 0.9334
17:16:03.612   Training iter 600, batch loss 1.5264, batch acc 0.9296
17:16:03.614 Training @ 448 epoch...
17:16:04.181   Training iter 50, batch loss 1.5304, batch acc 0.9268
17:16:04.731   Training iter 100, batch loss 1.5287, batch acc 0.9268
17:16:05.275   Training iter 150, batch loss 1.5261, batch acc 0.9288
17:16:05.846   Training iter 200, batch loss 1.5310, batch acc 0.9262
17:16:06.422   Training iter 250, batch loss 1.5291, batch acc 0.9328
17:16:06.957   Training iter 300, batch loss 1.5321, batch acc 0.9228
17:16:07.481   Training iter 350, batch loss 1.5288, batch acc 0.9288
17:16:07.999   Training iter 400, batch loss 1.5291, batch acc 0.9322
17:16:08.506   Training iter 450, batch loss 1.5297, batch acc 0.9276
17:16:09.022   Training iter 500, batch loss 1.5315, batch acc 0.9228
17:16:09.552   Training iter 550, batch loss 1.5296, batch acc 0.9274
17:16:10.068   Training iter 600, batch loss 1.5292, batch acc 0.9296
17:16:10.070 Training @ 449 epoch...
17:16:10.591   Training iter 50, batch loss 1.5334, batch acc 0.9218
17:16:11.116   Training iter 100, batch loss 1.5290, batch acc 0.9284
17:16:11.636   Training iter 150, batch loss 1.5260, batch acc 0.9342
17:16:12.149   Training iter 200, batch loss 1.5273, batch acc 0.9258
17:16:12.669   Training iter 250, batch loss 1.5290, batch acc 0.9298
17:16:13.179   Training iter 300, batch loss 1.5323, batch acc 0.9242
17:16:13.675   Training iter 350, batch loss 1.5281, batch acc 0.9268
17:16:14.185   Training iter 400, batch loss 1.5316, batch acc 0.9266
17:16:14.693   Training iter 450, batch loss 1.5325, batch acc 0.9242
17:16:15.213   Training iter 500, batch loss 1.5248, batch acc 0.9354
17:16:15.735   Training iter 550, batch loss 1.5330, batch acc 0.9230
17:16:16.258   Training iter 600, batch loss 1.5283, batch acc 0.9284
17:16:16.260 Training @ 450 epoch...
17:16:16.786   Training iter 50, batch loss 1.5248, batch acc 0.9322
17:16:17.301   Training iter 100, batch loss 1.5312, batch acc 0.9238
17:16:17.829   Training iter 150, batch loss 1.5293, batch acc 0.9272
17:16:18.357   Training iter 200, batch loss 1.5284, batch acc 0.9284
17:16:18.887   Training iter 250, batch loss 1.5291, batch acc 0.9268
17:16:19.417   Training iter 300, batch loss 1.5260, batch acc 0.9310
17:16:19.912   Training iter 350, batch loss 1.5268, batch acc 0.9312
17:16:20.414   Training iter 400, batch loss 1.5314, batch acc 0.9260
17:16:20.913   Training iter 450, batch loss 1.5294, batch acc 0.9304
17:16:21.420   Training iter 500, batch loss 1.5352, batch acc 0.9212
17:16:21.917   Training iter 550, batch loss 1.5320, batch acc 0.9252
17:16:22.421   Training iter 600, batch loss 1.5317, batch acc 0.9256
17:16:22.423 Testing @ 450 epoch...
17:16:22.468     Testing, total mean loss 1.53646, total acc 0.92460
17:16:22.468 Training @ 451 epoch...
17:16:22.974   Training iter 50, batch loss 1.5299, batch acc 0.9276
17:16:23.471   Training iter 100, batch loss 1.5259, batch acc 0.9296
17:16:23.964   Training iter 150, batch loss 1.5287, batch acc 0.9262
17:16:24.467   Training iter 200, batch loss 1.5269, batch acc 0.9304
17:16:24.967   Training iter 250, batch loss 1.5303, batch acc 0.9262
17:16:25.490   Training iter 300, batch loss 1.5319, batch acc 0.9258
17:16:26.067   Training iter 350, batch loss 1.5344, batch acc 0.9194
17:16:26.632   Training iter 400, batch loss 1.5301, batch acc 0.9284
17:16:27.187   Training iter 450, batch loss 1.5315, batch acc 0.9274
17:16:27.751   Training iter 500, batch loss 1.5307, batch acc 0.9216
17:16:28.310   Training iter 550, batch loss 1.5243, batch acc 0.9344
17:16:28.867   Training iter 600, batch loss 1.5295, batch acc 0.9276
17:16:28.869 Training @ 452 epoch...
17:16:29.435   Training iter 50, batch loss 1.5306, batch acc 0.9288
17:16:30.022   Training iter 100, batch loss 1.5304, batch acc 0.9246
17:16:30.609   Training iter 150, batch loss 1.5283, batch acc 0.9284
17:16:31.202   Training iter 200, batch loss 1.5255, batch acc 0.9282
17:16:31.789   Training iter 250, batch loss 1.5353, batch acc 0.9238
17:16:32.382   Training iter 300, batch loss 1.5318, batch acc 0.9246
17:16:32.945   Training iter 350, batch loss 1.5278, batch acc 0.9324
17:16:33.504   Training iter 400, batch loss 1.5288, batch acc 0.9278
17:16:34.024   Training iter 450, batch loss 1.5265, batch acc 0.9306
17:16:34.547   Training iter 500, batch loss 1.5303, batch acc 0.9312
17:16:35.062   Training iter 550, batch loss 1.5315, batch acc 0.9228
17:16:35.581   Training iter 600, batch loss 1.5275, batch acc 0.9280
17:16:35.583 Training @ 453 epoch...
17:16:36.117   Training iter 50, batch loss 1.5284, batch acc 0.9286
17:16:36.653   Training iter 100, batch loss 1.5310, batch acc 0.9300
17:16:37.195   Training iter 150, batch loss 1.5305, batch acc 0.9262
17:16:37.748   Training iter 200, batch loss 1.5276, batch acc 0.9300
17:16:38.307   Training iter 250, batch loss 1.5267, batch acc 0.9254
17:16:38.851   Training iter 300, batch loss 1.5289, batch acc 0.9292
17:16:39.410   Training iter 350, batch loss 1.5321, batch acc 0.9248
17:16:39.935   Training iter 400, batch loss 1.5276, batch acc 0.9304
17:16:40.467   Training iter 450, batch loss 1.5305, batch acc 0.9274
17:16:40.987   Training iter 500, batch loss 1.5282, batch acc 0.9266
17:16:41.504   Training iter 550, batch loss 1.5330, batch acc 0.9230
17:16:42.050   Training iter 600, batch loss 1.5292, batch acc 0.9262
17:16:42.052 Training @ 454 epoch...
17:16:42.602   Training iter 50, batch loss 1.5259, batch acc 0.9348
17:16:43.163   Training iter 100, batch loss 1.5240, batch acc 0.9346
17:16:43.727   Training iter 150, batch loss 1.5294, batch acc 0.9274
17:16:44.290   Training iter 200, batch loss 1.5297, batch acc 0.9332
17:16:44.842   Training iter 250, batch loss 1.5299, batch acc 0.9276
17:16:45.393   Training iter 300, batch loss 1.5288, batch acc 0.9262
17:16:45.946   Training iter 350, batch loss 1.5300, batch acc 0.9306
17:16:46.508   Training iter 400, batch loss 1.5338, batch acc 0.9190
17:16:47.071   Training iter 450, batch loss 1.5341, batch acc 0.9206
17:16:47.630   Training iter 500, batch loss 1.5297, batch acc 0.9280
17:16:48.144   Training iter 550, batch loss 1.5295, batch acc 0.9240
17:16:48.648   Training iter 600, batch loss 1.5307, batch acc 0.9252
17:16:48.650 Training @ 455 epoch...
17:16:49.173   Training iter 50, batch loss 1.5288, batch acc 0.9282
17:16:49.691   Training iter 100, batch loss 1.5305, batch acc 0.9240
17:16:50.212   Training iter 150, batch loss 1.5287, batch acc 0.9282
17:16:50.723   Training iter 200, batch loss 1.5319, batch acc 0.9252
17:16:51.244   Training iter 250, batch loss 1.5298, batch acc 0.9248
17:16:51.771   Training iter 300, batch loss 1.5295, batch acc 0.9296
17:16:52.305   Training iter 350, batch loss 1.5293, batch acc 0.9274
17:16:52.840   Training iter 400, batch loss 1.5302, batch acc 0.9248
17:16:53.381   Training iter 450, batch loss 1.5303, batch acc 0.9266
17:16:53.914   Training iter 500, batch loss 1.5282, batch acc 0.9304
17:16:54.483   Training iter 550, batch loss 1.5282, batch acc 0.9306
17:16:55.040   Training iter 600, batch loss 1.5283, batch acc 0.9328
17:16:55.042 Testing @ 455 epoch...
17:16:55.091     Testing, total mean loss 1.53584, total acc 0.92640
17:16:55.091 Training @ 456 epoch...
17:16:55.640   Training iter 50, batch loss 1.5359, batch acc 0.9200
17:16:56.172   Training iter 100, batch loss 1.5297, batch acc 0.9274
17:16:56.741   Training iter 150, batch loss 1.5266, batch acc 0.9322
17:16:57.322   Training iter 200, batch loss 1.5310, batch acc 0.9256
17:16:57.844   Training iter 250, batch loss 1.5341, batch acc 0.9246
17:16:58.379   Training iter 300, batch loss 1.5284, batch acc 0.9308
17:16:58.906   Training iter 350, batch loss 1.5289, batch acc 0.9256
17:16:59.448   Training iter 400, batch loss 1.5253, batch acc 0.9340
17:16:59.995   Training iter 450, batch loss 1.5272, batch acc 0.9284
17:17:00.548   Training iter 500, batch loss 1.5241, batch acc 0.9320
17:17:01.095   Training iter 550, batch loss 1.5308, batch acc 0.9258
17:17:01.678   Training iter 600, batch loss 1.5304, batch acc 0.9236
17:17:01.680 Training @ 457 epoch...
17:17:02.266   Training iter 50, batch loss 1.5329, batch acc 0.9246
17:17:02.844   Training iter 100, batch loss 1.5325, batch acc 0.9234
17:17:03.423   Training iter 150, batch loss 1.5278, batch acc 0.9298
17:17:03.975   Training iter 200, batch loss 1.5292, batch acc 0.9264
17:17:04.507   Training iter 250, batch loss 1.5309, batch acc 0.9270
17:17:05.050   Training iter 300, batch loss 1.5301, batch acc 0.9240
17:17:05.608   Training iter 350, batch loss 1.5260, batch acc 0.9318
17:17:06.154   Training iter 400, batch loss 1.5291, batch acc 0.9266
17:17:06.708   Training iter 450, batch loss 1.5269, batch acc 0.9316
17:17:07.252   Training iter 500, batch loss 1.5285, batch acc 0.9318
17:17:07.789   Training iter 550, batch loss 1.5300, batch acc 0.9268
17:17:08.316   Training iter 600, batch loss 1.5292, batch acc 0.9262
17:17:08.318 Training @ 458 epoch...
17:17:08.843   Training iter 50, batch loss 1.5296, batch acc 0.9254
17:17:09.373   Training iter 100, batch loss 1.5332, batch acc 0.9192
17:17:09.898   Training iter 150, batch loss 1.5302, batch acc 0.9214
17:17:10.426   Training iter 200, batch loss 1.5319, batch acc 0.9252
17:17:10.942   Training iter 250, batch loss 1.5339, batch acc 0.9266
17:17:11.467   Training iter 300, batch loss 1.5256, batch acc 0.9322
17:17:11.990   Training iter 350, batch loss 1.5300, batch acc 0.9268
17:17:12.521   Training iter 400, batch loss 1.5270, batch acc 0.9294
17:17:13.064   Training iter 450, batch loss 1.5296, batch acc 0.9298
17:17:13.587   Training iter 500, batch loss 1.5293, batch acc 0.9296
17:17:14.120   Training iter 550, batch loss 1.5259, batch acc 0.9278
17:17:14.684   Training iter 600, batch loss 1.5283, batch acc 0.9300
17:17:14.685 Training @ 459 epoch...
17:17:15.270   Training iter 50, batch loss 1.5309, batch acc 0.9276
17:17:15.848   Training iter 100, batch loss 1.5291, batch acc 0.9296
17:17:16.387   Training iter 150, batch loss 1.5287, batch acc 0.9282
17:17:16.881   Training iter 200, batch loss 1.5263, batch acc 0.9338
17:17:17.386   Training iter 250, batch loss 1.5253, batch acc 0.9328
17:17:17.907   Training iter 300, batch loss 1.5345, batch acc 0.9206
17:17:18.440   Training iter 350, batch loss 1.5288, batch acc 0.9240
17:17:18.961   Training iter 400, batch loss 1.5287, batch acc 0.9258
17:17:19.474   Training iter 450, batch loss 1.5265, batch acc 0.9322
17:17:19.997   Training iter 500, batch loss 1.5324, batch acc 0.9224
17:17:20.526   Training iter 550, batch loss 1.5320, batch acc 0.9252
17:17:21.042   Training iter 600, batch loss 1.5289, batch acc 0.9268
17:17:21.044 Training @ 460 epoch...
17:17:21.569   Training iter 50, batch loss 1.5321, batch acc 0.9254
17:17:22.099   Training iter 100, batch loss 1.5303, batch acc 0.9262
17:17:22.621   Training iter 150, batch loss 1.5309, batch acc 0.9284
17:17:23.136   Training iter 200, batch loss 1.5273, batch acc 0.9314
17:17:23.646   Training iter 250, batch loss 1.5275, batch acc 0.9316
17:17:24.169   Training iter 300, batch loss 1.5302, batch acc 0.9262
17:17:24.655   Training iter 350, batch loss 1.5253, batch acc 0.9296
17:17:25.127   Training iter 400, batch loss 1.5337, batch acc 0.9204
17:17:25.642   Training iter 450, batch loss 1.5282, batch acc 0.9300
17:17:26.142   Training iter 500, batch loss 1.5247, batch acc 0.9328
17:17:26.638   Training iter 550, batch loss 1.5305, batch acc 0.9262
17:17:27.154   Training iter 600, batch loss 1.5316, batch acc 0.9248
17:17:27.156 Testing @ 460 epoch...
17:17:27.201     Testing, total mean loss 1.53523, total acc 0.92520
17:17:27.202 Training @ 461 epoch...
17:17:27.747   Training iter 50, batch loss 1.5313, batch acc 0.9244
17:17:28.273   Training iter 100, batch loss 1.5251, batch acc 0.9316
17:17:28.799   Training iter 150, batch loss 1.5283, batch acc 0.9246
17:17:29.340   Training iter 200, batch loss 1.5305, batch acc 0.9244
17:17:29.863   Training iter 250, batch loss 1.5275, batch acc 0.9296
17:17:30.386   Training iter 300, batch loss 1.5297, batch acc 0.9260
17:17:30.919   Training iter 350, batch loss 1.5269, batch acc 0.9328
17:17:31.454   Training iter 400, batch loss 1.5323, batch acc 0.9266
17:17:31.977   Training iter 450, batch loss 1.5291, batch acc 0.9294
17:17:32.502   Training iter 500, batch loss 1.5275, batch acc 0.9308
17:17:33.030   Training iter 550, batch loss 1.5314, batch acc 0.9262
17:17:33.548   Training iter 600, batch loss 1.5332, batch acc 0.9214
17:17:33.550 Training @ 462 epoch...
17:17:34.087   Training iter 50, batch loss 1.5333, batch acc 0.9244
17:17:34.619   Training iter 100, batch loss 1.5299, batch acc 0.9310
17:17:35.137   Training iter 150, batch loss 1.5286, batch acc 0.9232
17:17:35.659   Training iter 200, batch loss 1.5309, batch acc 0.9274
17:17:36.179   Training iter 250, batch loss 1.5225, batch acc 0.9300
17:17:36.701   Training iter 300, batch loss 1.5280, batch acc 0.9310
17:17:37.223   Training iter 350, batch loss 1.5268, batch acc 0.9302
17:17:37.743   Training iter 400, batch loss 1.5283, batch acc 0.9282
17:17:38.277   Training iter 450, batch loss 1.5329, batch acc 0.9274
17:17:38.795   Training iter 500, batch loss 1.5298, batch acc 0.9254
17:17:39.322   Training iter 550, batch loss 1.5273, batch acc 0.9334
17:17:39.841   Training iter 600, batch loss 1.5349, batch acc 0.9216
17:17:39.843 Training @ 463 epoch...
17:17:40.377   Training iter 50, batch loss 1.5289, batch acc 0.9296
17:17:40.894   Training iter 100, batch loss 1.5309, batch acc 0.9298
17:17:41.424   Training iter 150, batch loss 1.5305, batch acc 0.9256
17:17:41.949   Training iter 200, batch loss 1.5319, batch acc 0.9218
17:17:42.485   Training iter 250, batch loss 1.5309, batch acc 0.9226
17:17:43.018   Training iter 300, batch loss 1.5322, batch acc 0.9240
17:17:43.533   Training iter 350, batch loss 1.5318, batch acc 0.9250
17:17:44.063   Training iter 400, batch loss 1.5257, batch acc 0.9340
17:17:44.588   Training iter 450, batch loss 1.5288, batch acc 0.9266
17:17:45.101   Training iter 500, batch loss 1.5281, batch acc 0.9300
17:17:45.621   Training iter 550, batch loss 1.5249, batch acc 0.9346
17:17:46.152   Training iter 600, batch loss 1.5275, batch acc 0.9284
17:17:46.153 Training @ 464 epoch...
17:17:46.669   Training iter 50, batch loss 1.5305, batch acc 0.9282
17:17:47.166   Training iter 100, batch loss 1.5316, batch acc 0.9234
17:17:47.695   Training iter 150, batch loss 1.5290, batch acc 0.9278
17:17:48.192   Training iter 200, batch loss 1.5283, batch acc 0.9260
17:17:48.678   Training iter 250, batch loss 1.5279, batch acc 0.9304
17:17:49.178   Training iter 300, batch loss 1.5267, batch acc 0.9330
17:17:49.682   Training iter 350, batch loss 1.5281, batch acc 0.9306
17:17:50.172   Training iter 400, batch loss 1.5305, batch acc 0.9256
17:17:50.661   Training iter 450, batch loss 1.5281, batch acc 0.9260
17:17:51.149   Training iter 500, batch loss 1.5314, batch acc 0.9278
17:17:51.630   Training iter 550, batch loss 1.5281, batch acc 0.9262
17:17:52.113   Training iter 600, batch loss 1.5323, batch acc 0.9224
17:17:52.114 Training @ 465 epoch...
17:17:52.606   Training iter 50, batch loss 1.5260, batch acc 0.9312
17:17:53.094   Training iter 100, batch loss 1.5307, batch acc 0.9276
17:17:53.589   Training iter 150, batch loss 1.5282, batch acc 0.9302
17:17:54.082   Training iter 200, batch loss 1.5297, batch acc 0.9244
17:17:54.592   Training iter 250, batch loss 1.5366, batch acc 0.9180
17:17:55.088   Training iter 300, batch loss 1.5271, batch acc 0.9282
17:17:55.595   Training iter 350, batch loss 1.5253, batch acc 0.9286
17:17:56.087   Training iter 400, batch loss 1.5287, batch acc 0.9310
17:17:56.579   Training iter 450, batch loss 1.5333, batch acc 0.9230
17:17:57.081   Training iter 500, batch loss 1.5265, batch acc 0.9330
17:17:57.586   Training iter 550, batch loss 1.5278, batch acc 0.9304
17:17:58.105   Training iter 600, batch loss 1.5315, batch acc 0.9292
17:17:58.107 Testing @ 465 epoch...
17:17:58.152     Testing, total mean loss 1.53733, total acc 0.92260
17:17:58.152 Training @ 466 epoch...
17:17:58.678   Training iter 50, batch loss 1.5302, batch acc 0.9258
17:17:59.220   Training iter 100, batch loss 1.5255, batch acc 0.9338
17:17:59.752   Training iter 150, batch loss 1.5319, batch acc 0.9218
17:18:00.295   Training iter 200, batch loss 1.5291, batch acc 0.9258
17:18:00.838   Training iter 250, batch loss 1.5298, batch acc 0.9218
17:18:01.367   Training iter 300, batch loss 1.5341, batch acc 0.9220
17:18:01.954   Training iter 350, batch loss 1.5342, batch acc 0.9210
17:18:02.535   Training iter 400, batch loss 1.5268, batch acc 0.9300
17:18:03.123   Training iter 450, batch loss 1.5255, batch acc 0.9342
17:18:03.692   Training iter 500, batch loss 1.5313, batch acc 0.9274
17:18:04.237   Training iter 550, batch loss 1.5256, batch acc 0.9318
17:18:04.800   Training iter 600, batch loss 1.5276, batch acc 0.9280
17:18:04.802 Training @ 467 epoch...
17:18:05.365   Training iter 50, batch loss 1.5283, batch acc 0.9298
17:18:05.942   Training iter 100, batch loss 1.5274, batch acc 0.9272
17:18:06.519   Training iter 150, batch loss 1.5298, batch acc 0.9282
17:18:07.078   Training iter 200, batch loss 1.5304, batch acc 0.9292
17:18:07.627   Training iter 250, batch loss 1.5244, batch acc 0.9336
17:18:08.136   Training iter 300, batch loss 1.5319, batch acc 0.9228
17:18:08.640   Training iter 350, batch loss 1.5287, batch acc 0.9268
17:18:09.150   Training iter 400, batch loss 1.5298, batch acc 0.9274
17:18:09.690   Training iter 450, batch loss 1.5288, batch acc 0.9288
17:18:10.220   Training iter 500, batch loss 1.5295, batch acc 0.9312
17:18:10.769   Training iter 550, batch loss 1.5303, batch acc 0.9208
17:18:11.313   Training iter 600, batch loss 1.5324, batch acc 0.9262
17:18:11.314 Training @ 468 epoch...
17:18:11.856   Training iter 50, batch loss 1.5330, batch acc 0.9258
17:18:12.388   Training iter 100, batch loss 1.5281, batch acc 0.9266
17:18:12.929   Training iter 150, batch loss 1.5244, batch acc 0.9322
17:18:13.463   Training iter 200, batch loss 1.5301, batch acc 0.9280
17:18:14.017   Training iter 250, batch loss 1.5275, batch acc 0.9290
17:18:14.582   Training iter 300, batch loss 1.5291, batch acc 0.9246
17:18:15.144   Training iter 350, batch loss 1.5298, batch acc 0.9296
17:18:15.877   Training iter 400, batch loss 1.5322, batch acc 0.9260
17:18:16.464   Training iter 450, batch loss 1.5258, batch acc 0.9320
17:18:16.996   Training iter 500, batch loss 1.5287, batch acc 0.9296
17:18:17.537   Training iter 550, batch loss 1.5317, batch acc 0.9236
17:18:18.055   Training iter 600, batch loss 1.5308, batch acc 0.9266
17:18:18.057 Training @ 469 epoch...
17:18:18.572   Training iter 50, batch loss 1.5305, batch acc 0.9236
17:18:19.096   Training iter 100, batch loss 1.5290, batch acc 0.9266
17:18:19.617   Training iter 150, batch loss 1.5312, batch acc 0.9246
17:18:20.136   Training iter 200, batch loss 1.5248, batch acc 0.9368
17:18:20.654   Training iter 250, batch loss 1.5342, batch acc 0.9240
17:18:21.173   Training iter 300, batch loss 1.5260, batch acc 0.9292
17:18:21.703   Training iter 350, batch loss 1.5286, batch acc 0.9278
17:18:22.242   Training iter 400, batch loss 1.5275, batch acc 0.9288
17:18:22.797   Training iter 450, batch loss 1.5276, batch acc 0.9316
17:18:23.346   Training iter 500, batch loss 1.5332, batch acc 0.9202
17:18:23.879   Training iter 550, batch loss 1.5285, batch acc 0.9306
17:18:24.428   Training iter 600, batch loss 1.5293, batch acc 0.9278
17:18:24.430 Training @ 470 epoch...
17:18:24.970   Training iter 50, batch loss 1.5316, batch acc 0.9284
17:18:25.512   Training iter 100, batch loss 1.5312, batch acc 0.9242
17:18:26.069   Training iter 150, batch loss 1.5288, batch acc 0.9278
17:18:26.605   Training iter 200, batch loss 1.5306, batch acc 0.9266
17:18:27.153   Training iter 250, batch loss 1.5295, batch acc 0.9300
17:18:27.732   Training iter 300, batch loss 1.5257, batch acc 0.9298
17:18:28.272   Training iter 350, batch loss 1.5289, batch acc 0.9288
17:18:28.816   Training iter 400, batch loss 1.5297, batch acc 0.9298
17:18:29.360   Training iter 450, batch loss 1.5308, batch acc 0.9242
17:18:29.903   Training iter 500, batch loss 1.5269, batch acc 0.9288
17:18:30.437   Training iter 550, batch loss 1.5259, batch acc 0.9332
17:18:30.966   Training iter 600, batch loss 1.5304, batch acc 0.9244
17:18:30.968 Testing @ 470 epoch...
17:18:31.014     Testing, total mean loss 1.53508, total acc 0.92540
17:18:31.014 Training @ 471 epoch...
17:18:31.559   Training iter 50, batch loss 1.5254, batch acc 0.9302
17:18:32.085   Training iter 100, batch loss 1.5245, batch acc 0.9332
17:18:32.600   Training iter 150, batch loss 1.5330, batch acc 0.9208
17:18:33.161   Training iter 200, batch loss 1.5260, batch acc 0.9308
17:18:33.702   Training iter 250, batch loss 1.5302, batch acc 0.9292
17:18:34.244   Training iter 300, batch loss 1.5289, batch acc 0.9312
17:18:34.807   Training iter 350, batch loss 1.5323, batch acc 0.9232
17:18:35.366   Training iter 400, batch loss 1.5286, batch acc 0.9284
17:18:35.930   Training iter 450, batch loss 1.5306, batch acc 0.9298
17:18:36.492   Training iter 500, batch loss 1.5306, batch acc 0.9274
17:18:37.052   Training iter 550, batch loss 1.5291, batch acc 0.9290
17:18:37.591   Training iter 600, batch loss 1.5315, batch acc 0.9214
17:18:37.593 Training @ 472 epoch...
17:18:38.130   Training iter 50, batch loss 1.5298, batch acc 0.9270
17:18:38.656   Training iter 100, batch loss 1.5267, batch acc 0.9320
17:18:39.198   Training iter 150, batch loss 1.5295, batch acc 0.9256
17:18:39.742   Training iter 200, batch loss 1.5285, batch acc 0.9266
17:18:40.280   Training iter 250, batch loss 1.5343, batch acc 0.9246
17:18:40.807   Training iter 300, batch loss 1.5304, batch acc 0.9278
17:18:41.347   Training iter 350, batch loss 1.5263, batch acc 0.9334
17:18:41.882   Training iter 400, batch loss 1.5303, batch acc 0.9284
17:18:42.429   Training iter 450, batch loss 1.5281, batch acc 0.9288
17:18:43.034   Training iter 500, batch loss 1.5327, batch acc 0.9202
17:18:43.635   Training iter 550, batch loss 1.5257, batch acc 0.9312
17:18:44.240   Training iter 600, batch loss 1.5281, batch acc 0.9278
17:18:44.242 Training @ 473 epoch...
17:18:44.799   Training iter 50, batch loss 1.5268, batch acc 0.9298
17:18:45.348   Training iter 100, batch loss 1.5304, batch acc 0.9300
17:18:45.899   Training iter 150, batch loss 1.5333, batch acc 0.9262
17:18:46.454   Training iter 200, batch loss 1.5279, batch acc 0.9298
17:18:46.991   Training iter 250, batch loss 1.5272, batch acc 0.9266
17:18:47.536   Training iter 300, batch loss 1.5290, batch acc 0.9264
17:18:48.066   Training iter 350, batch loss 1.5250, batch acc 0.9326
17:18:48.592   Training iter 400, batch loss 1.5307, batch acc 0.9246
17:18:49.121   Training iter 450, batch loss 1.5356, batch acc 0.9232
17:18:49.658   Training iter 500, batch loss 1.5271, batch acc 0.9296
17:18:50.188   Training iter 550, batch loss 1.5263, batch acc 0.9286
17:18:50.721   Training iter 600, batch loss 1.5306, batch acc 0.9256
17:18:50.722 Training @ 474 epoch...
17:18:51.256   Training iter 50, batch loss 1.5305, batch acc 0.9298
17:18:51.783   Training iter 100, batch loss 1.5273, batch acc 0.9316
17:18:52.317   Training iter 150, batch loss 1.5312, batch acc 0.9284
17:18:52.842   Training iter 200, batch loss 1.5312, batch acc 0.9276
17:18:53.381   Training iter 250, batch loss 1.5295, batch acc 0.9312
17:18:53.912   Training iter 300, batch loss 1.5248, batch acc 0.9286
17:18:54.456   Training iter 350, batch loss 1.5260, batch acc 0.9322
17:18:54.988   Training iter 400, batch loss 1.5273, batch acc 0.9308
17:18:55.550   Training iter 450, batch loss 1.5273, batch acc 0.9296
17:18:56.103   Training iter 500, batch loss 1.5304, batch acc 0.9256
17:18:56.643   Training iter 550, batch loss 1.5326, batch acc 0.9246
17:18:57.188   Training iter 600, batch loss 1.5326, batch acc 0.9188
17:18:57.189 Training @ 475 epoch...
17:18:57.727   Training iter 50, batch loss 1.5295, batch acc 0.9242
17:18:58.263   Training iter 100, batch loss 1.5296, batch acc 0.9264
17:18:58.779   Training iter 150, batch loss 1.5294, batch acc 0.9276
17:18:59.313   Training iter 200, batch loss 1.5284, batch acc 0.9260
17:18:59.839   Training iter 250, batch loss 1.5290, batch acc 0.9328
17:19:00.370   Training iter 300, batch loss 1.5304, batch acc 0.9266
17:19:00.888   Training iter 350, batch loss 1.5318, batch acc 0.9274
17:19:01.443   Training iter 400, batch loss 1.5323, batch acc 0.9232
17:19:02.195   Training iter 450, batch loss 1.5272, batch acc 0.9318
17:19:02.834   Training iter 500, batch loss 1.5295, batch acc 0.9260
17:19:03.356   Training iter 550, batch loss 1.5237, batch acc 0.9370
17:19:03.875   Training iter 600, batch loss 1.5289, batch acc 0.9296
17:19:03.877 Testing @ 475 epoch...
17:19:03.922     Testing, total mean loss 1.53524, total acc 0.92500
17:19:03.922 Training @ 476 epoch...
17:19:04.438   Training iter 50, batch loss 1.5311, batch acc 0.9270
17:19:04.952   Training iter 100, batch loss 1.5289, batch acc 0.9300
17:19:05.465   Training iter 150, batch loss 1.5305, batch acc 0.9244
17:19:05.978   Training iter 200, batch loss 1.5257, batch acc 0.9312
17:19:06.508   Training iter 250, batch loss 1.5289, batch acc 0.9260
17:19:07.027   Training iter 300, batch loss 1.5287, batch acc 0.9276
17:19:07.528   Training iter 350, batch loss 1.5281, batch acc 0.9320
17:19:08.010   Training iter 400, batch loss 1.5286, batch acc 0.9272
17:19:08.504   Training iter 450, batch loss 1.5327, batch acc 0.9228
17:19:08.999   Training iter 500, batch loss 1.5321, batch acc 0.9292
17:19:09.515   Training iter 550, batch loss 1.5258, batch acc 0.9312
17:19:09.996   Training iter 600, batch loss 1.5280, batch acc 0.9276
17:19:09.998 Training @ 477 epoch...
17:19:10.498   Training iter 50, batch loss 1.5268, batch acc 0.9272
17:19:10.994   Training iter 100, batch loss 1.5312, batch acc 0.9260
17:19:11.472   Training iter 150, batch loss 1.5293, batch acc 0.9290
17:19:11.958   Training iter 200, batch loss 1.5225, batch acc 0.9340
17:19:12.455   Training iter 250, batch loss 1.5273, batch acc 0.9312
17:19:12.938   Training iter 300, batch loss 1.5293, batch acc 0.9252
17:19:13.438   Training iter 350, batch loss 1.5306, batch acc 0.9242
17:19:13.946   Training iter 400, batch loss 1.5312, batch acc 0.9258
17:19:14.469   Training iter 450, batch loss 1.5287, batch acc 0.9324
17:19:14.973   Training iter 500, batch loss 1.5333, batch acc 0.9230
17:19:15.460   Training iter 550, batch loss 1.5303, batch acc 0.9232
17:19:15.944   Training iter 600, batch loss 1.5286, batch acc 0.9290
17:19:15.945 Training @ 478 epoch...
17:19:16.434   Training iter 50, batch loss 1.5316, batch acc 0.9260
17:19:16.898   Training iter 100, batch loss 1.5325, batch acc 0.9270
17:19:17.349   Training iter 150, batch loss 1.5334, batch acc 0.9228
17:19:17.851   Training iter 200, batch loss 1.5317, batch acc 0.9272
17:19:18.349   Training iter 250, batch loss 1.5304, batch acc 0.9242
17:19:18.839   Training iter 300, batch loss 1.5244, batch acc 0.9330
17:19:19.332   Training iter 350, batch loss 1.5251, batch acc 0.9326
17:19:19.834   Training iter 400, batch loss 1.5293, batch acc 0.9266
17:19:20.337   Training iter 450, batch loss 1.5271, batch acc 0.9308
17:19:20.862   Training iter 500, batch loss 1.5268, batch acc 0.9280
17:19:21.412   Training iter 550, batch loss 1.5290, batch acc 0.9296
17:19:21.949   Training iter 600, batch loss 1.5281, batch acc 0.9278
17:19:21.951 Training @ 479 epoch...
17:19:22.504   Training iter 50, batch loss 1.5280, batch acc 0.9274
17:19:23.053   Training iter 100, batch loss 1.5276, batch acc 0.9266
17:19:23.608   Training iter 150, batch loss 1.5275, batch acc 0.9296
17:19:24.165   Training iter 200, batch loss 1.5269, batch acc 0.9294
17:19:24.720   Training iter 250, batch loss 1.5311, batch acc 0.9278
17:19:25.268   Training iter 300, batch loss 1.5335, batch acc 0.9230
17:19:25.807   Training iter 350, batch loss 1.5303, batch acc 0.9246
17:19:26.347   Training iter 400, batch loss 1.5293, batch acc 0.9286
17:19:26.871   Training iter 450, batch loss 1.5263, batch acc 0.9308
17:19:27.401   Training iter 500, batch loss 1.5277, batch acc 0.9292
17:19:27.934   Training iter 550, batch loss 1.5287, batch acc 0.9310
17:19:28.445   Training iter 600, batch loss 1.5329, batch acc 0.9186
17:19:28.446 Training @ 480 epoch...
17:19:28.940   Training iter 50, batch loss 1.5281, batch acc 0.9294
17:19:29.453   Training iter 100, batch loss 1.5318, batch acc 0.9232
17:19:29.951   Training iter 150, batch loss 1.5277, batch acc 0.9288
17:19:30.459   Training iter 200, batch loss 1.5282, batch acc 0.9312
17:19:30.946   Training iter 250, batch loss 1.5346, batch acc 0.9238
17:19:31.455   Training iter 300, batch loss 1.5255, batch acc 0.9346
17:19:31.954   Training iter 350, batch loss 1.5324, batch acc 0.9236
17:19:32.462   Training iter 400, batch loss 1.5297, batch acc 0.9250
17:19:32.956   Training iter 450, batch loss 1.5286, batch acc 0.9290
17:19:33.449   Training iter 500, batch loss 1.5291, batch acc 0.9244
17:19:33.934   Training iter 550, batch loss 1.5296, batch acc 0.9256
17:19:34.437   Training iter 600, batch loss 1.5240, batch acc 0.9340
17:19:34.439 Testing @ 480 epoch...
17:19:34.484     Testing, total mean loss 1.53540, total acc 0.92540
17:19:34.484 Training @ 481 epoch...
17:19:34.989   Training iter 50, batch loss 1.5285, batch acc 0.9294
17:19:35.502   Training iter 100, batch loss 1.5309, batch acc 0.9232
17:19:36.009   Training iter 150, batch loss 1.5334, batch acc 0.9234
17:19:36.525   Training iter 200, batch loss 1.5296, batch acc 0.9308
17:19:37.060   Training iter 250, batch loss 1.5282, batch acc 0.9230
17:19:37.616   Training iter 300, batch loss 1.5278, batch acc 0.9302
17:19:38.169   Training iter 350, batch loss 1.5276, batch acc 0.9268
17:19:38.721   Training iter 400, batch loss 1.5297, batch acc 0.9278
17:19:39.270   Training iter 450, batch loss 1.5289, batch acc 0.9272
17:19:39.829   Training iter 500, batch loss 1.5309, batch acc 0.9244
17:19:40.392   Training iter 550, batch loss 1.5256, batch acc 0.9340
17:19:40.983   Training iter 600, batch loss 1.5280, batch acc 0.9264
17:19:40.984 Training @ 482 epoch...
17:19:41.572   Training iter 50, batch loss 1.5264, batch acc 0.9302
17:19:42.150   Training iter 100, batch loss 1.5286, batch acc 0.9288
17:19:42.693   Training iter 150, batch loss 1.5247, batch acc 0.9356
17:19:43.190   Training iter 200, batch loss 1.5281, batch acc 0.9300
17:19:43.690   Training iter 250, batch loss 1.5270, batch acc 0.9302
17:19:44.212   Training iter 300, batch loss 1.5273, batch acc 0.9282
17:19:44.741   Training iter 350, batch loss 1.5288, batch acc 0.9282
17:19:45.262   Training iter 400, batch loss 1.5339, batch acc 0.9240
17:19:45.780   Training iter 450, batch loss 1.5264, batch acc 0.9312
17:19:46.292   Training iter 500, batch loss 1.5333, batch acc 0.9212
17:19:46.806   Training iter 550, batch loss 1.5328, batch acc 0.9210
17:19:47.330   Training iter 600, batch loss 1.5307, batch acc 0.9274
17:19:47.331 Training @ 483 epoch...
17:19:47.832   Training iter 50, batch loss 1.5270, batch acc 0.9300
17:19:48.325   Training iter 100, batch loss 1.5275, batch acc 0.9292
17:19:48.797   Training iter 150, batch loss 1.5287, batch acc 0.9270
17:19:49.290   Training iter 200, batch loss 1.5305, batch acc 0.9282
17:19:49.787   Training iter 250, batch loss 1.5297, batch acc 0.9256
17:19:50.273   Training iter 300, batch loss 1.5311, batch acc 0.9242
17:19:50.753   Training iter 350, batch loss 1.5270, batch acc 0.9310
17:19:51.228   Training iter 400, batch loss 1.5256, batch acc 0.9316
17:19:51.714   Training iter 450, batch loss 1.5282, batch acc 0.9272
17:19:52.197   Training iter 500, batch loss 1.5299, batch acc 0.9294
17:19:52.681   Training iter 550, batch loss 1.5313, batch acc 0.9248
17:19:53.185   Training iter 600, batch loss 1.5318, batch acc 0.9258
17:19:53.187 Training @ 484 epoch...
17:19:53.705   Training iter 50, batch loss 1.5278, batch acc 0.9260
17:19:54.220   Training iter 100, batch loss 1.5273, batch acc 0.9308
17:19:54.729   Training iter 150, batch loss 1.5257, batch acc 0.9324
17:19:55.263   Training iter 200, batch loss 1.5262, batch acc 0.9280
17:19:55.809   Training iter 250, batch loss 1.5333, batch acc 0.9216
17:19:56.361   Training iter 300, batch loss 1.5283, batch acc 0.9290
17:19:56.903   Training iter 350, batch loss 1.5308, batch acc 0.9272
17:19:57.458   Training iter 400, batch loss 1.5262, batch acc 0.9292
17:19:57.998   Training iter 450, batch loss 1.5303, batch acc 0.9298
17:19:58.524   Training iter 500, batch loss 1.5303, batch acc 0.9292
17:19:59.060   Training iter 550, batch loss 1.5306, batch acc 0.9280
17:19:59.563   Training iter 600, batch loss 1.5322, batch acc 0.9214
17:19:59.565 Training @ 485 epoch...
17:20:00.069   Training iter 50, batch loss 1.5291, batch acc 0.9280
17:20:00.606   Training iter 100, batch loss 1.5273, batch acc 0.9328
17:20:01.137   Training iter 150, batch loss 1.5287, batch acc 0.9274
17:20:01.683   Training iter 200, batch loss 1.5323, batch acc 0.9236
17:20:02.225   Training iter 250, batch loss 1.5265, batch acc 0.9282
17:20:02.726   Training iter 300, batch loss 1.5260, batch acc 0.9280
17:20:03.237   Training iter 350, batch loss 1.5251, batch acc 0.9354
17:20:03.740   Training iter 400, batch loss 1.5321, batch acc 0.9246
17:20:04.272   Training iter 450, batch loss 1.5250, batch acc 0.9376
17:20:04.794   Training iter 500, batch loss 1.5288, batch acc 0.9276
17:20:05.300   Training iter 550, batch loss 1.5331, batch acc 0.9252
17:20:05.776   Training iter 600, batch loss 1.5346, batch acc 0.9202
17:20:05.778 Testing @ 485 epoch...
17:20:05.823     Testing, total mean loss 1.53528, total acc 0.92530
17:20:05.824 Training @ 486 epoch...
17:20:06.318   Training iter 50, batch loss 1.5272, batch acc 0.9312
17:20:06.797   Training iter 100, batch loss 1.5316, batch acc 0.9248
17:20:07.271   Training iter 150, batch loss 1.5277, batch acc 0.9266
17:20:07.743   Training iter 200, batch loss 1.5300, batch acc 0.9276
17:20:08.238   Training iter 250, batch loss 1.5284, batch acc 0.9290
17:20:08.716   Training iter 300, batch loss 1.5274, batch acc 0.9316
17:20:09.218   Training iter 350, batch loss 1.5348, batch acc 0.9214
17:20:09.735   Training iter 400, batch loss 1.5306, batch acc 0.9278
17:20:10.257   Training iter 450, batch loss 1.5294, batch acc 0.9260
17:20:10.772   Training iter 500, batch loss 1.5267, batch acc 0.9310
17:20:11.299   Training iter 550, batch loss 1.5250, batch acc 0.9294
17:20:11.817   Training iter 600, batch loss 1.5292, batch acc 0.9276
17:20:11.819 Training @ 487 epoch...
17:20:12.341   Training iter 50, batch loss 1.5280, batch acc 0.9284
17:20:12.863   Training iter 100, batch loss 1.5311, batch acc 0.9222
17:20:13.373   Training iter 150, batch loss 1.5238, batch acc 0.9334
17:20:13.890   Training iter 200, batch loss 1.5295, batch acc 0.9294
17:20:14.396   Training iter 250, batch loss 1.5310, batch acc 0.9246
17:20:14.872   Training iter 300, batch loss 1.5251, batch acc 0.9370
17:20:15.329   Training iter 350, batch loss 1.5338, batch acc 0.9280
17:20:15.799   Training iter 400, batch loss 1.5305, batch acc 0.9296
17:20:16.285   Training iter 450, batch loss 1.5291, batch acc 0.9204
17:20:16.761   Training iter 500, batch loss 1.5283, batch acc 0.9286
17:20:17.229   Training iter 550, batch loss 1.5285, batch acc 0.9288
17:20:17.695   Training iter 600, batch loss 1.5288, batch acc 0.9290
17:20:17.697 Training @ 488 epoch...
17:20:18.156   Training iter 50, batch loss 1.5296, batch acc 0.9262
17:20:18.621   Training iter 100, batch loss 1.5321, batch acc 0.9274
17:20:19.086   Training iter 150, batch loss 1.5239, batch acc 0.9348
17:20:19.567   Training iter 200, batch loss 1.5361, batch acc 0.9164
17:20:20.047   Training iter 250, batch loss 1.5262, batch acc 0.9268
17:20:20.529   Training iter 300, batch loss 1.5314, batch acc 0.9256
17:20:21.001   Training iter 350, batch loss 1.5281, batch acc 0.9304
17:20:21.467   Training iter 400, batch loss 1.5288, batch acc 0.9278
17:20:21.932   Training iter 450, batch loss 1.5245, batch acc 0.9348
17:20:22.412   Training iter 500, batch loss 1.5293, batch acc 0.9308
17:20:22.890   Training iter 550, batch loss 1.5274, batch acc 0.9298
17:20:23.372   Training iter 600, batch loss 1.5308, batch acc 0.9280
17:20:23.374 Training @ 489 epoch...
17:20:23.842   Training iter 50, batch loss 1.5288, batch acc 0.9298
17:20:24.308   Training iter 100, batch loss 1.5279, batch acc 0.9256
17:20:24.774   Training iter 150, batch loss 1.5285, batch acc 0.9264
17:20:25.267   Training iter 200, batch loss 1.5273, batch acc 0.9304
17:20:25.769   Training iter 250, batch loss 1.5310, batch acc 0.9270
17:20:26.264   Training iter 300, batch loss 1.5357, batch acc 0.9224
17:20:26.754   Training iter 350, batch loss 1.5298, batch acc 0.9278
17:20:27.258   Training iter 400, batch loss 1.5277, batch acc 0.9314
17:20:27.769   Training iter 450, batch loss 1.5243, batch acc 0.9302
17:20:28.278   Training iter 500, batch loss 1.5271, batch acc 0.9294
17:20:28.792   Training iter 550, batch loss 1.5274, batch acc 0.9336
17:20:29.315   Training iter 600, batch loss 1.5320, batch acc 0.9238
17:20:29.317 Training @ 490 epoch...
17:20:29.855   Training iter 50, batch loss 1.5261, batch acc 0.9324
17:20:30.404   Training iter 100, batch loss 1.5288, batch acc 0.9272
17:20:30.935   Training iter 150, batch loss 1.5292, batch acc 0.9284
17:20:31.462   Training iter 200, batch loss 1.5324, batch acc 0.9224
17:20:32.017   Training iter 250, batch loss 1.5287, batch acc 0.9278
17:20:32.578   Training iter 300, batch loss 1.5299, batch acc 0.9270
17:20:33.129   Training iter 350, batch loss 1.5302, batch acc 0.9298
17:20:33.652   Training iter 400, batch loss 1.5288, batch acc 0.9286
17:20:34.129   Training iter 450, batch loss 1.5279, batch acc 0.9294
17:20:34.631   Training iter 500, batch loss 1.5272, batch acc 0.9276
17:20:35.139   Training iter 550, batch loss 1.5308, batch acc 0.9258
17:20:35.643   Training iter 600, batch loss 1.5271, batch acc 0.9256
17:20:35.645 Testing @ 490 epoch...
17:20:35.690     Testing, total mean loss 1.53537, total acc 0.92460
17:20:35.690 Training @ 491 epoch...
17:20:36.214   Training iter 50, batch loss 1.5246, batch acc 0.9320
17:20:36.705   Training iter 100, batch loss 1.5284, batch acc 0.9288
17:20:37.197   Training iter 150, batch loss 1.5281, batch acc 0.9298
17:20:37.685   Training iter 200, batch loss 1.5319, batch acc 0.9206
17:20:38.174   Training iter 250, batch loss 1.5275, batch acc 0.9280
17:20:38.655   Training iter 300, batch loss 1.5263, batch acc 0.9322
17:20:39.136   Training iter 350, batch loss 1.5306, batch acc 0.9272
17:20:39.638   Training iter 400, batch loss 1.5328, batch acc 0.9254
17:20:40.152   Training iter 450, batch loss 1.5319, batch acc 0.9274
17:20:40.661   Training iter 500, batch loss 1.5302, batch acc 0.9276
17:20:41.161   Training iter 550, batch loss 1.5284, batch acc 0.9280
17:20:41.674   Training iter 600, batch loss 1.5265, batch acc 0.9322
17:20:41.676 Training @ 492 epoch...
17:20:42.213   Training iter 50, batch loss 1.5286, batch acc 0.9294
17:20:42.733   Training iter 100, batch loss 1.5331, batch acc 0.9224
17:20:43.256   Training iter 150, batch loss 1.5292, batch acc 0.9332
17:20:43.781   Training iter 200, batch loss 1.5277, batch acc 0.9278
17:20:44.353   Training iter 250, batch loss 1.5276, batch acc 0.9312
17:20:44.890   Training iter 300, batch loss 1.5254, batch acc 0.9314
17:20:45.401   Training iter 350, batch loss 1.5297, batch acc 0.9264
17:20:45.889   Training iter 400, batch loss 1.5280, batch acc 0.9318
17:20:46.378   Training iter 450, batch loss 1.5298, batch acc 0.9282
17:20:46.865   Training iter 500, batch loss 1.5305, batch acc 0.9232
17:20:47.348   Training iter 550, batch loss 1.5319, batch acc 0.9210
17:20:47.843   Training iter 600, batch loss 1.5258, batch acc 0.9332
17:20:47.844 Training @ 493 epoch...
17:20:48.321   Training iter 50, batch loss 1.5327, batch acc 0.9262
17:20:48.778   Training iter 100, batch loss 1.5271, batch acc 0.9340
17:20:49.242   Training iter 150, batch loss 1.5245, batch acc 0.9352
17:20:49.693   Training iter 200, batch loss 1.5314, batch acc 0.9258
17:20:50.151   Training iter 250, batch loss 1.5278, batch acc 0.9286
17:20:50.622   Training iter 300, batch loss 1.5312, batch acc 0.9280
17:20:51.083   Training iter 350, batch loss 1.5261, batch acc 0.9314
17:20:51.551   Training iter 400, batch loss 1.5286, batch acc 0.9266
17:20:52.008   Training iter 450, batch loss 1.5312, batch acc 0.9240
17:20:52.468   Training iter 500, batch loss 1.5269, batch acc 0.9290
17:20:52.932   Training iter 550, batch loss 1.5313, batch acc 0.9224
17:20:53.398   Training iter 600, batch loss 1.5268, batch acc 0.9238
17:20:53.400 Training @ 494 epoch...
17:20:53.865   Training iter 50, batch loss 1.5282, batch acc 0.9278
17:20:54.340   Training iter 100, batch loss 1.5297, batch acc 0.9262
17:20:54.808   Training iter 150, batch loss 1.5290, batch acc 0.9278
17:20:55.284   Training iter 200, batch loss 1.5305, batch acc 0.9246
17:20:55.769   Training iter 250, batch loss 1.5260, batch acc 0.9360
17:20:56.257   Training iter 300, batch loss 1.5283, batch acc 0.9302
17:20:56.738   Training iter 350, batch loss 1.5278, batch acc 0.9278
17:20:57.219   Training iter 400, batch loss 1.5307, batch acc 0.9248
17:20:57.692   Training iter 450, batch loss 1.5328, batch acc 0.9206
17:20:58.213   Training iter 500, batch loss 1.5262, batch acc 0.9328
17:20:58.714   Training iter 550, batch loss 1.5275, batch acc 0.9316
17:20:59.224   Training iter 600, batch loss 1.5296, batch acc 0.9254
17:20:59.226 Training @ 495 epoch...
17:20:59.756   Training iter 50, batch loss 1.5309, batch acc 0.9244
17:21:00.255   Training iter 100, batch loss 1.5269, batch acc 0.9320
17:21:00.759   Training iter 150, batch loss 1.5301, batch acc 0.9292
17:21:01.275   Training iter 200, batch loss 1.5286, batch acc 0.9286
17:21:01.816   Training iter 250, batch loss 1.5294, batch acc 0.9292
17:21:02.376   Training iter 300, batch loss 1.5287, batch acc 0.9252
17:21:02.936   Training iter 350, batch loss 1.5311, batch acc 0.9226
17:21:03.470   Training iter 400, batch loss 1.5274, batch acc 0.9314
17:21:04.002   Training iter 450, batch loss 1.5290, batch acc 0.9276
17:21:04.519   Training iter 500, batch loss 1.5227, batch acc 0.9352
17:21:05.041   Training iter 550, batch loss 1.5301, batch acc 0.9274
17:21:05.570   Training iter 600, batch loss 1.5305, batch acc 0.9280
17:21:05.572 Testing @ 495 epoch...
17:21:05.617     Testing, total mean loss 1.53504, total acc 0.92590
17:21:05.617 Training @ 496 epoch...
17:21:06.149   Training iter 50, batch loss 1.5253, batch acc 0.9300
17:21:06.696   Training iter 100, batch loss 1.5310, batch acc 0.9248
17:21:07.237   Training iter 150, batch loss 1.5330, batch acc 0.9238
17:21:07.777   Training iter 200, batch loss 1.5295, batch acc 0.9264
17:21:08.279   Training iter 250, batch loss 1.5246, batch acc 0.9322
17:21:08.755   Training iter 300, batch loss 1.5278, batch acc 0.9292
17:21:09.239   Training iter 350, batch loss 1.5315, batch acc 0.9272
17:21:09.714   Training iter 400, batch loss 1.5244, batch acc 0.9314
17:21:10.200   Training iter 450, batch loss 1.5317, batch acc 0.9276
17:21:10.694   Training iter 500, batch loss 1.5308, batch acc 0.9294
17:21:11.189   Training iter 550, batch loss 1.5270, batch acc 0.9310
17:21:11.693   Training iter 600, batch loss 1.5293, batch acc 0.9262
17:21:11.695 Training @ 497 epoch...
17:21:12.201   Training iter 50, batch loss 1.5303, batch acc 0.9282
17:21:12.694   Training iter 100, batch loss 1.5273, batch acc 0.9262
17:21:13.181   Training iter 150, batch loss 1.5251, batch acc 0.9338
17:21:13.656   Training iter 200, batch loss 1.5295, batch acc 0.9290
17:21:14.134   Training iter 250, batch loss 1.5292, batch acc 0.9296
17:21:14.625   Training iter 300, batch loss 1.5277, batch acc 0.9288
17:21:15.107   Training iter 350, batch loss 1.5304, batch acc 0.9230
17:21:15.631   Training iter 400, batch loss 1.5284, batch acc 0.9290
17:21:16.138   Training iter 450, batch loss 1.5309, batch acc 0.9274
17:21:16.626   Training iter 500, batch loss 1.5301, batch acc 0.9242
17:21:17.109   Training iter 550, batch loss 1.5297, batch acc 0.9260
17:21:17.621   Training iter 600, batch loss 1.5274, batch acc 0.9318
17:21:17.622 Training @ 498 epoch...
17:21:18.147   Training iter 50, batch loss 1.5290, batch acc 0.9302
17:21:18.669   Training iter 100, batch loss 1.5283, batch acc 0.9308
17:21:19.181   Training iter 150, batch loss 1.5291, batch acc 0.9298
17:21:19.681   Training iter 200, batch loss 1.5256, batch acc 0.9316
17:21:20.162   Training iter 250, batch loss 1.5331, batch acc 0.9242
17:21:20.657   Training iter 300, batch loss 1.5325, batch acc 0.9242
17:21:21.132   Training iter 350, batch loss 1.5325, batch acc 0.9204
17:21:21.605   Training iter 400, batch loss 1.5229, batch acc 0.9360
17:21:22.078   Training iter 450, batch loss 1.5261, batch acc 0.9306
17:21:22.568   Training iter 500, batch loss 1.5279, batch acc 0.9298
17:21:23.047   Training iter 550, batch loss 1.5296, batch acc 0.9254
17:21:23.555   Training iter 600, batch loss 1.5291, batch acc 0.9260
17:21:23.557 Training @ 499 epoch...
17:21:24.085   Training iter 50, batch loss 1.5262, batch acc 0.9296
17:21:24.609   Training iter 100, batch loss 1.5281, batch acc 0.9300
17:21:25.128   Training iter 150, batch loss 1.5288, batch acc 0.9282
17:21:25.650   Training iter 200, batch loss 1.5290, batch acc 0.9288
17:21:26.178   Training iter 250, batch loss 1.5277, batch acc 0.9276
17:21:26.713   Training iter 300, batch loss 1.5279, batch acc 0.9322
17:21:27.254   Training iter 350, batch loss 1.5289, batch acc 0.9278
17:21:27.793   Training iter 400, batch loss 1.5291, batch acc 0.9274
17:21:28.373   Training iter 450, batch loss 1.5301, batch acc 0.9250
17:21:28.943   Training iter 500, batch loss 1.5347, batch acc 0.9192
17:21:29.514   Training iter 550, batch loss 1.5279, batch acc 0.9316
17:21:30.027   Training iter 600, batch loss 1.5276, batch acc 0.9322
======================================================
17:21:30.028 Testing @ final epoch...
17:21:30.073     Testing, total mean loss 1.53465, total acc 0.92500
training time: 3202 seconds
