======================================================
learning_rate: 0.001
weight_decay: 0.01
momentum: 0.01
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Relu
loss: Euclidean
result dir: ./result/exp_1
======================================================
09:31:00.337 Training @ 0 epoch...
09:31:00.823   Training iter 50, batch loss 0.5105, batch acc 0.1068
09:31:01.334   Training iter 100, batch loss 0.5067, batch acc 0.1054
09:31:01.898   Training iter 150, batch loss 0.5020, batch acc 0.1040
09:31:02.429   Training iter 200, batch loss 0.5013, batch acc 0.1042
09:31:02.909   Training iter 250, batch loss 0.4979, batch acc 0.1086
09:31:03.380   Training iter 300, batch loss 0.4969, batch acc 0.1028
09:31:03.857   Training iter 350, batch loss 0.4950, batch acc 0.1082
09:31:04.347   Training iter 400, batch loss 0.4951, batch acc 0.0950
09:31:04.849   Training iter 450, batch loss 0.4908, batch acc 0.1090
09:31:05.314   Training iter 500, batch loss 0.4915, batch acc 0.1034
09:31:05.771   Training iter 550, batch loss 0.4894, batch acc 0.0996
09:31:06.225   Training iter 600, batch loss 0.4871, batch acc 0.1078
09:31:06.227 Testing @ 0 epoch...
09:31:06.262     Testing, total mean loss 0.48748, total acc 0.10430
09:31:06.262 Plot @ 0 epoch...
09:31:06.262 Training @ 1 epoch...
09:31:06.764   Training iter 50, batch loss 0.4866, batch acc 0.1028
09:31:07.239   Training iter 100, batch loss 0.4879, batch acc 0.1044
09:31:07.685   Training iter 150, batch loss 0.4839, batch acc 0.1032
09:31:08.128   Training iter 200, batch loss 0.4840, batch acc 0.1028
09:31:08.588   Training iter 250, batch loss 0.4829, batch acc 0.1014
09:31:09.025   Training iter 300, batch loss 0.4825, batch acc 0.0992
09:31:09.451   Training iter 350, batch loss 0.4812, batch acc 0.0932
09:31:09.885   Training iter 400, batch loss 0.4811, batch acc 0.0950
09:31:10.318   Training iter 450, batch loss 0.4800, batch acc 0.0982
09:31:10.754   Training iter 500, batch loss 0.4785, batch acc 0.0972
09:31:11.168   Training iter 550, batch loss 0.4791, batch acc 0.0996
09:31:11.608   Training iter 600, batch loss 0.4763, batch acc 0.1078
09:31:11.610 Training @ 2 epoch...
09:31:12.032   Training iter 50, batch loss 0.4768, batch acc 0.1014
09:31:12.440   Training iter 100, batch loss 0.4763, batch acc 0.1032
09:31:12.862   Training iter 150, batch loss 0.4755, batch acc 0.1068
09:31:13.282   Training iter 200, batch loss 0.4751, batch acc 0.1080
09:31:13.739   Training iter 250, batch loss 0.4735, batch acc 0.1132
09:31:14.189   Training iter 300, batch loss 0.4731, batch acc 0.1136
09:31:14.679   Training iter 350, batch loss 0.4731, batch acc 0.1208
09:31:15.170   Training iter 400, batch loss 0.4726, batch acc 0.1130
09:31:15.651   Training iter 450, batch loss 0.4717, batch acc 0.1194
09:31:16.132   Training iter 500, batch loss 0.4714, batch acc 0.1224
09:31:16.603   Training iter 550, batch loss 0.4711, batch acc 0.1230
09:31:17.081   Training iter 600, batch loss 0.4695, batch acc 0.1242
09:31:17.083 Training @ 3 epoch...
09:31:17.566   Training iter 50, batch loss 0.4692, batch acc 0.1248
09:31:18.047   Training iter 100, batch loss 0.4694, batch acc 0.1316
09:31:18.514   Training iter 150, batch loss 0.4689, batch acc 0.1318
09:31:18.987   Training iter 200, batch loss 0.4653, batch acc 0.1430
09:31:19.432   Training iter 250, batch loss 0.4669, batch acc 0.1342
09:31:19.854   Training iter 300, batch loss 0.4670, batch acc 0.1368
09:31:20.279   Training iter 350, batch loss 0.4665, batch acc 0.1358
09:31:20.722   Training iter 400, batch loss 0.4626, batch acc 0.1544
09:31:21.139   Training iter 450, batch loss 0.4661, batch acc 0.1462
09:31:21.561   Training iter 500, batch loss 0.4640, batch acc 0.1530
09:31:21.996   Training iter 550, batch loss 0.4647, batch acc 0.1526
09:31:22.436   Training iter 600, batch loss 0.4623, batch acc 0.1550
09:31:22.438 Training @ 4 epoch...
09:31:22.888   Training iter 50, batch loss 0.4617, batch acc 0.1598
09:31:23.300   Training iter 100, batch loss 0.4635, batch acc 0.1546
09:31:23.719   Training iter 150, batch loss 0.4619, batch acc 0.1650
09:31:24.130   Training iter 200, batch loss 0.4604, batch acc 0.1644
09:31:24.553   Training iter 250, batch loss 0.4595, batch acc 0.1642
09:31:24.981   Training iter 300, batch loss 0.4595, batch acc 0.1670
09:31:25.413   Training iter 350, batch loss 0.4597, batch acc 0.1694
09:31:25.838   Training iter 400, batch loss 0.4606, batch acc 0.1602
09:31:26.251   Training iter 450, batch loss 0.4584, batch acc 0.1702
09:31:26.668   Training iter 500, batch loss 0.4564, batch acc 0.1692
09:31:27.083   Training iter 550, batch loss 0.4568, batch acc 0.1806
09:31:27.519   Training iter 600, batch loss 0.4558, batch acc 0.1828
09:31:27.521 Training @ 5 epoch...
09:31:27.952   Training iter 50, batch loss 0.4560, batch acc 0.1820
09:31:28.374   Training iter 100, batch loss 0.4560, batch acc 0.1860
09:31:28.800   Training iter 150, batch loss 0.4558, batch acc 0.1860
09:31:29.227   Training iter 200, batch loss 0.4540, batch acc 0.1888
09:31:29.654   Training iter 250, batch loss 0.4555, batch acc 0.1822
09:31:30.087   Training iter 300, batch loss 0.4528, batch acc 0.1954
09:31:30.518   Training iter 350, batch loss 0.4535, batch acc 0.1928
09:31:30.952   Training iter 400, batch loss 0.4529, batch acc 0.1866
09:31:31.382   Training iter 450, batch loss 0.4513, batch acc 0.1890
09:31:31.779   Training iter 500, batch loss 0.4527, batch acc 0.1974
09:31:32.184   Training iter 550, batch loss 0.4525, batch acc 0.1968
09:31:32.603   Training iter 600, batch loss 0.4514, batch acc 0.2066
09:31:32.604 Testing @ 5 epoch...
09:31:32.638     Testing, total mean loss 0.45001, total acc 0.21200
09:31:32.639 Training @ 6 epoch...
09:31:33.057   Training iter 50, batch loss 0.4501, batch acc 0.2012
09:31:33.471   Training iter 100, batch loss 0.4508, batch acc 0.2104
09:31:33.874   Training iter 150, batch loss 0.4513, batch acc 0.2024
09:31:34.291   Training iter 200, batch loss 0.4491, batch acc 0.2146
09:31:34.758   Training iter 250, batch loss 0.4499, batch acc 0.2080
09:31:35.218   Training iter 300, batch loss 0.4482, batch acc 0.2176
09:31:35.666   Training iter 350, batch loss 0.4489, batch acc 0.2082
09:31:36.107   Training iter 400, batch loss 0.4467, batch acc 0.2182
09:31:36.587   Training iter 450, batch loss 0.4467, batch acc 0.2236
09:31:37.055   Training iter 500, batch loss 0.4467, batch acc 0.2242
09:31:37.494   Training iter 550, batch loss 0.4460, batch acc 0.2236
09:31:37.941   Training iter 600, batch loss 0.4464, batch acc 0.2300
09:31:37.943 Training @ 7 epoch...
09:31:38.383   Training iter 50, batch loss 0.4465, batch acc 0.2186
09:31:38.828   Training iter 100, batch loss 0.4457, batch acc 0.2242
09:31:39.302   Training iter 150, batch loss 0.4465, batch acc 0.2250
09:31:39.768   Training iter 200, batch loss 0.4431, batch acc 0.2454
09:31:40.223   Training iter 250, batch loss 0.4436, batch acc 0.2312
09:31:40.643   Training iter 300, batch loss 0.4438, batch acc 0.2330
09:31:41.058   Training iter 350, batch loss 0.4427, batch acc 0.2446
09:31:41.494   Training iter 400, batch loss 0.4427, batch acc 0.2336
09:31:41.947   Training iter 450, batch loss 0.4411, batch acc 0.2450
09:31:42.364   Training iter 500, batch loss 0.4414, batch acc 0.2384
09:31:42.800   Training iter 550, batch loss 0.4425, batch acc 0.2472
09:31:43.230   Training iter 600, batch loss 0.4403, batch acc 0.2510
09:31:43.232 Training @ 8 epoch...
09:31:43.646   Training iter 50, batch loss 0.4419, batch acc 0.2456
09:31:44.059   Training iter 100, batch loss 0.4397, batch acc 0.2512
09:31:44.465   Training iter 150, batch loss 0.4400, batch acc 0.2522
09:31:44.888   Training iter 200, batch loss 0.4389, batch acc 0.2572
09:31:45.295   Training iter 250, batch loss 0.4395, batch acc 0.2494
09:31:45.738   Training iter 300, batch loss 0.4369, batch acc 0.2666
09:31:46.182   Training iter 350, batch loss 0.4398, batch acc 0.2588
09:31:46.606   Training iter 400, batch loss 0.4386, batch acc 0.2586
09:31:47.035   Training iter 450, batch loss 0.4375, batch acc 0.2532
09:31:47.509   Training iter 500, batch loss 0.4359, batch acc 0.2762
09:31:48.016   Training iter 550, batch loss 0.4357, batch acc 0.2726
09:31:48.484   Training iter 600, batch loss 0.4348, batch acc 0.2868
09:31:48.486 Training @ 9 epoch...
09:31:48.939   Training iter 50, batch loss 0.4356, batch acc 0.2686
09:31:49.412   Training iter 100, batch loss 0.4347, batch acc 0.2760
09:31:49.862   Training iter 150, batch loss 0.4361, batch acc 0.2718
09:31:50.282   Training iter 200, batch loss 0.4341, batch acc 0.2724
09:31:50.712   Training iter 250, batch loss 0.4342, batch acc 0.2806
09:31:51.147   Training iter 300, batch loss 0.4329, batch acc 0.2784
09:31:51.626   Training iter 350, batch loss 0.4320, batch acc 0.2966
09:31:52.081   Training iter 400, batch loss 0.4312, batch acc 0.2932
09:31:52.536   Training iter 450, batch loss 0.4316, batch acc 0.2950
09:31:52.975   Training iter 500, batch loss 0.4319, batch acc 0.2882
09:31:53.418   Training iter 550, batch loss 0.4323, batch acc 0.2864
09:31:53.846   Training iter 600, batch loss 0.4298, batch acc 0.2928
09:31:53.847 Training @ 10 epoch...
09:31:54.274   Training iter 50, batch loss 0.4292, batch acc 0.2854
09:31:54.690   Training iter 100, batch loss 0.4314, batch acc 0.2926
09:31:55.100   Training iter 150, batch loss 0.4271, batch acc 0.3130
09:31:55.514   Training iter 200, batch loss 0.4300, batch acc 0.2980
09:31:55.935   Training iter 250, batch loss 0.4282, batch acc 0.3014
09:31:56.349   Training iter 300, batch loss 0.4291, batch acc 0.3010
09:31:56.768   Training iter 350, batch loss 0.4266, batch acc 0.3108
09:31:57.181   Training iter 400, batch loss 0.4257, batch acc 0.3150
09:31:57.597   Training iter 450, batch loss 0.4258, batch acc 0.3192
09:31:58.092   Training iter 500, batch loss 0.4258, batch acc 0.3166
09:31:58.511   Training iter 550, batch loss 0.4285, batch acc 0.2952
09:31:58.947   Training iter 600, batch loss 0.4254, batch acc 0.3112
09:31:58.949 Testing @ 10 epoch...
09:31:58.983     Testing, total mean loss 0.42354, total acc 0.32690
09:31:58.983 Training @ 11 epoch...
09:31:59.450   Training iter 50, batch loss 0.4245, batch acc 0.3158
09:31:59.888   Training iter 100, batch loss 0.4258, batch acc 0.3062
09:32:00.328   Training iter 150, batch loss 0.4234, batch acc 0.3254
09:32:00.768   Training iter 200, batch loss 0.4229, batch acc 0.3230
09:32:01.219   Training iter 250, batch loss 0.4247, batch acc 0.3232
09:32:01.731   Training iter 300, batch loss 0.4233, batch acc 0.3212
09:32:02.249   Training iter 350, batch loss 0.4217, batch acc 0.3306
09:32:02.729   Training iter 400, batch loss 0.4219, batch acc 0.3266
09:32:03.185   Training iter 450, batch loss 0.4218, batch acc 0.3278
09:32:03.609   Training iter 500, batch loss 0.4213, batch acc 0.3334
09:32:04.049   Training iter 550, batch loss 0.4200, batch acc 0.3366
09:32:04.549   Training iter 600, batch loss 0.4199, batch acc 0.3366
09:32:04.551 Training @ 12 epoch...
09:32:05.038   Training iter 50, batch loss 0.4195, batch acc 0.3430
09:32:05.513   Training iter 100, batch loss 0.4206, batch acc 0.3328
09:32:05.975   Training iter 150, batch loss 0.4168, batch acc 0.3536
09:32:06.446   Training iter 200, batch loss 0.4183, batch acc 0.3400
09:32:06.907   Training iter 250, batch loss 0.4178, batch acc 0.3546
09:32:07.348   Training iter 300, batch loss 0.4193, batch acc 0.3382
09:32:07.797   Training iter 350, batch loss 0.4184, batch acc 0.3430
09:32:08.229   Training iter 400, batch loss 0.4170, batch acc 0.3498
09:32:08.671   Training iter 450, batch loss 0.4160, batch acc 0.3614
09:32:09.092   Training iter 500, batch loss 0.4167, batch acc 0.3476
09:32:09.506   Training iter 550, batch loss 0.4167, batch acc 0.3470
09:32:09.930   Training iter 600, batch loss 0.4151, batch acc 0.3580
09:32:09.931 Training @ 13 epoch...
09:32:10.346   Training iter 50, batch loss 0.4149, batch acc 0.3580
09:32:10.764   Training iter 100, batch loss 0.4147, batch acc 0.3692
09:32:11.150   Training iter 150, batch loss 0.4162, batch acc 0.3468
09:32:11.567   Training iter 200, batch loss 0.4130, batch acc 0.3662
09:32:11.987   Training iter 250, batch loss 0.4138, batch acc 0.3616
09:32:12.413   Training iter 300, batch loss 0.4135, batch acc 0.3582
09:32:12.861   Training iter 350, batch loss 0.4134, batch acc 0.3596
09:32:13.288   Training iter 400, batch loss 0.4124, batch acc 0.3740
09:32:13.716   Training iter 450, batch loss 0.4109, batch acc 0.3764
09:32:14.140   Training iter 500, batch loss 0.4115, batch acc 0.3760
09:32:14.548   Training iter 550, batch loss 0.4115, batch acc 0.3796
09:32:14.981   Training iter 600, batch loss 0.4097, batch acc 0.3832
09:32:14.983 Training @ 14 epoch...
09:32:15.397   Training iter 50, batch loss 0.4104, batch acc 0.3796
09:32:15.831   Training iter 100, batch loss 0.4098, batch acc 0.3826
09:32:16.290   Training iter 150, batch loss 0.4087, batch acc 0.3876
09:32:16.827   Training iter 200, batch loss 0.4103, batch acc 0.3730
09:32:17.341   Training iter 250, batch loss 0.4085, batch acc 0.3862
09:32:17.877   Training iter 300, batch loss 0.4082, batch acc 0.3892
09:32:18.335   Training iter 350, batch loss 0.4090, batch acc 0.3912
09:32:18.774   Training iter 400, batch loss 0.4077, batch acc 0.3902
09:32:19.192   Training iter 450, batch loss 0.4085, batch acc 0.3816
09:32:19.627   Training iter 500, batch loss 0.4063, batch acc 0.4072
09:32:20.082   Training iter 550, batch loss 0.4065, batch acc 0.3928
09:32:20.527   Training iter 600, batch loss 0.4068, batch acc 0.3876
09:32:20.528 Training @ 15 epoch...
09:32:20.984   Training iter 50, batch loss 0.4056, batch acc 0.3976
09:32:21.400   Training iter 100, batch loss 0.4047, batch acc 0.4016
09:32:21.819   Training iter 150, batch loss 0.4051, batch acc 0.4052
09:32:22.232   Training iter 200, batch loss 0.4060, batch acc 0.3966
09:32:22.641   Training iter 250, batch loss 0.4044, batch acc 0.4088
09:32:23.056   Training iter 300, batch loss 0.4038, batch acc 0.3986
09:32:23.456   Training iter 350, batch loss 0.4041, batch acc 0.4070
09:32:23.859   Training iter 400, batch loss 0.4036, batch acc 0.4050
09:32:24.267   Training iter 450, batch loss 0.4030, batch acc 0.4146
09:32:24.701   Training iter 500, batch loss 0.4027, batch acc 0.4148
09:32:25.169   Training iter 550, batch loss 0.4020, batch acc 0.4196
09:32:25.644   Training iter 600, batch loss 0.4031, batch acc 0.4138
09:32:25.646 Testing @ 15 epoch...
09:32:25.679     Testing, total mean loss 0.39991, total acc 0.42850
09:32:25.680 Training @ 16 epoch...
09:32:26.150   Training iter 50, batch loss 0.4039, batch acc 0.3986
09:32:26.576   Training iter 100, batch loss 0.4019, batch acc 0.4214
09:32:26.999   Training iter 150, batch loss 0.4013, batch acc 0.4114
09:32:27.425   Training iter 200, batch loss 0.4006, batch acc 0.4198
09:32:27.870   Training iter 250, batch loss 0.4009, batch acc 0.4194
09:32:28.292   Training iter 300, batch loss 0.3991, batch acc 0.4240
09:32:28.703   Training iter 350, batch loss 0.3977, batch acc 0.4370
09:32:29.122   Training iter 400, batch loss 0.3999, batch acc 0.4262
09:32:29.557   Training iter 450, batch loss 0.3979, batch acc 0.4430
09:32:30.012   Training iter 500, batch loss 0.3988, batch acc 0.4356
09:32:30.557   Training iter 550, batch loss 0.3987, batch acc 0.4348
09:32:31.076   Training iter 600, batch loss 0.3966, batch acc 0.4444
09:32:31.078 Training @ 17 epoch...
09:32:31.589   Training iter 50, batch loss 0.3979, batch acc 0.4370
09:32:32.118   Training iter 100, batch loss 0.3971, batch acc 0.4370
09:32:32.649   Training iter 150, batch loss 0.3976, batch acc 0.4314
09:32:33.197   Training iter 200, batch loss 0.3965, batch acc 0.4436
09:32:33.723   Training iter 250, batch loss 0.3969, batch acc 0.4332
09:32:34.264   Training iter 300, batch loss 0.3953, batch acc 0.4464
09:32:34.804   Training iter 350, batch loss 0.3957, batch acc 0.4474
09:32:35.340   Training iter 400, batch loss 0.3954, batch acc 0.4490
09:32:35.865   Training iter 450, batch loss 0.3948, batch acc 0.4528
09:32:36.395   Training iter 500, batch loss 0.3940, batch acc 0.4546
09:32:36.932   Training iter 550, batch loss 0.3934, batch acc 0.4560
09:32:37.477   Training iter 600, batch loss 0.3937, batch acc 0.4564
09:32:37.478 Training @ 18 epoch...
09:32:37.999   Training iter 50, batch loss 0.3922, batch acc 0.4646
09:32:38.520   Training iter 100, batch loss 0.3929, batch acc 0.4508
09:32:39.047   Training iter 150, batch loss 0.3915, batch acc 0.4702
09:32:39.573   Training iter 200, batch loss 0.3932, batch acc 0.4528
09:32:40.097   Training iter 250, batch loss 0.3913, batch acc 0.4600
09:32:40.623   Training iter 300, batch loss 0.3936, batch acc 0.4520
09:32:41.144   Training iter 350, batch loss 0.3927, batch acc 0.4628
09:32:41.669   Training iter 400, batch loss 0.3903, batch acc 0.4794
09:32:42.183   Training iter 450, batch loss 0.3915, batch acc 0.4688
09:32:42.689   Training iter 500, batch loss 0.3920, batch acc 0.4618
09:32:43.204   Training iter 550, batch loss 0.3905, batch acc 0.4726
09:32:43.718   Training iter 600, batch loss 0.3890, batch acc 0.4722
09:32:43.720 Training @ 19 epoch...
09:32:44.227   Training iter 50, batch loss 0.3900, batch acc 0.4754
09:32:44.742   Training iter 100, batch loss 0.3884, batch acc 0.4828
09:32:45.258   Training iter 150, batch loss 0.3889, batch acc 0.4744
09:32:45.799   Training iter 200, batch loss 0.3880, batch acc 0.4778
09:32:46.329   Training iter 250, batch loss 0.3894, batch acc 0.4754
09:32:46.853   Training iter 300, batch loss 0.3879, batch acc 0.4806
09:32:47.380   Training iter 350, batch loss 0.3895, batch acc 0.4776
09:32:47.905   Training iter 400, batch loss 0.3861, batch acc 0.4936
09:32:48.439   Training iter 450, batch loss 0.3875, batch acc 0.4896
09:32:48.994   Training iter 500, batch loss 0.3869, batch acc 0.4836
09:32:49.553   Training iter 550, batch loss 0.3854, batch acc 0.4918
09:32:50.118   Training iter 600, batch loss 0.3870, batch acc 0.4836
09:32:50.120 Training @ 20 epoch...
09:32:50.690   Training iter 50, batch loss 0.3857, batch acc 0.4896
09:32:51.258   Training iter 100, batch loss 0.3857, batch acc 0.4942
09:32:51.834   Training iter 150, batch loss 0.3849, batch acc 0.4968
09:32:52.418   Training iter 200, batch loss 0.3849, batch acc 0.5024
09:32:52.998   Training iter 250, batch loss 0.3830, batch acc 0.4946
09:32:53.568   Training iter 300, batch loss 0.3843, batch acc 0.4984
09:32:54.139   Training iter 350, batch loss 0.3850, batch acc 0.4916
09:32:54.712   Training iter 400, batch loss 0.3830, batch acc 0.5058
09:32:55.292   Training iter 450, batch loss 0.3864, batch acc 0.4826
09:32:55.837   Training iter 500, batch loss 0.3830, batch acc 0.5108
09:32:56.378   Training iter 550, batch loss 0.3840, batch acc 0.5002
09:32:56.916   Training iter 600, batch loss 0.3811, batch acc 0.5250
09:32:56.917 Testing @ 20 epoch...
09:32:56.957     Testing, total mean loss 0.38026, total acc 0.52150
09:32:56.958 Training @ 21 epoch...
09:32:57.530   Training iter 50, batch loss 0.3816, batch acc 0.5152
09:32:58.092   Training iter 100, batch loss 0.3802, batch acc 0.5198
09:32:58.618   Training iter 150, batch loss 0.3824, batch acc 0.5066
09:32:59.156   Training iter 200, batch loss 0.3816, batch acc 0.5176
09:32:59.700   Training iter 250, batch loss 0.3819, batch acc 0.5086
09:33:00.257   Training iter 300, batch loss 0.3814, batch acc 0.5102
09:33:00.818   Training iter 350, batch loss 0.3798, batch acc 0.5124
09:33:01.425   Training iter 400, batch loss 0.3796, batch acc 0.5194
09:33:01.984   Training iter 450, batch loss 0.3806, batch acc 0.5128
09:33:02.493   Training iter 500, batch loss 0.3820, batch acc 0.5062
09:33:02.965   Training iter 550, batch loss 0.3782, batch acc 0.5354
09:33:03.466   Training iter 600, batch loss 0.3790, batch acc 0.5192
09:33:03.468 Training @ 22 epoch...
09:33:03.979   Training iter 50, batch loss 0.3788, batch acc 0.5252
09:33:04.493   Training iter 100, batch loss 0.3791, batch acc 0.5242
09:33:05.018   Training iter 150, batch loss 0.3779, batch acc 0.5314
09:33:05.537   Training iter 200, batch loss 0.3790, batch acc 0.5242
09:33:06.058   Training iter 250, batch loss 0.3767, batch acc 0.5366
09:33:06.570   Training iter 300, batch loss 0.3775, batch acc 0.5358
09:33:07.088   Training iter 350, batch loss 0.3779, batch acc 0.5274
09:33:07.614   Training iter 400, batch loss 0.3752, batch acc 0.5410
09:33:08.200   Training iter 450, batch loss 0.3760, batch acc 0.5378
09:33:08.883   Training iter 500, batch loss 0.3765, batch acc 0.5296
09:33:09.583   Training iter 550, batch loss 0.3753, batch acc 0.5412
09:33:10.291   Training iter 600, batch loss 0.3772, batch acc 0.5244
09:33:10.294 Training @ 23 epoch...
09:33:11.005   Training iter 50, batch loss 0.3760, batch acc 0.5294
09:33:11.584   Training iter 100, batch loss 0.3750, batch acc 0.5464
09:33:12.096   Training iter 150, batch loss 0.3748, batch acc 0.5454
09:33:12.611   Training iter 200, batch loss 0.3737, batch acc 0.5468
09:33:13.123   Training iter 250, batch loss 0.3741, batch acc 0.5422
09:33:13.611   Training iter 300, batch loss 0.3736, batch acc 0.5468
09:33:14.099   Training iter 350, batch loss 0.3746, batch acc 0.5412
09:33:14.599   Training iter 400, batch loss 0.3747, batch acc 0.5474
09:33:15.091   Training iter 450, batch loss 0.3719, batch acc 0.5604
09:33:15.585   Training iter 500, batch loss 0.3718, batch acc 0.5530
09:33:16.081   Training iter 550, batch loss 0.3740, batch acc 0.5438
09:33:16.583   Training iter 600, batch loss 0.3732, batch acc 0.5488
09:33:16.585 Training @ 24 epoch...
09:33:17.087   Training iter 50, batch loss 0.3724, batch acc 0.5540
09:33:17.596   Training iter 100, batch loss 0.3734, batch acc 0.5448
09:33:18.099   Training iter 150, batch loss 0.3722, batch acc 0.5598
09:33:18.611   Training iter 200, batch loss 0.3712, batch acc 0.5558
09:33:19.114   Training iter 250, batch loss 0.3701, batch acc 0.5588
09:33:19.608   Training iter 300, batch loss 0.3701, batch acc 0.5680
09:33:20.128   Training iter 350, batch loss 0.3730, batch acc 0.5522
09:33:20.629   Training iter 400, batch loss 0.3701, batch acc 0.5742
09:33:21.151   Training iter 450, batch loss 0.3675, batch acc 0.5754
09:33:21.676   Training iter 500, batch loss 0.3690, batch acc 0.5682
09:33:22.207   Training iter 550, batch loss 0.3692, batch acc 0.5568
09:33:22.748   Training iter 600, batch loss 0.3708, batch acc 0.5576
09:33:22.750 Training @ 25 epoch...
09:33:23.297   Training iter 50, batch loss 0.3687, batch acc 0.5684
09:33:23.835   Training iter 100, batch loss 0.3676, batch acc 0.5742
09:33:24.378   Training iter 150, batch loss 0.3673, batch acc 0.5784
09:33:24.920   Training iter 200, batch loss 0.3670, batch acc 0.5760
09:33:25.470   Training iter 250, batch loss 0.3695, batch acc 0.5662
09:33:25.997   Training iter 300, batch loss 0.3675, batch acc 0.5772
09:33:26.524   Training iter 350, batch loss 0.3673, batch acc 0.5722
09:33:27.047   Training iter 400, batch loss 0.3687, batch acc 0.5676
09:33:27.575   Training iter 450, batch loss 0.3678, batch acc 0.5668
09:33:28.093   Training iter 500, batch loss 0.3670, batch acc 0.5792
09:33:28.595   Training iter 550, batch loss 0.3672, batch acc 0.5756
09:33:29.106   Training iter 600, batch loss 0.3661, batch acc 0.5840
09:33:29.108 Testing @ 25 epoch...
09:33:29.143     Testing, total mean loss 0.36377, total acc 0.59350
09:33:29.143 Training @ 26 epoch...
09:33:29.642   Training iter 50, batch loss 0.3654, batch acc 0.5842
09:33:30.137   Training iter 100, batch loss 0.3654, batch acc 0.5814
09:33:30.623   Training iter 150, batch loss 0.3659, batch acc 0.5876
09:33:31.104   Training iter 200, batch loss 0.3660, batch acc 0.5756
09:33:31.583   Training iter 250, batch loss 0.3652, batch acc 0.5802
09:33:32.051   Training iter 300, batch loss 0.3664, batch acc 0.5824
09:33:32.539   Training iter 350, batch loss 0.3647, batch acc 0.5824
09:33:33.030   Training iter 400, batch loss 0.3643, batch acc 0.5868
09:33:33.495   Training iter 450, batch loss 0.3646, batch acc 0.5766
09:33:33.970   Training iter 500, batch loss 0.3612, batch acc 0.6090
09:33:34.435   Training iter 550, batch loss 0.3624, batch acc 0.5890
09:33:34.909   Training iter 600, batch loss 0.3643, batch acc 0.5856
09:33:34.911 Training @ 27 epoch...
09:33:35.415   Training iter 50, batch loss 0.3631, batch acc 0.5826
09:33:35.891   Training iter 100, batch loss 0.3638, batch acc 0.5880
09:33:36.367   Training iter 150, batch loss 0.3628, batch acc 0.6026
09:33:36.841   Training iter 200, batch loss 0.3616, batch acc 0.5970
09:33:37.380   Training iter 250, batch loss 0.3624, batch acc 0.5952
09:33:37.927   Training iter 300, batch loss 0.3613, batch acc 0.6038
09:33:38.450   Training iter 350, batch loss 0.3627, batch acc 0.5860
09:33:38.950   Training iter 400, batch loss 0.3619, batch acc 0.5960
09:33:39.467   Training iter 450, batch loss 0.3597, batch acc 0.6022
09:33:39.994   Training iter 500, batch loss 0.3609, batch acc 0.6004
09:33:40.510   Training iter 550, batch loss 0.3606, batch acc 0.5948
09:33:41.020   Training iter 600, batch loss 0.3605, batch acc 0.6056
09:33:41.022 Training @ 28 epoch...
09:33:41.553   Training iter 50, batch loss 0.3619, batch acc 0.5946
09:33:42.110   Training iter 100, batch loss 0.3600, batch acc 0.6086
09:33:42.665   Training iter 150, batch loss 0.3601, batch acc 0.6054
09:33:43.205   Training iter 200, batch loss 0.3607, batch acc 0.5996
09:33:43.653   Training iter 250, batch loss 0.3598, batch acc 0.5998
09:33:44.111   Training iter 300, batch loss 0.3582, batch acc 0.6116
09:33:44.578   Training iter 350, batch loss 0.3584, batch acc 0.6084
09:33:45.046   Training iter 400, batch loss 0.3586, batch acc 0.6050
09:33:45.512   Training iter 450, batch loss 0.3588, batch acc 0.6098
09:33:46.018   Training iter 500, batch loss 0.3569, batch acc 0.6190
09:33:46.482   Training iter 550, batch loss 0.3563, batch acc 0.6194
09:33:46.941   Training iter 600, batch loss 0.3580, batch acc 0.6052
09:33:46.943 Training @ 29 epoch...
09:33:47.402   Training iter 50, batch loss 0.3590, batch acc 0.6094
09:33:47.858   Training iter 100, batch loss 0.3572, batch acc 0.6130
09:33:48.346   Training iter 150, batch loss 0.3567, batch acc 0.6174
09:33:48.843   Training iter 200, batch loss 0.3570, batch acc 0.6062
09:33:49.343   Training iter 250, batch loss 0.3567, batch acc 0.6110
09:33:49.843   Training iter 300, batch loss 0.3571, batch acc 0.6198
09:33:50.359   Training iter 350, batch loss 0.3557, batch acc 0.6184
09:33:50.858   Training iter 400, batch loss 0.3551, batch acc 0.6232
09:33:51.341   Training iter 450, batch loss 0.3546, batch acc 0.6256
09:33:51.815   Training iter 500, batch loss 0.3559, batch acc 0.6198
09:33:52.294   Training iter 550, batch loss 0.3553, batch acc 0.6162
09:33:52.776   Training iter 600, batch loss 0.3548, batch acc 0.6236
09:33:52.778 Training @ 30 epoch...
09:33:53.291   Training iter 50, batch loss 0.3554, batch acc 0.6198
09:33:53.807   Training iter 100, batch loss 0.3540, batch acc 0.6220
09:33:54.317   Training iter 150, batch loss 0.3537, batch acc 0.6274
09:33:54.838   Training iter 200, batch loss 0.3546, batch acc 0.6238
09:33:55.355   Training iter 250, batch loss 0.3546, batch acc 0.6254
09:33:55.843   Training iter 300, batch loss 0.3533, batch acc 0.6238
09:33:56.345   Training iter 350, batch loss 0.3534, batch acc 0.6292
09:33:56.832   Training iter 400, batch loss 0.3530, batch acc 0.6352
09:33:57.331   Training iter 450, batch loss 0.3516, batch acc 0.6310
09:33:57.836   Training iter 500, batch loss 0.3541, batch acc 0.6198
09:33:58.343   Training iter 550, batch loss 0.3525, batch acc 0.6302
09:33:58.844   Training iter 600, batch loss 0.3534, batch acc 0.6254
09:33:58.845 Testing @ 30 epoch...
09:33:58.881     Testing, total mean loss 0.34984, total acc 0.64250
09:33:58.881 Training @ 31 epoch...
09:33:59.357   Training iter 50, batch loss 0.3520, batch acc 0.6250
09:33:59.831   Training iter 100, batch loss 0.3519, batch acc 0.6364
09:34:00.323   Training iter 150, batch loss 0.3531, batch acc 0.6288
09:34:00.833   Training iter 200, batch loss 0.3507, batch acc 0.6400
09:34:01.434   Training iter 250, batch loss 0.3514, batch acc 0.6206
09:34:02.145   Training iter 300, batch loss 0.3528, batch acc 0.6274
09:34:02.822   Training iter 350, batch loss 0.3511, batch acc 0.6348
09:34:03.508   Training iter 400, batch loss 0.3501, batch acc 0.6396
09:34:04.133   Training iter 450, batch loss 0.3497, batch acc 0.6382
09:34:04.637   Training iter 500, batch loss 0.3507, batch acc 0.6364
09:34:05.142   Training iter 550, batch loss 0.3509, batch acc 0.6394
09:34:05.633   Training iter 600, batch loss 0.3486, batch acc 0.6494
09:34:05.635 Training @ 32 epoch...
09:34:06.113   Training iter 50, batch loss 0.3499, batch acc 0.6430
09:34:06.562   Training iter 100, batch loss 0.3519, batch acc 0.6264
09:34:07.025   Training iter 150, batch loss 0.3495, batch acc 0.6424
09:34:07.490   Training iter 200, batch loss 0.3475, batch acc 0.6468
09:34:07.953   Training iter 250, batch loss 0.3505, batch acc 0.6262
09:34:08.412   Training iter 300, batch loss 0.3468, batch acc 0.6598
09:34:08.865   Training iter 350, batch loss 0.3505, batch acc 0.6318
09:34:09.323   Training iter 400, batch loss 0.3479, batch acc 0.6388
09:34:09.770   Training iter 450, batch loss 0.3477, batch acc 0.6550
09:34:10.241   Training iter 500, batch loss 0.3467, batch acc 0.6520
09:34:10.734   Training iter 550, batch loss 0.3475, batch acc 0.6428
09:34:11.225   Training iter 600, batch loss 0.3473, batch acc 0.6528
09:34:11.227 Training @ 33 epoch...
09:34:11.723   Training iter 50, batch loss 0.3463, batch acc 0.6506
09:34:12.213   Training iter 100, batch loss 0.3487, batch acc 0.6420
09:34:12.750   Training iter 150, batch loss 0.3472, batch acc 0.6472
09:34:13.316   Training iter 200, batch loss 0.3478, batch acc 0.6424
09:34:13.879   Training iter 250, batch loss 0.3459, batch acc 0.6534
09:34:14.423   Training iter 300, batch loss 0.3481, batch acc 0.6422
09:34:14.911   Training iter 350, batch loss 0.3461, batch acc 0.6566
09:34:15.406   Training iter 400, batch loss 0.3440, batch acc 0.6674
09:34:15.910   Training iter 450, batch loss 0.3461, batch acc 0.6396
09:34:16.394   Training iter 500, batch loss 0.3447, batch acc 0.6554
09:34:16.870   Training iter 550, batch loss 0.3460, batch acc 0.6512
09:34:17.357   Training iter 600, batch loss 0.3441, batch acc 0.6618
09:34:17.359 Training @ 34 epoch...
09:34:17.831   Training iter 50, batch loss 0.3452, batch acc 0.6510
09:34:18.305   Training iter 100, batch loss 0.3452, batch acc 0.6496
09:34:18.775   Training iter 150, batch loss 0.3443, batch acc 0.6722
09:34:19.241   Training iter 200, batch loss 0.3438, batch acc 0.6624
09:34:19.711   Training iter 250, batch loss 0.3442, batch acc 0.6592
09:34:20.200   Training iter 300, batch loss 0.3434, batch acc 0.6538
09:34:20.682   Training iter 350, batch loss 0.3432, batch acc 0.6508
09:34:21.157   Training iter 400, batch loss 0.3431, batch acc 0.6642
09:34:21.626   Training iter 450, batch loss 0.3444, batch acc 0.6488
09:34:22.111   Training iter 500, batch loss 0.3430, batch acc 0.6678
09:34:22.596   Training iter 550, batch loss 0.3443, batch acc 0.6502
09:34:23.083   Training iter 600, batch loss 0.3430, batch acc 0.6578
09:34:23.084 Training @ 35 epoch...
09:34:23.552   Training iter 50, batch loss 0.3435, batch acc 0.6560
09:34:24.018   Training iter 100, batch loss 0.3404, batch acc 0.6798
09:34:24.481   Training iter 150, batch loss 0.3410, batch acc 0.6656
09:34:24.965   Training iter 200, batch loss 0.3429, batch acc 0.6576
09:34:25.498   Training iter 250, batch loss 0.3413, batch acc 0.6586
09:34:26.034   Training iter 300, batch loss 0.3416, batch acc 0.6658
09:34:26.568   Training iter 350, batch loss 0.3418, batch acc 0.6588
09:34:27.138   Training iter 400, batch loss 0.3412, batch acc 0.6630
09:34:27.719   Training iter 450, batch loss 0.3427, batch acc 0.6616
09:34:28.308   Training iter 500, batch loss 0.3421, batch acc 0.6624
09:34:28.870   Training iter 550, batch loss 0.3411, batch acc 0.6690
09:34:29.433   Training iter 600, batch loss 0.3407, batch acc 0.6662
09:34:29.435 Testing @ 35 epoch...
09:34:29.475     Testing, total mean loss 0.33797, total acc 0.67920
09:34:29.475 Training @ 36 epoch...
09:34:30.063   Training iter 50, batch loss 0.3425, batch acc 0.6576
09:34:30.632   Training iter 100, batch loss 0.3418, batch acc 0.6622
09:34:31.190   Training iter 150, batch loss 0.3395, batch acc 0.6720
09:34:31.721   Training iter 200, batch loss 0.3394, batch acc 0.6694
09:34:32.257   Training iter 250, batch loss 0.3395, batch acc 0.6698
09:34:32.792   Training iter 300, batch loss 0.3405, batch acc 0.6664
09:34:33.319   Training iter 350, batch loss 0.3366, batch acc 0.6770
09:34:33.833   Training iter 400, batch loss 0.3410, batch acc 0.6642
09:34:34.348   Training iter 450, batch loss 0.3400, batch acc 0.6714
09:34:34.881   Training iter 500, batch loss 0.3371, batch acc 0.6812
09:34:35.431   Training iter 550, batch loss 0.3384, batch acc 0.6770
09:34:35.965   Training iter 600, batch loss 0.3378, batch acc 0.6698
09:34:35.967 Training @ 37 epoch...
09:34:36.518   Training iter 50, batch loss 0.3378, batch acc 0.6742
09:34:37.071   Training iter 100, batch loss 0.3379, batch acc 0.6734
09:34:37.616   Training iter 150, batch loss 0.3399, batch acc 0.6620
09:34:38.165   Training iter 200, batch loss 0.3390, batch acc 0.6754
09:34:38.724   Training iter 250, batch loss 0.3375, batch acc 0.6744
09:34:39.252   Training iter 300, batch loss 0.3379, batch acc 0.6712
09:34:39.755   Training iter 350, batch loss 0.3362, batch acc 0.6814
09:34:40.259   Training iter 400, batch loss 0.3356, batch acc 0.6864
09:34:40.765   Training iter 450, batch loss 0.3376, batch acc 0.6744
09:34:41.294   Training iter 500, batch loss 0.3363, batch acc 0.6806
09:34:41.867   Training iter 550, batch loss 0.3366, batch acc 0.6750
09:34:42.442   Training iter 600, batch loss 0.3363, batch acc 0.6820
09:34:42.444 Training @ 38 epoch...
09:34:43.020   Training iter 50, batch loss 0.3336, batch acc 0.6976
09:34:43.590   Training iter 100, batch loss 0.3353, batch acc 0.6790
09:34:44.169   Training iter 150, batch loss 0.3370, batch acc 0.6698
09:34:44.744   Training iter 200, batch loss 0.3355, batch acc 0.6860
09:34:45.332   Training iter 250, batch loss 0.3363, batch acc 0.6790
09:34:45.912   Training iter 300, batch loss 0.3378, batch acc 0.6698
09:34:46.487   Training iter 350, batch loss 0.3341, batch acc 0.6904
09:34:47.034   Training iter 400, batch loss 0.3353, batch acc 0.6816
09:34:47.574   Training iter 450, batch loss 0.3358, batch acc 0.6794
09:34:48.113   Training iter 500, batch loss 0.3353, batch acc 0.6756
09:34:48.683   Training iter 550, batch loss 0.3349, batch acc 0.6766
09:34:49.222   Training iter 600, batch loss 0.3332, batch acc 0.6862
09:34:49.224 Training @ 39 epoch...
09:34:49.762   Training iter 50, batch loss 0.3355, batch acc 0.6854
09:34:50.273   Training iter 100, batch loss 0.3359, batch acc 0.6774
09:34:50.778   Training iter 150, batch loss 0.3343, batch acc 0.6808
09:34:51.313   Training iter 200, batch loss 0.3336, batch acc 0.6824
09:34:51.848   Training iter 250, batch loss 0.3346, batch acc 0.6858
09:34:52.391   Training iter 300, batch loss 0.3344, batch acc 0.6788
09:34:52.925   Training iter 350, batch loss 0.3325, batch acc 0.6930
09:34:53.456   Training iter 400, batch loss 0.3339, batch acc 0.6844
09:34:53.974   Training iter 450, batch loss 0.3327, batch acc 0.6858
09:34:54.526   Training iter 500, batch loss 0.3307, batch acc 0.6854
09:34:55.070   Training iter 550, batch loss 0.3306, batch acc 0.6960
09:34:55.629   Training iter 600, batch loss 0.3313, batch acc 0.6916
09:34:55.630 Training @ 40 epoch...
09:34:56.177   Training iter 50, batch loss 0.3326, batch acc 0.6928
09:34:56.663   Training iter 100, batch loss 0.3312, batch acc 0.7010
09:34:57.158   Training iter 150, batch loss 0.3311, batch acc 0.6972
09:34:57.651   Training iter 200, batch loss 0.3316, batch acc 0.6954
09:34:58.158   Training iter 250, batch loss 0.3308, batch acc 0.6898
09:34:58.643   Training iter 300, batch loss 0.3317, batch acc 0.6878
09:34:59.127   Training iter 350, batch loss 0.3317, batch acc 0.6862
09:34:59.625   Training iter 400, batch loss 0.3310, batch acc 0.6896
09:35:00.130   Training iter 450, batch loss 0.3328, batch acc 0.6826
09:35:00.629   Training iter 500, batch loss 0.3303, batch acc 0.6866
09:35:01.136   Training iter 550, batch loss 0.3311, batch acc 0.6868
09:35:01.676   Training iter 600, batch loss 0.3310, batch acc 0.6862
09:35:01.678 Testing @ 40 epoch...
09:35:01.714     Testing, total mean loss 0.32775, total acc 0.70500
09:35:01.714 Training @ 41 epoch...
09:35:02.243   Training iter 50, batch loss 0.3308, batch acc 0.6994
09:35:02.761   Training iter 100, batch loss 0.3288, batch acc 0.6986
09:35:03.287   Training iter 150, batch loss 0.3296, batch acc 0.6942
09:35:03.813   Training iter 200, batch loss 0.3291, batch acc 0.6974
09:35:04.349   Training iter 250, batch loss 0.3316, batch acc 0.6860
09:35:04.884   Training iter 300, batch loss 0.3297, batch acc 0.6956
09:35:05.414   Training iter 350, batch loss 0.3284, batch acc 0.6950
09:35:05.909   Training iter 400, batch loss 0.3306, batch acc 0.6852
09:35:06.416   Training iter 450, batch loss 0.3296, batch acc 0.6986
09:35:06.956   Training iter 500, batch loss 0.3287, batch acc 0.7056
09:35:07.477   Training iter 550, batch loss 0.3290, batch acc 0.6876
09:35:08.002   Training iter 600, batch loss 0.3284, batch acc 0.6982
09:35:08.003 Training @ 42 epoch...
09:35:08.513   Training iter 50, batch loss 0.3286, batch acc 0.6948
09:35:09.011   Training iter 100, batch loss 0.3278, batch acc 0.6936
09:35:09.514   Training iter 150, batch loss 0.3288, batch acc 0.7032
09:35:10.017   Training iter 200, batch loss 0.3286, batch acc 0.6934
09:35:10.564   Training iter 250, batch loss 0.3268, batch acc 0.7056
09:35:11.104   Training iter 300, batch loss 0.3276, batch acc 0.7030
09:35:11.635   Training iter 350, batch loss 0.3282, batch acc 0.6950
09:35:12.150   Training iter 400, batch loss 0.3273, batch acc 0.7086
09:35:12.618   Training iter 450, batch loss 0.3275, batch acc 0.6996
09:35:13.094   Training iter 500, batch loss 0.3292, batch acc 0.6942
09:35:13.579   Training iter 550, batch loss 0.3253, batch acc 0.6980
09:35:14.056   Training iter 600, batch loss 0.3266, batch acc 0.7068
09:35:14.057 Training @ 43 epoch...
09:35:14.546   Training iter 50, batch loss 0.3254, batch acc 0.7092
09:35:15.045   Training iter 100, batch loss 0.3277, batch acc 0.6992
09:35:15.580   Training iter 150, batch loss 0.3291, batch acc 0.6868
09:35:16.059   Training iter 200, batch loss 0.3281, batch acc 0.7002
09:35:16.531   Training iter 250, batch loss 0.3255, batch acc 0.7116
09:35:17.049   Training iter 300, batch loss 0.3249, batch acc 0.7072
09:35:17.598   Training iter 350, batch loss 0.3254, batch acc 0.7104
09:35:18.113   Training iter 400, batch loss 0.3264, batch acc 0.7020
09:35:18.621   Training iter 450, batch loss 0.3237, batch acc 0.7114
09:35:19.140   Training iter 500, batch loss 0.3241, batch acc 0.7172
09:35:19.662   Training iter 550, batch loss 0.3239, batch acc 0.7094
09:35:20.173   Training iter 600, batch loss 0.3267, batch acc 0.6912
09:35:20.175 Training @ 44 epoch...
09:35:20.666   Training iter 50, batch loss 0.3246, batch acc 0.7074
09:35:21.147   Training iter 100, batch loss 0.3256, batch acc 0.7090
09:35:21.641   Training iter 150, batch loss 0.3252, batch acc 0.7050
09:35:22.136   Training iter 200, batch loss 0.3231, batch acc 0.7082
09:35:22.650   Training iter 250, batch loss 0.3235, batch acc 0.7150
09:35:23.161   Training iter 300, batch loss 0.3271, batch acc 0.6986
09:35:23.657   Training iter 350, batch loss 0.3239, batch acc 0.7180
09:35:24.142   Training iter 400, batch loss 0.3225, batch acc 0.7094
09:35:24.641   Training iter 450, batch loss 0.3222, batch acc 0.7092
09:35:25.147   Training iter 500, batch loss 0.3224, batch acc 0.7096
09:35:25.639   Training iter 550, batch loss 0.3255, batch acc 0.6954
09:35:26.132   Training iter 600, batch loss 0.3246, batch acc 0.7068
09:35:26.133 Training @ 45 epoch...
09:35:26.625   Training iter 50, batch loss 0.3236, batch acc 0.7060
09:35:27.125   Training iter 100, batch loss 0.3227, batch acc 0.7116
09:35:27.622   Training iter 150, batch loss 0.3216, batch acc 0.7132
09:35:28.142   Training iter 200, batch loss 0.3241, batch acc 0.7088
09:35:28.719   Training iter 250, batch loss 0.3232, batch acc 0.7030
09:35:29.421   Training iter 300, batch loss 0.3241, batch acc 0.7042
09:35:30.146   Training iter 350, batch loss 0.3237, batch acc 0.7102
09:35:30.869   Training iter 400, batch loss 0.3209, batch acc 0.7262
09:35:31.443   Training iter 450, batch loss 0.3228, batch acc 0.7110
09:35:31.987   Training iter 500, batch loss 0.3207, batch acc 0.7172
09:35:32.523   Training iter 550, batch loss 0.3212, batch acc 0.7124
09:35:33.055   Training iter 600, batch loss 0.3214, batch acc 0.7184
09:35:33.057 Testing @ 45 epoch...
09:35:33.093     Testing, total mean loss 0.31889, total acc 0.72700
09:35:33.093 Training @ 46 epoch...
09:35:33.617   Training iter 50, batch loss 0.3216, batch acc 0.7190
09:35:34.138   Training iter 100, batch loss 0.3204, batch acc 0.7190
09:35:34.656   Training iter 150, batch loss 0.3213, batch acc 0.7126
09:35:35.171   Training iter 200, batch loss 0.3224, batch acc 0.7092
09:35:35.650   Training iter 250, batch loss 0.3216, batch acc 0.7116
09:35:36.131   Training iter 300, batch loss 0.3213, batch acc 0.7086
09:35:36.614   Training iter 350, batch loss 0.3219, batch acc 0.7124
09:35:37.109   Training iter 400, batch loss 0.3220, batch acc 0.7116
09:35:37.591   Training iter 450, batch loss 0.3174, batch acc 0.7222
09:35:38.090   Training iter 500, batch loss 0.3184, batch acc 0.7290
09:35:38.583   Training iter 550, batch loss 0.3207, batch acc 0.7182
09:35:39.081   Training iter 600, batch loss 0.3213, batch acc 0.7082
09:35:39.082 Training @ 47 epoch...
09:35:39.578   Training iter 50, batch loss 0.3196, batch acc 0.7212
09:35:40.076   Training iter 100, batch loss 0.3197, batch acc 0.7148
09:35:40.583   Training iter 150, batch loss 0.3200, batch acc 0.7180
09:35:41.085   Training iter 200, batch loss 0.3207, batch acc 0.7138
09:35:41.596   Training iter 250, batch loss 0.3193, batch acc 0.7188
09:35:42.126   Training iter 300, batch loss 0.3209, batch acc 0.7128
09:35:42.659   Training iter 350, batch loss 0.3201, batch acc 0.7070
09:35:43.192   Training iter 400, batch loss 0.3189, batch acc 0.7248
09:35:43.708   Training iter 450, batch loss 0.3182, batch acc 0.7244
09:35:44.239   Training iter 500, batch loss 0.3184, batch acc 0.7152
09:35:44.758   Training iter 550, batch loss 0.3168, batch acc 0.7312
09:35:45.268   Training iter 600, batch loss 0.3187, batch acc 0.7198
09:35:45.270 Training @ 48 epoch...
09:35:45.783   Training iter 50, batch loss 0.3199, batch acc 0.7162
09:35:46.293   Training iter 100, batch loss 0.3178, batch acc 0.7306
09:35:46.799   Training iter 150, batch loss 0.3166, batch acc 0.7268
09:35:47.307   Training iter 200, batch loss 0.3172, batch acc 0.7208
09:35:47.812   Training iter 250, batch loss 0.3172, batch acc 0.7274
09:35:48.340   Training iter 300, batch loss 0.3183, batch acc 0.7204
09:35:48.874   Training iter 350, batch loss 0.3177, batch acc 0.7214
09:35:49.409   Training iter 400, batch loss 0.3168, batch acc 0.7304
09:35:49.961   Training iter 450, batch loss 0.3196, batch acc 0.7056
09:35:50.520   Training iter 500, batch loss 0.3175, batch acc 0.7148
09:35:51.079   Training iter 550, batch loss 0.3154, batch acc 0.7246
09:35:51.613   Training iter 600, batch loss 0.3185, batch acc 0.7190
09:35:51.615 Training @ 49 epoch...
09:35:52.142   Training iter 50, batch loss 0.3180, batch acc 0.7226
09:35:52.605   Training iter 100, batch loss 0.3179, batch acc 0.7200
09:35:53.080   Training iter 150, batch loss 0.3179, batch acc 0.7146
09:35:53.565   Training iter 200, batch loss 0.3158, batch acc 0.7300
09:35:54.045   Training iter 250, batch loss 0.3166, batch acc 0.7208
09:35:54.523   Training iter 300, batch loss 0.3166, batch acc 0.7240
09:35:55.008   Training iter 350, batch loss 0.3155, batch acc 0.7230
09:35:55.511   Training iter 400, batch loss 0.3147, batch acc 0.7330
09:35:55.991   Training iter 450, batch loss 0.3155, batch acc 0.7234
09:35:56.483   Training iter 500, batch loss 0.3154, batch acc 0.7262
09:35:56.949   Training iter 550, batch loss 0.3161, batch acc 0.7214
09:35:57.423   Training iter 600, batch loss 0.3144, batch acc 0.7306
09:35:57.424 Training @ 50 epoch...
09:35:57.909   Training iter 50, batch loss 0.3169, batch acc 0.7222
09:35:58.398   Training iter 100, batch loss 0.3172, batch acc 0.7170
09:35:58.882   Training iter 150, batch loss 0.3144, batch acc 0.7286
09:35:59.375   Training iter 200, batch loss 0.3133, batch acc 0.7362
09:35:59.857   Training iter 250, batch loss 0.3145, batch acc 0.7240
09:36:00.373   Training iter 300, batch loss 0.3151, batch acc 0.7292
09:36:00.877   Training iter 350, batch loss 0.3152, batch acc 0.7300
09:36:01.367   Training iter 400, batch loss 0.3137, batch acc 0.7300
09:36:01.886   Training iter 450, batch loss 0.3143, batch acc 0.7184
09:36:02.495   Training iter 500, batch loss 0.3122, batch acc 0.7330
09:36:03.052   Training iter 550, batch loss 0.3165, batch acc 0.7152
09:36:03.569   Training iter 600, batch loss 0.3131, batch acc 0.7342
09:36:03.570 Testing @ 50 epoch...
09:36:03.607     Testing, total mean loss 0.31114, total acc 0.74220
09:36:03.607 Training @ 51 epoch...
09:36:04.112   Training iter 50, batch loss 0.3129, batch acc 0.7328
09:36:04.662   Training iter 100, batch loss 0.3133, batch acc 0.7210
09:36:05.243   Training iter 150, batch loss 0.3113, batch acc 0.7362
09:36:05.830   Training iter 200, batch loss 0.3125, batch acc 0.7312
09:36:06.437   Training iter 250, batch loss 0.3142, batch acc 0.7304
09:36:07.027   Training iter 300, batch loss 0.3142, batch acc 0.7294
09:36:07.558   Training iter 350, batch loss 0.3132, batch acc 0.7252
09:36:08.010   Training iter 400, batch loss 0.3150, batch acc 0.7194
09:36:08.495   Training iter 450, batch loss 0.3132, batch acc 0.7326
09:36:08.971   Training iter 500, batch loss 0.3129, batch acc 0.7294
09:36:09.451   Training iter 550, batch loss 0.3142, batch acc 0.7186
09:36:09.917   Training iter 600, batch loss 0.3124, batch acc 0.7384
09:36:09.919 Training @ 52 epoch...
09:36:10.413   Training iter 50, batch loss 0.3121, batch acc 0.7282
09:36:10.891   Training iter 100, batch loss 0.3118, batch acc 0.7400
09:36:11.372   Training iter 150, batch loss 0.3116, batch acc 0.7396
09:36:11.861   Training iter 200, batch loss 0.3125, batch acc 0.7268
09:36:12.391   Training iter 250, batch loss 0.3115, batch acc 0.7332
09:36:12.874   Training iter 300, batch loss 0.3149, batch acc 0.7248
09:36:13.388   Training iter 350, batch loss 0.3099, batch acc 0.7326
09:36:13.903   Training iter 400, batch loss 0.3107, batch acc 0.7350
09:36:14.404   Training iter 450, batch loss 0.3111, batch acc 0.7318
09:36:14.906   Training iter 500, batch loss 0.3132, batch acc 0.7278
09:36:15.428   Training iter 550, batch loss 0.3132, batch acc 0.7214
09:36:15.933   Training iter 600, batch loss 0.3100, batch acc 0.7338
09:36:15.935 Training @ 53 epoch...
09:36:16.428   Training iter 50, batch loss 0.3108, batch acc 0.7334
09:36:16.907   Training iter 100, batch loss 0.3123, batch acc 0.7276
09:36:17.382   Training iter 150, batch loss 0.3113, batch acc 0.7292
09:36:17.867   Training iter 200, batch loss 0.3093, batch acc 0.7434
09:36:18.361   Training iter 250, batch loss 0.3121, batch acc 0.7232
09:36:18.875   Training iter 300, batch loss 0.3107, batch acc 0.7388
09:36:19.378   Training iter 350, batch loss 0.3103, batch acc 0.7434
09:36:19.876   Training iter 400, batch loss 0.3093, batch acc 0.7378
09:36:20.384   Training iter 450, batch loss 0.3113, batch acc 0.7276
09:36:20.884   Training iter 500, batch loss 0.3083, batch acc 0.7396
09:36:21.371   Training iter 550, batch loss 0.3109, batch acc 0.7314
09:36:21.858   Training iter 600, batch loss 0.3094, batch acc 0.7304
09:36:21.860 Training @ 54 epoch...
09:36:22.362   Training iter 50, batch loss 0.3077, batch acc 0.7456
09:36:22.868   Training iter 100, batch loss 0.3109, batch acc 0.7348
09:36:23.391   Training iter 150, batch loss 0.3071, batch acc 0.7408
09:36:23.932   Training iter 200, batch loss 0.3101, batch acc 0.7326
09:36:24.475   Training iter 250, batch loss 0.3083, batch acc 0.7364
09:36:25.021   Training iter 300, batch loss 0.3098, batch acc 0.7354
09:36:25.571   Training iter 350, batch loss 0.3104, batch acc 0.7382
09:36:26.115   Training iter 400, batch loss 0.3087, batch acc 0.7280
09:36:26.657   Training iter 450, batch loss 0.3088, batch acc 0.7350
09:36:27.208   Training iter 500, batch loss 0.3093, batch acc 0.7364
09:36:27.754   Training iter 550, batch loss 0.3088, batch acc 0.7360
09:36:28.302   Training iter 600, batch loss 0.3102, batch acc 0.7346
09:36:28.304 Training @ 55 epoch...
09:36:28.846   Training iter 50, batch loss 0.3094, batch acc 0.7332
09:36:29.387   Training iter 100, batch loss 0.3076, batch acc 0.7474
09:36:29.907   Training iter 150, batch loss 0.3086, batch acc 0.7370
09:36:30.395   Training iter 200, batch loss 0.3081, batch acc 0.7414
09:36:30.909   Training iter 250, batch loss 0.3084, batch acc 0.7384
09:36:31.395   Training iter 300, batch loss 0.3077, batch acc 0.7342
09:36:31.872   Training iter 350, batch loss 0.3075, batch acc 0.7434
09:36:32.341   Training iter 400, batch loss 0.3088, batch acc 0.7412
09:36:32.801   Training iter 450, batch loss 0.3079, batch acc 0.7442
09:36:33.286   Training iter 500, batch loss 0.3071, batch acc 0.7304
09:36:33.801   Training iter 550, batch loss 0.3071, batch acc 0.7372
09:36:34.312   Training iter 600, batch loss 0.3062, batch acc 0.7348
09:36:34.313 Testing @ 55 epoch...
09:36:34.350     Testing, total mean loss 0.30432, total acc 0.75430
09:36:34.350 Training @ 56 epoch...
09:36:34.854   Training iter 50, batch loss 0.3101, batch acc 0.7316
09:36:35.353   Training iter 100, batch loss 0.3050, batch acc 0.7464
09:36:35.855   Training iter 150, batch loss 0.3081, batch acc 0.7336
09:36:36.361   Training iter 200, batch loss 0.3061, batch acc 0.7388
09:36:36.856   Training iter 250, batch loss 0.3085, batch acc 0.7400
09:36:37.351   Training iter 300, batch loss 0.3080, batch acc 0.7340
09:36:37.875   Training iter 350, batch loss 0.3054, batch acc 0.7488
09:36:38.396   Training iter 400, batch loss 0.3040, batch acc 0.7532
09:36:38.928   Training iter 450, batch loss 0.3061, batch acc 0.7338
09:36:39.482   Training iter 500, batch loss 0.3065, batch acc 0.7422
09:36:39.964   Training iter 550, batch loss 0.3057, batch acc 0.7408
09:36:40.469   Training iter 600, batch loss 0.3057, batch acc 0.7392
09:36:40.471 Training @ 57 epoch...
09:36:40.965   Training iter 50, batch loss 0.3048, batch acc 0.7366
09:36:41.444   Training iter 100, batch loss 0.3076, batch acc 0.7334
09:36:41.920   Training iter 150, batch loss 0.3055, batch acc 0.7352
09:36:42.388   Training iter 200, batch loss 0.3049, batch acc 0.7456
09:36:42.882   Training iter 250, batch loss 0.3060, batch acc 0.7342
09:36:43.362   Training iter 300, batch loss 0.3047, batch acc 0.7482
09:36:43.867   Training iter 350, batch loss 0.3071, batch acc 0.7362
09:36:44.364   Training iter 400, batch loss 0.3056, batch acc 0.7420
09:36:44.885   Training iter 450, batch loss 0.3043, batch acc 0.7462
09:36:45.415   Training iter 500, batch loss 0.3050, batch acc 0.7456
09:36:45.924   Training iter 550, batch loss 0.3046, batch acc 0.7516
09:36:46.413   Training iter 600, batch loss 0.3040, batch acc 0.7488
09:36:46.415 Training @ 58 epoch...
09:36:46.923   Training iter 50, batch loss 0.3065, batch acc 0.7324
09:36:47.427   Training iter 100, batch loss 0.3035, batch acc 0.7528
09:36:47.909   Training iter 150, batch loss 0.3048, batch acc 0.7402
09:36:48.396   Training iter 200, batch loss 0.3051, batch acc 0.7390
09:36:48.861   Training iter 250, batch loss 0.3038, batch acc 0.7450
09:36:49.323   Training iter 300, batch loss 0.3053, batch acc 0.7390
09:36:49.819   Training iter 350, batch loss 0.3027, batch acc 0.7524
09:36:50.340   Training iter 400, batch loss 0.3072, batch acc 0.7256
09:36:50.853   Training iter 450, batch loss 0.3031, batch acc 0.7538
09:36:51.356   Training iter 500, batch loss 0.3033, batch acc 0.7452
09:36:51.871   Training iter 550, batch loss 0.3018, batch acc 0.7558
09:36:52.406   Training iter 600, batch loss 0.3027, batch acc 0.7450
09:36:52.408 Training @ 59 epoch...
09:36:52.954   Training iter 50, batch loss 0.3036, batch acc 0.7430
09:36:53.502   Training iter 100, batch loss 0.3057, batch acc 0.7422
09:36:54.050   Training iter 150, batch loss 0.3028, batch acc 0.7430
09:36:54.584   Training iter 200, batch loss 0.3044, batch acc 0.7390
09:36:55.130   Training iter 250, batch loss 0.3046, batch acc 0.7442
09:36:55.675   Training iter 300, batch loss 0.3013, batch acc 0.7486
09:36:56.194   Training iter 350, batch loss 0.3011, batch acc 0.7580
09:36:56.702   Training iter 400, batch loss 0.3013, batch acc 0.7522
09:36:57.198   Training iter 450, batch loss 0.3027, batch acc 0.7474
09:36:57.699   Training iter 500, batch loss 0.3049, batch acc 0.7344
09:36:58.221   Training iter 550, batch loss 0.3008, batch acc 0.7522
09:36:58.741   Training iter 600, batch loss 0.3021, batch acc 0.7474
09:36:58.743 Training @ 60 epoch...
09:36:59.261   Training iter 50, batch loss 0.3022, batch acc 0.7548
09:36:59.791   Training iter 100, batch loss 0.3011, batch acc 0.7500
09:37:00.331   Training iter 150, batch loss 0.3035, batch acc 0.7460
09:37:00.817   Training iter 200, batch loss 0.3017, batch acc 0.7408
09:37:01.290   Training iter 250, batch loss 0.3024, batch acc 0.7460
09:37:01.812   Training iter 300, batch loss 0.2994, batch acc 0.7450
09:37:02.330   Training iter 350, batch loss 0.3034, batch acc 0.7384
09:37:02.870   Training iter 400, batch loss 0.3021, batch acc 0.7486
09:37:03.397   Training iter 450, batch loss 0.3017, batch acc 0.7540
09:37:03.936   Training iter 500, batch loss 0.3020, batch acc 0.7426
09:37:04.486   Training iter 550, batch loss 0.3006, batch acc 0.7536
09:37:05.018   Training iter 600, batch loss 0.3015, batch acc 0.7518
09:37:05.020 Testing @ 60 epoch...
09:37:05.056     Testing, total mean loss 0.29828, total acc 0.76310
09:37:05.056 Training @ 61 epoch...
09:37:05.616   Training iter 50, batch loss 0.3019, batch acc 0.7418
09:37:06.183   Training iter 100, batch loss 0.3007, batch acc 0.7490
09:37:06.753   Training iter 150, batch loss 0.3016, batch acc 0.7464
09:37:07.321   Training iter 200, batch loss 0.3016, batch acc 0.7494
09:37:07.867   Training iter 250, batch loss 0.3000, batch acc 0.7538
09:37:08.437   Training iter 300, batch loss 0.2982, batch acc 0.7556
09:37:08.996   Training iter 350, batch loss 0.3026, batch acc 0.7412
09:37:09.552   Training iter 400, batch loss 0.3001, batch acc 0.7574
09:37:10.112   Training iter 450, batch loss 0.3017, batch acc 0.7464
09:37:10.672   Training iter 500, batch loss 0.3020, batch acc 0.7440
09:37:11.235   Training iter 550, batch loss 0.2994, batch acc 0.7466
09:37:11.781   Training iter 600, batch loss 0.2983, batch acc 0.7582
09:37:11.782 Training @ 62 epoch...
09:37:12.309   Training iter 50, batch loss 0.3006, batch acc 0.7548
09:37:12.823   Training iter 100, batch loss 0.2980, batch acc 0.7524
09:37:13.339   Training iter 150, batch loss 0.3021, batch acc 0.7434
09:37:13.849   Training iter 200, batch loss 0.3016, batch acc 0.7458
09:37:14.362   Training iter 250, batch loss 0.2999, batch acc 0.7492
09:37:14.861   Training iter 300, batch loss 0.3004, batch acc 0.7548
09:37:15.368   Training iter 350, batch loss 0.2973, batch acc 0.7576
09:37:15.895   Training iter 400, batch loss 0.3012, batch acc 0.7452
09:37:16.402   Training iter 450, batch loss 0.2988, batch acc 0.7454
09:37:16.892   Training iter 500, batch loss 0.2993, batch acc 0.7558
09:37:17.380   Training iter 550, batch loss 0.2989, batch acc 0.7464
09:37:17.879   Training iter 600, batch loss 0.2966, batch acc 0.7614
09:37:17.881 Training @ 63 epoch...
09:37:18.413   Training iter 50, batch loss 0.2986, batch acc 0.7454
09:37:18.934   Training iter 100, batch loss 0.2968, batch acc 0.7496
09:37:19.433   Training iter 150, batch loss 0.3005, batch acc 0.7496
09:37:19.948   Training iter 200, batch loss 0.2983, batch acc 0.7620
09:37:20.462   Training iter 250, batch loss 0.2977, batch acc 0.7568
09:37:20.958   Training iter 300, batch loss 0.2983, batch acc 0.7558
09:37:21.441   Training iter 350, batch loss 0.2971, batch acc 0.7542
09:37:21.937   Training iter 400, batch loss 0.3007, batch acc 0.7466
09:37:22.461   Training iter 450, batch loss 0.2993, batch acc 0.7518
09:37:22.985   Training iter 500, batch loss 0.2972, batch acc 0.7524
09:37:23.522   Training iter 550, batch loss 0.2975, batch acc 0.7536
09:37:24.044   Training iter 600, batch loss 0.3001, batch acc 0.7506
09:37:24.046 Training @ 64 epoch...
09:37:24.576   Training iter 50, batch loss 0.2974, batch acc 0.7520
09:37:25.119   Training iter 100, batch loss 0.2993, batch acc 0.7498
09:37:25.666   Training iter 150, batch loss 0.2973, batch acc 0.7516
09:37:26.191   Training iter 200, batch loss 0.2978, batch acc 0.7562
09:37:26.716   Training iter 250, batch loss 0.2972, batch acc 0.7538
09:37:27.248   Training iter 300, batch loss 0.2974, batch acc 0.7570
09:37:27.772   Training iter 350, batch loss 0.2989, batch acc 0.7486
09:37:28.282   Training iter 400, batch loss 0.2977, batch acc 0.7458
09:37:28.774   Training iter 450, batch loss 0.2952, batch acc 0.7600
09:37:29.268   Training iter 500, batch loss 0.2968, batch acc 0.7632
09:37:29.762   Training iter 550, batch loss 0.2953, batch acc 0.7580
09:37:30.308   Training iter 600, batch loss 0.2987, batch acc 0.7454
09:37:30.310 Training @ 65 epoch...
09:37:30.836   Training iter 50, batch loss 0.2964, batch acc 0.7600
09:37:31.365   Training iter 100, batch loss 0.2970, batch acc 0.7546
09:37:31.889   Training iter 150, batch loss 0.2975, batch acc 0.7552
09:37:32.411   Training iter 200, batch loss 0.2955, batch acc 0.7578
09:37:32.940   Training iter 250, batch loss 0.2991, batch acc 0.7470
09:37:33.449   Training iter 300, batch loss 0.2968, batch acc 0.7556
09:37:33.926   Training iter 350, batch loss 0.2948, batch acc 0.7598
09:37:34.404   Training iter 400, batch loss 0.2966, batch acc 0.7524
09:37:34.891   Training iter 450, batch loss 0.2959, batch acc 0.7538
09:37:35.380   Training iter 500, batch loss 0.2973, batch acc 0.7494
09:37:35.856   Training iter 550, batch loss 0.2948, batch acc 0.7558
09:37:36.334   Training iter 600, batch loss 0.2950, batch acc 0.7636
09:37:36.336 Testing @ 65 epoch...
09:37:36.372     Testing, total mean loss 0.29289, total acc 0.77090
09:37:36.372 Training @ 66 epoch...
09:37:36.854   Training iter 50, batch loss 0.2947, batch acc 0.7566
09:37:37.379   Training iter 100, batch loss 0.2960, batch acc 0.7588
09:37:37.915   Training iter 150, batch loss 0.2955, batch acc 0.7558
09:37:38.470   Training iter 200, batch loss 0.2976, batch acc 0.7508
09:37:39.027   Training iter 250, batch loss 0.2943, batch acc 0.7604
09:37:39.569   Training iter 300, batch loss 0.2953, batch acc 0.7598
09:37:40.033   Training iter 350, batch loss 0.2971, batch acc 0.7344
09:37:40.535   Training iter 400, batch loss 0.2940, batch acc 0.7664
09:37:40.994   Training iter 450, batch loss 0.2970, batch acc 0.7504
09:37:41.464   Training iter 500, batch loss 0.2937, batch acc 0.7626
09:37:41.963   Training iter 550, batch loss 0.2944, batch acc 0.7636
09:37:42.489   Training iter 600, batch loss 0.2952, batch acc 0.7616
09:37:42.490 Training @ 67 epoch...
09:37:42.997   Training iter 50, batch loss 0.2925, batch acc 0.7608
09:37:43.488   Training iter 100, batch loss 0.2955, batch acc 0.7554
09:37:43.979   Training iter 150, batch loss 0.2938, batch acc 0.7522
09:37:44.476   Training iter 200, batch loss 0.2948, batch acc 0.7632
09:37:44.961   Training iter 250, batch loss 0.2946, batch acc 0.7554
09:37:45.454   Training iter 300, batch loss 0.2964, batch acc 0.7602
09:37:45.953   Training iter 350, batch loss 0.2948, batch acc 0.7624
09:37:46.451   Training iter 400, batch loss 0.2955, batch acc 0.7476
09:37:46.939   Training iter 450, batch loss 0.2935, batch acc 0.7602
09:37:47.435   Training iter 500, batch loss 0.2946, batch acc 0.7652
09:37:47.932   Training iter 550, batch loss 0.2935, batch acc 0.7614
09:37:48.411   Training iter 600, batch loss 0.2933, batch acc 0.7628
09:37:48.413 Training @ 68 epoch...
09:37:48.883   Training iter 50, batch loss 0.2939, batch acc 0.7626
09:37:49.348   Training iter 100, batch loss 0.2947, batch acc 0.7542
09:37:49.809   Training iter 150, batch loss 0.2948, batch acc 0.7590
09:37:50.287   Training iter 200, batch loss 0.2939, batch acc 0.7638
09:37:50.766   Training iter 250, batch loss 0.2934, batch acc 0.7592
09:37:51.245   Training iter 300, batch loss 0.2916, batch acc 0.7610
09:37:51.724   Training iter 350, batch loss 0.2932, batch acc 0.7580
09:37:52.206   Training iter 400, batch loss 0.2909, batch acc 0.7610
09:37:52.695   Training iter 450, batch loss 0.2932, batch acc 0.7666
09:37:53.175   Training iter 500, batch loss 0.2942, batch acc 0.7526
09:37:53.655   Training iter 550, batch loss 0.2935, batch acc 0.7560
09:37:54.136   Training iter 600, batch loss 0.2938, batch acc 0.7644
09:37:54.137 Training @ 69 epoch...
09:37:54.624   Training iter 50, batch loss 0.2939, batch acc 0.7568
09:37:55.131   Training iter 100, batch loss 0.2943, batch acc 0.7524
09:37:55.618   Training iter 150, batch loss 0.2926, batch acc 0.7674
09:37:56.094   Training iter 200, batch loss 0.2908, batch acc 0.7604
09:37:56.569   Training iter 250, batch loss 0.2926, batch acc 0.7584
09:37:57.037   Training iter 300, batch loss 0.2908, batch acc 0.7736
09:37:57.527   Training iter 350, batch loss 0.2952, batch acc 0.7502
09:37:58.011   Training iter 400, batch loss 0.2915, batch acc 0.7644
09:37:58.496   Training iter 450, batch loss 0.2910, batch acc 0.7672
09:37:58.986   Training iter 500, batch loss 0.2930, batch acc 0.7598
09:37:59.470   Training iter 550, batch loss 0.2931, batch acc 0.7610
09:37:59.967   Training iter 600, batch loss 0.2909, batch acc 0.7658
09:37:59.969 Training @ 70 epoch...
09:38:00.484   Training iter 50, batch loss 0.2923, batch acc 0.7488
09:38:00.973   Training iter 100, batch loss 0.2915, batch acc 0.7626
09:38:01.482   Training iter 150, batch loss 0.2919, batch acc 0.7606
09:38:02.049   Training iter 200, batch loss 0.2927, batch acc 0.7682
09:38:02.598   Training iter 250, batch loss 0.2909, batch acc 0.7704
09:38:03.137   Training iter 300, batch loss 0.2913, batch acc 0.7614
09:38:03.685   Training iter 350, batch loss 0.2929, batch acc 0.7604
09:38:04.247   Training iter 400, batch loss 0.2921, batch acc 0.7630
09:38:04.809   Training iter 450, batch loss 0.2910, batch acc 0.7612
09:38:05.378   Training iter 500, batch loss 0.2908, batch acc 0.7666
09:38:05.923   Training iter 550, batch loss 0.2918, batch acc 0.7616
09:38:06.475   Training iter 600, batch loss 0.2897, batch acc 0.7700
09:38:06.477 Testing @ 70 epoch...
09:38:06.516     Testing, total mean loss 0.28806, total acc 0.77510
09:38:06.516 Training @ 71 epoch...
09:38:07.044   Training iter 50, batch loss 0.2914, batch acc 0.7602
09:38:07.557   Training iter 100, batch loss 0.2891, batch acc 0.7652
09:38:08.075   Training iter 150, batch loss 0.2934, batch acc 0.7606
09:38:08.578   Training iter 200, batch loss 0.2914, batch acc 0.7664
09:38:09.085   Training iter 250, batch loss 0.2895, batch acc 0.7650
09:38:09.581   Training iter 300, batch loss 0.2899, batch acc 0.7740
09:38:10.094   Training iter 350, batch loss 0.2896, batch acc 0.7632
09:38:10.601   Training iter 400, batch loss 0.2927, batch acc 0.7582
09:38:11.098   Training iter 450, batch loss 0.2893, batch acc 0.7658
09:38:11.595   Training iter 500, batch loss 0.2914, batch acc 0.7614
09:38:12.061   Training iter 550, batch loss 0.2905, batch acc 0.7598
09:38:12.531   Training iter 600, batch loss 0.2898, batch acc 0.7646
09:38:12.533 Training @ 72 epoch...
09:38:12.997   Training iter 50, batch loss 0.2908, batch acc 0.7608
09:38:13.448   Training iter 100, batch loss 0.2897, batch acc 0.7576
09:38:13.912   Training iter 150, batch loss 0.2909, batch acc 0.7544
09:38:14.401   Training iter 200, batch loss 0.2915, batch acc 0.7630
09:38:14.863   Training iter 250, batch loss 0.2896, batch acc 0.7784
09:38:15.338   Training iter 300, batch loss 0.2886, batch acc 0.7728
09:38:15.811   Training iter 350, batch loss 0.2894, batch acc 0.7712
09:38:16.274   Training iter 400, batch loss 0.2885, batch acc 0.7712
09:38:16.762   Training iter 450, batch loss 0.2903, batch acc 0.7616
09:38:17.273   Training iter 500, batch loss 0.2885, batch acc 0.7626
09:38:17.756   Training iter 550, batch loss 0.2894, batch acc 0.7640
09:38:18.229   Training iter 600, batch loss 0.2903, batch acc 0.7626
09:38:18.231 Training @ 73 epoch...
09:38:18.686   Training iter 50, batch loss 0.2890, batch acc 0.7536
09:38:19.136   Training iter 100, batch loss 0.2896, batch acc 0.7652
09:38:19.584   Training iter 150, batch loss 0.2899, batch acc 0.7708
09:38:20.037   Training iter 200, batch loss 0.2891, batch acc 0.7638
09:38:20.514   Training iter 250, batch loss 0.2882, batch acc 0.7662
09:38:20.984   Training iter 300, batch loss 0.2876, batch acc 0.7746
09:38:21.454   Training iter 350, batch loss 0.2914, batch acc 0.7560
09:38:21.933   Training iter 400, batch loss 0.2885, batch acc 0.7656
09:38:22.410   Training iter 450, batch loss 0.2865, batch acc 0.7766
09:38:22.896   Training iter 500, batch loss 0.2877, batch acc 0.7692
09:38:23.382   Training iter 550, batch loss 0.2902, batch acc 0.7706
09:38:23.872   Training iter 600, batch loss 0.2891, batch acc 0.7630
09:38:23.873 Training @ 74 epoch...
09:38:24.382   Training iter 50, batch loss 0.2874, batch acc 0.7744
09:38:24.888   Training iter 100, batch loss 0.2873, batch acc 0.7682
09:38:25.414   Training iter 150, batch loss 0.2870, batch acc 0.7782
09:38:25.924   Training iter 200, batch loss 0.2888, batch acc 0.7652
09:38:26.438   Training iter 250, batch loss 0.2852, batch acc 0.7676
09:38:26.947   Training iter 300, batch loss 0.2925, batch acc 0.7560
09:38:27.469   Training iter 350, batch loss 0.2900, batch acc 0.7604
09:38:27.975   Training iter 400, batch loss 0.2876, batch acc 0.7690
09:38:28.487   Training iter 450, batch loss 0.2884, batch acc 0.7658
09:38:29.002   Training iter 500, batch loss 0.2874, batch acc 0.7692
09:38:29.487   Training iter 550, batch loss 0.2850, batch acc 0.7736
09:38:29.961   Training iter 600, batch loss 0.2899, batch acc 0.7586
09:38:29.962 Training @ 75 epoch...
09:38:30.471   Training iter 50, batch loss 0.2874, batch acc 0.7778
09:38:30.976   Training iter 100, batch loss 0.2857, batch acc 0.7738
09:38:31.488   Training iter 150, batch loss 0.2875, batch acc 0.7584
09:38:32.040   Training iter 200, batch loss 0.2872, batch acc 0.7630
09:38:32.584   Training iter 250, batch loss 0.2845, batch acc 0.7868
09:38:33.150   Training iter 300, batch loss 0.2883, batch acc 0.7654
09:38:33.659   Training iter 350, batch loss 0.2890, batch acc 0.7614
09:38:34.159   Training iter 400, batch loss 0.2863, batch acc 0.7640
09:38:34.665   Training iter 450, batch loss 0.2868, batch acc 0.7658
09:38:35.170   Training iter 500, batch loss 0.2876, batch acc 0.7682
09:38:35.652   Training iter 550, batch loss 0.2877, batch acc 0.7640
09:38:36.133   Training iter 600, batch loss 0.2887, batch acc 0.7678
09:38:36.135 Testing @ 75 epoch...
09:38:36.171     Testing, total mean loss 0.28373, total acc 0.77890
09:38:36.171 Training @ 76 epoch...
09:38:36.642   Training iter 50, batch loss 0.2861, batch acc 0.7652
09:38:37.116   Training iter 100, batch loss 0.2873, batch acc 0.7668
09:38:37.605   Training iter 150, batch loss 0.2862, batch acc 0.7756
09:38:38.099   Training iter 200, batch loss 0.2880, batch acc 0.7580
09:38:38.581   Training iter 250, batch loss 0.2857, batch acc 0.7706
09:38:39.052   Training iter 300, batch loss 0.2869, batch acc 0.7688
09:38:39.530   Training iter 350, batch loss 0.2875, batch acc 0.7640
09:38:40.022   Training iter 400, batch loss 0.2878, batch acc 0.7744
09:38:40.515   Training iter 450, batch loss 0.2851, batch acc 0.7748
09:38:41.011   Training iter 500, batch loss 0.2843, batch acc 0.7784
09:38:41.507   Training iter 550, batch loss 0.2863, batch acc 0.7612
09:38:42.054   Training iter 600, batch loss 0.2856, batch acc 0.7698
09:38:42.056 Training @ 77 epoch...
09:38:42.622   Training iter 50, batch loss 0.2866, batch acc 0.7670
09:38:43.182   Training iter 100, batch loss 0.2868, batch acc 0.7696
09:38:43.724   Training iter 150, batch loss 0.2828, batch acc 0.7786
09:38:44.322   Training iter 200, batch loss 0.2869, batch acc 0.7646
09:38:44.914   Training iter 250, batch loss 0.2865, batch acc 0.7670
09:38:45.516   Training iter 300, batch loss 0.2838, batch acc 0.7796
09:38:46.075   Training iter 350, batch loss 0.2861, batch acc 0.7716
09:38:46.612   Training iter 400, batch loss 0.2873, batch acc 0.7628
09:38:47.125   Training iter 450, batch loss 0.2846, batch acc 0.7706
09:38:47.617   Training iter 500, batch loss 0.2841, batch acc 0.7692
09:38:48.111   Training iter 550, batch loss 0.2857, batch acc 0.7676
09:38:48.653   Training iter 600, batch loss 0.2860, batch acc 0.7726
09:38:48.655 Training @ 78 epoch...
09:38:49.212   Training iter 50, batch loss 0.2849, batch acc 0.7722
09:38:49.760   Training iter 100, batch loss 0.2831, batch acc 0.7752
09:38:50.317   Training iter 150, batch loss 0.2846, batch acc 0.7734
09:38:50.850   Training iter 200, batch loss 0.2851, batch acc 0.7698
09:38:51.358   Training iter 250, batch loss 0.2866, batch acc 0.7676
09:38:51.859   Training iter 300, batch loss 0.2833, batch acc 0.7720
09:38:52.374   Training iter 350, batch loss 0.2856, batch acc 0.7744
09:38:52.893   Training iter 400, batch loss 0.2850, batch acc 0.7684
09:38:53.414   Training iter 450, batch loss 0.2860, batch acc 0.7648
09:38:53.920   Training iter 500, batch loss 0.2838, batch acc 0.7704
09:38:54.436   Training iter 550, batch loss 0.2846, batch acc 0.7752
09:38:54.945   Training iter 600, batch loss 0.2851, batch acc 0.7678
09:38:54.946 Training @ 79 epoch...
09:38:55.473   Training iter 50, batch loss 0.2853, batch acc 0.7736
09:38:55.967   Training iter 100, batch loss 0.2837, batch acc 0.7744
09:38:56.484   Training iter 150, batch loss 0.2826, batch acc 0.7788
09:38:56.990   Training iter 200, batch loss 0.2832, batch acc 0.7726
09:38:57.499   Training iter 250, batch loss 0.2853, batch acc 0.7656
09:38:58.033   Training iter 300, batch loss 0.2863, batch acc 0.7702
09:38:58.550   Training iter 350, batch loss 0.2846, batch acc 0.7678
09:38:59.076   Training iter 400, batch loss 0.2829, batch acc 0.7738
09:38:59.593   Training iter 450, batch loss 0.2854, batch acc 0.7634
09:39:00.109   Training iter 500, batch loss 0.2830, batch acc 0.7732
09:39:00.634   Training iter 550, batch loss 0.2820, batch acc 0.7830
09:39:01.174   Training iter 600, batch loss 0.2842, batch acc 0.7702
09:39:01.176 Training @ 80 epoch...
09:39:01.726   Training iter 50, batch loss 0.2855, batch acc 0.7666
09:39:02.430   Training iter 100, batch loss 0.2821, batch acc 0.7768
09:39:03.164   Training iter 150, batch loss 0.2829, batch acc 0.7788
09:39:03.780   Training iter 200, batch loss 0.2838, batch acc 0.7694
09:39:04.314   Training iter 250, batch loss 0.2831, batch acc 0.7686
09:39:04.849   Training iter 300, batch loss 0.2833, batch acc 0.7762
09:39:05.371   Training iter 350, batch loss 0.2815, batch acc 0.7794
09:39:05.868   Training iter 400, batch loss 0.2852, batch acc 0.7682
09:39:06.343   Training iter 450, batch loss 0.2844, batch acc 0.7680
09:39:06.833   Training iter 500, batch loss 0.2840, batch acc 0.7736
09:39:07.376   Training iter 550, batch loss 0.2833, batch acc 0.7740
09:39:07.901   Training iter 600, batch loss 0.2806, batch acc 0.7762
09:39:07.903 Testing @ 80 epoch...
09:39:07.939     Testing, total mean loss 0.27981, total acc 0.78600
09:39:07.939 Training @ 81 epoch...
09:39:08.463   Training iter 50, batch loss 0.2814, batch acc 0.7862
09:39:08.961   Training iter 100, batch loss 0.2816, batch acc 0.7714
09:39:09.443   Training iter 150, batch loss 0.2814, batch acc 0.7792
09:39:09.938   Training iter 200, batch loss 0.2805, batch acc 0.7808
09:39:10.445   Training iter 250, batch loss 0.2844, batch acc 0.7652
09:39:10.930   Training iter 300, batch loss 0.2826, batch acc 0.7664
09:39:11.410   Training iter 350, batch loss 0.2826, batch acc 0.7742
09:39:11.885   Training iter 400, batch loss 0.2834, batch acc 0.7660
09:39:12.384   Training iter 450, batch loss 0.2817, batch acc 0.7786
09:39:12.884   Training iter 500, batch loss 0.2836, batch acc 0.7638
09:39:13.383   Training iter 550, batch loss 0.2846, batch acc 0.7724
09:39:13.891   Training iter 600, batch loss 0.2828, batch acc 0.7758
09:39:13.892 Training @ 82 epoch...
09:39:14.408   Training iter 50, batch loss 0.2808, batch acc 0.7830
09:39:14.907   Training iter 100, batch loss 0.2816, batch acc 0.7726
09:39:15.375   Training iter 150, batch loss 0.2843, batch acc 0.7672
09:39:15.841   Training iter 200, batch loss 0.2810, batch acc 0.7734
09:39:16.327   Training iter 250, batch loss 0.2818, batch acc 0.7814
09:39:16.829   Training iter 300, batch loss 0.2822, batch acc 0.7714
09:39:17.316   Training iter 350, batch loss 0.2811, batch acc 0.7810
09:39:17.812   Training iter 400, batch loss 0.2810, batch acc 0.7752
09:39:18.350   Training iter 450, batch loss 0.2832, batch acc 0.7646
09:39:18.887   Training iter 500, batch loss 0.2809, batch acc 0.7762
09:39:19.389   Training iter 550, batch loss 0.2815, batch acc 0.7762
09:39:19.871   Training iter 600, batch loss 0.2826, batch acc 0.7748
09:39:19.873 Training @ 83 epoch...
09:39:20.379   Training iter 50, batch loss 0.2796, batch acc 0.7768
09:39:20.834   Training iter 100, batch loss 0.2831, batch acc 0.7708
09:39:21.285   Training iter 150, batch loss 0.2824, batch acc 0.7696
09:39:21.739   Training iter 200, batch loss 0.2806, batch acc 0.7734
09:39:22.185   Training iter 250, batch loss 0.2795, batch acc 0.7870
09:39:22.631   Training iter 300, batch loss 0.2808, batch acc 0.7766
09:39:23.082   Training iter 350, batch loss 0.2824, batch acc 0.7648
09:39:23.532   Training iter 400, batch loss 0.2801, batch acc 0.7770
09:39:23.994   Training iter 450, batch loss 0.2832, batch acc 0.7740
09:39:24.464   Training iter 500, batch loss 0.2798, batch acc 0.7798
09:39:24.939   Training iter 550, batch loss 0.2812, batch acc 0.7782
09:39:25.433   Training iter 600, batch loss 0.2807, batch acc 0.7732
09:39:25.435 Training @ 84 epoch...
09:39:25.905   Training iter 50, batch loss 0.2822, batch acc 0.7730
09:39:26.390   Training iter 100, batch loss 0.2801, batch acc 0.7780
09:39:26.910   Training iter 150, batch loss 0.2805, batch acc 0.7814
09:39:27.442   Training iter 200, batch loss 0.2814, batch acc 0.7702
09:39:27.969   Training iter 250, batch loss 0.2834, batch acc 0.7596
09:39:28.487   Training iter 300, batch loss 0.2807, batch acc 0.7840
09:39:29.005   Training iter 350, batch loss 0.2795, batch acc 0.7806
09:39:29.559   Training iter 400, batch loss 0.2791, batch acc 0.7820
09:39:30.134   Training iter 450, batch loss 0.2799, batch acc 0.7808
09:39:30.743   Training iter 500, batch loss 0.2796, batch acc 0.7714
09:39:31.344   Training iter 550, batch loss 0.2805, batch acc 0.7758
09:39:31.894   Training iter 600, batch loss 0.2782, batch acc 0.7772
09:39:31.896 Training @ 85 epoch...
09:39:32.471   Training iter 50, batch loss 0.2804, batch acc 0.7668
09:39:33.064   Training iter 100, batch loss 0.2788, batch acc 0.7782
09:39:33.658   Training iter 150, batch loss 0.2783, batch acc 0.7848
09:39:34.256   Training iter 200, batch loss 0.2790, batch acc 0.7820
09:39:34.836   Training iter 250, batch loss 0.2817, batch acc 0.7708
09:39:35.394   Training iter 300, batch loss 0.2814, batch acc 0.7688
09:39:35.906   Training iter 350, batch loss 0.2817, batch acc 0.7732
09:39:36.413   Training iter 400, batch loss 0.2804, batch acc 0.7756
09:39:36.930   Training iter 450, batch loss 0.2793, batch acc 0.7872
09:39:37.442   Training iter 500, batch loss 0.2772, batch acc 0.7742
09:39:37.943   Training iter 550, batch loss 0.2798, batch acc 0.7798
09:39:38.442   Training iter 600, batch loss 0.2789, batch acc 0.7812
09:39:38.444 Testing @ 85 epoch...
09:39:38.480     Testing, total mean loss 0.27625, total acc 0.78980
09:39:38.480 Training @ 86 epoch...
09:39:38.997   Training iter 50, batch loss 0.2815, batch acc 0.7730
09:39:39.516   Training iter 100, batch loss 0.2803, batch acc 0.7702
09:39:40.015   Training iter 150, batch loss 0.2799, batch acc 0.7812
09:39:40.533   Training iter 200, batch loss 0.2774, batch acc 0.7862
09:39:41.036   Training iter 250, batch loss 0.2785, batch acc 0.7816
09:39:41.551   Training iter 300, batch loss 0.2762, batch acc 0.7846
09:39:42.046   Training iter 350, batch loss 0.2810, batch acc 0.7680
09:39:42.550   Training iter 400, batch loss 0.2783, batch acc 0.7708
09:39:43.061   Training iter 450, batch loss 0.2792, batch acc 0.7754
09:39:43.557   Training iter 500, batch loss 0.2784, batch acc 0.7792
09:39:44.067   Training iter 550, batch loss 0.2785, batch acc 0.7772
09:39:44.567   Training iter 600, batch loss 0.2794, batch acc 0.7796
09:39:44.569 Training @ 87 epoch...
09:39:45.076   Training iter 50, batch loss 0.2798, batch acc 0.7808
09:39:45.621   Training iter 100, batch loss 0.2787, batch acc 0.7706
09:39:46.179   Training iter 150, batch loss 0.2763, batch acc 0.7834
09:39:46.736   Training iter 200, batch loss 0.2778, batch acc 0.7826
09:39:47.301   Training iter 250, batch loss 0.2786, batch acc 0.7720
09:39:47.871   Training iter 300, batch loss 0.2779, batch acc 0.7804
09:39:48.421   Training iter 350, batch loss 0.2776, batch acc 0.7764
09:39:48.952   Training iter 400, batch loss 0.2811, batch acc 0.7642
09:39:49.486   Training iter 450, batch loss 0.2811, batch acc 0.7722
09:39:50.008   Training iter 500, batch loss 0.2766, batch acc 0.7866
09:39:50.524   Training iter 550, batch loss 0.2764, batch acc 0.7870
09:39:51.037   Training iter 600, batch loss 0.2790, batch acc 0.7800
09:39:51.039 Training @ 88 epoch...
09:39:51.514   Training iter 50, batch loss 0.2792, batch acc 0.7768
09:39:51.980   Training iter 100, batch loss 0.2777, batch acc 0.7836
09:39:52.447   Training iter 150, batch loss 0.2746, batch acc 0.7862
09:39:52.937   Training iter 200, batch loss 0.2795, batch acc 0.7782
09:39:53.441   Training iter 250, batch loss 0.2792, batch acc 0.7668
09:39:53.943   Training iter 300, batch loss 0.2781, batch acc 0.7706
09:39:54.448   Training iter 350, batch loss 0.2762, batch acc 0.7892
09:39:54.924   Training iter 400, batch loss 0.2768, batch acc 0.7822
09:39:55.458   Training iter 450, batch loss 0.2768, batch acc 0.7802
09:39:56.045   Training iter 500, batch loss 0.2789, batch acc 0.7732
09:39:56.508   Training iter 550, batch loss 0.2777, batch acc 0.7758
09:39:56.977   Training iter 600, batch loss 0.2784, batch acc 0.7806
09:39:56.978 Training @ 89 epoch...
09:39:57.458   Training iter 50, batch loss 0.2748, batch acc 0.7902
09:39:57.962   Training iter 100, batch loss 0.2759, batch acc 0.7806
09:39:58.542   Training iter 150, batch loss 0.2778, batch acc 0.7740
09:39:59.039   Training iter 200, batch loss 0.2785, batch acc 0.7744
09:39:59.536   Training iter 250, batch loss 0.2775, batch acc 0.7780
09:40:00.024   Training iter 300, batch loss 0.2783, batch acc 0.7792
09:40:00.525   Training iter 350, batch loss 0.2777, batch acc 0.7784
09:40:01.005   Training iter 400, batch loss 0.2755, batch acc 0.7860
09:40:01.580   Training iter 450, batch loss 0.2772, batch acc 0.7816
09:40:02.148   Training iter 500, batch loss 0.2782, batch acc 0.7766
09:40:02.671   Training iter 550, batch loss 0.2788, batch acc 0.7704
09:40:03.196   Training iter 600, batch loss 0.2753, batch acc 0.7824
09:40:03.198 Training @ 90 epoch...
09:40:03.731   Training iter 50, batch loss 0.2775, batch acc 0.7766
09:40:04.256   Training iter 100, batch loss 0.2774, batch acc 0.7806
09:40:04.791   Training iter 150, batch loss 0.2743, batch acc 0.7820
09:40:05.313   Training iter 200, batch loss 0.2777, batch acc 0.7820
09:40:05.844   Training iter 250, batch loss 0.2773, batch acc 0.7812
09:40:06.394   Training iter 300, batch loss 0.2742, batch acc 0.7900
09:40:06.903   Training iter 350, batch loss 0.2772, batch acc 0.7760
09:40:07.378   Training iter 400, batch loss 0.2777, batch acc 0.7722
09:40:07.895   Training iter 450, batch loss 0.2776, batch acc 0.7736
09:40:08.450   Training iter 500, batch loss 0.2744, batch acc 0.7844
09:40:08.976   Training iter 550, batch loss 0.2768, batch acc 0.7770
09:40:09.468   Training iter 600, batch loss 0.2761, batch acc 0.7820
09:40:09.470 Testing @ 90 epoch...
09:40:09.505     Testing, total mean loss 0.27302, total acc 0.79330
09:40:09.505 Training @ 91 epoch...
09:40:10.013   Training iter 50, batch loss 0.2755, batch acc 0.7840
09:40:10.545   Training iter 100, batch loss 0.2795, batch acc 0.7676
09:40:11.058   Training iter 150, batch loss 0.2770, batch acc 0.7784
09:40:11.574   Training iter 200, batch loss 0.2754, batch acc 0.7856
09:40:12.092   Training iter 250, batch loss 0.2743, batch acc 0.7842
09:40:12.618   Training iter 300, batch loss 0.2741, batch acc 0.7834
09:40:13.148   Training iter 350, batch loss 0.2780, batch acc 0.7792
09:40:13.646   Training iter 400, batch loss 0.2776, batch acc 0.7758
09:40:14.153   Training iter 450, batch loss 0.2754, batch acc 0.7832
09:40:14.672   Training iter 500, batch loss 0.2740, batch acc 0.7874
09:40:15.176   Training iter 550, batch loss 0.2757, batch acc 0.7728
09:40:15.699   Training iter 600, batch loss 0.2743, batch acc 0.7862
09:40:15.700 Training @ 92 epoch...
09:40:16.211   Training iter 50, batch loss 0.2755, batch acc 0.7846
09:40:16.713   Training iter 100, batch loss 0.2757, batch acc 0.7780
09:40:17.214   Training iter 150, batch loss 0.2745, batch acc 0.7840
09:40:17.745   Training iter 200, batch loss 0.2780, batch acc 0.7674
09:40:18.269   Training iter 250, batch loss 0.2755, batch acc 0.7872
09:40:18.789   Training iter 300, batch loss 0.2753, batch acc 0.7836
09:40:19.332   Training iter 350, batch loss 0.2746, batch acc 0.7872
09:40:19.869   Training iter 400, batch loss 0.2760, batch acc 0.7778
09:40:20.415   Training iter 450, batch loss 0.2718, batch acc 0.7844
09:40:20.944   Training iter 500, batch loss 0.2765, batch acc 0.7816
09:40:21.469   Training iter 550, batch loss 0.2750, batch acc 0.7820
09:40:21.989   Training iter 600, batch loss 0.2752, batch acc 0.7744
09:40:21.991 Training @ 93 epoch...
09:40:22.510   Training iter 50, batch loss 0.2738, batch acc 0.7778
09:40:23.025   Training iter 100, batch loss 0.2749, batch acc 0.7846
09:40:23.519   Training iter 150, batch loss 0.2729, batch acc 0.7830
09:40:24.006   Training iter 200, batch loss 0.2734, batch acc 0.7852
09:40:24.486   Training iter 250, batch loss 0.2728, batch acc 0.7934
09:40:24.979   Training iter 300, batch loss 0.2781, batch acc 0.7686
09:40:25.498   Training iter 350, batch loss 0.2766, batch acc 0.7754
09:40:26.017   Training iter 400, batch loss 0.2747, batch acc 0.7866
09:40:26.498   Training iter 450, batch loss 0.2742, batch acc 0.7824
09:40:26.991   Training iter 500, batch loss 0.2749, batch acc 0.7848
09:40:27.492   Training iter 550, batch loss 0.2747, batch acc 0.7764
09:40:27.997   Training iter 600, batch loss 0.2755, batch acc 0.7784
09:40:27.999 Training @ 94 epoch...
09:40:28.521   Training iter 50, batch loss 0.2762, batch acc 0.7766
09:40:29.036   Training iter 100, batch loss 0.2734, batch acc 0.7874
09:40:29.546   Training iter 150, batch loss 0.2740, batch acc 0.7820
09:40:30.058   Training iter 200, batch loss 0.2725, batch acc 0.7846
09:40:30.534   Training iter 250, batch loss 0.2753, batch acc 0.7742
09:40:31.030   Training iter 300, batch loss 0.2749, batch acc 0.7838
09:40:31.512   Training iter 350, batch loss 0.2732, batch acc 0.7862
09:40:31.987   Training iter 400, batch loss 0.2750, batch acc 0.7826
09:40:32.460   Training iter 450, batch loss 0.2742, batch acc 0.7764
09:40:32.928   Training iter 500, batch loss 0.2725, batch acc 0.7848
09:40:33.444   Training iter 550, batch loss 0.2747, batch acc 0.7780
09:40:33.977   Training iter 600, batch loss 0.2736, batch acc 0.7902
09:40:33.978 Training @ 95 epoch...
09:40:34.502   Training iter 50, batch loss 0.2747, batch acc 0.7808
09:40:35.027   Training iter 100, batch loss 0.2749, batch acc 0.7760
09:40:35.560   Training iter 150, batch loss 0.2728, batch acc 0.7890
09:40:36.085   Training iter 200, batch loss 0.2730, batch acc 0.7810
09:40:36.598   Training iter 250, batch loss 0.2730, batch acc 0.7850
09:40:37.114   Training iter 300, batch loss 0.2755, batch acc 0.7752
09:40:37.641   Training iter 350, batch loss 0.2732, batch acc 0.7812
09:40:38.179   Training iter 400, batch loss 0.2739, batch acc 0.7834
09:40:38.714   Training iter 450, batch loss 0.2748, batch acc 0.7790
09:40:39.236   Training iter 500, batch loss 0.2718, batch acc 0.7862
09:40:39.735   Training iter 550, batch loss 0.2719, batch acc 0.7910
09:40:40.240   Training iter 600, batch loss 0.2733, batch acc 0.7858
09:40:40.242 Testing @ 95 epoch...
09:40:40.278     Testing, total mean loss 0.27007, total acc 0.79480
09:40:40.278 Training @ 96 epoch...
09:40:40.789   Training iter 50, batch loss 0.2719, batch acc 0.7836
09:40:41.275   Training iter 100, batch loss 0.2712, batch acc 0.7886
09:40:41.761   Training iter 150, batch loss 0.2758, batch acc 0.7778
09:40:42.258   Training iter 200, batch loss 0.2721, batch acc 0.7858
09:40:42.747   Training iter 250, batch loss 0.2716, batch acc 0.7856
09:40:43.241   Training iter 300, batch loss 0.2741, batch acc 0.7850
09:40:43.714   Training iter 350, batch loss 0.2740, batch acc 0.7808
09:40:44.205   Training iter 400, batch loss 0.2704, batch acc 0.7858
09:40:44.704   Training iter 450, batch loss 0.2745, batch acc 0.7742
09:40:45.190   Training iter 500, batch loss 0.2751, batch acc 0.7824
09:40:45.692   Training iter 550, batch loss 0.2725, batch acc 0.7886
09:40:46.193   Training iter 600, batch loss 0.2727, batch acc 0.7820
09:40:46.195 Training @ 97 epoch...
09:40:46.705   Training iter 50, batch loss 0.2726, batch acc 0.7862
09:40:47.223   Training iter 100, batch loss 0.2741, batch acc 0.7814
09:40:47.733   Training iter 150, batch loss 0.2744, batch acc 0.7810
09:40:48.249   Training iter 200, batch loss 0.2688, batch acc 0.7962
09:40:48.734   Training iter 250, batch loss 0.2722, batch acc 0.7870
09:40:49.237   Training iter 300, batch loss 0.2724, batch acc 0.7808
09:40:49.770   Training iter 350, batch loss 0.2733, batch acc 0.7776
09:40:50.301   Training iter 400, batch loss 0.2725, batch acc 0.7782
09:40:50.836   Training iter 450, batch loss 0.2750, batch acc 0.7770
09:40:51.360   Training iter 500, batch loss 0.2724, batch acc 0.7792
09:40:51.891   Training iter 550, batch loss 0.2721, batch acc 0.7820
09:40:52.450   Training iter 600, batch loss 0.2697, batch acc 0.7972
09:40:52.452 Training @ 98 epoch...
09:40:53.028   Training iter 50, batch loss 0.2722, batch acc 0.7906
09:40:53.601   Training iter 100, batch loss 0.2720, batch acc 0.7846
09:40:54.150   Training iter 150, batch loss 0.2722, batch acc 0.7802
09:40:54.695   Training iter 200, batch loss 0.2734, batch acc 0.7792
09:40:55.230   Training iter 250, batch loss 0.2737, batch acc 0.7802
09:40:55.750   Training iter 300, batch loss 0.2709, batch acc 0.7940
09:40:56.281   Training iter 350, batch loss 0.2692, batch acc 0.7944
09:40:56.798   Training iter 400, batch loss 0.2725, batch acc 0.7798
09:40:57.317   Training iter 450, batch loss 0.2714, batch acc 0.7844
09:40:57.851   Training iter 500, batch loss 0.2727, batch acc 0.7810
09:40:58.374   Training iter 550, batch loss 0.2710, batch acc 0.7810
09:40:58.908   Training iter 600, batch loss 0.2717, batch acc 0.7820
09:40:58.909 Training @ 99 epoch...
09:40:59.459   Training iter 50, batch loss 0.2705, batch acc 0.7922
09:41:00.004   Training iter 100, batch loss 0.2709, batch acc 0.7852
09:41:00.532   Training iter 150, batch loss 0.2716, batch acc 0.7944
09:41:01.050   Training iter 200, batch loss 0.2705, batch acc 0.7906
09:41:01.595   Training iter 250, batch loss 0.2695, batch acc 0.7872
09:41:02.133   Training iter 300, batch loss 0.2708, batch acc 0.7848
09:41:02.659   Training iter 350, batch loss 0.2724, batch acc 0.7838
09:41:03.175   Training iter 400, batch loss 0.2697, batch acc 0.7838
09:41:03.697   Training iter 450, batch loss 0.2743, batch acc 0.7750
09:41:04.248   Training iter 500, batch loss 0.2725, batch acc 0.7826
09:41:04.775   Training iter 550, batch loss 0.2717, batch acc 0.7766
09:41:05.302   Training iter 600, batch loss 0.2722, batch acc 0.7810
09:41:05.303 Training @ 100 epoch...
09:41:05.824   Training iter 50, batch loss 0.2723, batch acc 0.7834
09:41:06.327   Training iter 100, batch loss 0.2725, batch acc 0.7852
09:41:06.849   Training iter 150, batch loss 0.2682, batch acc 0.7982
09:41:07.366   Training iter 200, batch loss 0.2721, batch acc 0.7788
09:41:07.872   Training iter 250, batch loss 0.2733, batch acc 0.7768
09:41:08.381   Training iter 300, batch loss 0.2724, batch acc 0.7790
09:41:08.881   Training iter 350, batch loss 0.2705, batch acc 0.7886
09:41:09.376   Training iter 400, batch loss 0.2696, batch acc 0.7880
09:41:09.850   Training iter 450, batch loss 0.2675, batch acc 0.7946
09:41:10.361   Training iter 500, batch loss 0.2699, batch acc 0.7890
09:41:10.849   Training iter 550, batch loss 0.2740, batch acc 0.7706
09:41:11.327   Training iter 600, batch loss 0.2683, batch acc 0.7938
09:41:11.329 Testing @ 100 epoch...
09:41:11.365     Testing, total mean loss 0.26737, total acc 0.79870
09:41:11.365 Plot @ 100 epoch...
09:41:11.365 Training @ 101 epoch...
09:41:11.845   Training iter 50, batch loss 0.2727, batch acc 0.7772
09:41:12.330   Training iter 100, batch loss 0.2700, batch acc 0.7886
09:41:12.822   Training iter 150, batch loss 0.2715, batch acc 0.7824
09:41:13.309   Training iter 200, batch loss 0.2711, batch acc 0.7876
09:41:13.782   Training iter 250, batch loss 0.2705, batch acc 0.7874
09:41:14.291   Training iter 300, batch loss 0.2685, batch acc 0.7906
09:41:14.776   Training iter 350, batch loss 0.2713, batch acc 0.7818
09:41:15.274   Training iter 400, batch loss 0.2696, batch acc 0.7858
09:41:15.766   Training iter 450, batch loss 0.2712, batch acc 0.7854
09:41:16.224   Training iter 500, batch loss 0.2707, batch acc 0.7870
09:41:16.663   Training iter 550, batch loss 0.2691, batch acc 0.7878
09:41:17.100   Training iter 600, batch loss 0.2681, batch acc 0.7924
09:41:17.102 Training @ 102 epoch...
09:41:17.568   Training iter 50, batch loss 0.2714, batch acc 0.7794
09:41:18.065   Training iter 100, batch loss 0.2721, batch acc 0.7786
09:41:18.548   Training iter 150, batch loss 0.2690, batch acc 0.7904
09:41:19.029   Training iter 200, batch loss 0.2713, batch acc 0.7802
09:41:19.506   Training iter 250, batch loss 0.2712, batch acc 0.7824
09:41:20.002   Training iter 300, batch loss 0.2702, batch acc 0.7914
09:41:20.485   Training iter 350, batch loss 0.2693, batch acc 0.7912
09:41:20.970   Training iter 400, batch loss 0.2697, batch acc 0.7890
09:41:21.503   Training iter 450, batch loss 0.2647, batch acc 0.8004
09:41:22.023   Training iter 500, batch loss 0.2699, batch acc 0.7866
09:41:22.557   Training iter 550, batch loss 0.2679, batch acc 0.7894
09:41:23.104   Training iter 600, batch loss 0.2714, batch acc 0.7788
09:41:23.106 Training @ 103 epoch...
09:41:23.654   Training iter 50, batch loss 0.2693, batch acc 0.7860
09:41:24.190   Training iter 100, batch loss 0.2712, batch acc 0.7834
09:41:24.734   Training iter 150, batch loss 0.2672, batch acc 0.7954
09:41:25.284   Training iter 200, batch loss 0.2691, batch acc 0.7780
09:41:25.864   Training iter 250, batch loss 0.2698, batch acc 0.7846
09:41:26.449   Training iter 300, batch loss 0.2698, batch acc 0.7876
09:41:27.036   Training iter 350, batch loss 0.2693, batch acc 0.7904
09:41:27.592   Training iter 400, batch loss 0.2702, batch acc 0.7934
09:41:28.122   Training iter 450, batch loss 0.2691, batch acc 0.7834
09:41:28.605   Training iter 500, batch loss 0.2682, batch acc 0.7904
09:41:29.095   Training iter 550, batch loss 0.2698, batch acc 0.7844
09:41:29.583   Training iter 600, batch loss 0.2694, batch acc 0.7850
09:41:29.585 Training @ 104 epoch...
09:41:30.115   Training iter 50, batch loss 0.2684, batch acc 0.7954
09:41:30.649   Training iter 100, batch loss 0.2675, batch acc 0.7956
09:41:31.179   Training iter 150, batch loss 0.2706, batch acc 0.7860
09:41:31.702   Training iter 200, batch loss 0.2709, batch acc 0.7772
09:41:32.237   Training iter 250, batch loss 0.2669, batch acc 0.7972
09:41:32.774   Training iter 300, batch loss 0.2683, batch acc 0.7830
09:41:33.304   Training iter 350, batch loss 0.2696, batch acc 0.7802
09:41:33.825   Training iter 400, batch loss 0.2700, batch acc 0.7852
09:41:34.352   Training iter 450, batch loss 0.2684, batch acc 0.7904
09:41:34.870   Training iter 500, batch loss 0.2693, batch acc 0.7836
09:41:35.371   Training iter 550, batch loss 0.2685, batch acc 0.7912
09:41:35.884   Training iter 600, batch loss 0.2679, batch acc 0.7878
09:41:35.886 Training @ 105 epoch...
09:41:36.398   Training iter 50, batch loss 0.2674, batch acc 0.7958
09:41:36.900   Training iter 100, batch loss 0.2694, batch acc 0.7872
09:41:37.420   Training iter 150, batch loss 0.2712, batch acc 0.7832
09:41:37.965   Training iter 200, batch loss 0.2678, batch acc 0.7888
09:41:38.511   Training iter 250, batch loss 0.2682, batch acc 0.7846
09:41:39.047   Training iter 300, batch loss 0.2659, batch acc 0.7944
09:41:39.582   Training iter 350, batch loss 0.2677, batch acc 0.7958
09:41:40.136   Training iter 400, batch loss 0.2685, batch acc 0.7846
09:41:40.683   Training iter 450, batch loss 0.2673, batch acc 0.7928
09:41:41.221   Training iter 500, batch loss 0.2682, batch acc 0.7850
09:41:41.749   Training iter 550, batch loss 0.2685, batch acc 0.7878
09:41:42.291   Training iter 600, batch loss 0.2706, batch acc 0.7790
09:41:42.293 Testing @ 105 epoch...
09:41:42.329     Testing, total mean loss 0.26490, total acc 0.80070
09:41:42.329 Training @ 106 epoch...
09:41:42.878   Training iter 50, batch loss 0.2652, batch acc 0.7938
09:41:43.422   Training iter 100, batch loss 0.2684, batch acc 0.7880
09:41:43.928   Training iter 150, batch loss 0.2671, batch acc 0.7878
09:41:44.426   Training iter 200, batch loss 0.2699, batch acc 0.7760
09:41:44.944   Training iter 250, batch loss 0.2697, batch acc 0.7922
09:41:45.462   Training iter 300, batch loss 0.2684, batch acc 0.7886
09:41:45.957   Training iter 350, batch loss 0.2678, batch acc 0.7894
09:41:46.463   Training iter 400, batch loss 0.2680, batch acc 0.7856
09:41:46.957   Training iter 450, batch loss 0.2669, batch acc 0.7962
09:41:47.441   Training iter 500, batch loss 0.2691, batch acc 0.7812
09:41:47.938   Training iter 550, batch loss 0.2671, batch acc 0.7932
09:41:48.448   Training iter 600, batch loss 0.2677, batch acc 0.7926
09:41:48.449 Training @ 107 epoch...
09:41:48.931   Training iter 50, batch loss 0.2681, batch acc 0.7842
09:41:49.408   Training iter 100, batch loss 0.2659, batch acc 0.7892
09:41:49.883   Training iter 150, batch loss 0.2673, batch acc 0.7936
09:41:50.369   Training iter 200, batch loss 0.2703, batch acc 0.7840
09:41:50.865   Training iter 250, batch loss 0.2688, batch acc 0.7846
09:41:51.375   Training iter 300, batch loss 0.2685, batch acc 0.7914
09:41:51.892   Training iter 350, batch loss 0.2650, batch acc 0.7994
09:41:52.401   Training iter 400, batch loss 0.2691, batch acc 0.7848
09:41:52.913   Training iter 450, batch loss 0.2683, batch acc 0.7902
09:41:53.432   Training iter 500, batch loss 0.2665, batch acc 0.7850
09:41:53.967   Training iter 550, batch loss 0.2654, batch acc 0.7918
09:41:54.518   Training iter 600, batch loss 0.2665, batch acc 0.7932
09:41:54.520 Training @ 108 epoch...
09:41:55.056   Training iter 50, batch loss 0.2676, batch acc 0.7832
09:41:55.567   Training iter 100, batch loss 0.2675, batch acc 0.7870
09:41:56.103   Training iter 150, batch loss 0.2672, batch acc 0.7946
09:41:56.654   Training iter 200, batch loss 0.2672, batch acc 0.7908
09:41:57.195   Training iter 250, batch loss 0.2674, batch acc 0.7922
09:41:57.747   Training iter 300, batch loss 0.2654, batch acc 0.7920
09:41:58.301   Training iter 350, batch loss 0.2679, batch acc 0.7834
09:41:58.831   Training iter 400, batch loss 0.2675, batch acc 0.7818
09:41:59.355   Training iter 450, batch loss 0.2668, batch acc 0.7902
09:41:59.859   Training iter 500, batch loss 0.2644, batch acc 0.7994
09:42:00.405   Training iter 550, batch loss 0.2677, batch acc 0.7790
09:42:00.913   Training iter 600, batch loss 0.2676, batch acc 0.8014
09:42:00.914 Training @ 109 epoch...
09:42:01.427   Training iter 50, batch loss 0.2662, batch acc 0.7946
09:42:01.938   Training iter 100, batch loss 0.2666, batch acc 0.7888
09:42:02.463   Training iter 150, batch loss 0.2634, batch acc 0.7980
09:42:02.970   Training iter 200, batch loss 0.2657, batch acc 0.7958
09:42:03.503   Training iter 250, batch loss 0.2704, batch acc 0.7744
09:42:04.032   Training iter 300, batch loss 0.2646, batch acc 0.7966
09:42:04.555   Training iter 350, batch loss 0.2685, batch acc 0.7894
09:42:05.095   Training iter 400, batch loss 0.2676, batch acc 0.7872
09:42:05.655   Training iter 450, batch loss 0.2659, batch acc 0.7946
09:42:06.188   Training iter 500, batch loss 0.2670, batch acc 0.7816
09:42:06.715   Training iter 550, batch loss 0.2644, batch acc 0.7960
09:42:07.249   Training iter 600, batch loss 0.2685, batch acc 0.7834
09:42:07.251 Training @ 110 epoch...
09:42:07.776   Training iter 50, batch loss 0.2659, batch acc 0.7940
09:42:08.288   Training iter 100, batch loss 0.2676, batch acc 0.7896
09:42:08.806   Training iter 150, batch loss 0.2683, batch acc 0.7806
09:42:09.326   Training iter 200, batch loss 0.2654, batch acc 0.7884
09:42:09.867   Training iter 250, batch loss 0.2653, batch acc 0.7902
09:42:10.408   Training iter 300, batch loss 0.2690, batch acc 0.7772
09:42:10.939   Training iter 350, batch loss 0.2649, batch acc 0.7990
09:42:11.471   Training iter 400, batch loss 0.2679, batch acc 0.7860
09:42:12.008   Training iter 450, batch loss 0.2653, batch acc 0.7984
09:42:12.550   Training iter 500, batch loss 0.2641, batch acc 0.7988
09:42:13.099   Training iter 550, batch loss 0.2658, batch acc 0.7844
09:42:13.637   Training iter 600, batch loss 0.2640, batch acc 0.8000
09:42:13.639 Testing @ 110 epoch...
09:42:13.674     Testing, total mean loss 0.26263, total acc 0.80390
09:42:13.674 Training @ 111 epoch...
09:42:14.238   Training iter 50, batch loss 0.2671, batch acc 0.7960
09:42:14.780   Training iter 100, batch loss 0.2655, batch acc 0.7900
09:42:15.332   Training iter 150, batch loss 0.2653, batch acc 0.7834
09:42:15.841   Training iter 200, batch loss 0.2673, batch acc 0.7834
09:42:16.364   Training iter 250, batch loss 0.2650, batch acc 0.7910
09:42:16.892   Training iter 300, batch loss 0.2654, batch acc 0.7964
09:42:17.436   Training iter 350, batch loss 0.2664, batch acc 0.7876
09:42:17.967   Training iter 400, batch loss 0.2655, batch acc 0.7886
09:42:18.491   Training iter 450, batch loss 0.2657, batch acc 0.7864
09:42:19.015   Training iter 500, batch loss 0.2664, batch acc 0.7926
09:42:19.531   Training iter 550, batch loss 0.2640, batch acc 0.8030
09:42:20.039   Training iter 600, batch loss 0.2648, batch acc 0.7942
09:42:20.041 Training @ 112 epoch...
09:42:20.527   Training iter 50, batch loss 0.2649, batch acc 0.7974
09:42:21.004   Training iter 100, batch loss 0.2652, batch acc 0.7962
09:42:21.471   Training iter 150, batch loss 0.2662, batch acc 0.7826
09:42:21.942   Training iter 200, batch loss 0.2655, batch acc 0.7910
09:42:22.456   Training iter 250, batch loss 0.2654, batch acc 0.7860
09:42:22.987   Training iter 300, batch loss 0.2636, batch acc 0.7998
09:42:23.519   Training iter 350, batch loss 0.2665, batch acc 0.7870
09:42:24.148   Training iter 400, batch loss 0.2649, batch acc 0.7908
09:42:24.825   Training iter 450, batch loss 0.2663, batch acc 0.7902
09:42:25.515   Training iter 500, batch loss 0.2671, batch acc 0.7886
09:42:26.192   Training iter 550, batch loss 0.2631, batch acc 0.7926
09:42:26.827   Training iter 600, batch loss 0.2648, batch acc 0.7906
09:42:26.829 Training @ 113 epoch...
09:42:27.503   Training iter 50, batch loss 0.2657, batch acc 0.7902
09:42:28.140   Training iter 100, batch loss 0.2686, batch acc 0.7816
09:42:28.806   Training iter 150, batch loss 0.2635, batch acc 0.7846
09:42:29.391   Training iter 200, batch loss 0.2645, batch acc 0.7886
09:42:29.938   Training iter 250, batch loss 0.2667, batch acc 0.7820
09:42:30.496   Training iter 300, batch loss 0.2645, batch acc 0.8022
09:42:31.045   Training iter 350, batch loss 0.2652, batch acc 0.7946
09:42:31.602   Training iter 400, batch loss 0.2621, batch acc 0.8008
09:42:32.123   Training iter 450, batch loss 0.2632, batch acc 0.7912
09:42:32.658   Training iter 500, batch loss 0.2646, batch acc 0.8042
09:42:33.192   Training iter 550, batch loss 0.2642, batch acc 0.7924
09:42:33.716   Training iter 600, batch loss 0.2655, batch acc 0.7888
09:42:33.718 Training @ 114 epoch...
09:42:34.253   Training iter 50, batch loss 0.2622, batch acc 0.8038
09:42:34.788   Training iter 100, batch loss 0.2645, batch acc 0.7894
09:42:35.325   Training iter 150, batch loss 0.2645, batch acc 0.7856
09:42:35.832   Training iter 200, batch loss 0.2637, batch acc 0.7982
09:42:36.349   Training iter 250, batch loss 0.2638, batch acc 0.7904
09:42:36.856   Training iter 300, batch loss 0.2661, batch acc 0.7884
09:42:37.387   Training iter 350, batch loss 0.2656, batch acc 0.7898
09:42:37.921   Training iter 400, batch loss 0.2655, batch acc 0.7898
09:42:38.444   Training iter 450, batch loss 0.2636, batch acc 0.7948
09:42:38.957   Training iter 500, batch loss 0.2653, batch acc 0.7910
09:42:39.479   Training iter 550, batch loss 0.2625, batch acc 0.7936
09:42:39.995   Training iter 600, batch loss 0.2662, batch acc 0.7900
09:42:39.996 Training @ 115 epoch...
09:42:40.521   Training iter 50, batch loss 0.2647, batch acc 0.7944
09:42:41.037   Training iter 100, batch loss 0.2662, batch acc 0.7836
09:42:41.545   Training iter 150, batch loss 0.2642, batch acc 0.7916
09:42:42.085   Training iter 200, batch loss 0.2635, batch acc 0.7956
09:42:42.620   Training iter 250, batch loss 0.2635, batch acc 0.7916
09:42:43.164   Training iter 300, batch loss 0.2645, batch acc 0.7854
09:42:43.696   Training iter 350, batch loss 0.2656, batch acc 0.7856
09:42:44.250   Training iter 400, batch loss 0.2626, batch acc 0.7978
09:42:44.790   Training iter 450, batch loss 0.2647, batch acc 0.7916
09:42:45.326   Training iter 500, batch loss 0.2633, batch acc 0.8024
09:42:45.860   Training iter 550, batch loss 0.2613, batch acc 0.8030
09:42:46.415   Training iter 600, batch loss 0.2644, batch acc 0.7916
09:42:46.417 Testing @ 115 epoch...
09:42:46.453     Testing, total mean loss 0.26054, total acc 0.80560
09:42:46.453 Training @ 116 epoch...
09:42:47.009   Training iter 50, batch loss 0.2636, batch acc 0.7954
09:42:47.547   Training iter 100, batch loss 0.2650, batch acc 0.7890
09:42:48.040   Training iter 150, batch loss 0.2627, batch acc 0.7972
09:42:48.556   Training iter 200, batch loss 0.2646, batch acc 0.7988
09:42:49.072   Training iter 250, batch loss 0.2647, batch acc 0.7972
09:42:49.574   Training iter 300, batch loss 0.2618, batch acc 0.7980
09:42:50.072   Training iter 350, batch loss 0.2637, batch acc 0.7952
09:42:50.557   Training iter 400, batch loss 0.2610, batch acc 0.7976
09:42:51.033   Training iter 450, batch loss 0.2637, batch acc 0.7862
09:42:51.515   Training iter 500, batch loss 0.2631, batch acc 0.7964
09:42:52.007   Training iter 550, batch loss 0.2672, batch acc 0.7776
09:42:52.503   Training iter 600, batch loss 0.2628, batch acc 0.7902
09:42:52.505 Training @ 117 epoch...
09:42:52.988   Training iter 50, batch loss 0.2647, batch acc 0.7850
09:42:53.478   Training iter 100, batch loss 0.2611, batch acc 0.8016
09:42:53.978   Training iter 150, batch loss 0.2635, batch acc 0.7930
09:42:54.483   Training iter 200, batch loss 0.2641, batch acc 0.7902
09:42:54.986   Training iter 250, batch loss 0.2619, batch acc 0.7962
09:42:55.481   Training iter 300, batch loss 0.2648, batch acc 0.7944
09:42:55.968   Training iter 350, batch loss 0.2610, batch acc 0.7990
09:42:56.510   Training iter 400, batch loss 0.2641, batch acc 0.7974
09:42:57.065   Training iter 450, batch loss 0.2626, batch acc 0.7888
09:42:57.623   Training iter 500, batch loss 0.2625, batch acc 0.7976
09:42:58.178   Training iter 550, batch loss 0.2634, batch acc 0.7892
09:42:58.704   Training iter 600, batch loss 0.2654, batch acc 0.7890
09:42:58.706 Training @ 118 epoch...
09:42:59.255   Training iter 50, batch loss 0.2643, batch acc 0.7918
09:42:59.796   Training iter 100, batch loss 0.2634, batch acc 0.7950
09:43:00.341   Training iter 150, batch loss 0.2632, batch acc 0.7978
09:43:00.925   Training iter 200, batch loss 0.2651, batch acc 0.7868
09:43:01.545   Training iter 250, batch loss 0.2606, batch acc 0.8002
09:43:02.174   Training iter 300, batch loss 0.2635, batch acc 0.7976
09:43:02.793   Training iter 350, batch loss 0.2619, batch acc 0.7954
09:43:03.370   Training iter 400, batch loss 0.2634, batch acc 0.7914
09:43:03.890   Training iter 450, batch loss 0.2615, batch acc 0.7926
09:43:04.386   Training iter 500, batch loss 0.2623, batch acc 0.7952
09:43:04.881   Training iter 550, batch loss 0.2638, batch acc 0.7884
09:43:05.382   Training iter 600, batch loss 0.2615, batch acc 0.7964
09:43:05.384 Training @ 119 epoch...
09:43:05.888   Training iter 50, batch loss 0.2639, batch acc 0.7914
09:43:06.403   Training iter 100, batch loss 0.2611, batch acc 0.7992
09:43:06.911   Training iter 150, batch loss 0.2630, batch acc 0.7962
09:43:07.422   Training iter 200, batch loss 0.2638, batch acc 0.7832
09:43:07.936   Training iter 250, batch loss 0.2616, batch acc 0.7990
09:43:08.451   Training iter 300, batch loss 0.2601, batch acc 0.7958
09:43:08.962   Training iter 350, batch loss 0.2644, batch acc 0.7922
09:43:09.485   Training iter 400, batch loss 0.2625, batch acc 0.7922
09:43:09.995   Training iter 450, batch loss 0.2640, batch acc 0.7916
09:43:10.533   Training iter 500, batch loss 0.2623, batch acc 0.7974
09:43:11.048   Training iter 550, batch loss 0.2630, batch acc 0.7906
09:43:11.564   Training iter 600, batch loss 0.2601, batch acc 0.8010
09:43:11.565 Training @ 120 epoch...
09:43:12.137   Training iter 50, batch loss 0.2620, batch acc 0.7924
09:43:12.747   Training iter 100, batch loss 0.2609, batch acc 0.7988
09:43:13.268   Training iter 150, batch loss 0.2643, batch acc 0.7848
09:43:13.817   Training iter 200, batch loss 0.2624, batch acc 0.7956
09:43:14.450   Training iter 250, batch loss 0.2632, batch acc 0.7882
09:43:15.082   Training iter 300, batch loss 0.2624, batch acc 0.7992
09:43:15.629   Training iter 350, batch loss 0.2636, batch acc 0.7876
09:43:16.188   Training iter 400, batch loss 0.2599, batch acc 0.8028
09:43:16.769   Training iter 450, batch loss 0.2601, batch acc 0.7998
09:43:17.337   Training iter 500, batch loss 0.2599, batch acc 0.7970
09:43:17.889   Training iter 550, batch loss 0.2637, batch acc 0.7974
09:43:18.440   Training iter 600, batch loss 0.2633, batch acc 0.7954
09:43:18.442 Testing @ 120 epoch...
09:43:18.477     Testing, total mean loss 0.25861, total acc 0.80730
09:43:18.478 Training @ 121 epoch...
09:43:19.042   Training iter 50, batch loss 0.2625, batch acc 0.7894
09:43:19.592   Training iter 100, batch loss 0.2639, batch acc 0.7886
09:43:20.133   Training iter 150, batch loss 0.2625, batch acc 0.7894
09:43:20.661   Training iter 200, batch loss 0.2619, batch acc 0.7888
09:43:21.144   Training iter 250, batch loss 0.2632, batch acc 0.7926
09:43:21.604   Training iter 300, batch loss 0.2609, batch acc 0.7986
09:43:22.071   Training iter 350, batch loss 0.2637, batch acc 0.7930
09:43:22.534   Training iter 400, batch loss 0.2615, batch acc 0.7962
09:43:22.992   Training iter 450, batch loss 0.2603, batch acc 0.8006
09:43:23.450   Training iter 500, batch loss 0.2594, batch acc 0.8068
09:43:23.892   Training iter 550, batch loss 0.2609, batch acc 0.7994
09:43:24.363   Training iter 600, batch loss 0.2604, batch acc 0.7984
09:43:24.365 Training @ 122 epoch...
09:43:24.837   Training iter 50, batch loss 0.2599, batch acc 0.7930
09:43:25.305   Training iter 100, batch loss 0.2613, batch acc 0.7912
09:43:25.748   Training iter 150, batch loss 0.2606, batch acc 0.7994
09:43:26.194   Training iter 200, batch loss 0.2586, batch acc 0.8084
09:43:26.654   Training iter 250, batch loss 0.2643, batch acc 0.7870
09:43:27.125   Training iter 300, batch loss 0.2617, batch acc 0.7944
09:43:27.627   Training iter 350, batch loss 0.2618, batch acc 0.7940
09:43:28.103   Training iter 400, batch loss 0.2628, batch acc 0.7912
09:43:28.577   Training iter 450, batch loss 0.2626, batch acc 0.8012
09:43:29.054   Training iter 500, batch loss 0.2621, batch acc 0.7894
09:43:29.526   Training iter 550, batch loss 0.2602, batch acc 0.8024
09:43:30.013   Training iter 600, batch loss 0.2609, batch acc 0.7956
09:43:30.014 Training @ 123 epoch...
09:43:30.513   Training iter 50, batch loss 0.2616, batch acc 0.7988
09:43:30.990   Training iter 100, batch loss 0.2653, batch acc 0.7882
09:43:31.462   Training iter 150, batch loss 0.2630, batch acc 0.7862
09:43:31.950   Training iter 200, batch loss 0.2603, batch acc 0.8060
09:43:32.445   Training iter 250, batch loss 0.2625, batch acc 0.7908
09:43:32.917   Training iter 300, batch loss 0.2575, batch acc 0.8030
09:43:33.364   Training iter 350, batch loss 0.2604, batch acc 0.7956
09:43:33.808   Training iter 400, batch loss 0.2609, batch acc 0.7960
09:43:34.256   Training iter 450, batch loss 0.2612, batch acc 0.7916
09:43:34.727   Training iter 500, batch loss 0.2605, batch acc 0.7968
09:43:35.192   Training iter 550, batch loss 0.2603, batch acc 0.8020
09:43:35.644   Training iter 600, batch loss 0.2592, batch acc 0.7984
09:43:35.646 Training @ 124 epoch...
09:43:36.125   Training iter 50, batch loss 0.2605, batch acc 0.7998
09:43:36.575   Training iter 100, batch loss 0.2605, batch acc 0.8000
09:43:37.027   Training iter 150, batch loss 0.2610, batch acc 0.7944
09:43:37.468   Training iter 200, batch loss 0.2606, batch acc 0.8024
09:43:37.903   Training iter 250, batch loss 0.2576, batch acc 0.8068
09:43:38.349   Training iter 300, batch loss 0.2610, batch acc 0.7918
09:43:38.789   Training iter 350, batch loss 0.2614, batch acc 0.7884
09:43:39.235   Training iter 400, batch loss 0.2624, batch acc 0.7934
09:43:39.681   Training iter 450, batch loss 0.2627, batch acc 0.7940
09:43:40.146   Training iter 500, batch loss 0.2597, batch acc 0.7938
09:43:40.591   Training iter 550, batch loss 0.2582, batch acc 0.8026
09:43:41.023   Training iter 600, batch loss 0.2626, batch acc 0.7886
09:43:41.024 Training @ 125 epoch...
09:43:41.486   Training iter 50, batch loss 0.2591, batch acc 0.7954
09:43:41.937   Training iter 100, batch loss 0.2632, batch acc 0.7872
09:43:42.414   Training iter 150, batch loss 0.2615, batch acc 0.7896
09:43:42.892   Training iter 200, batch loss 0.2599, batch acc 0.7964
09:43:43.347   Training iter 250, batch loss 0.2586, batch acc 0.8124
09:43:43.789   Training iter 300, batch loss 0.2604, batch acc 0.7954
09:43:44.231   Training iter 350, batch loss 0.2611, batch acc 0.7994
09:43:44.681   Training iter 400, batch loss 0.2606, batch acc 0.7948
09:43:45.136   Training iter 450, batch loss 0.2588, batch acc 0.7948
09:43:45.601   Training iter 500, batch loss 0.2583, batch acc 0.8032
09:43:46.085   Training iter 550, batch loss 0.2600, batch acc 0.8026
09:43:46.576   Training iter 600, batch loss 0.2627, batch acc 0.7890
09:43:46.578 Testing @ 125 epoch...
09:43:46.613     Testing, total mean loss 0.25683, total acc 0.80900
09:43:46.613 Training @ 126 epoch...
09:43:47.120   Training iter 50, batch loss 0.2589, batch acc 0.8012
09:43:47.638   Training iter 100, batch loss 0.2620, batch acc 0.7850
09:43:48.140   Training iter 150, batch loss 0.2616, batch acc 0.7978
09:43:48.659   Training iter 200, batch loss 0.2612, batch acc 0.7978
09:43:49.158   Training iter 250, batch loss 0.2595, batch acc 0.7964
09:43:49.661   Training iter 300, batch loss 0.2598, batch acc 0.8038
09:43:50.177   Training iter 350, batch loss 0.2591, batch acc 0.7956
09:43:50.677   Training iter 400, batch loss 0.2597, batch acc 0.7990
09:43:51.176   Training iter 450, batch loss 0.2599, batch acc 0.8014
09:43:51.679   Training iter 500, batch loss 0.2577, batch acc 0.8036
09:43:52.230   Training iter 550, batch loss 0.2609, batch acc 0.7972
09:43:52.770   Training iter 600, batch loss 0.2598, batch acc 0.7896
09:43:52.772 Training @ 127 epoch...
09:43:53.322   Training iter 50, batch loss 0.2614, batch acc 0.7898
09:43:53.913   Training iter 100, batch loss 0.2584, batch acc 0.8006
09:43:54.489   Training iter 150, batch loss 0.2610, batch acc 0.7916
09:43:55.037   Training iter 200, batch loss 0.2601, batch acc 0.7954
09:43:55.541   Training iter 250, batch loss 0.2570, batch acc 0.8050
09:43:56.036   Training iter 300, batch loss 0.2596, batch acc 0.7916
09:43:56.529   Training iter 350, batch loss 0.2606, batch acc 0.7968
09:43:57.038   Training iter 400, batch loss 0.2600, batch acc 0.7982
09:43:57.547   Training iter 450, batch loss 0.2587, batch acc 0.8020
09:43:58.060   Training iter 500, batch loss 0.2594, batch acc 0.7956
09:43:58.529   Training iter 550, batch loss 0.2598, batch acc 0.8002
09:43:58.992   Training iter 600, batch loss 0.2602, batch acc 0.8034
09:43:58.993 Training @ 128 epoch...
09:43:59.476   Training iter 50, batch loss 0.2580, batch acc 0.8008
09:43:59.941   Training iter 100, batch loss 0.2584, batch acc 0.8060
09:44:00.446   Training iter 150, batch loss 0.2605, batch acc 0.7870
09:44:00.935   Training iter 200, batch loss 0.2589, batch acc 0.8004
09:44:01.487   Training iter 250, batch loss 0.2599, batch acc 0.7948
09:44:02.153   Training iter 300, batch loss 0.2610, batch acc 0.7948
09:44:02.730   Training iter 350, batch loss 0.2600, batch acc 0.7952
09:44:03.269   Training iter 400, batch loss 0.2601, batch acc 0.7940
09:44:03.801   Training iter 450, batch loss 0.2581, batch acc 0.8010
09:44:04.345   Training iter 500, batch loss 0.2597, batch acc 0.8036
09:44:04.911   Training iter 550, batch loss 0.2587, batch acc 0.7972
09:44:05.488   Training iter 600, batch loss 0.2588, batch acc 0.8002
09:44:05.490 Training @ 129 epoch...
09:44:06.058   Training iter 50, batch loss 0.2592, batch acc 0.7928
09:44:06.608   Training iter 100, batch loss 0.2581, batch acc 0.7938
09:44:07.187   Training iter 150, batch loss 0.2594, batch acc 0.8020
09:44:07.726   Training iter 200, batch loss 0.2582, batch acc 0.8048
09:44:08.247   Training iter 250, batch loss 0.2577, batch acc 0.7988
09:44:08.749   Training iter 300, batch loss 0.2600, batch acc 0.7988
09:44:09.249   Training iter 350, batch loss 0.2571, batch acc 0.8014
09:44:09.750   Training iter 400, batch loss 0.2603, batch acc 0.7964
09:44:10.299   Training iter 450, batch loss 0.2600, batch acc 0.7984
09:44:10.849   Training iter 500, batch loss 0.2596, batch acc 0.7918
09:44:11.393   Training iter 550, batch loss 0.2586, batch acc 0.8028
09:44:11.958   Training iter 600, batch loss 0.2601, batch acc 0.7968
09:44:11.961 Training @ 130 epoch...
09:44:12.612   Training iter 50, batch loss 0.2576, batch acc 0.8010
09:44:13.213   Training iter 100, batch loss 0.2603, batch acc 0.7912
09:44:13.744   Training iter 150, batch loss 0.2572, batch acc 0.8004
09:44:14.291   Training iter 200, batch loss 0.2567, batch acc 0.8042
09:44:14.817   Training iter 250, batch loss 0.2604, batch acc 0.7920
09:44:15.357   Training iter 300, batch loss 0.2588, batch acc 0.7944
09:44:15.894   Training iter 350, batch loss 0.2598, batch acc 0.8018
09:44:16.453   Training iter 400, batch loss 0.2571, batch acc 0.8014
09:44:17.010   Training iter 450, batch loss 0.2618, batch acc 0.7916
09:44:17.535   Training iter 500, batch loss 0.2579, batch acc 0.8038
09:44:18.083   Training iter 550, batch loss 0.2594, batch acc 0.7996
09:44:18.632   Training iter 600, batch loss 0.2574, batch acc 0.8044
09:44:18.634 Testing @ 130 epoch...
09:44:18.673     Testing, total mean loss 0.25518, total acc 0.80990
09:44:18.673 Training @ 131 epoch...
09:44:19.228   Training iter 50, batch loss 0.2579, batch acc 0.7898
09:44:19.771   Training iter 100, batch loss 0.2588, batch acc 0.8034
09:44:20.301   Training iter 150, batch loss 0.2580, batch acc 0.8012
09:44:20.817   Training iter 200, batch loss 0.2606, batch acc 0.7934
09:44:21.324   Training iter 250, batch loss 0.2582, batch acc 0.7988
09:44:21.829   Training iter 300, batch loss 0.2561, batch acc 0.8092
09:44:22.350   Training iter 350, batch loss 0.2587, batch acc 0.7982
09:44:22.864   Training iter 400, batch loss 0.2587, batch acc 0.7916
09:44:23.383   Training iter 450, batch loss 0.2604, batch acc 0.7968
09:44:23.894   Training iter 500, batch loss 0.2593, batch acc 0.7980
09:44:24.413   Training iter 550, batch loss 0.2584, batch acc 0.7998
09:44:24.938   Training iter 600, batch loss 0.2556, batch acc 0.8076
09:44:24.940 Training @ 132 epoch...
09:44:25.469   Training iter 50, batch loss 0.2579, batch acc 0.7940
09:44:26.008   Training iter 100, batch loss 0.2626, batch acc 0.7804
09:44:26.550   Training iter 150, batch loss 0.2579, batch acc 0.8014
09:44:27.090   Training iter 200, batch loss 0.2566, batch acc 0.8086
09:44:27.647   Training iter 250, batch loss 0.2577, batch acc 0.8068
09:44:28.220   Training iter 300, batch loss 0.2576, batch acc 0.7996
09:44:28.782   Training iter 350, batch loss 0.2570, batch acc 0.8082
09:44:29.347   Training iter 400, batch loss 0.2578, batch acc 0.8004
09:44:29.900   Training iter 450, batch loss 0.2565, batch acc 0.8028
09:44:30.473   Training iter 500, batch loss 0.2599, batch acc 0.7902
09:44:31.024   Training iter 550, batch loss 0.2583, batch acc 0.7976
09:44:31.566   Training iter 600, batch loss 0.2571, batch acc 0.8046
09:44:31.568 Training @ 133 epoch...
09:44:32.133   Training iter 50, batch loss 0.2579, batch acc 0.7950
09:44:32.705   Training iter 100, batch loss 0.2564, batch acc 0.8004
09:44:33.263   Training iter 150, batch loss 0.2577, batch acc 0.8006
09:44:33.812   Training iter 200, batch loss 0.2586, batch acc 0.7982
09:44:34.378   Training iter 250, batch loss 0.2580, batch acc 0.8018
09:44:34.938   Training iter 300, batch loss 0.2585, batch acc 0.7964
09:44:35.523   Training iter 350, batch loss 0.2557, batch acc 0.8086
09:44:36.097   Training iter 400, batch loss 0.2582, batch acc 0.7928
09:44:36.684   Training iter 450, batch loss 0.2573, batch acc 0.7960
09:44:37.268   Training iter 500, batch loss 0.2562, batch acc 0.8136
09:44:37.846   Training iter 550, batch loss 0.2598, batch acc 0.7990
09:44:38.399   Training iter 600, batch loss 0.2588, batch acc 0.7906
09:44:38.401 Training @ 134 epoch...
09:44:38.928   Training iter 50, batch loss 0.2586, batch acc 0.7976
09:44:39.443   Training iter 100, batch loss 0.2561, batch acc 0.7994
09:44:39.945   Training iter 150, batch loss 0.2588, batch acc 0.8026
09:44:40.470   Training iter 200, batch loss 0.2574, batch acc 0.7952
09:44:40.978   Training iter 250, batch loss 0.2571, batch acc 0.8030
09:44:41.495   Training iter 300, batch loss 0.2567, batch acc 0.8010
09:44:41.992   Training iter 350, batch loss 0.2563, batch acc 0.8022
09:44:42.514   Training iter 400, batch loss 0.2560, batch acc 0.8012
09:44:43.032   Training iter 450, batch loss 0.2558, batch acc 0.8028
09:44:43.547   Training iter 500, batch loss 0.2559, batch acc 0.8038
09:44:44.068   Training iter 550, batch loss 0.2597, batch acc 0.7930
09:44:44.590   Training iter 600, batch loss 0.2613, batch acc 0.7950
09:44:44.591 Training @ 135 epoch...
09:44:45.107   Training iter 50, batch loss 0.2584, batch acc 0.8036
09:44:45.635   Training iter 100, batch loss 0.2580, batch acc 0.8000
09:44:46.148   Training iter 150, batch loss 0.2571, batch acc 0.7934
09:44:46.655   Training iter 200, batch loss 0.2569, batch acc 0.8028
09:44:47.216   Training iter 250, batch loss 0.2574, batch acc 0.7980
09:44:47.774   Training iter 300, batch loss 0.2598, batch acc 0.7956
09:44:48.330   Training iter 350, batch loss 0.2585, batch acc 0.7950
09:44:48.836   Training iter 400, batch loss 0.2564, batch acc 0.8038
09:44:49.329   Training iter 450, batch loss 0.2573, batch acc 0.7936
09:44:49.832   Training iter 500, batch loss 0.2546, batch acc 0.8106
09:44:50.315   Training iter 550, batch loss 0.2558, batch acc 0.8038
09:44:50.794   Training iter 600, batch loss 0.2560, batch acc 0.8008
09:44:50.796 Testing @ 135 epoch...
09:44:50.832     Testing, total mean loss 0.25365, total acc 0.81150
09:44:50.832 Training @ 136 epoch...
09:44:51.297   Training iter 50, batch loss 0.2553, batch acc 0.7986
09:44:51.764   Training iter 100, batch loss 0.2562, batch acc 0.7972
09:44:52.250   Training iter 150, batch loss 0.2583, batch acc 0.7988
09:44:52.723   Training iter 200, batch loss 0.2548, batch acc 0.8082
09:44:53.211   Training iter 250, batch loss 0.2579, batch acc 0.7958
09:44:53.705   Training iter 300, batch loss 0.2575, batch acc 0.8008
09:44:54.199   Training iter 350, batch loss 0.2558, batch acc 0.8068
09:44:54.697   Training iter 400, batch loss 0.2584, batch acc 0.7958
09:44:55.200   Training iter 450, batch loss 0.2586, batch acc 0.7986
09:44:55.702   Training iter 500, batch loss 0.2559, batch acc 0.8066
09:44:56.192   Training iter 550, batch loss 0.2584, batch acc 0.7914
09:44:56.693   Training iter 600, batch loss 0.2556, batch acc 0.8044
09:44:56.695 Training @ 137 epoch...
09:44:57.193   Training iter 50, batch loss 0.2579, batch acc 0.8048
09:44:57.677   Training iter 100, batch loss 0.2540, batch acc 0.8096
09:44:58.167   Training iter 150, batch loss 0.2559, batch acc 0.8006
09:44:58.657   Training iter 200, batch loss 0.2594, batch acc 0.7876
09:44:59.154   Training iter 250, batch loss 0.2581, batch acc 0.8042
09:44:59.644   Training iter 300, batch loss 0.2557, batch acc 0.8004
09:45:00.150   Training iter 350, batch loss 0.2549, batch acc 0.7992
09:45:00.676   Training iter 400, batch loss 0.2575, batch acc 0.7944
09:45:01.205   Training iter 450, batch loss 0.2566, batch acc 0.7998
09:45:01.760   Training iter 500, batch loss 0.2568, batch acc 0.8040
09:45:02.307   Training iter 550, batch loss 0.2586, batch acc 0.7936
09:45:02.827   Training iter 600, batch loss 0.2536, batch acc 0.8112
09:45:02.828 Training @ 138 epoch...
09:45:03.332   Training iter 50, batch loss 0.2552, batch acc 0.8028
09:45:03.838   Training iter 100, batch loss 0.2584, batch acc 0.7970
09:45:04.331   Training iter 150, batch loss 0.2557, batch acc 0.8130
09:45:04.831   Training iter 200, batch loss 0.2558, batch acc 0.7986
09:45:05.332   Training iter 250, batch loss 0.2563, batch acc 0.7986
09:45:05.833   Training iter 300, batch loss 0.2554, batch acc 0.8090
09:45:06.306   Training iter 350, batch loss 0.2547, batch acc 0.8038
09:45:06.756   Training iter 400, batch loss 0.2591, batch acc 0.7842
09:45:07.209   Training iter 450, batch loss 0.2565, batch acc 0.8042
09:45:07.682   Training iter 500, batch loss 0.2557, batch acc 0.8054
09:45:08.153   Training iter 550, batch loss 0.2566, batch acc 0.8006
09:45:08.630   Training iter 600, batch loss 0.2564, batch acc 0.7950
09:45:08.632 Training @ 139 epoch...
09:45:09.093   Training iter 50, batch loss 0.2567, batch acc 0.8042
09:45:09.571   Training iter 100, batch loss 0.2548, batch acc 0.8050
09:45:10.044   Training iter 150, batch loss 0.2535, batch acc 0.8024
09:45:10.526   Training iter 200, batch loss 0.2566, batch acc 0.8014
09:45:10.988   Training iter 250, batch loss 0.2571, batch acc 0.7918
09:45:11.461   Training iter 300, batch loss 0.2582, batch acc 0.8024
09:45:11.940   Training iter 350, batch loss 0.2551, batch acc 0.8034
09:45:12.459   Training iter 400, batch loss 0.2559, batch acc 0.7976
09:45:12.978   Training iter 450, batch loss 0.2554, batch acc 0.8084
09:45:13.474   Training iter 500, batch loss 0.2549, batch acc 0.7978
09:45:13.989   Training iter 550, batch loss 0.2578, batch acc 0.7940
09:45:14.493   Training iter 600, batch loss 0.2565, batch acc 0.8046
09:45:14.495 Training @ 140 epoch...
09:45:14.982   Training iter 50, batch loss 0.2559, batch acc 0.8072
09:45:15.498   Training iter 100, batch loss 0.2546, batch acc 0.7984
09:45:15.992   Training iter 150, batch loss 0.2582, batch acc 0.8008
09:45:16.489   Training iter 200, batch loss 0.2553, batch acc 0.8018
09:45:16.945   Training iter 250, batch loss 0.2562, batch acc 0.8014
09:45:17.394   Training iter 300, batch loss 0.2549, batch acc 0.8088
09:45:17.847   Training iter 350, batch loss 0.2538, batch acc 0.8106
09:45:18.338   Training iter 400, batch loss 0.2576, batch acc 0.7976
09:45:18.815   Training iter 450, batch loss 0.2561, batch acc 0.8044
09:45:19.254   Training iter 500, batch loss 0.2559, batch acc 0.7974
09:45:19.685   Training iter 550, batch loss 0.2557, batch acc 0.7970
09:45:20.159   Training iter 600, batch loss 0.2549, batch acc 0.7970
09:45:20.160 Testing @ 140 epoch...
09:45:20.196     Testing, total mean loss 0.25223, total acc 0.81350
09:45:20.196 Training @ 141 epoch...
09:45:20.679   Training iter 50, batch loss 0.2544, batch acc 0.8070
09:45:21.150   Training iter 100, batch loss 0.2576, batch acc 0.7956
09:45:21.621   Training iter 150, batch loss 0.2554, batch acc 0.8068
09:45:22.114   Training iter 200, batch loss 0.2537, batch acc 0.8072
09:45:22.604   Training iter 250, batch loss 0.2567, batch acc 0.7980
09:45:23.065   Training iter 300, batch loss 0.2580, batch acc 0.7918
09:45:23.534   Training iter 350, batch loss 0.2553, batch acc 0.8088
09:45:23.996   Training iter 400, batch loss 0.2540, batch acc 0.7994
09:45:24.474   Training iter 450, batch loss 0.2563, batch acc 0.8020
09:45:24.948   Training iter 500, batch loss 0.2533, batch acc 0.8108
09:45:25.439   Training iter 550, batch loss 0.2548, batch acc 0.7946
09:45:25.922   Training iter 600, batch loss 0.2565, batch acc 0.8010
09:45:25.924 Training @ 142 epoch...
09:45:26.420   Training iter 50, batch loss 0.2551, batch acc 0.8062
09:45:26.906   Training iter 100, batch loss 0.2570, batch acc 0.7952
09:45:27.390   Training iter 150, batch loss 0.2557, batch acc 0.8008
09:45:27.865   Training iter 200, batch loss 0.2561, batch acc 0.7982
09:45:28.359   Training iter 250, batch loss 0.2541, batch acc 0.8118
09:45:28.858   Training iter 300, batch loss 0.2550, batch acc 0.8118
09:45:29.358   Training iter 350, batch loss 0.2533, batch acc 0.7972
09:45:29.838   Training iter 400, batch loss 0.2557, batch acc 0.7986
09:45:30.336   Training iter 450, batch loss 0.2564, batch acc 0.7952
09:45:30.862   Training iter 500, batch loss 0.2558, batch acc 0.8026
09:45:31.363   Training iter 550, batch loss 0.2523, batch acc 0.8114
09:45:31.858   Training iter 600, batch loss 0.2564, batch acc 0.8002
09:45:31.860 Training @ 143 epoch...
09:45:32.355   Training iter 50, batch loss 0.2553, batch acc 0.8018
09:45:32.857   Training iter 100, batch loss 0.2555, batch acc 0.8084
09:45:33.341   Training iter 150, batch loss 0.2545, batch acc 0.7998
09:45:33.820   Training iter 200, batch loss 0.2550, batch acc 0.8020
09:45:34.309   Training iter 250, batch loss 0.2540, batch acc 0.8054
09:45:34.796   Training iter 300, batch loss 0.2546, batch acc 0.8012
09:45:35.293   Training iter 350, batch loss 0.2547, batch acc 0.8014
09:45:35.790   Training iter 400, batch loss 0.2589, batch acc 0.7920
09:45:36.304   Training iter 450, batch loss 0.2535, batch acc 0.8008
09:45:36.820   Training iter 500, batch loss 0.2544, batch acc 0.8074
09:45:37.336   Training iter 550, batch loss 0.2555, batch acc 0.8016
09:45:37.837   Training iter 600, batch loss 0.2536, batch acc 0.8070
09:45:37.839 Training @ 144 epoch...
09:45:38.338   Training iter 50, batch loss 0.2529, batch acc 0.8116
09:45:38.838   Training iter 100, batch loss 0.2531, batch acc 0.8078
09:45:39.343   Training iter 150, batch loss 0.2522, batch acc 0.8122
09:45:39.840   Training iter 200, batch loss 0.2540, batch acc 0.8056
09:45:40.348   Training iter 250, batch loss 0.2540, batch acc 0.8050
09:45:40.838   Training iter 300, batch loss 0.2552, batch acc 0.7970
09:45:41.344   Training iter 350, batch loss 0.2567, batch acc 0.7976
09:45:41.843   Training iter 400, batch loss 0.2584, batch acc 0.7846
09:45:42.345   Training iter 450, batch loss 0.2548, batch acc 0.8044
09:45:42.851   Training iter 500, batch loss 0.2544, batch acc 0.7968
09:45:43.356   Training iter 550, batch loss 0.2564, batch acc 0.8052
09:45:43.866   Training iter 600, batch loss 0.2544, batch acc 0.8030
09:45:43.868 Training @ 145 epoch...
09:45:44.400   Training iter 50, batch loss 0.2527, batch acc 0.8066
09:45:44.881   Training iter 100, batch loss 0.2546, batch acc 0.8010
09:45:45.391   Training iter 150, batch loss 0.2541, batch acc 0.7996
09:45:45.944   Training iter 200, batch loss 0.2539, batch acc 0.8100
09:45:46.502   Training iter 250, batch loss 0.2539, batch acc 0.8078
09:45:47.082   Training iter 300, batch loss 0.2571, batch acc 0.7932
09:45:47.584   Training iter 350, batch loss 0.2538, batch acc 0.8026
09:45:48.097   Training iter 400, batch loss 0.2560, batch acc 0.7990
09:45:48.641   Training iter 450, batch loss 0.2538, batch acc 0.8066
09:45:49.143   Training iter 500, batch loss 0.2544, batch acc 0.7980
09:45:49.645   Training iter 550, batch loss 0.2533, batch acc 0.8114
09:45:50.159   Training iter 600, batch loss 0.2559, batch acc 0.7992
09:45:50.161 Testing @ 145 epoch...
09:45:50.198     Testing, total mean loss 0.25092, total acc 0.81400
09:45:50.198 Training @ 146 epoch...
09:45:50.739   Training iter 50, batch loss 0.2562, batch acc 0.8036
09:45:51.252   Training iter 100, batch loss 0.2544, batch acc 0.8012
09:45:51.774   Training iter 150, batch loss 0.2535, batch acc 0.8008
09:45:52.334   Training iter 200, batch loss 0.2542, batch acc 0.8054
09:45:52.889   Training iter 250, batch loss 0.2527, batch acc 0.8056
09:45:53.471   Training iter 300, batch loss 0.2556, batch acc 0.7964
09:45:54.047   Training iter 350, batch loss 0.2530, batch acc 0.8050
09:45:54.635   Training iter 400, batch loss 0.2569, batch acc 0.8030
09:45:55.190   Training iter 450, batch loss 0.2527, batch acc 0.8014
09:45:55.735   Training iter 500, batch loss 0.2551, batch acc 0.8022
09:45:56.274   Training iter 550, batch loss 0.2531, batch acc 0.8034
09:45:56.820   Training iter 600, batch loss 0.2532, batch acc 0.8106
09:45:56.822 Training @ 147 epoch...
09:45:57.358   Training iter 50, batch loss 0.2551, batch acc 0.8048
09:45:57.891   Training iter 100, batch loss 0.2568, batch acc 0.7950
09:45:58.441   Training iter 150, batch loss 0.2561, batch acc 0.7938
09:45:58.976   Training iter 200, batch loss 0.2534, batch acc 0.8062
09:45:59.484   Training iter 250, batch loss 0.2536, batch acc 0.8058
09:45:59.997   Training iter 300, batch loss 0.2529, batch acc 0.8052
09:46:00.555   Training iter 350, batch loss 0.2510, batch acc 0.8182
09:46:01.079   Training iter 400, batch loss 0.2524, batch acc 0.8090
09:46:01.649   Training iter 450, batch loss 0.2546, batch acc 0.7990
09:46:02.203   Training iter 500, batch loss 0.2528, batch acc 0.8044
09:46:02.711   Training iter 550, batch loss 0.2539, batch acc 0.7970
09:46:03.229   Training iter 600, batch loss 0.2547, batch acc 0.7994
09:46:03.231 Training @ 148 epoch...
09:46:03.777   Training iter 50, batch loss 0.2516, batch acc 0.8092
09:46:04.329   Training iter 100, batch loss 0.2522, batch acc 0.8070
09:46:04.873   Training iter 150, batch loss 0.2544, batch acc 0.8052
09:46:05.387   Training iter 200, batch loss 0.2542, batch acc 0.7994
09:46:05.902   Training iter 250, batch loss 0.2523, batch acc 0.8036
09:46:06.426   Training iter 300, batch loss 0.2535, batch acc 0.8100
09:46:06.925   Training iter 350, batch loss 0.2558, batch acc 0.7948
09:46:07.406   Training iter 400, batch loss 0.2526, batch acc 0.8084
09:46:07.899   Training iter 450, batch loss 0.2544, batch acc 0.7980
09:46:08.395   Training iter 500, batch loss 0.2552, batch acc 0.8004
09:46:08.880   Training iter 550, batch loss 0.2555, batch acc 0.7990
09:46:09.362   Training iter 600, batch loss 0.2528, batch acc 0.8042
09:46:09.364 Training @ 149 epoch...
09:46:09.847   Training iter 50, batch loss 0.2525, batch acc 0.8080
09:46:10.355   Training iter 100, batch loss 0.2535, batch acc 0.8034
09:46:10.841   Training iter 150, batch loss 0.2546, batch acc 0.7984
09:46:11.328   Training iter 200, batch loss 0.2526, batch acc 0.8096
09:46:11.833   Training iter 250, batch loss 0.2538, batch acc 0.8070
09:46:12.352   Training iter 300, batch loss 0.2564, batch acc 0.7978
09:46:12.893   Training iter 350, batch loss 0.2513, batch acc 0.8100
09:46:13.393   Training iter 400, batch loss 0.2531, batch acc 0.7996
09:46:13.910   Training iter 450, batch loss 0.2545, batch acc 0.8026
09:46:14.465   Training iter 500, batch loss 0.2519, batch acc 0.8106
09:46:14.988   Training iter 550, batch loss 0.2542, batch acc 0.7938
09:46:15.539   Training iter 600, batch loss 0.2532, batch acc 0.8032
09:46:15.541 Training @ 150 epoch...
09:46:16.108   Training iter 50, batch loss 0.2505, batch acc 0.8102
09:46:16.660   Training iter 100, batch loss 0.2554, batch acc 0.7946
09:46:17.202   Training iter 150, batch loss 0.2524, batch acc 0.8076
09:46:17.748   Training iter 200, batch loss 0.2549, batch acc 0.8018
09:46:18.253   Training iter 250, batch loss 0.2546, batch acc 0.8002
09:46:18.738   Training iter 300, batch loss 0.2545, batch acc 0.8028
09:46:19.196   Training iter 350, batch loss 0.2515, batch acc 0.8134
09:46:19.666   Training iter 400, batch loss 0.2541, batch acc 0.8026
09:46:20.163   Training iter 450, batch loss 0.2542, batch acc 0.8008
09:46:20.679   Training iter 500, batch loss 0.2527, batch acc 0.8066
09:46:21.199   Training iter 550, batch loss 0.2531, batch acc 0.7980
09:46:21.719   Training iter 600, batch loss 0.2508, batch acc 0.8088
09:46:21.721 Testing @ 150 epoch...
09:46:21.759     Testing, total mean loss 0.24969, total acc 0.81470
09:46:21.759 Training @ 151 epoch...
09:46:22.272   Training iter 50, batch loss 0.2529, batch acc 0.8022
09:46:22.781   Training iter 100, batch loss 0.2543, batch acc 0.8034
09:46:23.281   Training iter 150, batch loss 0.2523, batch acc 0.8116
09:46:23.764   Training iter 200, batch loss 0.2521, batch acc 0.8110
09:46:24.238   Training iter 250, batch loss 0.2534, batch acc 0.7942
09:46:24.718   Training iter 300, batch loss 0.2525, batch acc 0.8032
09:46:25.306   Training iter 350, batch loss 0.2512, batch acc 0.8048
09:46:25.817   Training iter 400, batch loss 0.2542, batch acc 0.7996
09:46:26.330   Training iter 450, batch loss 0.2563, batch acc 0.7944
09:46:26.839   Training iter 500, batch loss 0.2533, batch acc 0.8112
09:46:27.356   Training iter 550, batch loss 0.2517, batch acc 0.8086
09:46:27.895   Training iter 600, batch loss 0.2517, batch acc 0.8060
09:46:27.896 Training @ 152 epoch...
09:46:28.429   Training iter 50, batch loss 0.2507, batch acc 0.8054
09:46:28.930   Training iter 100, batch loss 0.2485, batch acc 0.8114
09:46:29.442   Training iter 150, batch loss 0.2563, batch acc 0.7986
09:46:29.969   Training iter 200, batch loss 0.2542, batch acc 0.8018
09:46:30.507   Training iter 250, batch loss 0.2527, batch acc 0.8014
09:46:31.040   Training iter 300, batch loss 0.2561, batch acc 0.8000
09:46:31.571   Training iter 350, batch loss 0.2501, batch acc 0.8092
09:46:32.091   Training iter 400, batch loss 0.2538, batch acc 0.8012
09:46:32.621   Training iter 450, batch loss 0.2496, batch acc 0.8092
09:46:33.134   Training iter 500, batch loss 0.2537, batch acc 0.7992
09:46:33.639   Training iter 550, batch loss 0.2535, batch acc 0.8036
09:46:34.110   Training iter 600, batch loss 0.2539, batch acc 0.8094
09:46:34.112 Training @ 153 epoch...
09:46:34.585   Training iter 50, batch loss 0.2523, batch acc 0.8028
09:46:35.066   Training iter 100, batch loss 0.2536, batch acc 0.8012
09:46:35.526   Training iter 150, batch loss 0.2509, batch acc 0.8116
09:46:35.979   Training iter 200, batch loss 0.2544, batch acc 0.8074
09:46:36.445   Training iter 250, batch loss 0.2532, batch acc 0.8028
09:46:36.906   Training iter 300, batch loss 0.2516, batch acc 0.8072
09:46:37.374   Training iter 350, batch loss 0.2525, batch acc 0.8020
09:46:37.844   Training iter 400, batch loss 0.2491, batch acc 0.8090
09:46:38.339   Training iter 450, batch loss 0.2529, batch acc 0.8028
09:46:38.818   Training iter 500, batch loss 0.2549, batch acc 0.7992
09:46:39.296   Training iter 550, batch loss 0.2503, batch acc 0.8134
09:46:39.771   Training iter 600, batch loss 0.2547, batch acc 0.7946
09:46:39.773 Training @ 154 epoch...
09:46:40.285   Training iter 50, batch loss 0.2537, batch acc 0.8042
09:46:40.786   Training iter 100, batch loss 0.2521, batch acc 0.8098
09:46:41.280   Training iter 150, batch loss 0.2532, batch acc 0.8062
09:46:41.779   Training iter 200, batch loss 0.2533, batch acc 0.7980
09:46:42.268   Training iter 250, batch loss 0.2518, batch acc 0.8122
09:46:42.777   Training iter 300, batch loss 0.2553, batch acc 0.7986
09:46:43.351   Training iter 350, batch loss 0.2487, batch acc 0.8034
09:46:43.927   Training iter 400, batch loss 0.2526, batch acc 0.8080
09:46:44.496   Training iter 450, batch loss 0.2522, batch acc 0.7980
09:46:44.976   Training iter 500, batch loss 0.2504, batch acc 0.8088
09:46:45.480   Training iter 550, batch loss 0.2519, batch acc 0.8012
09:46:45.968   Training iter 600, batch loss 0.2527, batch acc 0.8060
09:46:45.970 Training @ 155 epoch...
09:46:46.498   Training iter 50, batch loss 0.2521, batch acc 0.7976
09:46:47.020   Training iter 100, batch loss 0.2523, batch acc 0.8052
09:46:47.501   Training iter 150, batch loss 0.2522, batch acc 0.8066
09:46:47.999   Training iter 200, batch loss 0.2497, batch acc 0.8156
09:46:48.479   Training iter 250, batch loss 0.2534, batch acc 0.8000
09:46:49.007   Training iter 300, batch loss 0.2520, batch acc 0.8098
09:46:49.528   Training iter 350, batch loss 0.2504, batch acc 0.8060
09:46:50.041   Training iter 400, batch loss 0.2523, batch acc 0.8028
09:46:50.545   Training iter 450, batch loss 0.2539, batch acc 0.7944
09:46:51.041   Training iter 500, batch loss 0.2507, batch acc 0.8052
09:46:51.529   Training iter 550, batch loss 0.2524, batch acc 0.8068
09:46:52.028   Training iter 600, batch loss 0.2536, batch acc 0.8056
09:46:52.030 Testing @ 155 epoch...
09:46:52.066     Testing, total mean loss 0.24855, total acc 0.81590
09:46:52.066 Training @ 156 epoch...
09:46:52.543   Training iter 50, batch loss 0.2544, batch acc 0.8026
09:46:53.024   Training iter 100, batch loss 0.2511, batch acc 0.8052
09:46:53.505   Training iter 150, batch loss 0.2511, batch acc 0.8062
09:46:54.005   Training iter 200, batch loss 0.2550, batch acc 0.7918
09:46:54.508   Training iter 250, batch loss 0.2511, batch acc 0.8130
09:46:55.029   Training iter 300, batch loss 0.2499, batch acc 0.8062
09:46:55.521   Training iter 350, batch loss 0.2481, batch acc 0.8160
09:46:55.972   Training iter 400, batch loss 0.2517, batch acc 0.8012
09:46:56.440   Training iter 450, batch loss 0.2532, batch acc 0.8032
09:46:56.927   Training iter 500, batch loss 0.2526, batch acc 0.8048
09:46:57.463   Training iter 550, batch loss 0.2501, batch acc 0.8072
09:46:57.985   Training iter 600, batch loss 0.2542, batch acc 0.8042
09:46:57.987 Training @ 157 epoch...
09:46:58.470   Training iter 50, batch loss 0.2498, batch acc 0.8158
09:46:58.979   Training iter 100, batch loss 0.2525, batch acc 0.8096
09:46:59.480   Training iter 150, batch loss 0.2519, batch acc 0.8004
09:47:00.005   Training iter 200, batch loss 0.2518, batch acc 0.7992
09:47:00.559   Training iter 250, batch loss 0.2533, batch acc 0.8076
09:47:01.161   Training iter 300, batch loss 0.2514, batch acc 0.8052
09:47:01.759   Training iter 350, batch loss 0.2532, batch acc 0.8040
09:47:02.339   Training iter 400, batch loss 0.2512, batch acc 0.7998
09:47:02.869   Training iter 450, batch loss 0.2531, batch acc 0.8056
09:47:03.380   Training iter 500, batch loss 0.2511, batch acc 0.8114
09:47:03.847   Training iter 550, batch loss 0.2519, batch acc 0.7984
09:47:04.325   Training iter 600, batch loss 0.2486, batch acc 0.8070
09:47:04.327 Training @ 158 epoch...
09:47:04.820   Training iter 50, batch loss 0.2510, batch acc 0.8028
09:47:05.297   Training iter 100, batch loss 0.2512, batch acc 0.8120
09:47:05.770   Training iter 150, batch loss 0.2512, batch acc 0.8034
09:47:06.255   Training iter 200, batch loss 0.2509, batch acc 0.8048
09:47:06.748   Training iter 250, batch loss 0.2513, batch acc 0.8116
09:47:07.226   Training iter 300, batch loss 0.2501, batch acc 0.8058
09:47:07.707   Training iter 350, batch loss 0.2504, batch acc 0.8048
09:47:08.216   Training iter 400, batch loss 0.2531, batch acc 0.8028
09:47:08.723   Training iter 450, batch loss 0.2534, batch acc 0.8034
09:47:09.192   Training iter 500, batch loss 0.2533, batch acc 0.7962
09:47:09.683   Training iter 550, batch loss 0.2504, batch acc 0.8048
09:47:10.199   Training iter 600, batch loss 0.2510, batch acc 0.8132
09:47:10.201 Training @ 159 epoch...
09:47:10.700   Training iter 50, batch loss 0.2501, batch acc 0.8064
09:47:11.193   Training iter 100, batch loss 0.2519, batch acc 0.8038
09:47:11.707   Training iter 150, batch loss 0.2498, batch acc 0.8104
09:47:12.202   Training iter 200, batch loss 0.2508, batch acc 0.8102
09:47:12.727   Training iter 250, batch loss 0.2516, batch acc 0.8076
09:47:13.250   Training iter 300, batch loss 0.2507, batch acc 0.8054
09:47:13.768   Training iter 350, batch loss 0.2525, batch acc 0.8022
09:47:14.274   Training iter 400, batch loss 0.2523, batch acc 0.8058
09:47:14.801   Training iter 450, batch loss 0.2548, batch acc 0.7962
09:47:15.335   Training iter 500, batch loss 0.2500, batch acc 0.8168
09:47:15.848   Training iter 550, batch loss 0.2500, batch acc 0.8012
09:47:16.343   Training iter 600, batch loss 0.2503, batch acc 0.8062
09:47:16.345 Training @ 160 epoch...
09:47:16.843   Training iter 50, batch loss 0.2484, batch acc 0.8096
09:47:17.339   Training iter 100, batch loss 0.2525, batch acc 0.8082
09:47:17.860   Training iter 150, batch loss 0.2530, batch acc 0.7998
09:47:18.412   Training iter 200, batch loss 0.2513, batch acc 0.8084
09:47:18.937   Training iter 250, batch loss 0.2509, batch acc 0.8054
09:47:19.454   Training iter 300, batch loss 0.2501, batch acc 0.8042
09:47:19.995   Training iter 350, batch loss 0.2514, batch acc 0.8054
09:47:20.573   Training iter 400, batch loss 0.2525, batch acc 0.8016
09:47:21.101   Training iter 450, batch loss 0.2516, batch acc 0.7992
09:47:21.601   Training iter 500, batch loss 0.2491, batch acc 0.8130
09:47:22.122   Training iter 550, batch loss 0.2484, batch acc 0.8114
09:47:22.660   Training iter 600, batch loss 0.2531, batch acc 0.8070
09:47:22.661 Testing @ 160 epoch...
09:47:22.697     Testing, total mean loss 0.24748, total acc 0.81690
09:47:22.697 Training @ 161 epoch...
09:47:23.224   Training iter 50, batch loss 0.2490, batch acc 0.8120
09:47:23.736   Training iter 100, batch loss 0.2537, batch acc 0.7916
09:47:24.249   Training iter 150, batch loss 0.2497, batch acc 0.8096
09:47:24.769   Training iter 200, batch loss 0.2507, batch acc 0.8048
09:47:25.289   Training iter 250, batch loss 0.2511, batch acc 0.8022
09:47:25.821   Training iter 300, batch loss 0.2517, batch acc 0.8008
09:47:26.327   Training iter 350, batch loss 0.2497, batch acc 0.8114
09:47:26.842   Training iter 400, batch loss 0.2522, batch acc 0.8076
09:47:27.367   Training iter 450, batch loss 0.2531, batch acc 0.8008
09:47:27.910   Training iter 500, batch loss 0.2507, batch acc 0.8110
09:47:28.442   Training iter 550, batch loss 0.2495, batch acc 0.8072
09:47:28.983   Training iter 600, batch loss 0.2488, batch acc 0.8150
09:47:28.985 Training @ 162 epoch...
09:47:29.521   Training iter 50, batch loss 0.2521, batch acc 0.8050
09:47:30.066   Training iter 100, batch loss 0.2516, batch acc 0.8022
09:47:30.607   Training iter 150, batch loss 0.2509, batch acc 0.8072
09:47:31.144   Training iter 200, batch loss 0.2477, batch acc 0.8152
09:47:31.691   Training iter 250, batch loss 0.2501, batch acc 0.8074
09:47:32.246   Training iter 300, batch loss 0.2471, batch acc 0.8142
09:47:32.813   Training iter 350, batch loss 0.2509, batch acc 0.8046
09:47:33.374   Training iter 400, batch loss 0.2520, batch acc 0.8054
09:47:33.931   Training iter 450, batch loss 0.2475, batch acc 0.8098
09:47:34.466   Training iter 500, batch loss 0.2522, batch acc 0.8006
09:47:34.995   Training iter 550, batch loss 0.2535, batch acc 0.8000
09:47:35.508   Training iter 600, batch loss 0.2521, batch acc 0.8048
09:47:35.509 Training @ 163 epoch...
09:47:36.017   Training iter 50, batch loss 0.2507, batch acc 0.8090
09:47:36.520   Training iter 100, batch loss 0.2517, batch acc 0.8088
09:47:37.026   Training iter 150, batch loss 0.2502, batch acc 0.8038
09:47:37.537   Training iter 200, batch loss 0.2490, batch acc 0.8130
09:47:38.053   Training iter 250, batch loss 0.2511, batch acc 0.8042
09:47:38.557   Training iter 300, batch loss 0.2517, batch acc 0.8092
09:47:39.064   Training iter 350, batch loss 0.2506, batch acc 0.8026
09:47:39.562   Training iter 400, batch loss 0.2521, batch acc 0.8000
09:47:40.066   Training iter 450, batch loss 0.2505, batch acc 0.7986
09:47:40.630   Training iter 500, batch loss 0.2473, batch acc 0.8172
09:47:41.197   Training iter 550, batch loss 0.2498, batch acc 0.8076
09:47:41.767   Training iter 600, batch loss 0.2506, batch acc 0.8076
09:47:41.769 Training @ 164 epoch...
09:47:42.285   Training iter 50, batch loss 0.2485, batch acc 0.8062
09:47:42.796   Training iter 100, batch loss 0.2490, batch acc 0.8124
09:47:43.310   Training iter 150, batch loss 0.2505, batch acc 0.8094
09:47:43.993   Training iter 200, batch loss 0.2481, batch acc 0.8076
09:47:44.521   Training iter 250, batch loss 0.2535, batch acc 0.7872
09:47:45.012   Training iter 300, batch loss 0.2482, batch acc 0.8148
09:47:45.505   Training iter 350, batch loss 0.2507, batch acc 0.8078
09:47:45.993   Training iter 400, batch loss 0.2525, batch acc 0.8066
09:47:46.487   Training iter 450, batch loss 0.2519, batch acc 0.8092
09:47:46.984   Training iter 500, batch loss 0.2499, batch acc 0.8050
09:47:47.477   Training iter 550, batch loss 0.2489, batch acc 0.8122
09:47:47.975   Training iter 600, batch loss 0.2511, batch acc 0.8060
09:47:47.977 Training @ 165 epoch...
09:47:48.485   Training iter 50, batch loss 0.2510, batch acc 0.8080
09:47:48.986   Training iter 100, batch loss 0.2472, batch acc 0.8144
09:47:49.507   Training iter 150, batch loss 0.2515, batch acc 0.8086
09:47:50.036   Training iter 200, batch loss 0.2514, batch acc 0.8014
09:47:50.561   Training iter 250, batch loss 0.2498, batch acc 0.8050
09:47:51.079   Training iter 300, batch loss 0.2495, batch acc 0.8134
09:47:51.592   Training iter 350, batch loss 0.2492, batch acc 0.8046
09:47:52.114   Training iter 400, batch loss 0.2511, batch acc 0.8086
09:47:52.641   Training iter 450, batch loss 0.2483, batch acc 0.8116
09:47:53.168   Training iter 500, batch loss 0.2526, batch acc 0.8018
09:47:53.714   Training iter 550, batch loss 0.2494, batch acc 0.8082
09:47:54.245   Training iter 600, batch loss 0.2495, batch acc 0.8012
09:47:54.247 Testing @ 165 epoch...
09:47:54.283     Testing, total mean loss 0.24649, total acc 0.81760
09:47:54.283 Training @ 166 epoch...
09:47:54.809   Training iter 50, batch loss 0.2478, batch acc 0.8164
09:47:55.325   Training iter 100, batch loss 0.2538, batch acc 0.7980
09:47:55.836   Training iter 150, batch loss 0.2499, batch acc 0.8026
09:47:56.355   Training iter 200, batch loss 0.2514, batch acc 0.8132
09:47:56.871   Training iter 250, batch loss 0.2490, batch acc 0.8060
09:47:57.399   Training iter 300, batch loss 0.2503, batch acc 0.8018
09:47:57.923   Training iter 350, batch loss 0.2510, batch acc 0.8092
09:47:58.444   Training iter 400, batch loss 0.2476, batch acc 0.8130
09:47:58.959   Training iter 450, batch loss 0.2489, batch acc 0.8122
09:47:59.481   Training iter 500, batch loss 0.2500, batch acc 0.8060
09:48:00.004   Training iter 550, batch loss 0.2479, batch acc 0.8122
09:48:00.552   Training iter 600, batch loss 0.2507, batch acc 0.8000
09:48:00.553 Training @ 167 epoch...
09:48:01.091   Training iter 50, batch loss 0.2523, batch acc 0.7988
09:48:01.627   Training iter 100, batch loss 0.2498, batch acc 0.8004
09:48:02.152   Training iter 150, batch loss 0.2502, batch acc 0.8038
09:48:02.706   Training iter 200, batch loss 0.2466, batch acc 0.8152
09:48:03.258   Training iter 250, batch loss 0.2472, batch acc 0.8190
09:48:03.785   Training iter 300, batch loss 0.2500, batch acc 0.8046
09:48:04.318   Training iter 350, batch loss 0.2527, batch acc 0.8034
09:48:04.852   Training iter 400, batch loss 0.2474, batch acc 0.8156
09:48:05.391   Training iter 450, batch loss 0.2513, batch acc 0.8008
09:48:05.938   Training iter 500, batch loss 0.2497, batch acc 0.8142
09:48:06.495   Training iter 550, batch loss 0.2503, batch acc 0.8072
09:48:07.042   Training iter 600, batch loss 0.2485, batch acc 0.8098
09:48:07.043 Training @ 168 epoch...
09:48:07.583   Training iter 50, batch loss 0.2504, batch acc 0.8088
09:48:08.120   Training iter 100, batch loss 0.2474, batch acc 0.8116
09:48:08.671   Training iter 150, batch loss 0.2497, batch acc 0.8108
09:48:09.205   Training iter 200, batch loss 0.2495, batch acc 0.7988
09:48:09.733   Training iter 250, batch loss 0.2503, batch acc 0.8080
09:48:10.270   Training iter 300, batch loss 0.2492, batch acc 0.8084
09:48:10.784   Training iter 350, batch loss 0.2496, batch acc 0.8088
09:48:11.299   Training iter 400, batch loss 0.2506, batch acc 0.8080
09:48:11.817   Training iter 450, batch loss 0.2486, batch acc 0.8168
09:48:12.357   Training iter 500, batch loss 0.2480, batch acc 0.8054
09:48:12.934   Training iter 550, batch loss 0.2487, batch acc 0.8098
09:48:13.508   Training iter 600, batch loss 0.2518, batch acc 0.7970
09:48:13.510 Training @ 169 epoch...
09:48:14.080   Training iter 50, batch loss 0.2495, batch acc 0.8030
09:48:14.640   Training iter 100, batch loss 0.2482, batch acc 0.8120
09:48:15.154   Training iter 150, batch loss 0.2506, batch acc 0.8016
09:48:15.666   Training iter 200, batch loss 0.2479, batch acc 0.8120
09:48:16.181   Training iter 250, batch loss 0.2459, batch acc 0.8192
09:48:16.713   Training iter 300, batch loss 0.2490, batch acc 0.8108
09:48:17.257   Training iter 350, batch loss 0.2500, batch acc 0.8026
09:48:17.785   Training iter 400, batch loss 0.2513, batch acc 0.8054
09:48:18.311   Training iter 450, batch loss 0.2526, batch acc 0.7938
09:48:18.805   Training iter 500, batch loss 0.2496, batch acc 0.8098
09:48:19.309   Training iter 550, batch loss 0.2496, batch acc 0.8094
09:48:19.815   Training iter 600, batch loss 0.2475, batch acc 0.8156
09:48:19.817 Training @ 170 epoch...
09:48:20.324   Training iter 50, batch loss 0.2520, batch acc 0.7972
09:48:20.821   Training iter 100, batch loss 0.2490, batch acc 0.8104
09:48:21.296   Training iter 150, batch loss 0.2489, batch acc 0.8132
09:48:21.763   Training iter 200, batch loss 0.2490, batch acc 0.8048
09:48:22.225   Training iter 250, batch loss 0.2491, batch acc 0.8052
09:48:22.684   Training iter 300, batch loss 0.2507, batch acc 0.8108
09:48:23.145   Training iter 350, batch loss 0.2471, batch acc 0.8108
09:48:23.604   Training iter 400, batch loss 0.2488, batch acc 0.8132
09:48:24.081   Training iter 450, batch loss 0.2468, batch acc 0.8102
09:48:24.577   Training iter 500, batch loss 0.2497, batch acc 0.8126
09:48:25.083   Training iter 550, batch loss 0.2470, batch acc 0.8130
09:48:25.589   Training iter 600, batch loss 0.2512, batch acc 0.7970
09:48:25.591 Testing @ 170 epoch...
09:48:25.627     Testing, total mean loss 0.24556, total acc 0.81820
09:48:25.627 Training @ 171 epoch...
09:48:26.113   Training iter 50, batch loss 0.2486, batch acc 0.8078
09:48:26.576   Training iter 100, batch loss 0.2479, batch acc 0.8100
09:48:27.044   Training iter 150, batch loss 0.2479, batch acc 0.8146
09:48:27.530   Training iter 200, batch loss 0.2502, batch acc 0.8080
09:48:28.025   Training iter 250, batch loss 0.2496, batch acc 0.8028
09:48:28.503   Training iter 300, batch loss 0.2497, batch acc 0.7982
09:48:28.992   Training iter 350, batch loss 0.2483, batch acc 0.8090
09:48:29.485   Training iter 400, batch loss 0.2476, batch acc 0.8120
09:48:29.973   Training iter 450, batch loss 0.2480, batch acc 0.8090
09:48:30.490   Training iter 500, batch loss 0.2509, batch acc 0.8068
09:48:31.021   Training iter 550, batch loss 0.2488, batch acc 0.8118
09:48:31.559   Training iter 600, batch loss 0.2498, batch acc 0.8108
09:48:31.561 Training @ 172 epoch...
09:48:32.087   Training iter 50, batch loss 0.2473, batch acc 0.8116
09:48:32.608   Training iter 100, batch loss 0.2476, batch acc 0.8096
09:48:33.127   Training iter 150, batch loss 0.2495, batch acc 0.8086
09:48:33.643   Training iter 200, batch loss 0.2520, batch acc 0.8034
09:48:34.157   Training iter 250, batch loss 0.2483, batch acc 0.8092
09:48:34.679   Training iter 300, batch loss 0.2478, batch acc 0.8116
09:48:35.184   Training iter 350, batch loss 0.2486, batch acc 0.8158
09:48:35.692   Training iter 400, batch loss 0.2451, batch acc 0.8208
09:48:36.199   Training iter 450, batch loss 0.2491, batch acc 0.8014
09:48:36.694   Training iter 500, batch loss 0.2488, batch acc 0.8052
09:48:37.205   Training iter 550, batch loss 0.2486, batch acc 0.8094
09:48:37.713   Training iter 600, batch loss 0.2525, batch acc 0.7958
09:48:37.715 Training @ 173 epoch...
09:48:38.234   Training iter 50, batch loss 0.2486, batch acc 0.8098
09:48:38.730   Training iter 100, batch loss 0.2523, batch acc 0.7996
09:48:39.234   Training iter 150, batch loss 0.2497, batch acc 0.8048
09:48:39.734   Training iter 200, batch loss 0.2504, batch acc 0.8056
09:48:40.247   Training iter 250, batch loss 0.2485, batch acc 0.8090
09:48:40.705   Training iter 300, batch loss 0.2492, batch acc 0.8100
09:48:41.149   Training iter 350, batch loss 0.2477, batch acc 0.8088
09:48:41.605   Training iter 400, batch loss 0.2468, batch acc 0.8130
09:48:42.142   Training iter 450, batch loss 0.2494, batch acc 0.8088
09:48:42.690   Training iter 500, batch loss 0.2461, batch acc 0.8152
09:48:43.208   Training iter 550, batch loss 0.2480, batch acc 0.8114
09:48:43.714   Training iter 600, batch loss 0.2465, batch acc 0.8108
09:48:43.716 Training @ 174 epoch...
09:48:44.230   Training iter 50, batch loss 0.2493, batch acc 0.8148
09:48:44.755   Training iter 100, batch loss 0.2471, batch acc 0.8166
09:48:45.264   Training iter 150, batch loss 0.2505, batch acc 0.8044
09:48:45.729   Training iter 200, batch loss 0.2464, batch acc 0.8128
09:48:46.196   Training iter 250, batch loss 0.2482, batch acc 0.8138
09:48:46.662   Training iter 300, batch loss 0.2465, batch acc 0.8128
09:48:47.110   Training iter 350, batch loss 0.2495, batch acc 0.8032
09:48:47.568   Training iter 400, batch loss 0.2505, batch acc 0.8038
09:48:48.049   Training iter 450, batch loss 0.2480, batch acc 0.8030
09:48:48.519   Training iter 500, batch loss 0.2452, batch acc 0.8126
09:48:49.026   Training iter 550, batch loss 0.2499, batch acc 0.8076
09:48:49.586   Training iter 600, batch loss 0.2501, batch acc 0.8014
09:48:49.588 Training @ 175 epoch...
09:48:50.154   Training iter 50, batch loss 0.2520, batch acc 0.8012
09:48:50.726   Training iter 100, batch loss 0.2491, batch acc 0.8042
09:48:51.265   Training iter 150, batch loss 0.2500, batch acc 0.7968
09:48:51.798   Training iter 200, batch loss 0.2482, batch acc 0.8150
09:48:52.335   Training iter 250, batch loss 0.2474, batch acc 0.8110
09:48:52.868   Training iter 300, batch loss 0.2481, batch acc 0.8086
09:48:53.409   Training iter 350, batch loss 0.2477, batch acc 0.8124
09:48:53.937   Training iter 400, batch loss 0.2477, batch acc 0.8184
09:48:54.479   Training iter 450, batch loss 0.2478, batch acc 0.8088
09:48:54.993   Training iter 500, batch loss 0.2462, batch acc 0.8192
09:48:55.507   Training iter 550, batch loss 0.2464, batch acc 0.8096
09:48:56.007   Training iter 600, batch loss 0.2485, batch acc 0.8042
09:48:56.009 Testing @ 175 epoch...
09:48:56.046     Testing, total mean loss 0.24470, total acc 0.81870
09:48:56.046 Training @ 176 epoch...
09:48:56.559   Training iter 50, batch loss 0.2470, batch acc 0.8148
09:48:57.075   Training iter 100, batch loss 0.2469, batch acc 0.8168
09:48:57.586   Training iter 150, batch loss 0.2472, batch acc 0.8112
09:48:58.103   Training iter 200, batch loss 0.2494, batch acc 0.8078
09:48:58.594   Training iter 250, batch loss 0.2501, batch acc 0.8094
09:48:59.073   Training iter 300, batch loss 0.2465, batch acc 0.8090
09:48:59.559   Training iter 350, batch loss 0.2492, batch acc 0.8060
09:49:00.062   Training iter 400, batch loss 0.2501, batch acc 0.8056
09:49:00.580   Training iter 450, batch loss 0.2472, batch acc 0.8092
09:49:01.091   Training iter 500, batch loss 0.2468, batch acc 0.8070
09:49:01.608   Training iter 550, batch loss 0.2479, batch acc 0.8064
09:49:02.155   Training iter 600, batch loss 0.2488, batch acc 0.8056
09:49:02.157 Training @ 177 epoch...
09:49:02.662   Training iter 50, batch loss 0.2490, batch acc 0.7982
09:49:03.153   Training iter 100, batch loss 0.2456, batch acc 0.8230
09:49:03.652   Training iter 150, batch loss 0.2459, batch acc 0.8094
09:49:04.163   Training iter 200, batch loss 0.2482, batch acc 0.8088
09:49:04.692   Training iter 250, batch loss 0.2488, batch acc 0.8134
09:49:05.237   Training iter 300, batch loss 0.2505, batch acc 0.8076
09:49:05.769   Training iter 350, batch loss 0.2480, batch acc 0.7998
09:49:06.302   Training iter 400, batch loss 0.2485, batch acc 0.8090
09:49:06.862   Training iter 450, batch loss 0.2460, batch acc 0.8156
09:49:07.387   Training iter 500, batch loss 0.2495, batch acc 0.8022
09:49:07.902   Training iter 550, batch loss 0.2500, batch acc 0.8040
09:49:08.421   Training iter 600, batch loss 0.2451, batch acc 0.8216
09:49:08.423 Training @ 178 epoch...
09:49:08.929   Training iter 50, batch loss 0.2476, batch acc 0.8114
09:49:09.432   Training iter 100, batch loss 0.2458, batch acc 0.8090
09:49:09.954   Training iter 150, batch loss 0.2483, batch acc 0.8120
09:49:10.492   Training iter 200, batch loss 0.2477, batch acc 0.8090
09:49:10.954   Training iter 250, batch loss 0.2487, batch acc 0.8072
09:49:11.405   Training iter 300, batch loss 0.2470, batch acc 0.8094
09:49:11.852   Training iter 350, batch loss 0.2477, batch acc 0.8118
09:49:12.368   Training iter 400, batch loss 0.2461, batch acc 0.8094
09:49:12.922   Training iter 450, batch loss 0.2460, batch acc 0.8182
09:49:13.482   Training iter 500, batch loss 0.2512, batch acc 0.8102
09:49:14.031   Training iter 550, batch loss 0.2487, batch acc 0.7994
09:49:14.521   Training iter 600, batch loss 0.2485, batch acc 0.8068
09:49:14.523 Training @ 179 epoch...
09:49:15.006   Training iter 50, batch loss 0.2486, batch acc 0.8042
09:49:15.496   Training iter 100, batch loss 0.2472, batch acc 0.8124
09:49:15.968   Training iter 150, batch loss 0.2497, batch acc 0.8048
09:49:16.444   Training iter 200, batch loss 0.2470, batch acc 0.8066
09:49:16.913   Training iter 250, batch loss 0.2487, batch acc 0.8112
09:49:17.378   Training iter 300, batch loss 0.2475, batch acc 0.8142
09:49:17.860   Training iter 350, batch loss 0.2445, batch acc 0.8172
09:49:18.325   Training iter 400, batch loss 0.2470, batch acc 0.8052
09:49:18.763   Training iter 450, batch loss 0.2508, batch acc 0.8036
09:49:19.207   Training iter 500, batch loss 0.2471, batch acc 0.8134
09:49:19.683   Training iter 550, batch loss 0.2447, batch acc 0.8142
09:49:20.179   Training iter 600, batch loss 0.2486, batch acc 0.8112
09:49:20.181 Training @ 180 epoch...
09:49:20.691   Training iter 50, batch loss 0.2507, batch acc 0.8108
09:49:21.206   Training iter 100, batch loss 0.2469, batch acc 0.8144
09:49:21.695   Training iter 150, batch loss 0.2482, batch acc 0.8018
09:49:22.193   Training iter 200, batch loss 0.2480, batch acc 0.8144
09:49:22.718   Training iter 250, batch loss 0.2444, batch acc 0.8218
09:49:23.260   Training iter 300, batch loss 0.2494, batch acc 0.8036
09:49:23.800   Training iter 350, batch loss 0.2478, batch acc 0.8046
09:49:24.338   Training iter 400, batch loss 0.2464, batch acc 0.8060
09:49:24.890   Training iter 450, batch loss 0.2481, batch acc 0.8070
09:49:25.458   Training iter 500, batch loss 0.2459, batch acc 0.8166
09:49:25.996   Training iter 550, batch loss 0.2461, batch acc 0.8092
09:49:26.535   Training iter 600, batch loss 0.2475, batch acc 0.8098
09:49:26.537 Testing @ 180 epoch...
09:49:26.573     Testing, total mean loss 0.24388, total acc 0.81950
09:49:26.573 Training @ 181 epoch...
09:49:27.094   Training iter 50, batch loss 0.2511, batch acc 0.7970
09:49:27.599   Training iter 100, batch loss 0.2477, batch acc 0.8086
09:49:28.111   Training iter 150, batch loss 0.2468, batch acc 0.8114
09:49:28.644   Training iter 200, batch loss 0.2475, batch acc 0.8128
09:49:29.174   Training iter 250, batch loss 0.2460, batch acc 0.8104
09:49:29.714   Training iter 300, batch loss 0.2464, batch acc 0.8074
09:49:30.259   Training iter 350, batch loss 0.2464, batch acc 0.8172
09:49:30.816   Training iter 400, batch loss 0.2476, batch acc 0.8086
09:49:31.347   Training iter 450, batch loss 0.2474, batch acc 0.8148
09:49:31.886   Training iter 500, batch loss 0.2476, batch acc 0.8082
09:49:32.414   Training iter 550, batch loss 0.2461, batch acc 0.8142
09:49:32.940   Training iter 600, batch loss 0.2470, batch acc 0.8110
09:49:32.942 Training @ 182 epoch...
09:49:33.487   Training iter 50, batch loss 0.2489, batch acc 0.8052
09:49:34.005   Training iter 100, batch loss 0.2453, batch acc 0.8160
09:49:34.489   Training iter 150, batch loss 0.2467, batch acc 0.8020
09:49:34.993   Training iter 200, batch loss 0.2457, batch acc 0.8172
09:49:35.483   Training iter 250, batch loss 0.2489, batch acc 0.8058
09:49:35.955   Training iter 300, batch loss 0.2472, batch acc 0.8096
09:49:36.428   Training iter 350, batch loss 0.2481, batch acc 0.8060
09:49:36.900   Training iter 400, batch loss 0.2465, batch acc 0.8170
09:49:37.373   Training iter 450, batch loss 0.2467, batch acc 0.8022
09:49:37.865   Training iter 500, batch loss 0.2465, batch acc 0.8184
09:49:38.354   Training iter 550, batch loss 0.2477, batch acc 0.8038
09:49:38.855   Training iter 600, batch loss 0.2474, batch acc 0.8174
09:49:38.856 Training @ 183 epoch...
09:49:39.321   Training iter 50, batch loss 0.2482, batch acc 0.7998
09:49:39.787   Training iter 100, batch loss 0.2450, batch acc 0.8088
09:49:40.256   Training iter 150, batch loss 0.2467, batch acc 0.8112
09:49:40.717   Training iter 200, batch loss 0.2491, batch acc 0.8084
09:49:41.176   Training iter 250, batch loss 0.2471, batch acc 0.8212
09:49:41.633   Training iter 300, batch loss 0.2487, batch acc 0.8106
09:49:42.092   Training iter 350, batch loss 0.2439, batch acc 0.8192
09:49:42.570   Training iter 400, batch loss 0.2467, batch acc 0.8140
09:49:43.031   Training iter 450, batch loss 0.2465, batch acc 0.8104
09:49:43.496   Training iter 500, batch loss 0.2465, batch acc 0.8066
09:49:43.970   Training iter 550, batch loss 0.2470, batch acc 0.8136
09:49:44.478   Training iter 600, batch loss 0.2487, batch acc 0.7992
09:49:44.480 Training @ 184 epoch...
09:49:44.989   Training iter 50, batch loss 0.2501, batch acc 0.8040
09:49:45.470   Training iter 100, batch loss 0.2468, batch acc 0.8110
09:49:45.929   Training iter 150, batch loss 0.2423, batch acc 0.8274
09:49:46.379   Training iter 200, batch loss 0.2494, batch acc 0.8016
09:49:46.830   Training iter 250, batch loss 0.2461, batch acc 0.8052
09:49:47.287   Training iter 300, batch loss 0.2457, batch acc 0.8130
09:49:47.748   Training iter 350, batch loss 0.2473, batch acc 0.8114
09:49:48.241   Training iter 400, batch loss 0.2439, batch acc 0.8178
09:49:48.743   Training iter 450, batch loss 0.2461, batch acc 0.8088
09:49:49.258   Training iter 500, batch loss 0.2492, batch acc 0.8040
09:49:49.778   Training iter 550, batch loss 0.2489, batch acc 0.8088
09:49:50.360   Training iter 600, batch loss 0.2463, batch acc 0.8120
09:49:50.362 Training @ 185 epoch...
09:49:50.890   Training iter 50, batch loss 0.2449, batch acc 0.8142
09:49:51.395   Training iter 100, batch loss 0.2489, batch acc 0.8052
09:49:51.900   Training iter 150, batch loss 0.2490, batch acc 0.8058
09:49:52.421   Training iter 200, batch loss 0.2471, batch acc 0.8118
09:49:52.958   Training iter 250, batch loss 0.2448, batch acc 0.8162
09:49:53.528   Training iter 300, batch loss 0.2472, batch acc 0.8082
09:49:54.064   Training iter 350, batch loss 0.2469, batch acc 0.8126
09:49:54.594   Training iter 400, batch loss 0.2452, batch acc 0.8130
09:49:55.132   Training iter 450, batch loss 0.2465, batch acc 0.8176
09:49:55.689   Training iter 500, batch loss 0.2442, batch acc 0.8166
09:49:56.201   Training iter 550, batch loss 0.2483, batch acc 0.8008
09:49:56.691   Training iter 600, batch loss 0.2474, batch acc 0.8056
09:49:56.693 Testing @ 185 epoch...
09:49:56.729     Testing, total mean loss 0.24313, total acc 0.81970
09:49:56.729 Training @ 186 epoch...
09:49:57.226   Training iter 50, batch loss 0.2448, batch acc 0.8122
09:49:57.735   Training iter 100, batch loss 0.2463, batch acc 0.8090
09:49:58.252   Training iter 150, batch loss 0.2476, batch acc 0.8086
09:49:58.749   Training iter 200, batch loss 0.2432, batch acc 0.8214
09:49:59.240   Training iter 250, batch loss 0.2474, batch acc 0.8086
09:49:59.742   Training iter 300, batch loss 0.2456, batch acc 0.8162
09:50:00.274   Training iter 350, batch loss 0.2474, batch acc 0.8066
09:50:00.805   Training iter 400, batch loss 0.2472, batch acc 0.8080
09:50:01.348   Training iter 450, batch loss 0.2488, batch acc 0.8092
09:50:01.912   Training iter 500, batch loss 0.2464, batch acc 0.8126
09:50:02.485   Training iter 550, batch loss 0.2469, batch acc 0.8060
09:50:03.038   Training iter 600, batch loss 0.2471, batch acc 0.8110
09:50:03.040 Training @ 187 epoch...
09:50:03.604   Training iter 50, batch loss 0.2477, batch acc 0.8142
09:50:04.165   Training iter 100, batch loss 0.2459, batch acc 0.8110
09:50:04.735   Training iter 150, batch loss 0.2456, batch acc 0.8090
09:50:05.304   Training iter 200, batch loss 0.2494, batch acc 0.8058
09:50:05.872   Training iter 250, batch loss 0.2461, batch acc 0.8126
09:50:06.440   Training iter 300, batch loss 0.2481, batch acc 0.8060
09:50:06.967   Training iter 350, batch loss 0.2474, batch acc 0.8140
09:50:07.469   Training iter 400, batch loss 0.2466, batch acc 0.8080
09:50:08.013   Training iter 450, batch loss 0.2456, batch acc 0.8114
09:50:08.577   Training iter 500, batch loss 0.2434, batch acc 0.8184
09:50:09.125   Training iter 550, batch loss 0.2470, batch acc 0.8036
09:50:09.694   Training iter 600, batch loss 0.2441, batch acc 0.8204
09:50:09.696 Training @ 188 epoch...
09:50:10.282   Training iter 50, batch loss 0.2464, batch acc 0.8144
09:50:10.868   Training iter 100, batch loss 0.2456, batch acc 0.8150
09:50:11.406   Training iter 150, batch loss 0.2439, batch acc 0.8210
09:50:11.930   Training iter 200, batch loss 0.2473, batch acc 0.8120
09:50:12.453   Training iter 250, batch loss 0.2494, batch acc 0.8006
09:50:12.956   Training iter 300, batch loss 0.2467, batch acc 0.8022
09:50:13.499   Training iter 350, batch loss 0.2458, batch acc 0.8124
09:50:14.024   Training iter 400, batch loss 0.2457, batch acc 0.8120
09:50:14.520   Training iter 450, batch loss 0.2452, batch acc 0.8152
09:50:15.003   Training iter 500, batch loss 0.2462, batch acc 0.8068
09:50:15.493   Training iter 550, batch loss 0.2477, batch acc 0.8034
09:50:15.969   Training iter 600, batch loss 0.2453, batch acc 0.8170
09:50:15.971 Training @ 189 epoch...
09:50:16.441   Training iter 50, batch loss 0.2454, batch acc 0.8162
09:50:16.902   Training iter 100, batch loss 0.2455, batch acc 0.8096
09:50:17.352   Training iter 150, batch loss 0.2454, batch acc 0.8112
09:50:17.817   Training iter 200, batch loss 0.2478, batch acc 0.8008
09:50:18.292   Training iter 250, batch loss 0.2473, batch acc 0.8088
09:50:18.760   Training iter 300, batch loss 0.2466, batch acc 0.8198
09:50:19.212   Training iter 350, batch loss 0.2420, batch acc 0.8218
09:50:19.660   Training iter 400, batch loss 0.2489, batch acc 0.8030
09:50:20.144   Training iter 450, batch loss 0.2465, batch acc 0.8044
09:50:20.632   Training iter 500, batch loss 0.2434, batch acc 0.8234
09:50:21.094   Training iter 550, batch loss 0.2461, batch acc 0.8108
09:50:21.587   Training iter 600, batch loss 0.2486, batch acc 0.8050
09:50:21.588 Training @ 190 epoch...
09:50:22.102   Training iter 50, batch loss 0.2445, batch acc 0.8164
09:50:22.625   Training iter 100, batch loss 0.2473, batch acc 0.8070
09:50:23.137   Training iter 150, batch loss 0.2445, batch acc 0.8146
09:50:23.645   Training iter 200, batch loss 0.2456, batch acc 0.8108
09:50:24.130   Training iter 250, batch loss 0.2487, batch acc 0.8022
09:50:24.656   Training iter 300, batch loss 0.2448, batch acc 0.8146
09:50:25.193   Training iter 350, batch loss 0.2458, batch acc 0.8144
09:50:25.731   Training iter 400, batch loss 0.2450, batch acc 0.8106
09:50:26.255   Training iter 450, batch loss 0.2456, batch acc 0.8136
09:50:26.773   Training iter 500, batch loss 0.2447, batch acc 0.8152
09:50:27.299   Training iter 550, batch loss 0.2480, batch acc 0.8086
09:50:27.832   Training iter 600, batch loss 0.2476, batch acc 0.8094
09:50:27.833 Testing @ 190 epoch...
09:50:27.870     Testing, total mean loss 0.24242, total acc 0.82050
09:50:27.870 Training @ 191 epoch...
09:50:28.404   Training iter 50, batch loss 0.2460, batch acc 0.8122
09:50:28.903   Training iter 100, batch loss 0.2457, batch acc 0.8112
09:50:29.423   Training iter 150, batch loss 0.2462, batch acc 0.8090
09:50:29.914   Training iter 200, batch loss 0.2444, batch acc 0.8180
09:50:30.440   Training iter 250, batch loss 0.2467, batch acc 0.8128
09:50:30.959   Training iter 300, batch loss 0.2438, batch acc 0.8130
09:50:31.518   Training iter 350, batch loss 0.2474, batch acc 0.8064
09:50:32.044   Training iter 400, batch loss 0.2467, batch acc 0.8070
09:50:32.530   Training iter 450, batch loss 0.2464, batch acc 0.8096
09:50:33.033   Training iter 500, batch loss 0.2457, batch acc 0.8166
09:50:33.518   Training iter 550, batch loss 0.2463, batch acc 0.8078
09:50:34.028   Training iter 600, batch loss 0.2451, batch acc 0.8154
09:50:34.030 Training @ 192 epoch...
09:50:34.526   Training iter 50, batch loss 0.2469, batch acc 0.8118
09:50:35.023   Training iter 100, batch loss 0.2450, batch acc 0.8086
09:50:35.509   Training iter 150, batch loss 0.2460, batch acc 0.8100
09:50:35.991   Training iter 200, batch loss 0.2441, batch acc 0.8162
09:50:36.492   Training iter 250, batch loss 0.2413, batch acc 0.8224
09:50:36.982   Training iter 300, batch loss 0.2469, batch acc 0.8068
09:50:37.476   Training iter 350, batch loss 0.2483, batch acc 0.8090
09:50:37.987   Training iter 400, batch loss 0.2485, batch acc 0.7990
09:50:38.507   Training iter 450, batch loss 0.2475, batch acc 0.8072
09:50:39.027   Training iter 500, batch loss 0.2466, batch acc 0.8146
09:50:39.554   Training iter 550, batch loss 0.2420, batch acc 0.8224
09:50:40.075   Training iter 600, batch loss 0.2457, batch acc 0.8128
09:50:40.076 Training @ 193 epoch...
09:50:40.612   Training iter 50, batch loss 0.2444, batch acc 0.8162
09:50:41.173   Training iter 100, batch loss 0.2453, batch acc 0.8114
09:50:41.713   Training iter 150, batch loss 0.2487, batch acc 0.8038
09:50:42.292   Training iter 200, batch loss 0.2451, batch acc 0.8120
09:50:42.857   Training iter 250, batch loss 0.2444, batch acc 0.8126
09:50:43.417   Training iter 300, batch loss 0.2458, batch acc 0.8166
09:50:43.961   Training iter 350, batch loss 0.2455, batch acc 0.8122
09:50:44.514   Training iter 400, batch loss 0.2450, batch acc 0.8112
09:50:45.065   Training iter 450, batch loss 0.2461, batch acc 0.8108
09:50:45.616   Training iter 500, batch loss 0.2443, batch acc 0.8166
09:50:46.163   Training iter 550, batch loss 0.2466, batch acc 0.8124
09:50:46.690   Training iter 600, batch loss 0.2458, batch acc 0.8048
09:50:46.692 Training @ 194 epoch...
09:50:47.206   Training iter 50, batch loss 0.2468, batch acc 0.8112
09:50:47.702   Training iter 100, batch loss 0.2471, batch acc 0.8126
09:50:48.210   Training iter 150, batch loss 0.2451, batch acc 0.8098
09:50:48.726   Training iter 200, batch loss 0.2481, batch acc 0.8042
09:50:49.246   Training iter 250, batch loss 0.2432, batch acc 0.8170
09:50:49.760   Training iter 300, batch loss 0.2455, batch acc 0.8148
09:50:50.270   Training iter 350, batch loss 0.2455, batch acc 0.8120
09:50:50.792   Training iter 400, batch loss 0.2456, batch acc 0.8062
09:50:51.327   Training iter 450, batch loss 0.2441, batch acc 0.8172
09:50:51.859   Training iter 500, batch loss 0.2447, batch acc 0.8164
09:50:52.366   Training iter 550, batch loss 0.2454, batch acc 0.8102
09:50:52.875   Training iter 600, batch loss 0.2443, batch acc 0.8112
09:50:52.877 Training @ 195 epoch...
09:50:53.401   Training iter 50, batch loss 0.2439, batch acc 0.8170
09:50:53.916   Training iter 100, batch loss 0.2463, batch acc 0.8046
09:50:54.431   Training iter 150, batch loss 0.2467, batch acc 0.8092
09:50:54.938   Training iter 200, batch loss 0.2441, batch acc 0.8132
09:50:55.462   Training iter 250, batch loss 0.2453, batch acc 0.8114
09:50:55.935   Training iter 300, batch loss 0.2426, batch acc 0.8240
09:50:56.423   Training iter 350, batch loss 0.2455, batch acc 0.8094
09:50:56.931   Training iter 400, batch loss 0.2455, batch acc 0.8136
09:50:57.458   Training iter 450, batch loss 0.2423, batch acc 0.8184
09:50:57.975   Training iter 500, batch loss 0.2493, batch acc 0.8004
09:50:58.498   Training iter 550, batch loss 0.2458, batch acc 0.8132
09:50:59.002   Training iter 600, batch loss 0.2465, batch acc 0.8088
09:50:59.004 Testing @ 195 epoch...
09:50:59.041     Testing, total mean loss 0.24175, total acc 0.82140
09:50:59.041 Training @ 196 epoch...
09:50:59.551   Training iter 50, batch loss 0.2462, batch acc 0.8128
09:51:00.066   Training iter 100, batch loss 0.2453, batch acc 0.8048
09:51:00.580   Training iter 150, batch loss 0.2433, batch acc 0.8178
09:51:01.084   Training iter 200, batch loss 0.2482, batch acc 0.8048
09:51:01.613   Training iter 250, batch loss 0.2459, batch acc 0.8150
09:51:02.161   Training iter 300, batch loss 0.2434, batch acc 0.8188
09:51:02.680   Training iter 350, batch loss 0.2437, batch acc 0.8148
09:51:03.195   Training iter 400, batch loss 0.2457, batch acc 0.8090
09:51:03.763   Training iter 450, batch loss 0.2461, batch acc 0.8056
09:51:04.331   Training iter 500, batch loss 0.2430, batch acc 0.8230
09:51:04.893   Training iter 550, batch loss 0.2449, batch acc 0.8108
09:51:05.390   Training iter 600, batch loss 0.2468, batch acc 0.8104
09:51:05.392 Training @ 197 epoch...
09:51:05.913   Training iter 50, batch loss 0.2463, batch acc 0.8086
09:51:06.453   Training iter 100, batch loss 0.2471, batch acc 0.8052
09:51:06.983   Training iter 150, batch loss 0.2457, batch acc 0.8082
09:51:07.500   Training iter 200, batch loss 0.2458, batch acc 0.8140
09:51:08.006   Training iter 250, batch loss 0.2471, batch acc 0.8140
09:51:08.489   Training iter 300, batch loss 0.2462, batch acc 0.8080
09:51:08.971   Training iter 350, batch loss 0.2436, batch acc 0.8138
09:51:09.495   Training iter 400, batch loss 0.2448, batch acc 0.8112
09:51:09.992   Training iter 450, batch loss 0.2438, batch acc 0.8162
09:51:10.638   Training iter 500, batch loss 0.2450, batch acc 0.8088
09:51:11.159   Training iter 550, batch loss 0.2430, batch acc 0.8200
09:51:11.688   Training iter 600, batch loss 0.2426, batch acc 0.8178
09:51:11.690 Training @ 198 epoch...
09:51:12.213   Training iter 50, batch loss 0.2481, batch acc 0.8018
09:51:12.752   Training iter 100, batch loss 0.2456, batch acc 0.8070
09:51:13.317   Training iter 150, batch loss 0.2447, batch acc 0.8090
09:51:13.888   Training iter 200, batch loss 0.2425, batch acc 0.8152
09:51:14.447   Training iter 250, batch loss 0.2437, batch acc 0.8152
09:51:14.994   Training iter 300, batch loss 0.2449, batch acc 0.8192
09:51:15.566   Training iter 350, batch loss 0.2454, batch acc 0.8162
09:51:16.128   Training iter 400, batch loss 0.2440, batch acc 0.8116
09:51:16.674   Training iter 450, batch loss 0.2443, batch acc 0.8120
09:51:17.212   Training iter 500, batch loss 0.2439, batch acc 0.8138
09:51:17.761   Training iter 550, batch loss 0.2479, batch acc 0.8128
09:51:18.315   Training iter 600, batch loss 0.2443, batch acc 0.8130
09:51:18.317 Training @ 199 epoch...
09:51:18.872   Training iter 50, batch loss 0.2429, batch acc 0.8174
09:51:19.393   Training iter 100, batch loss 0.2464, batch acc 0.8038
09:51:19.918   Training iter 150, batch loss 0.2445, batch acc 0.8182
09:51:20.448   Training iter 200, batch loss 0.2479, batch acc 0.8020
09:51:20.972   Training iter 250, batch loss 0.2432, batch acc 0.8218
09:51:21.490   Training iter 300, batch loss 0.2448, batch acc 0.8044
09:51:22.006   Training iter 350, batch loss 0.2464, batch acc 0.8098
09:51:22.529   Training iter 400, batch loss 0.2425, batch acc 0.8152
09:51:23.075   Training iter 450, batch loss 0.2450, batch acc 0.8098
09:51:23.621   Training iter 500, batch loss 0.2457, batch acc 0.8128
09:51:24.152   Training iter 550, batch loss 0.2458, batch acc 0.8098
09:51:24.672   Training iter 600, batch loss 0.2429, batch acc 0.8246
09:51:24.674 Training @ 200 epoch...
09:51:25.216   Training iter 50, batch loss 0.2470, batch acc 0.8064
09:51:25.762   Training iter 100, batch loss 0.2429, batch acc 0.8194
09:51:26.297   Training iter 150, batch loss 0.2458, batch acc 0.8124
09:51:26.828   Training iter 200, batch loss 0.2427, batch acc 0.8210
09:51:27.354   Training iter 250, batch loss 0.2440, batch acc 0.8192
09:51:27.905   Training iter 300, batch loss 0.2454, batch acc 0.8050
09:51:28.448   Training iter 350, batch loss 0.2456, batch acc 0.8074
09:51:28.965   Training iter 400, batch loss 0.2449, batch acc 0.8050
09:51:29.526   Training iter 450, batch loss 0.2428, batch acc 0.8168
09:51:30.105   Training iter 500, batch loss 0.2472, batch acc 0.8054
09:51:30.686   Training iter 550, batch loss 0.2419, batch acc 0.8216
09:51:31.265   Training iter 600, batch loss 0.2462, batch acc 0.8110
09:51:31.267 Testing @ 200 epoch...
09:51:31.306     Testing, total mean loss 0.24112, total acc 0.82240
09:51:31.306 Plot @ 200 epoch...
09:51:31.306 Training @ 201 epoch...
09:51:31.882   Training iter 50, batch loss 0.2443, batch acc 0.8188
09:51:32.451   Training iter 100, batch loss 0.2450, batch acc 0.8172
09:51:33.020   Training iter 150, batch loss 0.2429, batch acc 0.8148
09:51:33.593   Training iter 200, batch loss 0.2438, batch acc 0.8110
09:51:34.185   Training iter 250, batch loss 0.2444, batch acc 0.8138
09:51:34.739   Training iter 300, batch loss 0.2448, batch acc 0.8112
09:51:35.272   Training iter 350, batch loss 0.2490, batch acc 0.8030
09:51:35.806   Training iter 400, batch loss 0.2455, batch acc 0.8100
09:51:36.320   Training iter 450, batch loss 0.2465, batch acc 0.8068
09:51:36.831   Training iter 500, batch loss 0.2419, batch acc 0.8198
09:51:37.347   Training iter 550, batch loss 0.2434, batch acc 0.8088
09:51:37.880   Training iter 600, batch loss 0.2438, batch acc 0.8148
09:51:37.882 Training @ 202 epoch...
09:51:38.438   Training iter 50, batch loss 0.2475, batch acc 0.8022
09:51:38.976   Training iter 100, batch loss 0.2415, batch acc 0.8210
09:51:39.507   Training iter 150, batch loss 0.2429, batch acc 0.8128
09:51:40.052   Training iter 200, batch loss 0.2441, batch acc 0.8116
09:51:40.589   Training iter 250, batch loss 0.2445, batch acc 0.8086
09:51:41.197   Training iter 300, batch loss 0.2454, batch acc 0.8142
09:51:41.866   Training iter 350, batch loss 0.2466, batch acc 0.8072
09:51:42.359   Training iter 400, batch loss 0.2433, batch acc 0.8224
09:51:42.864   Training iter 450, batch loss 0.2448, batch acc 0.8036
09:51:43.389   Training iter 500, batch loss 0.2426, batch acc 0.8234
09:51:43.900   Training iter 550, batch loss 0.2448, batch acc 0.8188
09:51:44.416   Training iter 600, batch loss 0.2457, batch acc 0.8072
09:51:44.417 Training @ 203 epoch...
09:51:44.931   Training iter 50, batch loss 0.2412, batch acc 0.8244
09:51:45.440   Training iter 100, batch loss 0.2426, batch acc 0.8164
09:51:45.958   Training iter 150, batch loss 0.2438, batch acc 0.8134
09:51:46.450   Training iter 200, batch loss 0.2454, batch acc 0.8068
09:51:46.928   Training iter 250, batch loss 0.2438, batch acc 0.8174
09:51:47.409   Training iter 300, batch loss 0.2454, batch acc 0.8082
09:51:47.870   Training iter 350, batch loss 0.2457, batch acc 0.8172
09:51:48.347   Training iter 400, batch loss 0.2418, batch acc 0.8126
09:51:48.836   Training iter 450, batch loss 0.2444, batch acc 0.8132
09:51:49.343   Training iter 500, batch loss 0.2461, batch acc 0.8070
09:51:49.855   Training iter 550, batch loss 0.2469, batch acc 0.8040
09:51:50.360   Training iter 600, batch loss 0.2451, batch acc 0.8134
09:51:50.362 Training @ 204 epoch...
09:51:50.886   Training iter 50, batch loss 0.2453, batch acc 0.8120
09:51:51.407   Training iter 100, batch loss 0.2425, batch acc 0.8180
09:51:51.935   Training iter 150, batch loss 0.2457, batch acc 0.8100
09:51:52.451   Training iter 200, batch loss 0.2439, batch acc 0.8118
09:51:52.969   Training iter 250, batch loss 0.2425, batch acc 0.8176
09:51:53.476   Training iter 300, batch loss 0.2440, batch acc 0.8118
09:51:53.983   Training iter 350, batch loss 0.2439, batch acc 0.8128
09:51:54.488   Training iter 400, batch loss 0.2454, batch acc 0.8098
09:51:54.994   Training iter 450, batch loss 0.2463, batch acc 0.8134
09:51:55.514   Training iter 500, batch loss 0.2431, batch acc 0.8124
09:51:56.030   Training iter 550, batch loss 0.2432, batch acc 0.8154
09:51:56.567   Training iter 600, batch loss 0.2452, batch acc 0.8118
09:51:56.569 Training @ 205 epoch...
09:51:57.113   Training iter 50, batch loss 0.2443, batch acc 0.8138
09:51:57.649   Training iter 100, batch loss 0.2460, batch acc 0.8068
09:51:58.190   Training iter 150, batch loss 0.2432, batch acc 0.8160
09:51:58.730   Training iter 200, batch loss 0.2460, batch acc 0.8120
09:51:59.240   Training iter 250, batch loss 0.2450, batch acc 0.8112
09:51:59.757   Training iter 300, batch loss 0.2439, batch acc 0.8118
09:52:00.269   Training iter 350, batch loss 0.2416, batch acc 0.8230
09:52:00.838   Training iter 400, batch loss 0.2441, batch acc 0.8150
09:52:01.555   Training iter 450, batch loss 0.2425, batch acc 0.8128
09:52:02.223   Training iter 500, batch loss 0.2433, batch acc 0.8114
09:52:02.803   Training iter 550, batch loss 0.2450, batch acc 0.8122
09:52:03.374   Training iter 600, batch loss 0.2445, batch acc 0.8122
09:52:03.375 Testing @ 205 epoch...
09:52:03.411     Testing, total mean loss 0.24054, total acc 0.82280
09:52:03.412 Training @ 206 epoch...
09:52:03.940   Training iter 50, batch loss 0.2425, batch acc 0.8200
09:52:04.469   Training iter 100, batch loss 0.2447, batch acc 0.8148
09:52:05.003   Training iter 150, batch loss 0.2440, batch acc 0.8116
09:52:05.542   Training iter 200, batch loss 0.2425, batch acc 0.8134
09:52:06.078   Training iter 250, batch loss 0.2421, batch acc 0.8146
09:52:06.593   Training iter 300, batch loss 0.2442, batch acc 0.8052
09:52:07.131   Training iter 350, batch loss 0.2446, batch acc 0.8066
09:52:07.661   Training iter 400, batch loss 0.2432, batch acc 0.8202
09:52:08.181   Training iter 450, batch loss 0.2447, batch acc 0.8190
09:52:08.712   Training iter 500, batch loss 0.2429, batch acc 0.8178
09:52:09.237   Training iter 550, batch loss 0.2465, batch acc 0.8076
09:52:09.761   Training iter 600, batch loss 0.2464, batch acc 0.8068
09:52:09.763 Training @ 207 epoch...
09:52:10.297   Training iter 50, batch loss 0.2429, batch acc 0.8198
09:52:10.809   Training iter 100, batch loss 0.2444, batch acc 0.8110
09:52:11.307   Training iter 150, batch loss 0.2451, batch acc 0.8198
09:52:11.809   Training iter 200, batch loss 0.2443, batch acc 0.8170
09:52:12.305   Training iter 250, batch loss 0.2453, batch acc 0.8038
09:52:12.804   Training iter 300, batch loss 0.2434, batch acc 0.8150
09:52:13.311   Training iter 350, batch loss 0.2436, batch acc 0.8150
09:52:13.820   Training iter 400, batch loss 0.2425, batch acc 0.8178
09:52:14.327   Training iter 450, batch loss 0.2434, batch acc 0.8016
09:52:14.837   Training iter 500, batch loss 0.2446, batch acc 0.8156
09:52:15.340   Training iter 550, batch loss 0.2455, batch acc 0.8060
09:52:15.842   Training iter 600, batch loss 0.2418, batch acc 0.8166
09:52:15.844 Training @ 208 epoch...
09:52:16.363   Training iter 50, batch loss 0.2446, batch acc 0.8108
09:52:16.871   Training iter 100, batch loss 0.2444, batch acc 0.8064
09:52:17.387   Training iter 150, batch loss 0.2443, batch acc 0.8090
09:52:17.940   Training iter 200, batch loss 0.2441, batch acc 0.8076
09:52:18.481   Training iter 250, batch loss 0.2473, batch acc 0.8090
09:52:19.021   Training iter 300, batch loss 0.2424, batch acc 0.8198
09:52:19.543   Training iter 350, batch loss 0.2443, batch acc 0.8140
09:52:20.088   Training iter 400, batch loss 0.2443, batch acc 0.8148
09:52:20.623   Training iter 450, batch loss 0.2429, batch acc 0.8162
09:52:21.141   Training iter 500, batch loss 0.2399, batch acc 0.8224
09:52:21.637   Training iter 550, batch loss 0.2426, batch acc 0.8178
09:52:22.156   Training iter 600, batch loss 0.2444, batch acc 0.8128
09:52:22.157 Training @ 209 epoch...
09:52:22.686   Training iter 50, batch loss 0.2444, batch acc 0.8070
09:52:23.217   Training iter 100, batch loss 0.2457, batch acc 0.8102
09:52:23.723   Training iter 150, batch loss 0.2429, batch acc 0.8150
09:52:24.231   Training iter 200, batch loss 0.2428, batch acc 0.8158
09:52:24.760   Training iter 250, batch loss 0.2443, batch acc 0.8122
09:52:25.288   Training iter 300, batch loss 0.2447, batch acc 0.8178
09:52:25.792   Training iter 350, batch loss 0.2413, batch acc 0.8296
09:52:26.269   Training iter 400, batch loss 0.2425, batch acc 0.8150
09:52:26.746   Training iter 450, batch loss 0.2445, batch acc 0.8070
09:52:27.234   Training iter 500, batch loss 0.2421, batch acc 0.8164
09:52:27.728   Training iter 550, batch loss 0.2435, batch acc 0.8074
09:52:28.234   Training iter 600, batch loss 0.2456, batch acc 0.8072
09:52:28.235 Training @ 210 epoch...
09:52:28.729   Training iter 50, batch loss 0.2418, batch acc 0.8160
09:52:29.226   Training iter 100, batch loss 0.2443, batch acc 0.8086
09:52:29.726   Training iter 150, batch loss 0.2403, batch acc 0.8244
09:52:30.256   Training iter 200, batch loss 0.2408, batch acc 0.8182
09:52:30.792   Training iter 250, batch loss 0.2425, batch acc 0.8152
09:52:31.356   Training iter 300, batch loss 0.2444, batch acc 0.8180
09:52:31.923   Training iter 350, batch loss 0.2444, batch acc 0.8164
09:52:32.443   Training iter 400, batch loss 0.2443, batch acc 0.8130
09:52:32.972   Training iter 450, batch loss 0.2453, batch acc 0.8124
09:52:33.518   Training iter 500, batch loss 0.2455, batch acc 0.8082
09:52:34.067   Training iter 550, batch loss 0.2441, batch acc 0.8062
09:52:34.617   Training iter 600, batch loss 0.2454, batch acc 0.8058
09:52:34.619 Testing @ 210 epoch...
09:52:34.655     Testing, total mean loss 0.23999, total acc 0.82300
09:52:34.655 Training @ 211 epoch...
09:52:35.202   Training iter 50, batch loss 0.2432, batch acc 0.8134
09:52:35.727   Training iter 100, batch loss 0.2426, batch acc 0.8198
09:52:36.257   Training iter 150, batch loss 0.2458, batch acc 0.8102
09:52:36.771   Training iter 200, batch loss 0.2433, batch acc 0.8094
09:52:37.289   Training iter 250, batch loss 0.2427, batch acc 0.8190
09:52:37.798   Training iter 300, batch loss 0.2444, batch acc 0.8106
09:52:38.331   Training iter 350, batch loss 0.2426, batch acc 0.8150
09:52:38.863   Training iter 400, batch loss 0.2432, batch acc 0.8126
09:52:39.350   Training iter 450, batch loss 0.2432, batch acc 0.8202
09:52:39.850   Training iter 500, batch loss 0.2432, batch acc 0.8156
09:52:40.372   Training iter 550, batch loss 0.2426, batch acc 0.8108
09:52:40.892   Training iter 600, batch loss 0.2450, batch acc 0.8076
09:52:40.894 Training @ 212 epoch...
09:52:41.389   Training iter 50, batch loss 0.2401, batch acc 0.8224
09:52:41.881   Training iter 100, batch loss 0.2468, batch acc 0.8086
09:52:42.402   Training iter 150, batch loss 0.2409, batch acc 0.8156
09:52:42.921   Training iter 200, batch loss 0.2434, batch acc 0.8160
09:52:43.467   Training iter 250, batch loss 0.2435, batch acc 0.8146
09:52:43.972   Training iter 300, batch loss 0.2428, batch acc 0.8152
09:52:44.484   Training iter 350, batch loss 0.2413, batch acc 0.8170
09:52:45.011   Training iter 400, batch loss 0.2470, batch acc 0.8038
09:52:45.534   Training iter 450, batch loss 0.2449, batch acc 0.8098
09:52:46.039   Training iter 500, batch loss 0.2427, batch acc 0.8058
09:52:46.526   Training iter 550, batch loss 0.2431, batch acc 0.8168
09:52:47.030   Training iter 600, batch loss 0.2437, batch acc 0.8194
09:52:47.032 Training @ 213 epoch...
09:52:47.528   Training iter 50, batch loss 0.2412, batch acc 0.8184
09:52:48.028   Training iter 100, batch loss 0.2410, batch acc 0.8206
09:52:48.550   Training iter 150, batch loss 0.2438, batch acc 0.8050
09:52:49.091   Training iter 200, batch loss 0.2454, batch acc 0.8062
09:52:49.647   Training iter 250, batch loss 0.2440, batch acc 0.8174
09:52:50.196   Training iter 300, batch loss 0.2442, batch acc 0.8150
09:52:50.751   Training iter 350, batch loss 0.2431, batch acc 0.8154
09:52:51.296   Training iter 400, batch loss 0.2431, batch acc 0.8164
09:52:51.835   Training iter 450, batch loss 0.2444, batch acc 0.8086
09:52:52.382   Training iter 500, batch loss 0.2393, batch acc 0.8252
09:52:52.929   Training iter 550, batch loss 0.2440, batch acc 0.8108
09:52:53.482   Training iter 600, batch loss 0.2458, batch acc 0.8082
09:52:53.484 Training @ 214 epoch...
09:52:54.033   Training iter 50, batch loss 0.2405, batch acc 0.8176
09:52:54.560   Training iter 100, batch loss 0.2440, batch acc 0.8098
09:52:55.065   Training iter 150, batch loss 0.2432, batch acc 0.8126
09:52:55.549   Training iter 200, batch loss 0.2415, batch acc 0.8206
09:52:56.040   Training iter 250, batch loss 0.2442, batch acc 0.8160
09:52:56.525   Training iter 300, batch loss 0.2454, batch acc 0.8084
09:52:57.029   Training iter 350, batch loss 0.2399, batch acc 0.8178
09:52:57.535   Training iter 400, batch loss 0.2442, batch acc 0.8108
09:52:58.034   Training iter 450, batch loss 0.2439, batch acc 0.8134
09:52:58.559   Training iter 500, batch loss 0.2429, batch acc 0.8128
09:52:59.081   Training iter 550, batch loss 0.2456, batch acc 0.8126
09:52:59.614   Training iter 600, batch loss 0.2426, batch acc 0.8146
09:52:59.616 Training @ 215 epoch...
09:53:00.150   Training iter 50, batch loss 0.2440, batch acc 0.8122
09:53:00.679   Training iter 100, batch loss 0.2434, batch acc 0.8150
09:53:01.214   Training iter 150, batch loss 0.2441, batch acc 0.8122
09:53:01.754   Training iter 200, batch loss 0.2443, batch acc 0.8176
09:53:02.280   Training iter 250, batch loss 0.2420, batch acc 0.8200
09:53:02.808   Training iter 300, batch loss 0.2409, batch acc 0.8232
09:53:03.346   Training iter 350, batch loss 0.2421, batch acc 0.8180
09:53:03.878   Training iter 400, batch loss 0.2456, batch acc 0.8078
09:53:04.408   Training iter 450, batch loss 0.2422, batch acc 0.8102
09:53:04.945   Training iter 500, batch loss 0.2427, batch acc 0.8146
09:53:05.503   Training iter 550, batch loss 0.2437, batch acc 0.8110
09:53:06.061   Training iter 600, batch loss 0.2417, batch acc 0.8120
09:53:06.063 Testing @ 215 epoch...
09:53:06.101     Testing, total mean loss 0.23947, total acc 0.82380
09:53:06.101 Training @ 216 epoch...
09:53:06.680   Training iter 50, batch loss 0.2443, batch acc 0.8096
09:53:07.308   Training iter 100, batch loss 0.2431, batch acc 0.8148
09:53:07.904   Training iter 150, batch loss 0.2406, batch acc 0.8188
09:53:08.497   Training iter 200, batch loss 0.2436, batch acc 0.8082
09:53:09.091   Training iter 250, batch loss 0.2434, batch acc 0.8172
09:53:09.649   Training iter 300, batch loss 0.2418, batch acc 0.8114
09:53:10.218   Training iter 350, batch loss 0.2436, batch acc 0.8162
09:53:10.778   Training iter 400, batch loss 0.2426, batch acc 0.8102
09:53:11.450   Training iter 450, batch loss 0.2431, batch acc 0.8180
09:53:12.120   Training iter 500, batch loss 0.2419, batch acc 0.8178
09:53:12.805   Training iter 550, batch loss 0.2445, batch acc 0.8050
09:53:13.441   Training iter 600, batch loss 0.2431, batch acc 0.8220
09:53:13.443 Training @ 217 epoch...
09:53:13.985   Training iter 50, batch loss 0.2426, batch acc 0.8164
09:53:14.515   Training iter 100, batch loss 0.2417, batch acc 0.8156
09:53:15.038   Training iter 150, batch loss 0.2433, batch acc 0.8128
09:53:15.566   Training iter 200, batch loss 0.2428, batch acc 0.8194
09:53:16.088   Training iter 250, batch loss 0.2437, batch acc 0.8148
09:53:16.602   Training iter 300, batch loss 0.2424, batch acc 0.8170
09:53:17.112   Training iter 350, batch loss 0.2403, batch acc 0.8162
09:53:17.622   Training iter 400, batch loss 0.2447, batch acc 0.8100
09:53:18.132   Training iter 450, batch loss 0.2439, batch acc 0.8142
09:53:18.657   Training iter 500, batch loss 0.2434, batch acc 0.8058
09:53:19.189   Training iter 550, batch loss 0.2415, batch acc 0.8226
09:53:19.712   Training iter 600, batch loss 0.2442, batch acc 0.8076
09:53:19.713 Training @ 218 epoch...
09:53:20.249   Training iter 50, batch loss 0.2411, batch acc 0.8170
09:53:20.775   Training iter 100, batch loss 0.2428, batch acc 0.8196
09:53:21.284   Training iter 150, batch loss 0.2419, batch acc 0.8186
09:53:21.784   Training iter 200, batch loss 0.2403, batch acc 0.8192
09:53:22.287   Training iter 250, batch loss 0.2441, batch acc 0.8088
09:53:22.804   Training iter 300, batch loss 0.2456, batch acc 0.8094
09:53:23.338   Training iter 350, batch loss 0.2408, batch acc 0.8128
09:53:23.857   Training iter 400, batch loss 0.2470, batch acc 0.8088
09:53:24.386   Training iter 450, batch loss 0.2437, batch acc 0.8096
09:53:24.904   Training iter 500, batch loss 0.2434, batch acc 0.8130
09:53:25.430   Training iter 550, batch loss 0.2390, batch acc 0.8288
09:53:25.937   Training iter 600, batch loss 0.2435, batch acc 0.8072
09:53:25.939 Training @ 219 epoch...
09:53:26.464   Training iter 50, batch loss 0.2435, batch acc 0.8138
09:53:26.959   Training iter 100, batch loss 0.2417, batch acc 0.8176
09:53:27.464   Training iter 150, batch loss 0.2414, batch acc 0.8128
09:53:27.958   Training iter 200, batch loss 0.2421, batch acc 0.8272
09:53:28.446   Training iter 250, batch loss 0.2408, batch acc 0.8172
09:53:28.932   Training iter 300, batch loss 0.2409, batch acc 0.8134
09:53:29.395   Training iter 350, batch loss 0.2446, batch acc 0.8146
09:53:29.842   Training iter 400, batch loss 0.2434, batch acc 0.8130
09:53:30.296   Training iter 450, batch loss 0.2457, batch acc 0.8052
09:53:30.735   Training iter 500, batch loss 0.2445, batch acc 0.8094
09:53:31.250   Training iter 550, batch loss 0.2411, batch acc 0.8182
09:53:31.764   Training iter 600, batch loss 0.2426, batch acc 0.8152
09:53:31.766 Training @ 220 epoch...
09:53:32.282   Training iter 50, batch loss 0.2446, batch acc 0.8098
09:53:32.783   Training iter 100, batch loss 0.2406, batch acc 0.8252
09:53:33.250   Training iter 150, batch loss 0.2418, batch acc 0.8156
09:53:33.751   Training iter 200, batch loss 0.2398, batch acc 0.8192
09:53:34.243   Training iter 250, batch loss 0.2441, batch acc 0.8040
09:53:34.749   Training iter 300, batch loss 0.2427, batch acc 0.8106
09:53:35.283   Training iter 350, batch loss 0.2430, batch acc 0.8090
09:53:35.764   Training iter 400, batch loss 0.2399, batch acc 0.8286
09:53:36.244   Training iter 450, batch loss 0.2451, batch acc 0.8158
09:53:36.729   Training iter 500, batch loss 0.2450, batch acc 0.8088
09:53:37.251   Training iter 550, batch loss 0.2415, batch acc 0.8144
09:53:37.793   Training iter 600, batch loss 0.2428, batch acc 0.8160
09:53:37.795 Testing @ 220 epoch...
09:53:37.831     Testing, total mean loss 0.23898, total acc 0.82400
09:53:37.831 Training @ 221 epoch...
09:53:38.377   Training iter 50, batch loss 0.2409, batch acc 0.8230
09:53:38.913   Training iter 100, batch loss 0.2450, batch acc 0.8176
09:53:39.450   Training iter 150, batch loss 0.2398, batch acc 0.8212
09:53:39.994   Training iter 200, batch loss 0.2431, batch acc 0.8100
09:53:40.532   Training iter 250, batch loss 0.2432, batch acc 0.8098
09:53:41.036   Training iter 300, batch loss 0.2409, batch acc 0.8160
09:53:41.549   Training iter 350, batch loss 0.2438, batch acc 0.8130
09:53:42.075   Training iter 400, batch loss 0.2441, batch acc 0.8136
09:53:42.598   Training iter 450, batch loss 0.2420, batch acc 0.8114
09:53:43.067   Training iter 500, batch loss 0.2406, batch acc 0.8218
09:53:43.524   Training iter 550, batch loss 0.2430, batch acc 0.8122
09:53:43.983   Training iter 600, batch loss 0.2435, batch acc 0.8106
09:53:43.985 Training @ 222 epoch...
09:53:44.447   Training iter 50, batch loss 0.2412, batch acc 0.8148
09:53:44.902   Training iter 100, batch loss 0.2405, batch acc 0.8234
09:53:45.370   Training iter 150, batch loss 0.2428, batch acc 0.8110
09:53:45.856   Training iter 200, batch loss 0.2415, batch acc 0.8176
09:53:46.348   Training iter 250, batch loss 0.2424, batch acc 0.8124
09:53:46.821   Training iter 300, batch loss 0.2421, batch acc 0.8188
09:53:47.297   Training iter 350, batch loss 0.2429, batch acc 0.8162
09:53:47.783   Training iter 400, batch loss 0.2439, batch acc 0.8138
09:53:48.269   Training iter 450, batch loss 0.2421, batch acc 0.8184
09:53:48.752   Training iter 500, batch loss 0.2450, batch acc 0.8058
09:53:49.240   Training iter 550, batch loss 0.2446, batch acc 0.8076
09:53:49.738   Training iter 600, batch loss 0.2398, batch acc 0.8216
09:53:49.740 Training @ 223 epoch...
09:53:50.250   Training iter 50, batch loss 0.2441, batch acc 0.8058
09:53:50.773   Training iter 100, batch loss 0.2414, batch acc 0.8188
09:53:51.282   Training iter 150, batch loss 0.2402, batch acc 0.8220
09:53:51.807   Training iter 200, batch loss 0.2413, batch acc 0.8128
09:53:52.324   Training iter 250, batch loss 0.2443, batch acc 0.8096
09:53:52.850   Training iter 300, batch loss 0.2439, batch acc 0.8106
09:53:53.381   Training iter 350, batch loss 0.2402, batch acc 0.8184
09:53:53.882   Training iter 400, batch loss 0.2434, batch acc 0.8124
09:53:54.362   Training iter 450, batch loss 0.2443, batch acc 0.8064
09:53:54.855   Training iter 500, batch loss 0.2441, batch acc 0.8162
09:53:55.364   Training iter 550, batch loss 0.2396, batch acc 0.8250
09:53:55.846   Training iter 600, batch loss 0.2407, batch acc 0.8244
09:53:55.848 Training @ 224 epoch...
09:53:56.350   Training iter 50, batch loss 0.2424, batch acc 0.8166
09:53:56.868   Training iter 100, batch loss 0.2411, batch acc 0.8180
09:53:57.397   Training iter 150, batch loss 0.2418, batch acc 0.8248
09:53:57.968   Training iter 200, batch loss 0.2415, batch acc 0.8088
09:53:58.517   Training iter 250, batch loss 0.2428, batch acc 0.8058
09:53:58.995   Training iter 300, batch loss 0.2427, batch acc 0.8114
09:53:59.501   Training iter 350, batch loss 0.2407, batch acc 0.8144
09:53:59.987   Training iter 400, batch loss 0.2428, batch acc 0.8172
09:54:00.512   Training iter 450, batch loss 0.2419, batch acc 0.8144
09:54:01.101   Training iter 500, batch loss 0.2418, batch acc 0.8198
09:54:01.795   Training iter 550, batch loss 0.2431, batch acc 0.8136
09:54:02.362   Training iter 600, batch loss 0.2440, batch acc 0.8158
09:54:02.364 Training @ 225 epoch...
09:54:02.892   Training iter 50, batch loss 0.2405, batch acc 0.8266
09:54:03.405   Training iter 100, batch loss 0.2426, batch acc 0.8088
09:54:03.950   Training iter 150, batch loss 0.2440, batch acc 0.8150
09:54:04.470   Training iter 200, batch loss 0.2432, batch acc 0.8096
09:54:05.014   Training iter 250, batch loss 0.2438, batch acc 0.8146
09:54:05.520   Training iter 300, batch loss 0.2406, batch acc 0.8200
09:54:06.060   Training iter 350, batch loss 0.2417, batch acc 0.8164
09:54:06.591   Training iter 400, batch loss 0.2417, batch acc 0.8086
09:54:07.127   Training iter 450, batch loss 0.2420, batch acc 0.8164
09:54:07.654   Training iter 500, batch loss 0.2440, batch acc 0.8062
09:54:08.148   Training iter 550, batch loss 0.2392, batch acc 0.8262
09:54:08.642   Training iter 600, batch loss 0.2423, batch acc 0.8142
09:54:08.644 Testing @ 225 epoch...
09:54:08.680     Testing, total mean loss 0.23853, total acc 0.82480
09:54:08.680 Training @ 226 epoch...
09:54:09.199   Training iter 50, batch loss 0.2403, batch acc 0.8198
09:54:09.724   Training iter 100, batch loss 0.2413, batch acc 0.8178
09:54:10.286   Training iter 150, batch loss 0.2425, batch acc 0.8136
09:54:10.852   Training iter 200, batch loss 0.2426, batch acc 0.8144
09:54:11.423   Training iter 250, batch loss 0.2408, batch acc 0.8170
09:54:11.996   Training iter 300, batch loss 0.2454, batch acc 0.8068
09:54:12.574   Training iter 350, batch loss 0.2410, batch acc 0.8186
09:54:13.136   Training iter 400, batch loss 0.2410, batch acc 0.8118
09:54:13.718   Training iter 450, batch loss 0.2426, batch acc 0.8140
09:54:14.280   Training iter 500, batch loss 0.2426, batch acc 0.8164
09:54:14.828   Training iter 550, batch loss 0.2421, batch acc 0.8218
09:54:15.334   Training iter 600, batch loss 0.2421, batch acc 0.8160
09:54:15.336 Training @ 227 epoch...
09:54:15.851   Training iter 50, batch loss 0.2400, batch acc 0.8196
09:54:16.331   Training iter 100, batch loss 0.2408, batch acc 0.8154
09:54:16.811   Training iter 150, batch loss 0.2445, batch acc 0.8088
09:54:17.307   Training iter 200, batch loss 0.2417, batch acc 0.8114
09:54:17.827   Training iter 250, batch loss 0.2432, batch acc 0.8154
09:54:18.336   Training iter 300, batch loss 0.2416, batch acc 0.8166
09:54:18.844   Training iter 350, batch loss 0.2402, batch acc 0.8200
09:54:19.343   Training iter 400, batch loss 0.2422, batch acc 0.8124
09:54:19.848   Training iter 450, batch loss 0.2418, batch acc 0.8192
09:54:20.331   Training iter 500, batch loss 0.2435, batch acc 0.8156
09:54:20.787   Training iter 550, batch loss 0.2407, batch acc 0.8154
09:54:21.236   Training iter 600, batch loss 0.2432, batch acc 0.8172
09:54:21.237 Training @ 228 epoch...
09:54:21.698   Training iter 50, batch loss 0.2439, batch acc 0.8068
09:54:22.146   Training iter 100, batch loss 0.2420, batch acc 0.8136
09:54:22.623   Training iter 150, batch loss 0.2407, batch acc 0.8246
09:54:23.085   Training iter 200, batch loss 0.2414, batch acc 0.8104
09:54:23.542   Training iter 250, batch loss 0.2436, batch acc 0.8106
09:54:24.011   Training iter 300, batch loss 0.2414, batch acc 0.8172
09:54:24.481   Training iter 350, batch loss 0.2421, batch acc 0.8114
09:54:24.956   Training iter 400, batch loss 0.2421, batch acc 0.8140
09:54:25.435   Training iter 450, batch loss 0.2436, batch acc 0.8116
09:54:25.907   Training iter 500, batch loss 0.2404, batch acc 0.8280
09:54:26.407   Training iter 550, batch loss 0.2438, batch acc 0.8094
09:54:26.918   Training iter 600, batch loss 0.2375, batch acc 0.8290
09:54:26.919 Training @ 229 epoch...
09:54:27.465   Training iter 50, batch loss 0.2418, batch acc 0.8146
09:54:28.005   Training iter 100, batch loss 0.2408, batch acc 0.8184
09:54:28.528   Training iter 150, batch loss 0.2436, batch acc 0.8136
09:54:29.050   Training iter 200, batch loss 0.2389, batch acc 0.8176
09:54:29.578   Training iter 250, batch loss 0.2414, batch acc 0.8184
09:54:30.110   Training iter 300, batch loss 0.2445, batch acc 0.8096
09:54:30.593   Training iter 350, batch loss 0.2404, batch acc 0.8124
09:54:31.062   Training iter 400, batch loss 0.2417, batch acc 0.8174
09:54:31.515   Training iter 450, batch loss 0.2404, batch acc 0.8176
09:54:31.982   Training iter 500, batch loss 0.2421, batch acc 0.8172
09:54:32.455   Training iter 550, batch loss 0.2406, batch acc 0.8232
09:54:32.925   Training iter 600, batch loss 0.2452, batch acc 0.8068
09:54:32.926 Training @ 230 epoch...
09:54:33.411   Training iter 50, batch loss 0.2409, batch acc 0.8110
09:54:33.902   Training iter 100, batch loss 0.2392, batch acc 0.8272
09:54:34.421   Training iter 150, batch loss 0.2410, batch acc 0.8184
09:54:34.946   Training iter 200, batch loss 0.2414, batch acc 0.8196
09:54:35.463   Training iter 250, batch loss 0.2411, batch acc 0.8178
09:54:35.974   Training iter 300, batch loss 0.2449, batch acc 0.8050
09:54:36.484   Training iter 350, batch loss 0.2426, batch acc 0.8102
09:54:37.025   Training iter 400, batch loss 0.2434, batch acc 0.8090
09:54:37.555   Training iter 450, batch loss 0.2394, batch acc 0.8204
09:54:38.082   Training iter 500, batch loss 0.2419, batch acc 0.8164
09:54:38.608   Training iter 550, batch loss 0.2441, batch acc 0.8156
09:54:39.136   Training iter 600, batch loss 0.2407, batch acc 0.8210
09:54:39.138 Testing @ 230 epoch...
09:54:39.184     Testing, total mean loss 0.23809, total acc 0.82510
09:54:39.184 Training @ 231 epoch...
09:54:39.719   Training iter 50, batch loss 0.2434, batch acc 0.8112
09:54:40.281   Training iter 100, batch loss 0.2408, batch acc 0.8166
09:54:40.836   Training iter 150, batch loss 0.2436, batch acc 0.8132
09:54:41.403   Training iter 200, batch loss 0.2409, batch acc 0.8140
09:54:41.971   Training iter 250, batch loss 0.2389, batch acc 0.8246
09:54:42.541   Training iter 300, batch loss 0.2425, batch acc 0.8146
09:54:43.091   Training iter 350, batch loss 0.2402, batch acc 0.8202
09:54:43.588   Training iter 400, batch loss 0.2415, batch acc 0.8174
09:54:44.090   Training iter 450, batch loss 0.2423, batch acc 0.8192
09:54:44.599   Training iter 500, batch loss 0.2404, batch acc 0.8106
09:54:45.114   Training iter 550, batch loss 0.2428, batch acc 0.8132
09:54:45.645   Training iter 600, batch loss 0.2420, batch acc 0.8156
09:54:45.647 Training @ 232 epoch...
09:54:46.171   Training iter 50, batch loss 0.2428, batch acc 0.8218
09:54:46.683   Training iter 100, batch loss 0.2419, batch acc 0.8168
09:54:47.176   Training iter 150, batch loss 0.2383, batch acc 0.8272
09:54:47.653   Training iter 200, batch loss 0.2425, batch acc 0.8146
09:54:48.141   Training iter 250, batch loss 0.2422, batch acc 0.8122
09:54:48.638   Training iter 300, batch loss 0.2413, batch acc 0.8068
09:54:49.130   Training iter 350, batch loss 0.2413, batch acc 0.8152
09:54:49.621   Training iter 400, batch loss 0.2411, batch acc 0.8192
09:54:50.101   Training iter 450, batch loss 0.2397, batch acc 0.8228
09:54:50.592   Training iter 500, batch loss 0.2421, batch acc 0.8122
09:54:51.090   Training iter 550, batch loss 0.2420, batch acc 0.8134
09:54:51.583   Training iter 600, batch loss 0.2433, batch acc 0.8114
09:54:51.584 Training @ 233 epoch...
09:54:52.087   Training iter 50, batch loss 0.2422, batch acc 0.8162
09:54:52.586   Training iter 100, batch loss 0.2435, batch acc 0.8058
09:54:53.088   Training iter 150, batch loss 0.2436, batch acc 0.8076
09:54:53.588   Training iter 200, batch loss 0.2374, batch acc 0.8326
09:54:54.065   Training iter 250, batch loss 0.2425, batch acc 0.8116
09:54:54.534   Training iter 300, batch loss 0.2391, batch acc 0.8202
09:54:55.007   Training iter 350, batch loss 0.2411, batch acc 0.8212
09:54:55.481   Training iter 400, batch loss 0.2432, batch acc 0.8112
09:54:55.931   Training iter 450, batch loss 0.2433, batch acc 0.8062
09:54:56.381   Training iter 500, batch loss 0.2405, batch acc 0.8210
09:54:56.832   Training iter 550, batch loss 0.2416, batch acc 0.8140
09:54:57.285   Training iter 600, batch loss 0.2395, batch acc 0.8232
09:54:57.286 Training @ 234 epoch...
09:54:57.747   Training iter 50, batch loss 0.2441, batch acc 0.8060
09:54:58.220   Training iter 100, batch loss 0.2406, batch acc 0.8192
09:54:58.671   Training iter 150, batch loss 0.2413, batch acc 0.8174
09:54:59.137   Training iter 200, batch loss 0.2403, batch acc 0.8234
09:54:59.597   Training iter 250, batch loss 0.2440, batch acc 0.8078
09:55:00.069   Training iter 300, batch loss 0.2363, batch acc 0.8280
09:55:00.567   Training iter 350, batch loss 0.2406, batch acc 0.8180
09:55:01.074   Training iter 400, batch loss 0.2394, batch acc 0.8278
09:55:01.618   Training iter 450, batch loss 0.2419, batch acc 0.8134
09:55:02.177   Training iter 500, batch loss 0.2431, batch acc 0.8058
09:55:02.719   Training iter 550, batch loss 0.2427, batch acc 0.8186
09:55:03.244   Training iter 600, batch loss 0.2420, batch acc 0.8100
09:55:03.246 Training @ 235 epoch...
09:55:03.789   Training iter 50, batch loss 0.2397, batch acc 0.8146
09:55:04.331   Training iter 100, batch loss 0.2402, batch acc 0.8144
09:55:04.869   Training iter 150, batch loss 0.2423, batch acc 0.8168
09:55:05.409   Training iter 200, batch loss 0.2392, batch acc 0.8188
09:55:05.927   Training iter 250, batch loss 0.2429, batch acc 0.8088
09:55:06.416   Training iter 300, batch loss 0.2409, batch acc 0.8174
09:55:06.922   Training iter 350, batch loss 0.2413, batch acc 0.8190
09:55:07.430   Training iter 400, batch loss 0.2434, batch acc 0.8134
09:55:07.957   Training iter 450, batch loss 0.2415, batch acc 0.8184
09:55:08.488   Training iter 500, batch loss 0.2415, batch acc 0.8142
09:55:08.995   Training iter 550, batch loss 0.2420, batch acc 0.8162
09:55:09.492   Training iter 600, batch loss 0.2406, batch acc 0.8214
09:55:09.493 Testing @ 235 epoch...
09:55:09.531     Testing, total mean loss 0.23769, total acc 0.82510
09:55:09.531 Training @ 236 epoch...
09:55:10.037   Training iter 50, batch loss 0.2378, batch acc 0.8230
09:55:10.538   Training iter 100, batch loss 0.2424, batch acc 0.8200
09:55:11.046   Training iter 150, batch loss 0.2376, batch acc 0.8266
09:55:11.534   Training iter 200, batch loss 0.2398, batch acc 0.8190
09:55:12.037   Training iter 250, batch loss 0.2413, batch acc 0.8116
09:55:12.535   Training iter 300, batch loss 0.2451, batch acc 0.8116
09:55:13.053   Training iter 350, batch loss 0.2419, batch acc 0.8114
09:55:13.582   Training iter 400, batch loss 0.2418, batch acc 0.8180
09:55:14.093   Training iter 450, batch loss 0.2422, batch acc 0.8102
09:55:14.608   Training iter 500, batch loss 0.2409, batch acc 0.8148
09:55:15.131   Training iter 550, batch loss 0.2437, batch acc 0.8120
09:55:15.647   Training iter 600, batch loss 0.2402, batch acc 0.8158
09:55:15.649 Training @ 237 epoch...
09:55:16.158   Training iter 50, batch loss 0.2409, batch acc 0.8142
09:55:16.686   Training iter 100, batch loss 0.2427, batch acc 0.8088
09:55:17.222   Training iter 150, batch loss 0.2405, batch acc 0.8236
09:55:17.753   Training iter 200, batch loss 0.2396, batch acc 0.8168
09:55:18.292   Training iter 250, batch loss 0.2393, batch acc 0.8202
09:55:18.807   Training iter 300, batch loss 0.2419, batch acc 0.8182
09:55:19.334   Training iter 350, batch loss 0.2405, batch acc 0.8140
09:55:19.867   Training iter 400, batch loss 0.2416, batch acc 0.8174
09:55:20.417   Training iter 450, batch loss 0.2424, batch acc 0.8150
09:55:20.921   Training iter 500, batch loss 0.2414, batch acc 0.8116
09:55:21.401   Training iter 550, batch loss 0.2432, batch acc 0.8162
09:55:21.873   Training iter 600, batch loss 0.2397, batch acc 0.8208
09:55:21.875 Training @ 238 epoch...
09:55:22.354   Training iter 50, batch loss 0.2398, batch acc 0.8230
09:55:22.840   Training iter 100, batch loss 0.2412, batch acc 0.8194
09:55:23.333   Training iter 150, batch loss 0.2424, batch acc 0.8158
09:55:23.838   Training iter 200, batch loss 0.2434, batch acc 0.8092
09:55:24.353   Training iter 250, batch loss 0.2397, batch acc 0.8190
09:55:24.869   Training iter 300, batch loss 0.2400, batch acc 0.8114
09:55:25.392   Training iter 350, batch loss 0.2404, batch acc 0.8192
09:55:25.900   Training iter 400, batch loss 0.2409, batch acc 0.8164
09:55:26.402   Training iter 450, batch loss 0.2400, batch acc 0.8206
09:55:26.909   Training iter 500, batch loss 0.2424, batch acc 0.8114
09:55:27.457   Training iter 550, batch loss 0.2405, batch acc 0.8164
09:55:28.010   Training iter 600, batch loss 0.2421, batch acc 0.8162
09:55:28.012 Training @ 239 epoch...
09:55:28.563   Training iter 50, batch loss 0.2391, batch acc 0.8204
09:55:29.071   Training iter 100, batch loss 0.2428, batch acc 0.8128
09:55:29.567   Training iter 150, batch loss 0.2410, batch acc 0.8238
09:55:30.073   Training iter 200, batch loss 0.2410, batch acc 0.8158
09:55:30.599   Training iter 250, batch loss 0.2417, batch acc 0.8142
09:55:31.106   Training iter 300, batch loss 0.2365, batch acc 0.8260
09:55:31.578   Training iter 350, batch loss 0.2448, batch acc 0.8120
09:55:32.047   Training iter 400, batch loss 0.2407, batch acc 0.8112
09:55:32.519   Training iter 450, batch loss 0.2444, batch acc 0.8054
09:55:32.997   Training iter 500, batch loss 0.2408, batch acc 0.8178
09:55:33.457   Training iter 550, batch loss 0.2403, batch acc 0.8134
09:55:33.919   Training iter 600, batch loss 0.2388, batch acc 0.8274
09:55:33.920 Training @ 240 epoch...
09:55:34.412   Training iter 50, batch loss 0.2412, batch acc 0.8210
09:55:34.917   Training iter 100, batch loss 0.2394, batch acc 0.8204
09:55:35.420   Training iter 150, batch loss 0.2433, batch acc 0.8042
09:55:35.903   Training iter 200, batch loss 0.2396, batch acc 0.8144
09:55:36.362   Training iter 250, batch loss 0.2374, batch acc 0.8278
09:55:36.817   Training iter 300, batch loss 0.2402, batch acc 0.8212
09:55:37.274   Training iter 350, batch loss 0.2437, batch acc 0.8060
09:55:37.729   Training iter 400, batch loss 0.2405, batch acc 0.8194
09:55:38.194   Training iter 450, batch loss 0.2391, batch acc 0.8178
09:55:38.654   Training iter 500, batch loss 0.2412, batch acc 0.8170
09:55:39.120   Training iter 550, batch loss 0.2433, batch acc 0.8122
09:55:39.577   Training iter 600, batch loss 0.2421, batch acc 0.8166
09:55:39.579 Testing @ 240 epoch...
09:55:39.616     Testing, total mean loss 0.23731, total acc 0.82580
09:55:39.616 Training @ 241 epoch...
09:55:40.109   Training iter 50, batch loss 0.2427, batch acc 0.8118
09:55:40.572   Training iter 100, batch loss 0.2380, batch acc 0.8242
09:55:41.018   Training iter 150, batch loss 0.2435, batch acc 0.8078
09:55:41.469   Training iter 200, batch loss 0.2396, batch acc 0.8150
09:55:41.935   Training iter 250, batch loss 0.2428, batch acc 0.8156
09:55:42.392   Training iter 300, batch loss 0.2404, batch acc 0.8250
09:55:42.844   Training iter 350, batch loss 0.2407, batch acc 0.8162
09:55:43.319   Training iter 400, batch loss 0.2383, batch acc 0.8254
09:55:43.813   Training iter 450, batch loss 0.2396, batch acc 0.8212
09:55:44.298   Training iter 500, batch loss 0.2393, batch acc 0.8174
09:55:44.799   Training iter 550, batch loss 0.2435, batch acc 0.8078
09:55:45.300   Training iter 600, batch loss 0.2416, batch acc 0.8122
09:55:45.302 Training @ 242 epoch...
09:55:45.790   Training iter 50, batch loss 0.2385, batch acc 0.8248
09:55:46.299   Training iter 100, batch loss 0.2388, batch acc 0.8248
09:55:46.809   Training iter 150, batch loss 0.2420, batch acc 0.8146
09:55:47.326   Training iter 200, batch loss 0.2422, batch acc 0.8110
09:55:47.830   Training iter 250, batch loss 0.2409, batch acc 0.8222
09:55:48.325   Training iter 300, batch loss 0.2394, batch acc 0.8218
09:55:48.820   Training iter 350, batch loss 0.2395, batch acc 0.8232
09:55:49.350   Training iter 400, batch loss 0.2410, batch acc 0.8204
09:55:49.878   Training iter 450, batch loss 0.2417, batch acc 0.8014
09:55:50.407   Training iter 500, batch loss 0.2424, batch acc 0.8064
09:55:50.919   Training iter 550, batch loss 0.2417, batch acc 0.8164
09:55:51.423   Training iter 600, batch loss 0.2413, batch acc 0.8134
09:55:51.424 Training @ 243 epoch...
09:55:51.938   Training iter 50, batch loss 0.2415, batch acc 0.8100
09:55:52.440   Training iter 100, batch loss 0.2387, batch acc 0.8258
09:55:52.950   Training iter 150, batch loss 0.2409, batch acc 0.8200
09:55:53.466   Training iter 200, batch loss 0.2405, batch acc 0.8148
09:55:54.018   Training iter 250, batch loss 0.2396, batch acc 0.8198
09:55:54.525   Training iter 300, batch loss 0.2415, batch acc 0.8078
09:55:55.048   Training iter 350, batch loss 0.2428, batch acc 0.8164
09:55:55.559   Training iter 400, batch loss 0.2408, batch acc 0.8214
09:55:56.069   Training iter 450, batch loss 0.2407, batch acc 0.8126
09:55:56.576   Training iter 500, batch loss 0.2393, batch acc 0.8188
09:55:57.101   Training iter 550, batch loss 0.2409, batch acc 0.8158
09:55:57.615   Training iter 600, batch loss 0.2414, batch acc 0.8166
09:55:57.617 Training @ 244 epoch...
09:55:58.144   Training iter 50, batch loss 0.2410, batch acc 0.8110
09:55:58.648   Training iter 100, batch loss 0.2424, batch acc 0.8164
09:55:59.165   Training iter 150, batch loss 0.2407, batch acc 0.8154
09:55:59.685   Training iter 200, batch loss 0.2390, batch acc 0.8160
09:56:00.208   Training iter 250, batch loss 0.2410, batch acc 0.8128
09:56:00.777   Training iter 300, batch loss 0.2400, batch acc 0.8204
09:56:01.326   Training iter 350, batch loss 0.2423, batch acc 0.8212
09:56:01.880   Training iter 400, batch loss 0.2406, batch acc 0.8114
09:56:02.614   Training iter 450, batch loss 0.2413, batch acc 0.8236
09:56:03.130   Training iter 500, batch loss 0.2385, batch acc 0.8216
09:56:03.626   Training iter 550, batch loss 0.2420, batch acc 0.8122
09:56:04.144   Training iter 600, batch loss 0.2387, batch acc 0.8202
09:56:04.146 Training @ 245 epoch...
09:56:04.678   Training iter 50, batch loss 0.2430, batch acc 0.8050
09:56:05.197   Training iter 100, batch loss 0.2431, batch acc 0.8100
09:56:05.750   Training iter 150, batch loss 0.2402, batch acc 0.8174
09:56:06.281   Training iter 200, batch loss 0.2387, batch acc 0.8244
09:56:06.801   Training iter 250, batch loss 0.2424, batch acc 0.8140
09:56:07.332   Training iter 300, batch loss 0.2420, batch acc 0.8160
09:56:07.846   Training iter 350, batch loss 0.2400, batch acc 0.8182
09:56:08.349   Training iter 400, batch loss 0.2363, batch acc 0.8270
09:56:08.858   Training iter 450, batch loss 0.2421, batch acc 0.8094
09:56:09.375   Training iter 500, batch loss 0.2370, batch acc 0.8326
09:56:09.891   Training iter 550, batch loss 0.2393, batch acc 0.8232
09:56:10.415   Training iter 600, batch loss 0.2427, batch acc 0.8096
09:56:10.416 Testing @ 245 epoch...
09:56:10.452     Testing, total mean loss 0.23695, total acc 0.82570
09:56:10.453 Training @ 246 epoch...
09:56:10.974   Training iter 50, batch loss 0.2395, batch acc 0.8160
09:56:11.460   Training iter 100, batch loss 0.2429, batch acc 0.8116
09:56:11.946   Training iter 150, batch loss 0.2427, batch acc 0.8096
09:56:12.441   Training iter 200, batch loss 0.2394, batch acc 0.8200
09:56:12.958   Training iter 250, batch loss 0.2412, batch acc 0.8106
09:56:13.482   Training iter 300, batch loss 0.2407, batch acc 0.8128
09:56:14.022   Training iter 350, batch loss 0.2419, batch acc 0.8222
09:56:14.551   Training iter 400, batch loss 0.2380, batch acc 0.8188
09:56:15.084   Training iter 450, batch loss 0.2413, batch acc 0.8144
09:56:15.604   Training iter 500, batch loss 0.2412, batch acc 0.8274
09:56:16.152   Training iter 550, batch loss 0.2396, batch acc 0.8184
09:56:16.692   Training iter 600, batch loss 0.2375, batch acc 0.8206
09:56:16.694 Training @ 247 epoch...
09:56:17.306   Training iter 50, batch loss 0.2417, batch acc 0.8132
09:56:17.902   Training iter 100, batch loss 0.2432, batch acc 0.8132
09:56:18.446   Training iter 150, batch loss 0.2395, batch acc 0.8206
09:56:18.987   Training iter 200, batch loss 0.2414, batch acc 0.8136
09:56:19.507   Training iter 250, batch loss 0.2362, batch acc 0.8284
09:56:20.052   Training iter 300, batch loss 0.2392, batch acc 0.8224
09:56:20.592   Training iter 350, batch loss 0.2368, batch acc 0.8192
09:56:21.108   Training iter 400, batch loss 0.2427, batch acc 0.8118
09:56:21.600   Training iter 450, batch loss 0.2404, batch acc 0.8284
09:56:22.086   Training iter 500, batch loss 0.2415, batch acc 0.8120
09:56:22.550   Training iter 550, batch loss 0.2422, batch acc 0.8108
09:56:23.008   Training iter 600, batch loss 0.2403, batch acc 0.8112
09:56:23.009 Training @ 248 epoch...
09:56:23.475   Training iter 50, batch loss 0.2401, batch acc 0.8158
09:56:23.945   Training iter 100, batch loss 0.2399, batch acc 0.8156
09:56:24.424   Training iter 150, batch loss 0.2419, batch acc 0.8072
09:56:24.919   Training iter 200, batch loss 0.2398, batch acc 0.8224
09:56:25.432   Training iter 250, batch loss 0.2394, batch acc 0.8234
09:56:25.916   Training iter 300, batch loss 0.2422, batch acc 0.8142
09:56:26.392   Training iter 350, batch loss 0.2394, batch acc 0.8198
09:56:26.867   Training iter 400, batch loss 0.2387, batch acc 0.8196
09:56:27.331   Training iter 450, batch loss 0.2409, batch acc 0.8180
09:56:27.803   Training iter 500, batch loss 0.2412, batch acc 0.8178
09:56:28.280   Training iter 550, batch loss 0.2412, batch acc 0.8130
09:56:28.742   Training iter 600, batch loss 0.2397, batch acc 0.8198
09:56:28.744 Training @ 249 epoch...
09:56:29.215   Training iter 50, batch loss 0.2395, batch acc 0.8188
09:56:29.699   Training iter 100, batch loss 0.2404, batch acc 0.8136
09:56:30.207   Training iter 150, batch loss 0.2430, batch acc 0.8146
09:56:30.712   Training iter 200, batch loss 0.2389, batch acc 0.8248
09:56:31.202   Training iter 250, batch loss 0.2422, batch acc 0.8122
09:56:31.696   Training iter 300, batch loss 0.2412, batch acc 0.8236
09:56:32.220   Training iter 350, batch loss 0.2368, batch acc 0.8224
09:56:32.755   Training iter 400, batch loss 0.2418, batch acc 0.8078
09:56:33.302   Training iter 450, batch loss 0.2403, batch acc 0.8116
09:56:33.820   Training iter 500, batch loss 0.2407, batch acc 0.8210
09:56:34.332   Training iter 550, batch loss 0.2382, batch acc 0.8206
09:56:34.855   Training iter 600, batch loss 0.2404, batch acc 0.8142
09:56:34.857 Training @ 250 epoch...
09:56:35.366   Training iter 50, batch loss 0.2385, batch acc 0.8248
09:56:35.867   Training iter 100, batch loss 0.2409, batch acc 0.8190
09:56:36.367   Training iter 150, batch loss 0.2409, batch acc 0.8106
09:56:36.867   Training iter 200, batch loss 0.2411, batch acc 0.8190
09:56:37.371   Training iter 250, batch loss 0.2425, batch acc 0.8102
09:56:37.867   Training iter 300, batch loss 0.2389, batch acc 0.8228
09:56:38.339   Training iter 350, batch loss 0.2410, batch acc 0.8144
09:56:38.810   Training iter 400, batch loss 0.2396, batch acc 0.8172
09:56:39.267   Training iter 450, batch loss 0.2396, batch acc 0.8132
09:56:39.726   Training iter 500, batch loss 0.2399, batch acc 0.8182
09:56:40.200   Training iter 550, batch loss 0.2398, batch acc 0.8152
09:56:40.688   Training iter 600, batch loss 0.2399, batch acc 0.8230
09:56:40.690 Testing @ 250 epoch...
09:56:40.725     Testing, total mean loss 0.23661, total acc 0.82650
09:56:40.725 Training @ 251 epoch...
09:56:41.181   Training iter 50, batch loss 0.2398, batch acc 0.8154
09:56:41.607   Training iter 100, batch loss 0.2419, batch acc 0.8160
09:56:42.045   Training iter 150, batch loss 0.2404, batch acc 0.8156
09:56:42.481   Training iter 200, batch loss 0.2385, batch acc 0.8172
09:56:42.936   Training iter 250, batch loss 0.2428, batch acc 0.8186
09:56:43.392   Training iter 300, batch loss 0.2406, batch acc 0.8134
09:56:43.847   Training iter 350, batch loss 0.2431, batch acc 0.8108
09:56:44.303   Training iter 400, batch loss 0.2393, batch acc 0.8182
09:56:44.750   Training iter 450, batch loss 0.2394, batch acc 0.8190
09:56:45.202   Training iter 500, batch loss 0.2388, batch acc 0.8232
09:56:45.646   Training iter 550, batch loss 0.2400, batch acc 0.8134
09:56:46.128   Training iter 600, batch loss 0.2375, batch acc 0.8276
09:56:46.129 Training @ 252 epoch...
09:56:46.631   Training iter 50, batch loss 0.2413, batch acc 0.8156
09:56:47.104   Training iter 100, batch loss 0.2397, batch acc 0.8194
09:56:47.575   Training iter 150, batch loss 0.2388, batch acc 0.8180
09:56:48.057   Training iter 200, batch loss 0.2392, batch acc 0.8206
09:56:48.533   Training iter 250, batch loss 0.2403, batch acc 0.8164
09:56:49.026   Training iter 300, batch loss 0.2413, batch acc 0.8190
09:56:49.519   Training iter 350, batch loss 0.2427, batch acc 0.8022
09:56:50.013   Training iter 400, batch loss 0.2405, batch acc 0.8194
09:56:50.510   Training iter 450, batch loss 0.2400, batch acc 0.8146
09:56:51.003   Training iter 500, batch loss 0.2387, batch acc 0.8202
09:56:51.514   Training iter 550, batch loss 0.2394, batch acc 0.8236
09:56:52.023   Training iter 600, batch loss 0.2394, batch acc 0.8192
09:56:52.025 Training @ 253 epoch...
09:56:52.564   Training iter 50, batch loss 0.2393, batch acc 0.8206
09:56:53.107   Training iter 100, batch loss 0.2408, batch acc 0.8174
09:56:53.643   Training iter 150, batch loss 0.2381, batch acc 0.8250
09:56:54.118   Training iter 200, batch loss 0.2403, batch acc 0.8142
09:56:54.597   Training iter 250, batch loss 0.2415, batch acc 0.8118
09:56:55.060   Training iter 300, batch loss 0.2383, batch acc 0.8186
09:56:55.524   Training iter 350, batch loss 0.2397, batch acc 0.8194
09:56:55.961   Training iter 400, batch loss 0.2388, batch acc 0.8234
09:56:56.417   Training iter 450, batch loss 0.2417, batch acc 0.8160
09:56:56.861   Training iter 500, batch loss 0.2410, batch acc 0.8158
09:56:57.328   Training iter 550, batch loss 0.2404, batch acc 0.8172
09:56:57.805   Training iter 600, batch loss 0.2404, batch acc 0.8106
09:56:57.807 Training @ 254 epoch...
09:56:58.286   Training iter 50, batch loss 0.2401, batch acc 0.8150
09:56:58.747   Training iter 100, batch loss 0.2381, batch acc 0.8254
09:56:59.205   Training iter 150, batch loss 0.2376, batch acc 0.8234
09:56:59.682   Training iter 200, batch loss 0.2447, batch acc 0.8082
09:57:00.147   Training iter 250, batch loss 0.2400, batch acc 0.8158
09:57:00.638   Training iter 300, batch loss 0.2397, batch acc 0.8236
09:57:01.136   Training iter 350, batch loss 0.2395, batch acc 0.8132
09:57:01.617   Training iter 400, batch loss 0.2401, batch acc 0.8154
09:57:02.163   Training iter 450, batch loss 0.2414, batch acc 0.8142
09:57:02.730   Training iter 500, batch loss 0.2365, batch acc 0.8286
09:57:03.281   Training iter 550, batch loss 0.2419, batch acc 0.8146
09:57:03.848   Training iter 600, batch loss 0.2401, batch acc 0.8134
09:57:03.849 Training @ 255 epoch...
09:57:04.406   Training iter 50, batch loss 0.2416, batch acc 0.8082
09:57:04.931   Training iter 100, batch loss 0.2395, batch acc 0.8200
09:57:05.448   Training iter 150, batch loss 0.2384, batch acc 0.8252
09:57:05.999   Training iter 200, batch loss 0.2398, batch acc 0.8152
09:57:06.530   Training iter 250, batch loss 0.2381, batch acc 0.8226
09:57:07.091   Training iter 300, batch loss 0.2380, batch acc 0.8198
09:57:07.645   Training iter 350, batch loss 0.2421, batch acc 0.8114
09:57:08.193   Training iter 400, batch loss 0.2395, batch acc 0.8244
09:57:08.737   Training iter 450, batch loss 0.2376, batch acc 0.8228
09:57:09.286   Training iter 500, batch loss 0.2439, batch acc 0.8102
09:57:09.814   Training iter 550, batch loss 0.2402, batch acc 0.8164
09:57:10.374   Training iter 600, batch loss 0.2403, batch acc 0.8158
09:57:10.376 Testing @ 255 epoch...
09:57:10.413     Testing, total mean loss 0.23629, total acc 0.82680
09:57:10.413 Training @ 256 epoch...
09:57:10.931   Training iter 50, batch loss 0.2406, batch acc 0.8174
09:57:11.468   Training iter 100, batch loss 0.2366, batch acc 0.8238
09:57:11.992   Training iter 150, batch loss 0.2395, batch acc 0.8222
09:57:12.568   Training iter 200, batch loss 0.2383, batch acc 0.8186
09:57:13.096   Training iter 250, batch loss 0.2384, batch acc 0.8160
09:57:13.620   Training iter 300, batch loss 0.2401, batch acc 0.8080
09:57:14.140   Training iter 350, batch loss 0.2399, batch acc 0.8216
09:57:14.652   Training iter 400, batch loss 0.2425, batch acc 0.8128
09:57:15.158   Training iter 450, batch loss 0.2428, batch acc 0.8116
09:57:15.655   Training iter 500, batch loss 0.2399, batch acc 0.8232
09:57:16.152   Training iter 550, batch loss 0.2409, batch acc 0.8140
09:57:16.644   Training iter 600, batch loss 0.2386, batch acc 0.8198
09:57:16.646 Training @ 257 epoch...
09:57:17.146   Training iter 50, batch loss 0.2376, batch acc 0.8176
09:57:17.650   Training iter 100, batch loss 0.2405, batch acc 0.8174
09:57:18.162   Training iter 150, batch loss 0.2400, batch acc 0.8148
09:57:18.884   Training iter 200, batch loss 0.2387, batch acc 0.8238
09:57:19.623   Training iter 250, batch loss 0.2420, batch acc 0.8130
09:57:20.275   Training iter 300, batch loss 0.2412, batch acc 0.8126
09:57:20.817   Training iter 350, batch loss 0.2392, batch acc 0.8248
09:57:21.334   Training iter 400, batch loss 0.2373, batch acc 0.8240
09:57:21.850   Training iter 450, batch loss 0.2419, batch acc 0.8138
09:57:22.364   Training iter 500, batch loss 0.2399, batch acc 0.8180
09:57:22.886   Training iter 550, batch loss 0.2410, batch acc 0.8114
09:57:23.399   Training iter 600, batch loss 0.2380, batch acc 0.8248
09:57:23.400 Training @ 258 epoch...
09:57:23.929   Training iter 50, batch loss 0.2401, batch acc 0.8140
09:57:24.460   Training iter 100, batch loss 0.2412, batch acc 0.8142
09:57:24.968   Training iter 150, batch loss 0.2385, batch acc 0.8192
09:57:25.483   Training iter 200, batch loss 0.2420, batch acc 0.8158
09:57:25.961   Training iter 250, batch loss 0.2382, batch acc 0.8232
09:57:26.445   Training iter 300, batch loss 0.2411, batch acc 0.8156
09:57:26.920   Training iter 350, batch loss 0.2401, batch acc 0.8152
09:57:27.410   Training iter 400, batch loss 0.2382, batch acc 0.8196
09:57:27.907   Training iter 450, batch loss 0.2396, batch acc 0.8196
09:57:28.394   Training iter 500, batch loss 0.2398, batch acc 0.8182
09:57:28.888   Training iter 550, batch loss 0.2394, batch acc 0.8192
09:57:29.376   Training iter 600, batch loss 0.2386, batch acc 0.8182
09:57:29.378 Training @ 259 epoch...
09:57:29.876   Training iter 50, batch loss 0.2391, batch acc 0.8152
09:57:30.371   Training iter 100, batch loss 0.2413, batch acc 0.8128
09:57:30.871   Training iter 150, batch loss 0.2397, batch acc 0.8124
09:57:31.382   Training iter 200, batch loss 0.2417, batch acc 0.8130
09:57:31.872   Training iter 250, batch loss 0.2391, batch acc 0.8190
09:57:32.368   Training iter 300, batch loss 0.2389, batch acc 0.8268
09:57:32.874   Training iter 350, batch loss 0.2403, batch acc 0.8160
09:57:33.368   Training iter 400, batch loss 0.2394, batch acc 0.8160
09:57:33.856   Training iter 450, batch loss 0.2396, batch acc 0.8192
09:57:34.356   Training iter 500, batch loss 0.2370, batch acc 0.8272
09:57:34.862   Training iter 550, batch loss 0.2389, batch acc 0.8214
09:57:35.379   Training iter 600, batch loss 0.2411, batch acc 0.8162
09:57:35.381 Training @ 260 epoch...
09:57:35.889   Training iter 50, batch loss 0.2372, batch acc 0.8294
09:57:36.515   Training iter 100, batch loss 0.2423, batch acc 0.8120
09:57:37.064   Training iter 150, batch loss 0.2375, batch acc 0.8258
09:57:37.607   Training iter 200, batch loss 0.2379, batch acc 0.8234
09:57:38.146   Training iter 250, batch loss 0.2398, batch acc 0.8164
09:57:38.672   Training iter 300, batch loss 0.2388, batch acc 0.8200
09:57:39.198   Training iter 350, batch loss 0.2402, batch acc 0.8124
09:57:39.689   Training iter 400, batch loss 0.2402, batch acc 0.8204
09:57:40.176   Training iter 450, batch loss 0.2383, batch acc 0.8144
09:57:40.668   Training iter 500, batch loss 0.2420, batch acc 0.8088
09:57:41.134   Training iter 550, batch loss 0.2404, batch acc 0.8148
09:57:41.618   Training iter 600, batch loss 0.2407, batch acc 0.8170
09:57:41.620 Testing @ 260 epoch...
09:57:41.659     Testing, total mean loss 0.23599, total acc 0.82690
09:57:41.659 Training @ 261 epoch...
09:57:42.181   Training iter 50, batch loss 0.2371, batch acc 0.8250
09:57:42.693   Training iter 100, batch loss 0.2407, batch acc 0.8120
09:57:43.184   Training iter 150, batch loss 0.2405, batch acc 0.8158
09:57:43.650   Training iter 200, batch loss 0.2398, batch acc 0.8200
09:57:44.126   Training iter 250, batch loss 0.2383, batch acc 0.8184
09:57:44.610   Training iter 300, batch loss 0.2377, batch acc 0.8148
09:57:45.085   Training iter 350, batch loss 0.2422, batch acc 0.8116
09:57:45.558   Training iter 400, batch loss 0.2382, batch acc 0.8228
09:57:46.046   Training iter 450, batch loss 0.2414, batch acc 0.8110
09:57:46.543   Training iter 500, batch loss 0.2377, batch acc 0.8266
09:57:47.016   Training iter 550, batch loss 0.2419, batch acc 0.8160
09:57:47.474   Training iter 600, batch loss 0.2392, batch acc 0.8198
09:57:47.476 Training @ 262 epoch...
09:57:47.940   Training iter 50, batch loss 0.2394, batch acc 0.8230
09:57:48.398   Training iter 100, batch loss 0.2377, batch acc 0.8220
09:57:48.860   Training iter 150, batch loss 0.2403, batch acc 0.8184
09:57:49.337   Training iter 200, batch loss 0.2398, batch acc 0.8166
09:57:49.829   Training iter 250, batch loss 0.2392, batch acc 0.8150
09:57:50.336   Training iter 300, batch loss 0.2401, batch acc 0.8156
09:57:50.819   Training iter 350, batch loss 0.2394, batch acc 0.8202
09:57:51.302   Training iter 400, batch loss 0.2426, batch acc 0.8044
09:57:51.779   Training iter 450, batch loss 0.2358, batch acc 0.8270
09:57:52.273   Training iter 500, batch loss 0.2427, batch acc 0.8068
09:57:52.741   Training iter 550, batch loss 0.2377, batch acc 0.8254
09:57:53.208   Training iter 600, batch loss 0.2394, batch acc 0.8236
09:57:53.210 Training @ 263 epoch...
09:57:53.667   Training iter 50, batch loss 0.2369, batch acc 0.8292
09:57:54.125   Training iter 100, batch loss 0.2406, batch acc 0.8172
09:57:54.601   Training iter 150, batch loss 0.2380, batch acc 0.8250
09:57:55.078   Training iter 200, batch loss 0.2405, batch acc 0.8144
09:57:55.607   Training iter 250, batch loss 0.2375, batch acc 0.8242
09:57:56.087   Training iter 300, batch loss 0.2444, batch acc 0.8074
09:57:56.564   Training iter 350, batch loss 0.2397, batch acc 0.8134
09:57:57.063   Training iter 400, batch loss 0.2415, batch acc 0.8134
09:57:57.615   Training iter 450, batch loss 0.2383, batch acc 0.8170
09:57:58.120   Training iter 500, batch loss 0.2389, batch acc 0.8152
09:57:58.596   Training iter 550, batch loss 0.2365, batch acc 0.8256
09:57:59.064   Training iter 600, batch loss 0.2403, batch acc 0.8150
09:57:59.066 Training @ 264 epoch...
09:57:59.536   Training iter 50, batch loss 0.2402, batch acc 0.8130
09:58:00.002   Training iter 100, batch loss 0.2411, batch acc 0.8170
09:58:00.507   Training iter 150, batch loss 0.2398, batch acc 0.8192
09:58:00.992   Training iter 200, batch loss 0.2361, batch acc 0.8256
09:58:01.500   Training iter 250, batch loss 0.2412, batch acc 0.8100
09:58:01.997   Training iter 300, batch loss 0.2423, batch acc 0.8068
09:58:02.546   Training iter 350, batch loss 0.2386, batch acc 0.8256
09:58:03.085   Training iter 400, batch loss 0.2404, batch acc 0.8156
09:58:03.623   Training iter 450, batch loss 0.2394, batch acc 0.8156
09:58:04.163   Training iter 500, batch loss 0.2380, batch acc 0.8248
09:58:04.702   Training iter 550, batch loss 0.2368, batch acc 0.8268
09:58:05.235   Training iter 600, batch loss 0.2387, batch acc 0.8180
09:58:05.237 Training @ 265 epoch...
09:58:05.769   Training iter 50, batch loss 0.2400, batch acc 0.8302
09:58:06.270   Training iter 100, batch loss 0.2390, batch acc 0.8204
09:58:06.801   Training iter 150, batch loss 0.2376, batch acc 0.8178
09:58:07.328   Training iter 200, batch loss 0.2385, batch acc 0.8182
09:58:07.863   Training iter 250, batch loss 0.2388, batch acc 0.8180
09:58:08.389   Training iter 300, batch loss 0.2388, batch acc 0.8124
09:58:08.908   Training iter 350, batch loss 0.2396, batch acc 0.8212
09:58:09.425   Training iter 400, batch loss 0.2411, batch acc 0.8168
09:58:09.950   Training iter 450, batch loss 0.2398, batch acc 0.8128
09:58:10.474   Training iter 500, batch loss 0.2405, batch acc 0.8154
09:58:10.990   Training iter 550, batch loss 0.2385, batch acc 0.8238
09:58:11.521   Training iter 600, batch loss 0.2397, batch acc 0.8122
09:58:11.523 Testing @ 265 epoch...
09:58:11.559     Testing, total mean loss 0.23570, total acc 0.82740
09:58:11.559 Training @ 266 epoch...
09:58:12.112   Training iter 50, batch loss 0.2380, batch acc 0.8202
09:58:12.658   Training iter 100, batch loss 0.2397, batch acc 0.8216
09:58:13.205   Training iter 150, batch loss 0.2388, batch acc 0.8196
09:58:13.702   Training iter 200, batch loss 0.2400, batch acc 0.8226
09:58:14.221   Training iter 250, batch loss 0.2379, batch acc 0.8168
09:58:14.701   Training iter 300, batch loss 0.2414, batch acc 0.8098
09:58:15.183   Training iter 350, batch loss 0.2385, batch acc 0.8186
09:58:15.686   Training iter 400, batch loss 0.2386, batch acc 0.8198
09:58:16.246   Training iter 450, batch loss 0.2411, batch acc 0.8096
09:58:16.804   Training iter 500, batch loss 0.2399, batch acc 0.8208
09:58:17.372   Training iter 550, batch loss 0.2386, batch acc 0.8208
09:58:17.907   Training iter 600, batch loss 0.2388, batch acc 0.8200
09:58:17.909 Training @ 267 epoch...
09:58:18.416   Training iter 50, batch loss 0.2403, batch acc 0.8172
09:58:18.929   Training iter 100, batch loss 0.2417, batch acc 0.8176
09:58:19.440   Training iter 150, batch loss 0.2393, batch acc 0.8220
09:58:19.951   Training iter 200, batch loss 0.2381, batch acc 0.8232
09:58:20.474   Training iter 250, batch loss 0.2385, batch acc 0.8170
09:58:20.978   Training iter 300, batch loss 0.2409, batch acc 0.8150
09:58:21.478   Training iter 350, batch loss 0.2374, batch acc 0.8222
09:58:21.977   Training iter 400, batch loss 0.2399, batch acc 0.8156
09:58:22.493   Training iter 450, batch loss 0.2367, batch acc 0.8270
09:58:23.018   Training iter 500, batch loss 0.2387, batch acc 0.8188
09:58:23.555   Training iter 550, batch loss 0.2407, batch acc 0.8144
09:58:24.102   Training iter 600, batch loss 0.2385, batch acc 0.8124
09:58:24.104 Training @ 268 epoch...
09:58:24.664   Training iter 50, batch loss 0.2370, batch acc 0.8258
09:58:25.219   Training iter 100, batch loss 0.2391, batch acc 0.8182
09:58:25.761   Training iter 150, batch loss 0.2389, batch acc 0.8180
09:58:26.319   Training iter 200, batch loss 0.2389, batch acc 0.8210
09:58:26.862   Training iter 250, batch loss 0.2378, batch acc 0.8228
09:58:27.403   Training iter 300, batch loss 0.2393, batch acc 0.8208
09:58:27.945   Training iter 350, batch loss 0.2379, batch acc 0.8202
09:58:28.482   Training iter 400, batch loss 0.2398, batch acc 0.8124
09:58:29.022   Training iter 450, batch loss 0.2411, batch acc 0.8134
09:58:29.570   Training iter 500, batch loss 0.2391, batch acc 0.8210
09:58:30.107   Training iter 550, batch loss 0.2411, batch acc 0.8088
09:58:30.630   Training iter 600, batch loss 0.2399, batch acc 0.8184
09:58:30.632 Training @ 269 epoch...
09:58:31.153   Training iter 50, batch loss 0.2401, batch acc 0.8138
09:58:31.670   Training iter 100, batch loss 0.2366, batch acc 0.8238
09:58:32.176   Training iter 150, batch loss 0.2405, batch acc 0.8098
09:58:32.680   Training iter 200, batch loss 0.2390, batch acc 0.8182
09:58:33.188   Training iter 250, batch loss 0.2373, batch acc 0.8276
09:58:33.685   Training iter 300, batch loss 0.2408, batch acc 0.8114
09:58:34.185   Training iter 350, batch loss 0.2384, batch acc 0.8208
09:58:34.709   Training iter 400, batch loss 0.2385, batch acc 0.8140
09:58:35.223   Training iter 450, batch loss 0.2414, batch acc 0.8178
09:58:35.728   Training iter 500, batch loss 0.2382, batch acc 0.8244
09:58:36.210   Training iter 550, batch loss 0.2393, batch acc 0.8142
09:58:36.687   Training iter 600, batch loss 0.2393, batch acc 0.8232
09:58:36.689 Training @ 270 epoch...
09:58:37.168   Training iter 50, batch loss 0.2369, batch acc 0.8200
09:58:37.653   Training iter 100, batch loss 0.2389, batch acc 0.8160
09:58:38.142   Training iter 150, batch loss 0.2379, batch acc 0.8218
09:58:38.615   Training iter 200, batch loss 0.2419, batch acc 0.8040
09:58:39.103   Training iter 250, batch loss 0.2387, batch acc 0.8208
09:58:39.589   Training iter 300, batch loss 0.2381, batch acc 0.8258
09:58:40.091   Training iter 350, batch loss 0.2381, batch acc 0.8302
09:58:40.622   Training iter 400, batch loss 0.2378, batch acc 0.8232
09:58:41.156   Training iter 450, batch loss 0.2408, batch acc 0.8104
09:58:41.718   Training iter 500, batch loss 0.2412, batch acc 0.8188
09:58:42.257   Training iter 550, batch loss 0.2380, batch acc 0.8210
09:58:42.801   Training iter 600, batch loss 0.2404, batch acc 0.8116
09:58:42.802 Testing @ 270 epoch...
09:58:42.841     Testing, total mean loss 0.23543, total acc 0.82730
09:58:42.841 Training @ 271 epoch...
09:58:43.366   Training iter 50, batch loss 0.2392, batch acc 0.8172
09:58:43.873   Training iter 100, batch loss 0.2416, batch acc 0.8184
09:58:44.401   Training iter 150, batch loss 0.2424, batch acc 0.8120
09:58:44.965   Training iter 200, batch loss 0.2413, batch acc 0.8094
09:58:45.479   Training iter 250, batch loss 0.2353, batch acc 0.8236
09:58:45.932   Training iter 300, batch loss 0.2390, batch acc 0.8230
09:58:46.376   Training iter 350, batch loss 0.2372, batch acc 0.8204
09:58:46.817   Training iter 400, batch loss 0.2398, batch acc 0.8136
09:58:47.241   Training iter 450, batch loss 0.2421, batch acc 0.8098
09:58:47.661   Training iter 500, batch loss 0.2369, batch acc 0.8248
09:58:48.102   Training iter 550, batch loss 0.2374, batch acc 0.8228
09:58:48.547   Training iter 600, batch loss 0.2361, batch acc 0.8286
09:58:48.548 Training @ 272 epoch...
09:58:48.987   Training iter 50, batch loss 0.2405, batch acc 0.8156
09:58:49.420   Training iter 100, batch loss 0.2343, batch acc 0.8272
09:58:49.845   Training iter 150, batch loss 0.2390, batch acc 0.8180
09:58:50.291   Training iter 200, batch loss 0.2371, batch acc 0.8218
09:58:50.824   Training iter 250, batch loss 0.2400, batch acc 0.8204
09:58:51.375   Training iter 300, batch loss 0.2387, batch acc 0.8178
09:58:51.931   Training iter 350, batch loss 0.2386, batch acc 0.8152
09:58:52.402   Training iter 400, batch loss 0.2418, batch acc 0.8122
09:58:52.863   Training iter 450, batch loss 0.2382, batch acc 0.8158
09:58:53.308   Training iter 500, batch loss 0.2418, batch acc 0.8162
09:58:53.742   Training iter 550, batch loss 0.2387, batch acc 0.8228
09:58:54.173   Training iter 600, batch loss 0.2388, batch acc 0.8214
09:58:54.174 Training @ 273 epoch...
09:58:54.621   Training iter 50, batch loss 0.2380, batch acc 0.8202
09:58:55.067   Training iter 100, batch loss 0.2399, batch acc 0.8148
09:58:55.514   Training iter 150, batch loss 0.2397, batch acc 0.8172
09:58:55.940   Training iter 200, batch loss 0.2384, batch acc 0.8212
09:58:56.382   Training iter 250, batch loss 0.2386, batch acc 0.8164
09:58:56.830   Training iter 300, batch loss 0.2389, batch acc 0.8156
09:58:57.275   Training iter 350, batch loss 0.2389, batch acc 0.8160
09:58:57.704   Training iter 400, batch loss 0.2405, batch acc 0.8122
09:58:58.144   Training iter 450, batch loss 0.2366, batch acc 0.8256
09:58:58.567   Training iter 500, batch loss 0.2401, batch acc 0.8148
09:58:58.990   Training iter 550, batch loss 0.2376, batch acc 0.8320
09:58:59.419   Training iter 600, batch loss 0.2397, batch acc 0.8184
09:58:59.421 Training @ 274 epoch...
09:58:59.846   Training iter 50, batch loss 0.2380, batch acc 0.8220
09:59:00.260   Training iter 100, batch loss 0.2362, batch acc 0.8256
09:59:00.679   Training iter 150, batch loss 0.2393, batch acc 0.8252
09:59:01.145   Training iter 200, batch loss 0.2391, batch acc 0.8222
09:59:01.641   Training iter 250, batch loss 0.2370, batch acc 0.8196
09:59:02.148   Training iter 300, batch loss 0.2409, batch acc 0.8134
09:59:02.612   Training iter 350, batch loss 0.2408, batch acc 0.8134
09:59:03.078   Training iter 400, batch loss 0.2388, batch acc 0.8182
09:59:03.531   Training iter 450, batch loss 0.2376, batch acc 0.8162
09:59:03.966   Training iter 500, batch loss 0.2408, batch acc 0.8138
09:59:04.398   Training iter 550, batch loss 0.2404, batch acc 0.8150
09:59:04.833   Training iter 600, batch loss 0.2374, batch acc 0.8196
09:59:04.835 Training @ 275 epoch...
09:59:05.275   Training iter 50, batch loss 0.2402, batch acc 0.8146
09:59:05.713   Training iter 100, batch loss 0.2401, batch acc 0.8178
09:59:06.143   Training iter 150, batch loss 0.2419, batch acc 0.8128
09:59:06.545   Training iter 200, batch loss 0.2360, batch acc 0.8246
09:59:06.960   Training iter 250, batch loss 0.2392, batch acc 0.8160
09:59:07.396   Training iter 300, batch loss 0.2382, batch acc 0.8230
09:59:07.833   Training iter 350, batch loss 0.2354, batch acc 0.8296
09:59:08.243   Training iter 400, batch loss 0.2382, batch acc 0.8164
09:59:08.668   Training iter 450, batch loss 0.2403, batch acc 0.8158
09:59:09.071   Training iter 500, batch loss 0.2394, batch acc 0.8200
09:59:09.462   Training iter 550, batch loss 0.2375, batch acc 0.8224
09:59:09.885   Training iter 600, batch loss 0.2393, batch acc 0.8152
09:59:09.887 Testing @ 275 epoch...
09:59:09.922     Testing, total mean loss 0.23518, total acc 0.82780
09:59:09.922 Training @ 276 epoch...
09:59:10.352   Training iter 50, batch loss 0.2378, batch acc 0.8198
09:59:10.762   Training iter 100, batch loss 0.2366, batch acc 0.8300
09:59:11.179   Training iter 150, batch loss 0.2353, batch acc 0.8280
09:59:11.589   Training iter 200, batch loss 0.2417, batch acc 0.8142
09:59:11.995   Training iter 250, batch loss 0.2369, batch acc 0.8238
09:59:12.398   Training iter 300, batch loss 0.2403, batch acc 0.8146
09:59:12.833   Training iter 350, batch loss 0.2367, batch acc 0.8214
09:59:13.266   Training iter 400, batch loss 0.2393, batch acc 0.8140
09:59:13.719   Training iter 450, batch loss 0.2367, batch acc 0.8254
09:59:14.175   Training iter 500, batch loss 0.2400, batch acc 0.8080
09:59:14.592   Training iter 550, batch loss 0.2417, batch acc 0.8168
09:59:15.010   Training iter 600, batch loss 0.2421, batch acc 0.8116
09:59:15.012 Training @ 277 epoch...
09:59:15.429   Training iter 50, batch loss 0.2412, batch acc 0.8130
09:59:15.881   Training iter 100, batch loss 0.2367, batch acc 0.8194
09:59:16.379   Training iter 150, batch loss 0.2395, batch acc 0.8176
09:59:16.850   Training iter 200, batch loss 0.2383, batch acc 0.8178
09:59:17.333   Training iter 250, batch loss 0.2391, batch acc 0.8100
09:59:17.781   Training iter 300, batch loss 0.2367, batch acc 0.8294
09:59:18.253   Training iter 350, batch loss 0.2380, batch acc 0.8152
09:59:18.741   Training iter 400, batch loss 0.2400, batch acc 0.8170
09:59:19.194   Training iter 450, batch loss 0.2341, batch acc 0.8378
09:59:19.614   Training iter 500, batch loss 0.2394, batch acc 0.8158
09:59:20.054   Training iter 550, batch loss 0.2397, batch acc 0.8196
09:59:20.544   Training iter 600, batch loss 0.2419, batch acc 0.8152
09:59:20.545 Training @ 278 epoch...
09:59:21.017   Training iter 50, batch loss 0.2413, batch acc 0.8154
09:59:21.473   Training iter 100, batch loss 0.2397, batch acc 0.8122
09:59:21.927   Training iter 150, batch loss 0.2371, batch acc 0.8188
09:59:22.414   Training iter 200, batch loss 0.2389, batch acc 0.8218
09:59:22.896   Training iter 250, batch loss 0.2395, batch acc 0.8138
09:59:23.371   Training iter 300, batch loss 0.2384, batch acc 0.8178
09:59:23.822   Training iter 350, batch loss 0.2407, batch acc 0.8118
09:59:24.281   Training iter 400, batch loss 0.2376, batch acc 0.8186
09:59:24.756   Training iter 450, batch loss 0.2375, batch acc 0.8284
09:59:25.250   Training iter 500, batch loss 0.2383, batch acc 0.8208
09:59:25.723   Training iter 550, batch loss 0.2362, batch acc 0.8324
09:59:26.217   Training iter 600, batch loss 0.2387, batch acc 0.8190
09:59:26.219 Training @ 279 epoch...
09:59:26.719   Training iter 50, batch loss 0.2379, batch acc 0.8180
09:59:27.210   Training iter 100, batch loss 0.2379, batch acc 0.8260
09:59:27.716   Training iter 150, batch loss 0.2394, batch acc 0.8188
09:59:28.241   Training iter 200, batch loss 0.2372, batch acc 0.8188
09:59:28.758   Training iter 250, batch loss 0.2382, batch acc 0.8204
09:59:29.280   Training iter 300, batch loss 0.2388, batch acc 0.8292
09:59:29.790   Training iter 350, batch loss 0.2389, batch acc 0.8142
09:59:30.317   Training iter 400, batch loss 0.2392, batch acc 0.8166
09:59:30.857   Training iter 450, batch loss 0.2393, batch acc 0.8166
09:59:31.378   Training iter 500, batch loss 0.2396, batch acc 0.8116
09:59:31.904   Training iter 550, batch loss 0.2377, batch acc 0.8192
09:59:32.427   Training iter 600, batch loss 0.2393, batch acc 0.8192
09:59:32.429 Training @ 280 epoch...
09:59:32.958   Training iter 50, batch loss 0.2397, batch acc 0.8158
09:59:33.486   Training iter 100, batch loss 0.2390, batch acc 0.8268
09:59:33.997   Training iter 150, batch loss 0.2381, batch acc 0.8160
09:59:34.489   Training iter 200, batch loss 0.2404, batch acc 0.8150
09:59:34.985   Training iter 250, batch loss 0.2360, batch acc 0.8236
09:59:35.481   Training iter 300, batch loss 0.2391, batch acc 0.8172
09:59:35.958   Training iter 350, batch loss 0.2367, batch acc 0.8224
09:59:36.414   Training iter 400, batch loss 0.2379, batch acc 0.8246
09:59:36.877   Training iter 450, batch loss 0.2413, batch acc 0.8112
09:59:37.328   Training iter 500, batch loss 0.2385, batch acc 0.8138
09:59:37.796   Training iter 550, batch loss 0.2375, batch acc 0.8244
09:59:38.277   Training iter 600, batch loss 0.2387, batch acc 0.8208
09:59:38.278 Testing @ 280 epoch...
09:59:38.314     Testing, total mean loss 0.23494, total acc 0.82800
09:59:38.314 Training @ 281 epoch...
09:59:38.804   Training iter 50, batch loss 0.2373, batch acc 0.8190
09:59:39.281   Training iter 100, batch loss 0.2355, batch acc 0.8274
09:59:39.744   Training iter 150, batch loss 0.2404, batch acc 0.8140
09:59:40.240   Training iter 200, batch loss 0.2412, batch acc 0.8136
09:59:40.740   Training iter 250, batch loss 0.2375, batch acc 0.8218
09:59:41.236   Training iter 300, batch loss 0.2374, batch acc 0.8268
09:59:41.744   Training iter 350, batch loss 0.2407, batch acc 0.8098
09:59:42.244   Training iter 400, batch loss 0.2396, batch acc 0.8188
09:59:42.741   Training iter 450, batch loss 0.2400, batch acc 0.8142
09:59:43.240   Training iter 500, batch loss 0.2390, batch acc 0.8212
09:59:43.739   Training iter 550, batch loss 0.2379, batch acc 0.8226
09:59:44.282   Training iter 600, batch loss 0.2358, batch acc 0.8218
09:59:44.284 Training @ 282 epoch...
09:59:44.832   Training iter 50, batch loss 0.2390, batch acc 0.8194
09:59:45.375   Training iter 100, batch loss 0.2401, batch acc 0.8244
09:59:45.906   Training iter 150, batch loss 0.2373, batch acc 0.8158
09:59:46.424   Training iter 200, batch loss 0.2380, batch acc 0.8246
09:59:46.948   Training iter 250, batch loss 0.2379, batch acc 0.8198
09:59:47.479   Training iter 300, batch loss 0.2374, batch acc 0.8228
09:59:48.011   Training iter 350, batch loss 0.2394, batch acc 0.8178
09:59:48.521   Training iter 400, batch loss 0.2367, batch acc 0.8214
09:59:49.033   Training iter 450, batch loss 0.2372, batch acc 0.8158
09:59:49.553   Training iter 500, batch loss 0.2374, batch acc 0.8194
09:59:50.059   Training iter 550, batch loss 0.2420, batch acc 0.8136
09:59:50.556   Training iter 600, batch loss 0.2394, batch acc 0.8168
09:59:50.558 Training @ 283 epoch...
09:59:51.067   Training iter 50, batch loss 0.2359, batch acc 0.8210
09:59:51.556   Training iter 100, batch loss 0.2351, batch acc 0.8328
09:59:52.058   Training iter 150, batch loss 0.2391, batch acc 0.8152
09:59:52.560   Training iter 200, batch loss 0.2415, batch acc 0.8158
09:59:53.075   Training iter 250, batch loss 0.2385, batch acc 0.8238
09:59:53.613   Training iter 300, batch loss 0.2387, batch acc 0.8146
09:59:54.148   Training iter 350, batch loss 0.2376, batch acc 0.8186
09:59:54.668   Training iter 400, batch loss 0.2366, batch acc 0.8224
09:59:55.203   Training iter 450, batch loss 0.2398, batch acc 0.8172
09:59:55.731   Training iter 500, batch loss 0.2394, batch acc 0.8170
09:59:56.241   Training iter 550, batch loss 0.2393, batch acc 0.8166
09:59:56.759   Training iter 600, batch loss 0.2398, batch acc 0.8172
09:59:56.761 Training @ 284 epoch...
09:59:57.289   Training iter 50, batch loss 0.2397, batch acc 0.8192
09:59:57.809   Training iter 100, batch loss 0.2384, batch acc 0.8238
09:59:58.318   Training iter 150, batch loss 0.2377, batch acc 0.8250
09:59:58.813   Training iter 200, batch loss 0.2382, batch acc 0.8172
09:59:59.306   Training iter 250, batch loss 0.2371, batch acc 0.8200
09:59:59.808   Training iter 300, batch loss 0.2369, batch acc 0.8268
10:00:00.326   Training iter 350, batch loss 0.2380, batch acc 0.8218
10:00:00.876   Training iter 400, batch loss 0.2402, batch acc 0.8122
10:00:01.428   Training iter 450, batch loss 0.2397, batch acc 0.8156
10:00:02.007   Training iter 500, batch loss 0.2377, batch acc 0.8224
10:00:02.553   Training iter 550, batch loss 0.2398, batch acc 0.8098
10:00:03.059   Training iter 600, batch loss 0.2374, batch acc 0.8224
10:00:03.060 Training @ 285 epoch...
10:00:03.568   Training iter 50, batch loss 0.2363, batch acc 0.8254
10:00:04.069   Training iter 100, batch loss 0.2385, batch acc 0.8192
10:00:04.577   Training iter 150, batch loss 0.2353, batch acc 0.8236
10:00:05.072   Training iter 200, batch loss 0.2403, batch acc 0.8176
10:00:05.606   Training iter 250, batch loss 0.2406, batch acc 0.8146
10:00:06.150   Training iter 300, batch loss 0.2386, batch acc 0.8160
10:00:06.676   Training iter 350, batch loss 0.2365, batch acc 0.8192
10:00:07.209   Training iter 400, batch loss 0.2391, batch acc 0.8120
10:00:07.736   Training iter 450, batch loss 0.2382, batch acc 0.8254
10:00:08.271   Training iter 500, batch loss 0.2387, batch acc 0.8160
10:00:08.798   Training iter 550, batch loss 0.2387, batch acc 0.8270
10:00:09.311   Training iter 600, batch loss 0.2392, batch acc 0.8178
10:00:09.313 Testing @ 285 epoch...
10:00:09.348     Testing, total mean loss 0.23471, total acc 0.82810
10:00:09.348 Training @ 286 epoch...
10:00:09.854   Training iter 50, batch loss 0.2371, batch acc 0.8184
10:00:10.368   Training iter 100, batch loss 0.2368, batch acc 0.8232
10:00:10.866   Training iter 150, batch loss 0.2411, batch acc 0.8168
10:00:11.372   Training iter 200, batch loss 0.2376, batch acc 0.8230
10:00:11.862   Training iter 250, batch loss 0.2374, batch acc 0.8264
10:00:12.368   Training iter 300, batch loss 0.2369, batch acc 0.8206
10:00:12.876   Training iter 350, batch loss 0.2376, batch acc 0.8256
10:00:13.370   Training iter 400, batch loss 0.2392, batch acc 0.8166
10:00:13.883   Training iter 450, batch loss 0.2395, batch acc 0.8122
10:00:14.396   Training iter 500, batch loss 0.2364, batch acc 0.8248
10:00:14.896   Training iter 550, batch loss 0.2396, batch acc 0.8144
10:00:15.405   Training iter 600, batch loss 0.2403, batch acc 0.8132
10:00:15.407 Training @ 287 epoch...
10:00:15.935   Training iter 50, batch loss 0.2356, batch acc 0.8268
10:00:16.509   Training iter 100, batch loss 0.2386, batch acc 0.8186
10:00:17.056   Training iter 150, batch loss 0.2376, batch acc 0.8260
10:00:17.597   Training iter 200, batch loss 0.2389, batch acc 0.8180
10:00:18.155   Training iter 250, batch loss 0.2389, batch acc 0.8170
10:00:18.723   Training iter 300, batch loss 0.2401, batch acc 0.8132
10:00:19.292   Training iter 350, batch loss 0.2368, batch acc 0.8216
10:00:19.858   Training iter 400, batch loss 0.2381, batch acc 0.8248
10:00:20.443   Training iter 450, batch loss 0.2358, batch acc 0.8240
10:00:21.016   Training iter 500, batch loss 0.2393, batch acc 0.8206
10:00:21.570   Training iter 550, batch loss 0.2391, batch acc 0.8144
10:00:22.096   Training iter 600, batch loss 0.2403, batch acc 0.8108
10:00:22.098 Training @ 288 epoch...
10:00:22.613   Training iter 50, batch loss 0.2390, batch acc 0.8148
10:00:23.128   Training iter 100, batch loss 0.2399, batch acc 0.8162
10:00:23.642   Training iter 150, batch loss 0.2370, batch acc 0.8246
10:00:24.152   Training iter 200, batch loss 0.2364, batch acc 0.8210
10:00:24.676   Training iter 250, batch loss 0.2370, batch acc 0.8254
10:00:25.204   Training iter 300, batch loss 0.2382, batch acc 0.8202
10:00:25.732   Training iter 350, batch loss 0.2409, batch acc 0.8164
10:00:26.264   Training iter 400, batch loss 0.2401, batch acc 0.8086
10:00:26.784   Training iter 450, batch loss 0.2379, batch acc 0.8268
10:00:27.313   Training iter 500, batch loss 0.2362, batch acc 0.8288
10:00:27.844   Training iter 550, batch loss 0.2382, batch acc 0.8184
10:00:28.379   Training iter 600, batch loss 0.2380, batch acc 0.8164
10:00:28.381 Training @ 289 epoch...
10:00:28.918   Training iter 50, batch loss 0.2366, batch acc 0.8294
10:00:29.450   Training iter 100, batch loss 0.2376, batch acc 0.8294
10:00:29.973   Training iter 150, batch loss 0.2400, batch acc 0.8168
10:00:30.507   Training iter 200, batch loss 0.2363, batch acc 0.8218
10:00:31.047   Training iter 250, batch loss 0.2399, batch acc 0.8180
10:00:31.583   Training iter 300, batch loss 0.2393, batch acc 0.8136
10:00:32.130   Training iter 350, batch loss 0.2373, batch acc 0.8164
10:00:32.667   Training iter 400, batch loss 0.2400, batch acc 0.8152
10:00:33.200   Training iter 450, batch loss 0.2374, batch acc 0.8200
10:00:33.740   Training iter 500, batch loss 0.2396, batch acc 0.8162
10:00:34.279   Training iter 550, batch loss 0.2357, batch acc 0.8228
10:00:34.820   Training iter 600, batch loss 0.2383, batch acc 0.8162
10:00:34.821 Training @ 290 epoch...
10:00:35.372   Training iter 50, batch loss 0.2382, batch acc 0.8220
10:00:35.916   Training iter 100, batch loss 0.2388, batch acc 0.8194
10:00:36.462   Training iter 150, batch loss 0.2385, batch acc 0.8196
10:00:37.002   Training iter 200, batch loss 0.2361, batch acc 0.8344
10:00:37.540   Training iter 250, batch loss 0.2371, batch acc 0.8222
10:00:38.064   Training iter 300, batch loss 0.2380, batch acc 0.8142
10:00:38.615   Training iter 350, batch loss 0.2378, batch acc 0.8190
10:00:39.132   Training iter 400, batch loss 0.2400, batch acc 0.8158
10:00:39.642   Training iter 450, batch loss 0.2376, batch acc 0.8206
10:00:40.177   Training iter 500, batch loss 0.2384, batch acc 0.8164
10:00:40.705   Training iter 550, batch loss 0.2378, batch acc 0.8150
10:00:41.206   Training iter 600, batch loss 0.2393, batch acc 0.8186
10:00:41.208 Testing @ 290 epoch...
10:00:41.244     Testing, total mean loss 0.23450, total acc 0.82850
10:00:41.244 Training @ 291 epoch...
10:00:41.757   Training iter 50, batch loss 0.2384, batch acc 0.8176
10:00:42.282   Training iter 100, batch loss 0.2408, batch acc 0.8118
10:00:42.817   Training iter 150, batch loss 0.2390, batch acc 0.8196
10:00:43.340   Training iter 200, batch loss 0.2384, batch acc 0.8184
10:00:43.835   Training iter 250, batch loss 0.2376, batch acc 0.8226
10:00:44.379   Training iter 300, batch loss 0.2393, batch acc 0.8172
10:00:45.091   Training iter 350, batch loss 0.2358, batch acc 0.8280
10:00:45.801   Training iter 400, batch loss 0.2364, batch acc 0.8240
10:00:46.441   Training iter 450, batch loss 0.2385, batch acc 0.8158
10:00:46.930   Training iter 500, batch loss 0.2348, batch acc 0.8258
10:00:47.400   Training iter 550, batch loss 0.2384, batch acc 0.8222
10:00:47.929   Training iter 600, batch loss 0.2399, batch acc 0.8194
10:00:47.931 Training @ 292 epoch...
10:00:48.425   Training iter 50, batch loss 0.2376, batch acc 0.8196
10:00:48.916   Training iter 100, batch loss 0.2365, batch acc 0.8198
10:00:49.435   Training iter 150, batch loss 0.2392, batch acc 0.8210
10:00:49.966   Training iter 200, batch loss 0.2379, batch acc 0.8212
10:00:50.515   Training iter 250, batch loss 0.2395, batch acc 0.8198
10:00:51.036   Training iter 300, batch loss 0.2360, batch acc 0.8246
10:00:51.543   Training iter 350, batch loss 0.2396, batch acc 0.8148
10:00:52.050   Training iter 400, batch loss 0.2380, batch acc 0.8254
10:00:52.569   Training iter 450, batch loss 0.2379, batch acc 0.8270
10:00:53.050   Training iter 500, batch loss 0.2376, batch acc 0.8158
10:00:53.552   Training iter 550, batch loss 0.2388, batch acc 0.8146
10:00:54.050   Training iter 600, batch loss 0.2379, batch acc 0.8164
10:00:54.051 Training @ 293 epoch...
10:00:54.563   Training iter 50, batch loss 0.2383, batch acc 0.8174
10:00:55.059   Training iter 100, batch loss 0.2394, batch acc 0.8166
10:00:55.555   Training iter 150, batch loss 0.2398, batch acc 0.8112
10:00:56.053   Training iter 200, batch loss 0.2380, batch acc 0.8188
10:00:56.541   Training iter 250, batch loss 0.2367, batch acc 0.8270
10:00:57.034   Training iter 300, batch loss 0.2387, batch acc 0.8188
10:00:57.559   Training iter 350, batch loss 0.2351, batch acc 0.8252
10:00:58.033   Training iter 400, batch loss 0.2365, batch acc 0.8240
10:00:58.498   Training iter 450, batch loss 0.2397, batch acc 0.8176
10:00:58.951   Training iter 500, batch loss 0.2354, batch acc 0.8296
10:00:59.399   Training iter 550, batch loss 0.2394, batch acc 0.8122
10:00:59.840   Training iter 600, batch loss 0.2392, batch acc 0.8204
10:00:59.842 Training @ 294 epoch...
10:01:00.299   Training iter 50, batch loss 0.2377, batch acc 0.8224
10:01:00.781   Training iter 100, batch loss 0.2394, batch acc 0.8202
10:01:01.281   Training iter 150, batch loss 0.2394, batch acc 0.8198
10:01:01.845   Training iter 200, batch loss 0.2371, batch acc 0.8174
10:01:02.367   Training iter 250, batch loss 0.2383, batch acc 0.8170
10:01:02.876   Training iter 300, batch loss 0.2397, batch acc 0.8128
10:01:03.390   Training iter 350, batch loss 0.2367, batch acc 0.8208
10:01:03.955   Training iter 400, batch loss 0.2385, batch acc 0.8258
10:01:04.522   Training iter 450, batch loss 0.2382, batch acc 0.8152
10:01:05.078   Training iter 500, batch loss 0.2357, batch acc 0.8272
10:01:05.633   Training iter 550, batch loss 0.2366, batch acc 0.8182
10:01:06.178   Training iter 600, batch loss 0.2383, batch acc 0.8258
10:01:06.180 Training @ 295 epoch...
10:01:06.718   Training iter 50, batch loss 0.2370, batch acc 0.8234
10:01:07.268   Training iter 100, batch loss 0.2383, batch acc 0.8254
10:01:07.784   Training iter 150, batch loss 0.2379, batch acc 0.8266
10:01:08.295   Training iter 200, batch loss 0.2370, batch acc 0.8198
10:01:08.822   Training iter 250, batch loss 0.2356, batch acc 0.8224
10:01:09.335   Training iter 300, batch loss 0.2369, batch acc 0.8190
10:01:09.842   Training iter 350, batch loss 0.2389, batch acc 0.8180
10:01:10.351   Training iter 400, batch loss 0.2408, batch acc 0.8140
10:01:10.839   Training iter 450, batch loss 0.2384, batch acc 0.8188
10:01:11.324   Training iter 500, batch loss 0.2382, batch acc 0.8190
10:01:11.797   Training iter 550, batch loss 0.2372, batch acc 0.8216
10:01:12.287   Training iter 600, batch loss 0.2389, batch acc 0.8138
10:01:12.289 Testing @ 295 epoch...
10:01:12.325     Testing, total mean loss 0.23430, total acc 0.82860
10:01:12.325 Training @ 296 epoch...
10:01:12.848   Training iter 50, batch loss 0.2380, batch acc 0.8150
10:01:13.366   Training iter 100, batch loss 0.2363, batch acc 0.8260
10:01:13.881   Training iter 150, batch loss 0.2383, batch acc 0.8214
10:01:14.399   Training iter 200, batch loss 0.2380, batch acc 0.8182
10:01:14.922   Training iter 250, batch loss 0.2382, batch acc 0.8156
10:01:15.480   Training iter 300, batch loss 0.2424, batch acc 0.8156
10:01:16.026   Training iter 350, batch loss 0.2386, batch acc 0.8166
10:01:16.558   Training iter 400, batch loss 0.2371, batch acc 0.8300
10:01:17.093   Training iter 450, batch loss 0.2381, batch acc 0.8178
10:01:17.637   Training iter 500, batch loss 0.2364, batch acc 0.8246
10:01:18.177   Training iter 550, batch loss 0.2362, batch acc 0.8210
10:01:18.713   Training iter 600, batch loss 0.2373, batch acc 0.8246
10:01:18.714 Training @ 297 epoch...
10:01:19.244   Training iter 50, batch loss 0.2382, batch acc 0.8178
10:01:19.768   Training iter 100, batch loss 0.2373, batch acc 0.8188
10:01:20.313   Training iter 150, batch loss 0.2411, batch acc 0.8120
10:01:20.861   Training iter 200, batch loss 0.2367, batch acc 0.8254
10:01:21.402   Training iter 250, batch loss 0.2393, batch acc 0.8196
10:01:21.920   Training iter 300, batch loss 0.2397, batch acc 0.8158
10:01:22.433   Training iter 350, batch loss 0.2366, batch acc 0.8238
10:01:22.968   Training iter 400, batch loss 0.2373, batch acc 0.8218
10:01:23.487   Training iter 450, batch loss 0.2382, batch acc 0.8264
10:01:24.013   Training iter 500, batch loss 0.2366, batch acc 0.8240
10:01:24.531   Training iter 550, batch loss 0.2368, batch acc 0.8190
10:01:25.047   Training iter 600, batch loss 0.2364, batch acc 0.8180
10:01:25.049 Training @ 298 epoch...
10:01:25.575   Training iter 50, batch loss 0.2412, batch acc 0.8128
10:01:26.091   Training iter 100, batch loss 0.2389, batch acc 0.8208
10:01:26.596   Training iter 150, batch loss 0.2350, batch acc 0.8258
10:01:27.086   Training iter 200, batch loss 0.2396, batch acc 0.8184
10:01:27.580   Training iter 250, batch loss 0.2351, batch acc 0.8246
10:01:28.078   Training iter 300, batch loss 0.2404, batch acc 0.8104
10:01:28.564   Training iter 350, batch loss 0.2388, batch acc 0.8172
10:01:29.060   Training iter 400, batch loss 0.2364, batch acc 0.8292
10:01:29.584   Training iter 450, batch loss 0.2371, batch acc 0.8192
10:01:30.124   Training iter 500, batch loss 0.2391, batch acc 0.8158
10:01:30.647   Training iter 550, batch loss 0.2354, batch acc 0.8282
10:01:31.155   Training iter 600, batch loss 0.2369, batch acc 0.8198
10:01:31.156 Training @ 299 epoch...
10:01:31.663   Training iter 50, batch loss 0.2351, batch acc 0.8278
10:01:32.221   Training iter 100, batch loss 0.2377, batch acc 0.8226
10:01:32.761   Training iter 150, batch loss 0.2366, batch acc 0.8230
10:01:33.275   Training iter 200, batch loss 0.2420, batch acc 0.8004
10:01:33.746   Training iter 250, batch loss 0.2387, batch acc 0.8216
10:01:34.284   Training iter 300, batch loss 0.2376, batch acc 0.8178
10:01:34.790   Training iter 350, batch loss 0.2385, batch acc 0.8234
10:01:35.290   Training iter 400, batch loss 0.2392, batch acc 0.8216
10:01:35.780   Training iter 450, batch loss 0.2406, batch acc 0.8162
10:01:36.277   Training iter 500, batch loss 0.2352, batch acc 0.8256
10:01:36.765   Training iter 550, batch loss 0.2348, batch acc 0.8252
10:01:37.278   Training iter 600, batch loss 0.2375, batch acc 0.8188
10:01:37.280 Training @ 300 epoch...
10:01:37.795   Training iter 50, batch loss 0.2376, batch acc 0.8194
10:01:38.306   Training iter 100, batch loss 0.2379, batch acc 0.8240
10:01:38.786   Training iter 150, batch loss 0.2396, batch acc 0.8150
10:01:39.274   Training iter 200, batch loss 0.2398, batch acc 0.8118
10:01:39.772   Training iter 250, batch loss 0.2377, batch acc 0.8240
10:01:40.270   Training iter 300, batch loss 0.2368, batch acc 0.8182
10:01:40.756   Training iter 350, batch loss 0.2365, batch acc 0.8272
10:01:41.266   Training iter 400, batch loss 0.2368, batch acc 0.8204
10:01:41.767   Training iter 450, batch loss 0.2393, batch acc 0.8178
10:01:42.259   Training iter 500, batch loss 0.2350, batch acc 0.8280
10:01:42.756   Training iter 550, batch loss 0.2388, batch acc 0.8188
10:01:43.325   Training iter 600, batch loss 0.2373, batch acc 0.8192
10:01:43.326 Testing @ 300 epoch...
10:01:43.363     Testing, total mean loss 0.23410, total acc 0.82900
10:01:43.363 Plot @ 300 epoch...
10:01:43.363 Training @ 301 epoch...
10:01:43.937   Training iter 50, batch loss 0.2372, batch acc 0.8270
10:01:44.530   Training iter 100, batch loss 0.2396, batch acc 0.8176
10:01:45.063   Training iter 150, batch loss 0.2382, batch acc 0.8156
10:01:45.599   Training iter 200, batch loss 0.2383, batch acc 0.8150
10:01:46.146   Training iter 250, batch loss 0.2364, batch acc 0.8238
10:01:46.701   Training iter 300, batch loss 0.2367, batch acc 0.8264
10:01:47.242   Training iter 350, batch loss 0.2378, batch acc 0.8162
10:01:47.788   Training iter 400, batch loss 0.2396, batch acc 0.8178
10:01:48.355   Training iter 450, batch loss 0.2356, batch acc 0.8274
10:01:48.900   Training iter 500, batch loss 0.2420, batch acc 0.8098
10:01:49.440   Training iter 550, batch loss 0.2368, batch acc 0.8198
10:01:49.991   Training iter 600, batch loss 0.2341, batch acc 0.8308
10:01:49.993 Training @ 302 epoch...
10:01:50.558   Training iter 50, batch loss 0.2375, batch acc 0.8190
10:01:51.093   Training iter 100, batch loss 0.2372, batch acc 0.8192
10:01:51.637   Training iter 150, batch loss 0.2396, batch acc 0.8134
10:01:52.196   Training iter 200, batch loss 0.2378, batch acc 0.8234
10:01:52.781   Training iter 250, batch loss 0.2372, batch acc 0.8226
10:01:53.370   Training iter 300, batch loss 0.2380, batch acc 0.8172
10:01:53.944   Training iter 350, batch loss 0.2336, batch acc 0.8320
10:01:54.511   Training iter 400, batch loss 0.2367, batch acc 0.8254
10:01:55.096   Training iter 450, batch loss 0.2392, batch acc 0.8188
10:01:55.662   Training iter 500, batch loss 0.2377, batch acc 0.8216
10:01:56.211   Training iter 550, batch loss 0.2386, batch acc 0.8200
10:01:56.774   Training iter 600, batch loss 0.2389, batch acc 0.8132
10:01:56.776 Training @ 303 epoch...
10:01:57.350   Training iter 50, batch loss 0.2379, batch acc 0.8172
10:01:57.913   Training iter 100, batch loss 0.2368, batch acc 0.8252
10:01:58.445   Training iter 150, batch loss 0.2402, batch acc 0.8140
10:01:58.943   Training iter 200, batch loss 0.2372, batch acc 0.8198
10:01:59.452   Training iter 250, batch loss 0.2370, batch acc 0.8218
10:01:59.966   Training iter 300, batch loss 0.2375, batch acc 0.8152
10:02:00.523   Training iter 350, batch loss 0.2366, batch acc 0.8228
10:02:01.074   Training iter 400, batch loss 0.2372, batch acc 0.8254
10:02:01.645   Training iter 450, batch loss 0.2400, batch acc 0.8172
10:02:02.201   Training iter 500, batch loss 0.2352, batch acc 0.8220
10:02:02.713   Training iter 550, batch loss 0.2393, batch acc 0.8214
10:02:03.226   Training iter 600, batch loss 0.2367, batch acc 0.8248
10:02:03.227 Training @ 304 epoch...
10:02:03.747   Training iter 50, batch loss 0.2373, batch acc 0.8196
10:02:04.286   Training iter 100, batch loss 0.2355, batch acc 0.8268
10:02:04.818   Training iter 150, batch loss 0.2407, batch acc 0.8068
10:02:05.351   Training iter 200, batch loss 0.2397, batch acc 0.8144
10:02:05.876   Training iter 250, batch loss 0.2363, batch acc 0.8178
10:02:06.389   Training iter 300, batch loss 0.2360, batch acc 0.8274
10:02:06.917   Training iter 350, batch loss 0.2401, batch acc 0.8116
10:02:07.400   Training iter 400, batch loss 0.2363, batch acc 0.8298
10:02:07.881   Training iter 450, batch loss 0.2372, batch acc 0.8246
10:02:08.349   Training iter 500, batch loss 0.2381, batch acc 0.8180
10:02:08.805   Training iter 550, batch loss 0.2363, batch acc 0.8312
10:02:09.274   Training iter 600, batch loss 0.2378, batch acc 0.8186
10:02:09.275 Training @ 305 epoch...
10:02:09.789   Training iter 50, batch loss 0.2343, batch acc 0.8262
10:02:10.326   Training iter 100, batch loss 0.2395, batch acc 0.8162
10:02:10.850   Training iter 150, batch loss 0.2385, batch acc 0.8170
10:02:11.363   Training iter 200, batch loss 0.2369, batch acc 0.8238
10:02:11.876   Training iter 250, batch loss 0.2365, batch acc 0.8242
10:02:12.399   Training iter 300, batch loss 0.2398, batch acc 0.8170
10:02:12.941   Training iter 350, batch loss 0.2384, batch acc 0.8266
10:02:13.477   Training iter 400, batch loss 0.2397, batch acc 0.8046
10:02:13.988   Training iter 450, batch loss 0.2381, batch acc 0.8206
10:02:14.503   Training iter 500, batch loss 0.2348, batch acc 0.8232
10:02:15.023   Training iter 550, batch loss 0.2385, batch acc 0.8218
10:02:15.545   Training iter 600, batch loss 0.2358, batch acc 0.8268
10:02:15.546 Testing @ 305 epoch...
10:02:15.582     Testing, total mean loss 0.23392, total acc 0.82910
10:02:15.582 Training @ 306 epoch...
10:02:16.103   Training iter 50, batch loss 0.2382, batch acc 0.8124
10:02:16.593   Training iter 100, batch loss 0.2368, batch acc 0.8222
10:02:17.097   Training iter 150, batch loss 0.2374, batch acc 0.8230
10:02:17.594   Training iter 200, batch loss 0.2358, batch acc 0.8224
10:02:18.098   Training iter 250, batch loss 0.2378, batch acc 0.8282
10:02:18.609   Training iter 300, batch loss 0.2381, batch acc 0.8192
10:02:19.129   Training iter 350, batch loss 0.2365, batch acc 0.8250
10:02:19.644   Training iter 400, batch loss 0.2414, batch acc 0.8090
10:02:20.147   Training iter 450, batch loss 0.2369, batch acc 0.8240
10:02:20.631   Training iter 500, batch loss 0.2382, batch acc 0.8180
10:02:21.117   Training iter 550, batch loss 0.2368, batch acc 0.8194
10:02:21.587   Training iter 600, batch loss 0.2365, batch acc 0.8244
10:02:21.588 Training @ 307 epoch...
10:02:22.035   Training iter 50, batch loss 0.2354, batch acc 0.8238
10:02:22.510   Training iter 100, batch loss 0.2389, batch acc 0.8238
10:02:22.992   Training iter 150, batch loss 0.2362, batch acc 0.8180
10:02:23.482   Training iter 200, batch loss 0.2397, batch acc 0.8168
10:02:23.979   Training iter 250, batch loss 0.2363, batch acc 0.8256
10:02:24.467   Training iter 300, batch loss 0.2371, batch acc 0.8156
10:02:24.963   Training iter 350, batch loss 0.2401, batch acc 0.8230
10:02:25.468   Training iter 400, batch loss 0.2372, batch acc 0.8250
10:02:25.956   Training iter 450, batch loss 0.2374, batch acc 0.8210
10:02:26.463   Training iter 500, batch loss 0.2360, batch acc 0.8256
10:02:26.948   Training iter 550, batch loss 0.2381, batch acc 0.8164
10:02:27.458   Training iter 600, batch loss 0.2375, batch acc 0.8146
10:02:27.459 Training @ 308 epoch...
10:02:27.955   Training iter 50, batch loss 0.2385, batch acc 0.8134
10:02:28.458   Training iter 100, batch loss 0.2371, batch acc 0.8190
10:02:28.958   Training iter 150, batch loss 0.2359, batch acc 0.8234
10:02:29.473   Training iter 200, batch loss 0.2391, batch acc 0.8204
10:02:29.986   Training iter 250, batch loss 0.2384, batch acc 0.8118
10:02:30.493   Training iter 300, batch loss 0.2379, batch acc 0.8228
10:02:31.003   Training iter 350, batch loss 0.2386, batch acc 0.8168
10:02:31.506   Training iter 400, batch loss 0.2382, batch acc 0.8216
10:02:32.020   Training iter 450, batch loss 0.2375, batch acc 0.8216
10:02:32.549   Training iter 500, batch loss 0.2363, batch acc 0.8268
10:02:33.090   Training iter 550, batch loss 0.2350, batch acc 0.8316
10:02:33.638   Training iter 600, batch loss 0.2369, batch acc 0.8190
10:02:33.640 Training @ 309 epoch...
10:02:34.210   Training iter 50, batch loss 0.2360, batch acc 0.8206
10:02:34.700   Training iter 100, batch loss 0.2398, batch acc 0.8222
10:02:35.161   Training iter 150, batch loss 0.2380, batch acc 0.8148
10:02:35.633   Training iter 200, batch loss 0.2363, batch acc 0.8202
10:02:36.108   Training iter 250, batch loss 0.2424, batch acc 0.8080
10:02:36.584   Training iter 300, batch loss 0.2363, batch acc 0.8286
10:02:37.078   Training iter 350, batch loss 0.2381, batch acc 0.8174
10:02:37.603   Training iter 400, batch loss 0.2339, batch acc 0.8352
10:02:38.131   Training iter 450, batch loss 0.2392, batch acc 0.8122
10:02:38.653   Training iter 500, batch loss 0.2351, batch acc 0.8284
10:02:39.178   Training iter 550, batch loss 0.2373, batch acc 0.8204
10:02:39.712   Training iter 600, batch loss 0.2368, batch acc 0.8216
10:02:39.714 Training @ 310 epoch...
10:02:40.254   Training iter 50, batch loss 0.2371, batch acc 0.8208
10:02:40.793   Training iter 100, batch loss 0.2399, batch acc 0.8130
10:02:41.349   Training iter 150, batch loss 0.2396, batch acc 0.8178
10:02:41.920   Training iter 200, batch loss 0.2393, batch acc 0.8140
10:02:42.497   Training iter 250, batch loss 0.2353, batch acc 0.8272
10:02:43.064   Training iter 300, batch loss 0.2363, batch acc 0.8254
10:02:43.557   Training iter 350, batch loss 0.2337, batch acc 0.8260
10:02:44.043   Training iter 400, batch loss 0.2399, batch acc 0.8154
10:02:44.574   Training iter 450, batch loss 0.2395, batch acc 0.8176
10:02:45.133   Training iter 500, batch loss 0.2359, batch acc 0.8298
10:02:45.634   Training iter 550, batch loss 0.2351, batch acc 0.8230
10:02:46.143   Training iter 600, batch loss 0.2371, batch acc 0.8206
10:02:46.145 Testing @ 310 epoch...
10:02:46.181     Testing, total mean loss 0.23375, total acc 0.82920
10:02:46.181 Training @ 311 epoch...
10:02:46.673   Training iter 50, batch loss 0.2359, batch acc 0.8336
10:02:47.122   Training iter 100, batch loss 0.2375, batch acc 0.8210
10:02:47.612   Training iter 150, batch loss 0.2357, batch acc 0.8280
10:02:48.124   Training iter 200, batch loss 0.2342, batch acc 0.8320
10:02:48.682   Training iter 250, batch loss 0.2372, batch acc 0.8198
10:02:49.266   Training iter 300, batch loss 0.2399, batch acc 0.8128
10:02:49.785   Training iter 350, batch loss 0.2395, batch acc 0.8184
10:02:50.312   Training iter 400, batch loss 0.2388, batch acc 0.8190
10:02:50.918   Training iter 450, batch loss 0.2384, batch acc 0.8102
10:02:51.467   Training iter 500, batch loss 0.2378, batch acc 0.8188
10:02:51.973   Training iter 550, batch loss 0.2374, batch acc 0.8142
10:02:52.489   Training iter 600, batch loss 0.2362, batch acc 0.8216
10:02:52.491 Training @ 312 epoch...
10:02:53.076   Training iter 50, batch loss 0.2370, batch acc 0.8236
10:02:53.671   Training iter 100, batch loss 0.2374, batch acc 0.8172
10:02:54.188   Training iter 150, batch loss 0.2396, batch acc 0.8202
10:02:54.703   Training iter 200, batch loss 0.2353, batch acc 0.8188
10:02:55.202   Training iter 250, batch loss 0.2373, batch acc 0.8216
10:02:55.711   Training iter 300, batch loss 0.2350, batch acc 0.8266
10:02:56.212   Training iter 350, batch loss 0.2377, batch acc 0.8230
10:02:56.720   Training iter 400, batch loss 0.2390, batch acc 0.8162
10:02:57.232   Training iter 450, batch loss 0.2366, batch acc 0.8216
10:02:57.737   Training iter 500, batch loss 0.2365, batch acc 0.8210
10:02:58.260   Training iter 550, batch loss 0.2389, batch acc 0.8184
10:02:58.776   Training iter 600, batch loss 0.2376, batch acc 0.8224
10:02:58.778 Training @ 313 epoch...
10:02:59.299   Training iter 50, batch loss 0.2365, batch acc 0.8224
10:02:59.798   Training iter 100, batch loss 0.2366, batch acc 0.8148
10:03:00.308   Training iter 150, batch loss 0.2399, batch acc 0.8162
10:03:00.831   Training iter 200, batch loss 0.2347, batch acc 0.8294
10:03:01.363   Training iter 250, batch loss 0.2376, batch acc 0.8258
10:03:01.871   Training iter 300, batch loss 0.2370, batch acc 0.8194
10:03:02.345   Training iter 350, batch loss 0.2354, batch acc 0.8220
10:03:02.806   Training iter 400, batch loss 0.2372, batch acc 0.8210
10:03:03.268   Training iter 450, batch loss 0.2408, batch acc 0.8136
10:03:03.749   Training iter 500, batch loss 0.2389, batch acc 0.8162
10:03:04.223   Training iter 550, batch loss 0.2362, batch acc 0.8274
10:03:04.692   Training iter 600, batch loss 0.2366, batch acc 0.8238
10:03:04.694 Training @ 314 epoch...
10:03:05.168   Training iter 50, batch loss 0.2379, batch acc 0.8214
10:03:05.642   Training iter 100, batch loss 0.2371, batch acc 0.8228
10:03:06.146   Training iter 150, batch loss 0.2378, batch acc 0.8164
10:03:06.668   Training iter 200, batch loss 0.2346, batch acc 0.8250
10:03:07.177   Training iter 250, batch loss 0.2371, batch acc 0.8192
10:03:07.697   Training iter 300, batch loss 0.2392, batch acc 0.8266
10:03:08.184   Training iter 350, batch loss 0.2368, batch acc 0.8200
10:03:08.679   Training iter 400, batch loss 0.2357, batch acc 0.8248
10:03:09.152   Training iter 450, batch loss 0.2365, batch acc 0.8252
10:03:09.634   Training iter 500, batch loss 0.2385, batch acc 0.8172
10:03:10.126   Training iter 550, batch loss 0.2392, batch acc 0.8100
10:03:10.628   Training iter 600, batch loss 0.2368, batch acc 0.8228
10:03:10.630 Training @ 315 epoch...
10:03:11.143   Training iter 50, batch loss 0.2356, batch acc 0.8242
10:03:11.665   Training iter 100, batch loss 0.2419, batch acc 0.8040
10:03:12.198   Training iter 150, batch loss 0.2379, batch acc 0.8184
10:03:12.732   Training iter 200, batch loss 0.2376, batch acc 0.8180
10:03:13.274   Training iter 250, batch loss 0.2354, batch acc 0.8258
10:03:13.819   Training iter 300, batch loss 0.2367, batch acc 0.8216
10:03:14.370   Training iter 350, batch loss 0.2332, batch acc 0.8346
10:03:14.925   Training iter 400, batch loss 0.2380, batch acc 0.8194
10:03:15.483   Training iter 450, batch loss 0.2376, batch acc 0.8196
10:03:16.031   Training iter 500, batch loss 0.2358, batch acc 0.8214
10:03:16.582   Training iter 550, batch loss 0.2376, batch acc 0.8254
10:03:17.147   Training iter 600, batch loss 0.2395, batch acc 0.8198
10:03:17.148 Testing @ 315 epoch...
10:03:17.185     Testing, total mean loss 0.23359, total acc 0.82960
10:03:17.185 Training @ 316 epoch...
10:03:17.729   Training iter 50, batch loss 0.2382, batch acc 0.8220
10:03:18.246   Training iter 100, batch loss 0.2375, batch acc 0.8180
10:03:18.765   Training iter 150, batch loss 0.2350, batch acc 0.8234
10:03:19.293   Training iter 200, batch loss 0.2387, batch acc 0.8196
10:03:19.831   Training iter 250, batch loss 0.2357, batch acc 0.8210
10:03:20.358   Training iter 300, batch loss 0.2383, batch acc 0.8172
10:03:20.873   Training iter 350, batch loss 0.2388, batch acc 0.8198
10:03:21.391   Training iter 400, batch loss 0.2407, batch acc 0.8146
10:03:21.917   Training iter 450, batch loss 0.2370, batch acc 0.8246
10:03:22.445   Training iter 500, batch loss 0.2368, batch acc 0.8178
10:03:22.993   Training iter 550, batch loss 0.2356, batch acc 0.8274
10:03:23.545   Training iter 600, batch loss 0.2342, batch acc 0.8260
10:03:23.547 Training @ 317 epoch...
10:03:24.093   Training iter 50, batch loss 0.2369, batch acc 0.8188
10:03:24.635   Training iter 100, batch loss 0.2371, batch acc 0.8182
10:03:25.203   Training iter 150, batch loss 0.2364, batch acc 0.8220
10:03:25.748   Training iter 200, batch loss 0.2383, batch acc 0.8218
10:03:26.307   Training iter 250, batch loss 0.2375, batch acc 0.8206
10:03:26.874   Training iter 300, batch loss 0.2382, batch acc 0.8234
10:03:27.437   Training iter 350, batch loss 0.2358, batch acc 0.8316
10:03:27.996   Training iter 400, batch loss 0.2361, batch acc 0.8270
10:03:28.555   Training iter 450, batch loss 0.2388, batch acc 0.8098
10:03:29.170   Training iter 500, batch loss 0.2384, batch acc 0.8122
10:03:29.749   Training iter 550, batch loss 0.2335, batch acc 0.8306
10:03:30.330   Training iter 600, batch loss 0.2392, batch acc 0.8152
10:03:30.332 Training @ 318 epoch...
10:03:30.915   Training iter 50, batch loss 0.2399, batch acc 0.8180
10:03:31.510   Training iter 100, batch loss 0.2351, batch acc 0.8294
10:03:32.089   Training iter 150, batch loss 0.2358, batch acc 0.8224
10:03:32.667   Training iter 200, batch loss 0.2390, batch acc 0.8162
10:03:33.272   Training iter 250, batch loss 0.2367, batch acc 0.8134
10:03:33.838   Training iter 300, batch loss 0.2359, batch acc 0.8258
10:03:34.381   Training iter 350, batch loss 0.2376, batch acc 0.8182
10:03:34.920   Training iter 400, batch loss 0.2359, batch acc 0.8268
10:03:35.451   Training iter 450, batch loss 0.2377, batch acc 0.8164
10:03:35.958   Training iter 500, batch loss 0.2385, batch acc 0.8206
10:03:36.476   Training iter 550, batch loss 0.2383, batch acc 0.8226
10:03:36.996   Training iter 600, batch loss 0.2354, batch acc 0.8240
10:03:36.998 Training @ 319 epoch...
10:03:37.538   Training iter 50, batch loss 0.2360, batch acc 0.8250
10:03:38.082   Training iter 100, batch loss 0.2349, batch acc 0.8240
10:03:38.613   Training iter 150, batch loss 0.2376, batch acc 0.8200
10:03:39.129   Training iter 200, batch loss 0.2402, batch acc 0.8130
10:03:39.694   Training iter 250, batch loss 0.2372, batch acc 0.8174
10:03:40.274   Training iter 300, batch loss 0.2346, batch acc 0.8318
10:03:40.848   Training iter 350, batch loss 0.2382, batch acc 0.8162
10:03:41.355   Training iter 400, batch loss 0.2397, batch acc 0.8196
10:03:41.855   Training iter 450, batch loss 0.2374, batch acc 0.8146
10:03:42.436   Training iter 500, batch loss 0.2374, batch acc 0.8174
10:03:43.122   Training iter 550, batch loss 0.2373, batch acc 0.8198
10:03:43.749   Training iter 600, batch loss 0.2347, batch acc 0.8340
10:03:43.751 Training @ 320 epoch...
10:03:44.298   Training iter 50, batch loss 0.2412, batch acc 0.8136
10:03:44.842   Training iter 100, batch loss 0.2375, batch acc 0.8186
10:03:45.387   Training iter 150, batch loss 0.2369, batch acc 0.8218
10:03:45.932   Training iter 200, batch loss 0.2379, batch acc 0.8188
10:03:46.503   Training iter 250, batch loss 0.2367, batch acc 0.8226
10:03:47.072   Training iter 300, batch loss 0.2375, batch acc 0.8206
10:03:47.638   Training iter 350, batch loss 0.2352, batch acc 0.8280
10:03:48.211   Training iter 400, batch loss 0.2391, batch acc 0.8174
10:03:48.797   Training iter 450, batch loss 0.2351, batch acc 0.8230
10:03:49.374   Training iter 500, batch loss 0.2363, batch acc 0.8210
10:03:49.924   Training iter 550, batch loss 0.2350, batch acc 0.8268
10:03:50.465   Training iter 600, batch loss 0.2365, batch acc 0.8232
10:03:50.467 Testing @ 320 epoch...
10:03:50.505     Testing, total mean loss 0.23344, total acc 0.82960
10:03:50.505 Training @ 321 epoch...
10:03:51.058   Training iter 50, batch loss 0.2375, batch acc 0.8258
10:03:51.591   Training iter 100, batch loss 0.2368, batch acc 0.8224
10:03:52.117   Training iter 150, batch loss 0.2342, batch acc 0.8238
10:03:52.637   Training iter 200, batch loss 0.2359, batch acc 0.8216
10:03:53.171   Training iter 250, batch loss 0.2371, batch acc 0.8172
10:03:53.711   Training iter 300, batch loss 0.2387, batch acc 0.8132
10:03:54.258   Training iter 350, batch loss 0.2388, batch acc 0.8236
10:03:54.811   Training iter 400, batch loss 0.2369, batch acc 0.8180
10:03:55.403   Training iter 450, batch loss 0.2374, batch acc 0.8210
10:03:56.042   Training iter 500, batch loss 0.2359, batch acc 0.8254
10:03:56.674   Training iter 550, batch loss 0.2386, batch acc 0.8160
10:03:57.201   Training iter 600, batch loss 0.2368, batch acc 0.8220
10:03:57.203 Training @ 322 epoch...
10:03:57.746   Training iter 50, batch loss 0.2376, batch acc 0.8208
10:03:58.315   Training iter 100, batch loss 0.2381, batch acc 0.8150
10:03:58.888   Training iter 150, batch loss 0.2387, batch acc 0.8152
10:03:59.478   Training iter 200, batch loss 0.2370, batch acc 0.8226
10:04:00.100   Training iter 250, batch loss 0.2356, batch acc 0.8254
10:04:00.723   Training iter 300, batch loss 0.2397, batch acc 0.8160
10:04:01.332   Training iter 350, batch loss 0.2355, batch acc 0.8232
10:04:01.954   Training iter 400, batch loss 0.2348, batch acc 0.8232
10:04:02.548   Training iter 450, batch loss 0.2361, batch acc 0.8230
10:04:03.152   Training iter 500, batch loss 0.2361, batch acc 0.8248
10:04:03.754   Training iter 550, batch loss 0.2390, batch acc 0.8166
10:04:04.354   Training iter 600, batch loss 0.2359, batch acc 0.8280
10:04:04.356 Training @ 323 epoch...
10:04:04.967   Training iter 50, batch loss 0.2342, batch acc 0.8238
10:04:05.566   Training iter 100, batch loss 0.2385, batch acc 0.8242
10:04:06.158   Training iter 150, batch loss 0.2349, batch acc 0.8274
10:04:06.762   Training iter 200, batch loss 0.2390, batch acc 0.8210
10:04:07.370   Training iter 250, batch loss 0.2359, batch acc 0.8204
10:04:07.935   Training iter 300, batch loss 0.2377, batch acc 0.8172
10:04:08.520   Training iter 350, batch loss 0.2330, batch acc 0.8240
10:04:09.104   Training iter 400, batch loss 0.2390, batch acc 0.8180
10:04:09.636   Training iter 450, batch loss 0.2405, batch acc 0.8138
10:04:10.156   Training iter 500, batch loss 0.2360, batch acc 0.8214
10:04:10.676   Training iter 550, batch loss 0.2356, batch acc 0.8250
10:04:11.202   Training iter 600, batch loss 0.2398, batch acc 0.8166
10:04:11.204 Training @ 324 epoch...
10:04:11.723   Training iter 50, batch loss 0.2348, batch acc 0.8248
10:04:12.236   Training iter 100, batch loss 0.2363, batch acc 0.8278
10:04:12.792   Training iter 150, batch loss 0.2357, batch acc 0.8228
10:04:13.348   Training iter 200, batch loss 0.2360, batch acc 0.8250
10:04:13.902   Training iter 250, batch loss 0.2371, batch acc 0.8216
10:04:14.417   Training iter 300, batch loss 0.2370, batch acc 0.8194
10:04:14.939   Training iter 350, batch loss 0.2395, batch acc 0.8124
10:04:15.464   Training iter 400, batch loss 0.2388, batch acc 0.8178
10:04:15.977   Training iter 450, batch loss 0.2343, batch acc 0.8302
10:04:16.521   Training iter 500, batch loss 0.2382, batch acc 0.8162
10:04:17.039   Training iter 550, batch loss 0.2393, batch acc 0.8124
10:04:17.544   Training iter 600, batch loss 0.2367, batch acc 0.8226
10:04:17.546 Training @ 325 epoch...
10:04:18.055   Training iter 50, batch loss 0.2394, batch acc 0.8182
10:04:18.567   Training iter 100, batch loss 0.2367, batch acc 0.8170
10:04:19.081   Training iter 150, batch loss 0.2377, batch acc 0.8252
10:04:19.592   Training iter 200, batch loss 0.2360, batch acc 0.8242
10:04:20.109   Training iter 250, batch loss 0.2385, batch acc 0.8214
10:04:20.614   Training iter 300, batch loss 0.2382, batch acc 0.8142
10:04:21.121   Training iter 350, batch loss 0.2340, batch acc 0.8278
10:04:21.631   Training iter 400, batch loss 0.2371, batch acc 0.8216
10:04:22.157   Training iter 450, batch loss 0.2370, batch acc 0.8242
10:04:22.667   Training iter 500, batch loss 0.2348, batch acc 0.8228
10:04:23.172   Training iter 550, batch loss 0.2372, batch acc 0.8174
10:04:23.680   Training iter 600, batch loss 0.2368, batch acc 0.8234
10:04:23.682 Testing @ 325 epoch...
10:04:23.718     Testing, total mean loss 0.23329, total acc 0.82980
10:04:23.718 Training @ 326 epoch...
10:04:24.230   Training iter 50, batch loss 0.2366, batch acc 0.8196
10:04:24.747   Training iter 100, batch loss 0.2373, batch acc 0.8268
10:04:25.271   Training iter 150, batch loss 0.2374, batch acc 0.8146
10:04:25.781   Training iter 200, batch loss 0.2374, batch acc 0.8240
10:04:26.290   Training iter 250, batch loss 0.2367, batch acc 0.8206
10:04:26.794   Training iter 300, batch loss 0.2372, batch acc 0.8168
10:04:27.307   Training iter 350, batch loss 0.2386, batch acc 0.8194
10:04:27.823   Training iter 400, batch loss 0.2359, batch acc 0.8262
10:04:28.340   Training iter 450, batch loss 0.2361, batch acc 0.8282
10:04:28.866   Training iter 500, batch loss 0.2361, batch acc 0.8172
10:04:29.392   Training iter 550, batch loss 0.2357, batch acc 0.8228
10:04:29.917   Training iter 600, batch loss 0.2379, batch acc 0.8200
10:04:29.918 Training @ 327 epoch...
10:04:30.444   Training iter 50, batch loss 0.2364, batch acc 0.8166
10:04:30.942   Training iter 100, batch loss 0.2377, batch acc 0.8180
10:04:31.443   Training iter 150, batch loss 0.2366, batch acc 0.8292
10:04:31.973   Training iter 200, batch loss 0.2373, batch acc 0.8194
10:04:32.525   Training iter 250, batch loss 0.2385, batch acc 0.8178
10:04:33.079   Training iter 300, batch loss 0.2354, batch acc 0.8182
10:04:33.627   Training iter 350, batch loss 0.2382, batch acc 0.8200
10:04:34.192   Training iter 400, batch loss 0.2377, batch acc 0.8222
10:04:34.754   Training iter 450, batch loss 0.2368, batch acc 0.8178
10:04:35.314   Training iter 500, batch loss 0.2378, batch acc 0.8202
10:04:35.879   Training iter 550, batch loss 0.2367, batch acc 0.8282
10:04:36.443   Training iter 600, batch loss 0.2333, batch acc 0.8290
10:04:36.446 Training @ 328 epoch...
10:04:36.989   Training iter 50, batch loss 0.2357, batch acc 0.8252
10:04:37.514   Training iter 100, batch loss 0.2378, batch acc 0.8188
10:04:38.025   Training iter 150, batch loss 0.2367, batch acc 0.8208
10:04:38.541   Training iter 200, batch loss 0.2372, batch acc 0.8244
10:04:39.054   Training iter 250, batch loss 0.2346, batch acc 0.8228
10:04:39.561   Training iter 300, batch loss 0.2381, batch acc 0.8142
10:04:40.077   Training iter 350, batch loss 0.2374, batch acc 0.8270
10:04:40.590   Training iter 400, batch loss 0.2341, batch acc 0.8226
10:04:41.100   Training iter 450, batch loss 0.2385, batch acc 0.8200
10:04:41.605   Training iter 500, batch loss 0.2363, batch acc 0.8276
10:04:42.127   Training iter 550, batch loss 0.2369, batch acc 0.8190
10:04:42.650   Training iter 600, batch loss 0.2390, batch acc 0.8130
10:04:42.652 Training @ 329 epoch...
10:04:43.185   Training iter 50, batch loss 0.2390, batch acc 0.8198
10:04:43.685   Training iter 100, batch loss 0.2372, batch acc 0.8182
10:04:44.203   Training iter 150, batch loss 0.2348, batch acc 0.8346
10:04:44.733   Training iter 200, batch loss 0.2358, batch acc 0.8338
10:04:45.251   Training iter 250, batch loss 0.2345, batch acc 0.8174
10:04:45.776   Training iter 300, batch loss 0.2373, batch acc 0.8226
10:04:46.258   Training iter 350, batch loss 0.2355, batch acc 0.8318
10:04:46.738   Training iter 400, batch loss 0.2368, batch acc 0.8206
10:04:47.221   Training iter 450, batch loss 0.2372, batch acc 0.8188
10:04:47.699   Training iter 500, batch loss 0.2403, batch acc 0.8082
10:04:48.189   Training iter 550, batch loss 0.2375, batch acc 0.8158
10:04:48.664   Training iter 600, batch loss 0.2361, batch acc 0.8148
10:04:48.665 Training @ 330 epoch...
10:04:49.150   Training iter 50, batch loss 0.2367, batch acc 0.8278
10:04:49.639   Training iter 100, batch loss 0.2346, batch acc 0.8214
10:04:50.153   Training iter 150, batch loss 0.2390, batch acc 0.8164
10:04:50.660   Training iter 200, batch loss 0.2361, batch acc 0.8274
10:04:51.173   Training iter 250, batch loss 0.2352, batch acc 0.8266
10:04:51.682   Training iter 300, batch loss 0.2367, batch acc 0.8194
10:04:52.198   Training iter 350, batch loss 0.2384, batch acc 0.8172
10:04:52.717   Training iter 400, batch loss 0.2358, batch acc 0.8220
10:04:53.237   Training iter 450, batch loss 0.2381, batch acc 0.8146
10:04:53.747   Training iter 500, batch loss 0.2370, batch acc 0.8158
10:04:54.254   Training iter 550, batch loss 0.2395, batch acc 0.8184
10:04:54.796   Training iter 600, batch loss 0.2345, batch acc 0.8276
10:04:54.797 Testing @ 330 epoch...
10:04:54.836     Testing, total mean loss 0.23315, total acc 0.83020
10:04:54.836 Training @ 331 epoch...
10:04:55.360   Training iter 50, batch loss 0.2352, batch acc 0.8178
10:04:55.863   Training iter 100, batch loss 0.2366, batch acc 0.8294
10:04:56.393   Training iter 150, batch loss 0.2355, batch acc 0.8252
10:04:56.920   Training iter 200, batch loss 0.2404, batch acc 0.8160
10:04:57.470   Training iter 250, batch loss 0.2364, batch acc 0.8194
10:04:58.011   Training iter 300, batch loss 0.2368, batch acc 0.8116
10:04:58.553   Training iter 350, batch loss 0.2366, batch acc 0.8212
10:04:59.074   Training iter 400, batch loss 0.2357, batch acc 0.8286
10:04:59.584   Training iter 450, batch loss 0.2362, batch acc 0.8274
10:05:00.114   Training iter 500, batch loss 0.2378, batch acc 0.8220
10:05:00.657   Training iter 550, batch loss 0.2392, batch acc 0.8136
10:05:01.240   Training iter 600, batch loss 0.2349, batch acc 0.8266
10:05:01.242 Training @ 332 epoch...
10:05:01.789   Training iter 50, batch loss 0.2380, batch acc 0.8256
10:05:02.303   Training iter 100, batch loss 0.2374, batch acc 0.8212
10:05:02.815   Training iter 150, batch loss 0.2362, batch acc 0.8252
10:05:03.320   Training iter 200, batch loss 0.2365, batch acc 0.8254
10:05:03.802   Training iter 250, batch loss 0.2378, batch acc 0.8172
10:05:04.294   Training iter 300, batch loss 0.2350, batch acc 0.8244
10:05:04.792   Training iter 350, batch loss 0.2358, batch acc 0.8248
10:05:05.295   Training iter 400, batch loss 0.2371, batch acc 0.8174
10:05:05.775   Training iter 450, batch loss 0.2348, batch acc 0.8212
10:05:06.246   Training iter 500, batch loss 0.2385, batch acc 0.8188
10:05:06.760   Training iter 550, batch loss 0.2361, batch acc 0.8214
10:05:07.276   Training iter 600, batch loss 0.2376, batch acc 0.8154
10:05:07.277 Training @ 333 epoch...
10:05:07.790   Training iter 50, batch loss 0.2366, batch acc 0.8256
10:05:08.322   Training iter 100, batch loss 0.2352, batch acc 0.8202
10:05:08.837   Training iter 150, batch loss 0.2377, batch acc 0.8196
10:05:09.351   Training iter 200, batch loss 0.2380, batch acc 0.8232
10:05:09.864   Training iter 250, batch loss 0.2369, batch acc 0.8166
10:05:10.522   Training iter 300, batch loss 0.2382, batch acc 0.8176
10:05:11.183   Training iter 350, batch loss 0.2348, batch acc 0.8326
10:05:11.722   Training iter 400, batch loss 0.2362, batch acc 0.8298
10:05:12.204   Training iter 450, batch loss 0.2386, batch acc 0.8174
10:05:12.647   Training iter 500, batch loss 0.2368, batch acc 0.8122
10:05:13.103   Training iter 550, batch loss 0.2360, batch acc 0.8178
10:05:13.553   Training iter 600, batch loss 0.2356, batch acc 0.8280
10:05:13.555 Training @ 334 epoch...
10:05:14.024   Training iter 50, batch loss 0.2355, batch acc 0.8278
10:05:14.481   Training iter 100, batch loss 0.2375, batch acc 0.8244
10:05:14.950   Training iter 150, batch loss 0.2377, batch acc 0.8176
10:05:15.394   Training iter 200, batch loss 0.2384, batch acc 0.8176
10:05:15.835   Training iter 250, batch loss 0.2326, batch acc 0.8300
10:05:16.298   Training iter 300, batch loss 0.2359, batch acc 0.8234
10:05:16.777   Training iter 350, batch loss 0.2357, batch acc 0.8178
10:05:17.263   Training iter 400, batch loss 0.2371, batch acc 0.8222
10:05:17.745   Training iter 450, batch loss 0.2374, batch acc 0.8204
10:05:18.228   Training iter 500, batch loss 0.2393, batch acc 0.8132
10:05:18.712   Training iter 550, batch loss 0.2365, batch acc 0.8254
10:05:19.183   Training iter 600, batch loss 0.2367, batch acc 0.8200
10:05:19.185 Training @ 335 epoch...
10:05:19.728   Training iter 50, batch loss 0.2342, batch acc 0.8262
10:05:20.277   Training iter 100, batch loss 0.2359, batch acc 0.8230
10:05:20.827   Training iter 150, batch loss 0.2386, batch acc 0.8138
10:05:21.389   Training iter 200, batch loss 0.2366, batch acc 0.8230
10:05:21.938   Training iter 250, batch loss 0.2340, batch acc 0.8230
10:05:22.489   Training iter 300, batch loss 0.2389, batch acc 0.8204
10:05:23.028   Training iter 350, batch loss 0.2383, batch acc 0.8196
10:05:23.544   Training iter 400, batch loss 0.2381, batch acc 0.8206
10:05:24.059   Training iter 450, batch loss 0.2385, batch acc 0.8210
10:05:24.572   Training iter 500, batch loss 0.2353, batch acc 0.8238
10:05:25.092   Training iter 550, batch loss 0.2368, batch acc 0.8188
10:05:25.598   Training iter 600, batch loss 0.2349, batch acc 0.8266
10:05:25.600 Testing @ 335 epoch...
10:05:25.635     Testing, total mean loss 0.23302, total acc 0.82990
10:05:25.635 Training @ 336 epoch...
10:05:26.115   Training iter 50, batch loss 0.2383, batch acc 0.8168
10:05:26.603   Training iter 100, batch loss 0.2347, batch acc 0.8198
10:05:27.116   Training iter 150, batch loss 0.2352, batch acc 0.8244
10:05:27.622   Training iter 200, batch loss 0.2348, batch acc 0.8254
10:05:28.134   Training iter 250, batch loss 0.2365, batch acc 0.8222
10:05:28.646   Training iter 300, batch loss 0.2378, batch acc 0.8172
10:05:29.159   Training iter 350, batch loss 0.2342, batch acc 0.8252
10:05:29.662   Training iter 400, batch loss 0.2374, batch acc 0.8256
10:05:30.165   Training iter 450, batch loss 0.2356, batch acc 0.8202
10:05:30.638   Training iter 500, batch loss 0.2399, batch acc 0.8148
10:05:31.111   Training iter 550, batch loss 0.2379, batch acc 0.8278
10:05:31.563   Training iter 600, batch loss 0.2376, batch acc 0.8194
10:05:31.564 Training @ 337 epoch...
10:05:32.014   Training iter 50, batch loss 0.2373, batch acc 0.8210
10:05:32.471   Training iter 100, batch loss 0.2361, batch acc 0.8220
10:05:32.981   Training iter 150, batch loss 0.2358, batch acc 0.8178
10:05:33.463   Training iter 200, batch loss 0.2368, batch acc 0.8210
10:05:33.952   Training iter 250, batch loss 0.2347, batch acc 0.8296
10:05:34.423   Training iter 300, batch loss 0.2359, batch acc 0.8256
10:05:34.933   Training iter 350, batch loss 0.2373, batch acc 0.8194
10:05:35.439   Training iter 400, batch loss 0.2366, batch acc 0.8198
10:05:35.914   Training iter 450, batch loss 0.2381, batch acc 0.8134
10:05:36.410   Training iter 500, batch loss 0.2352, batch acc 0.8280
10:05:36.898   Training iter 550, batch loss 0.2383, batch acc 0.8184
10:05:37.399   Training iter 600, batch loss 0.2374, batch acc 0.8260
10:05:37.400 Training @ 338 epoch...
10:05:37.912   Training iter 50, batch loss 0.2357, batch acc 0.8256
10:05:38.432   Training iter 100, batch loss 0.2405, batch acc 0.8132
10:05:38.885   Training iter 150, batch loss 0.2358, batch acc 0.8166
10:05:39.344   Training iter 200, batch loss 0.2367, batch acc 0.8188
10:05:39.819   Training iter 250, batch loss 0.2345, batch acc 0.8288
10:05:40.309   Training iter 300, batch loss 0.2375, batch acc 0.8198
10:05:40.793   Training iter 350, batch loss 0.2365, batch acc 0.8226
10:05:41.240   Training iter 400, batch loss 0.2381, batch acc 0.8250
10:05:41.703   Training iter 450, batch loss 0.2363, batch acc 0.8226
10:05:42.173   Training iter 500, batch loss 0.2362, batch acc 0.8220
10:05:42.634   Training iter 550, batch loss 0.2369, batch acc 0.8218
10:05:43.115   Training iter 600, batch loss 0.2345, batch acc 0.8238
10:05:43.117 Training @ 339 epoch...
10:05:43.578   Training iter 50, batch loss 0.2385, batch acc 0.8106
10:05:44.021   Training iter 100, batch loss 0.2371, batch acc 0.8166
10:05:44.476   Training iter 150, batch loss 0.2357, batch acc 0.8228
10:05:44.942   Training iter 200, batch loss 0.2377, batch acc 0.8272
10:05:45.421   Training iter 250, batch loss 0.2366, batch acc 0.8210
10:05:45.884   Training iter 300, batch loss 0.2369, batch acc 0.8246
10:05:46.347   Training iter 350, batch loss 0.2366, batch acc 0.8250
10:05:46.804   Training iter 400, batch loss 0.2367, batch acc 0.8138
10:05:47.254   Training iter 450, batch loss 0.2355, batch acc 0.8284
10:05:47.709   Training iter 500, batch loss 0.2350, batch acc 0.8266
10:05:48.176   Training iter 550, batch loss 0.2364, batch acc 0.8252
10:05:48.634   Training iter 600, batch loss 0.2362, batch acc 0.8190
10:05:48.635 Training @ 340 epoch...
10:05:49.141   Training iter 50, batch loss 0.2361, batch acc 0.8284
10:05:49.637   Training iter 100, batch loss 0.2362, batch acc 0.8214
10:05:50.139   Training iter 150, batch loss 0.2357, batch acc 0.8276
10:05:50.611   Training iter 200, batch loss 0.2389, batch acc 0.8166
10:05:51.088   Training iter 250, batch loss 0.2371, batch acc 0.8240
10:05:51.534   Training iter 300, batch loss 0.2374, batch acc 0.8168
10:05:51.964   Training iter 350, batch loss 0.2370, batch acc 0.8196
10:05:52.410   Training iter 400, batch loss 0.2331, batch acc 0.8310
10:05:52.874   Training iter 450, batch loss 0.2357, batch acc 0.8224
10:05:53.336   Training iter 500, batch loss 0.2352, batch acc 0.8220
10:05:53.788   Training iter 550, batch loss 0.2383, batch acc 0.8204
10:05:54.244   Training iter 600, batch loss 0.2377, batch acc 0.8138
10:05:54.246 Testing @ 340 epoch...
10:05:54.281     Testing, total mean loss 0.23290, total acc 0.83030
10:05:54.281 Training @ 341 epoch...
10:05:54.750   Training iter 50, batch loss 0.2356, batch acc 0.8258
10:05:55.261   Training iter 100, batch loss 0.2340, batch acc 0.8266
10:05:55.728   Training iter 150, batch loss 0.2343, batch acc 0.8272
10:05:56.197   Training iter 200, batch loss 0.2369, batch acc 0.8208
10:05:56.649   Training iter 250, batch loss 0.2356, batch acc 0.8248
10:05:57.105   Training iter 300, batch loss 0.2372, batch acc 0.8234
10:05:57.563   Training iter 350, batch loss 0.2390, batch acc 0.8122
10:05:58.014   Training iter 400, batch loss 0.2394, batch acc 0.8134
10:05:58.486   Training iter 450, batch loss 0.2367, batch acc 0.8218
10:05:58.932   Training iter 500, batch loss 0.2359, batch acc 0.8218
10:05:59.373   Training iter 550, batch loss 0.2389, batch acc 0.8228
10:05:59.800   Training iter 600, batch loss 0.2348, batch acc 0.8224
10:05:59.802 Training @ 342 epoch...
10:06:00.248   Training iter 50, batch loss 0.2345, batch acc 0.8288
10:06:00.732   Training iter 100, batch loss 0.2358, batch acc 0.8266
10:06:01.208   Training iter 150, batch loss 0.2367, batch acc 0.8238
10:06:01.747   Training iter 200, batch loss 0.2358, batch acc 0.8204
10:06:02.283   Training iter 250, batch loss 0.2374, batch acc 0.8134
10:06:02.783   Training iter 300, batch loss 0.2358, batch acc 0.8208
10:06:03.295   Training iter 350, batch loss 0.2354, batch acc 0.8212
10:06:03.795   Training iter 400, batch loss 0.2361, batch acc 0.8244
10:06:04.291   Training iter 450, batch loss 0.2356, batch acc 0.8268
10:06:04.813   Training iter 500, batch loss 0.2380, batch acc 0.8230
10:06:05.379   Training iter 550, batch loss 0.2381, batch acc 0.8158
10:06:05.955   Training iter 600, batch loss 0.2388, batch acc 0.8176
10:06:05.957 Training @ 343 epoch...
10:06:06.480   Training iter 50, batch loss 0.2337, batch acc 0.8296
10:06:06.981   Training iter 100, batch loss 0.2359, batch acc 0.8202
10:06:07.479   Training iter 150, batch loss 0.2368, batch acc 0.8190
10:06:07.994   Training iter 200, batch loss 0.2352, batch acc 0.8194
10:06:08.558   Training iter 250, batch loss 0.2358, batch acc 0.8284
10:06:09.134   Training iter 300, batch loss 0.2384, batch acc 0.8198
10:06:09.708   Training iter 350, batch loss 0.2374, batch acc 0.8238
10:06:10.229   Training iter 400, batch loss 0.2363, batch acc 0.8182
10:06:10.733   Training iter 450, batch loss 0.2386, batch acc 0.8214
10:06:11.245   Training iter 500, batch loss 0.2344, batch acc 0.8178
10:06:11.746   Training iter 550, batch loss 0.2347, batch acc 0.8294
10:06:12.243   Training iter 600, batch loss 0.2404, batch acc 0.8186
10:06:12.245 Training @ 344 epoch...
10:06:12.750   Training iter 50, batch loss 0.2358, batch acc 0.8278
10:06:13.263   Training iter 100, batch loss 0.2352, batch acc 0.8280
10:06:13.765   Training iter 150, batch loss 0.2388, batch acc 0.8194
10:06:14.247   Training iter 200, batch loss 0.2369, batch acc 0.8200
10:06:14.722   Training iter 250, batch loss 0.2353, batch acc 0.8312
10:06:15.220   Training iter 300, batch loss 0.2364, batch acc 0.8184
10:06:15.734   Training iter 350, batch loss 0.2372, batch acc 0.8190
10:06:16.234   Training iter 400, batch loss 0.2349, batch acc 0.8206
10:06:16.737   Training iter 450, batch loss 0.2379, batch acc 0.8174
10:06:17.240   Training iter 500, batch loss 0.2329, batch acc 0.8290
10:06:17.740   Training iter 550, batch loss 0.2393, batch acc 0.8126
10:06:18.250   Training iter 600, batch loss 0.2368, batch acc 0.8208
10:06:18.252 Training @ 345 epoch...
10:06:18.737   Training iter 50, batch loss 0.2363, batch acc 0.8246
10:06:19.219   Training iter 100, batch loss 0.2354, batch acc 0.8292
10:06:19.706   Training iter 150, batch loss 0.2355, batch acc 0.8234
10:06:20.190   Training iter 200, batch loss 0.2365, batch acc 0.8202
10:06:20.678   Training iter 250, batch loss 0.2382, batch acc 0.8244
10:06:21.161   Training iter 300, batch loss 0.2354, batch acc 0.8244
10:06:21.638   Training iter 350, batch loss 0.2365, batch acc 0.8244
10:06:22.142   Training iter 400, batch loss 0.2363, batch acc 0.8220
10:06:22.650   Training iter 450, batch loss 0.2371, batch acc 0.8180
10:06:23.146   Training iter 500, batch loss 0.2345, batch acc 0.8234
10:06:23.626   Training iter 550, batch loss 0.2389, batch acc 0.8144
10:06:24.135   Training iter 600, batch loss 0.2363, batch acc 0.8174
10:06:24.136 Testing @ 345 epoch...
10:06:24.171     Testing, total mean loss 0.23278, total acc 0.83080
10:06:24.171 Training @ 346 epoch...
10:06:24.695   Training iter 50, batch loss 0.2359, batch acc 0.8208
10:06:25.221   Training iter 100, batch loss 0.2373, batch acc 0.8214
10:06:25.735   Training iter 150, batch loss 0.2361, batch acc 0.8220
10:06:26.254   Training iter 200, batch loss 0.2368, batch acc 0.8254
10:06:26.770   Training iter 250, batch loss 0.2330, batch acc 0.8300
10:06:27.284   Training iter 300, batch loss 0.2372, batch acc 0.8248
10:06:27.794   Training iter 350, batch loss 0.2372, batch acc 0.8212
10:06:28.301   Training iter 400, batch loss 0.2407, batch acc 0.8088
10:06:28.808   Training iter 450, batch loss 0.2379, batch acc 0.8182
10:06:29.302   Training iter 500, batch loss 0.2344, batch acc 0.8284
10:06:29.793   Training iter 550, batch loss 0.2379, batch acc 0.8182
10:06:30.282   Training iter 600, batch loss 0.2324, batch acc 0.8262
10:06:30.284 Training @ 347 epoch...
10:06:30.773   Training iter 50, batch loss 0.2370, batch acc 0.8190
10:06:31.261   Training iter 100, batch loss 0.2353, batch acc 0.8248
10:06:31.738   Training iter 150, batch loss 0.2369, batch acc 0.8212
10:06:32.225   Training iter 200, batch loss 0.2381, batch acc 0.8156
10:06:32.709   Training iter 250, batch loss 0.2339, batch acc 0.8258
10:06:33.212   Training iter 300, batch loss 0.2366, batch acc 0.8162
10:06:33.652   Training iter 350, batch loss 0.2362, batch acc 0.8218
10:06:34.083   Training iter 400, batch loss 0.2373, batch acc 0.8172
10:06:34.514   Training iter 450, batch loss 0.2350, batch acc 0.8258
10:06:34.963   Training iter 500, batch loss 0.2366, batch acc 0.8244
10:06:35.414   Training iter 550, batch loss 0.2372, batch acc 0.8308
10:06:35.858   Training iter 600, batch loss 0.2365, batch acc 0.8232
10:06:35.859 Training @ 348 epoch...
10:06:36.299   Training iter 50, batch loss 0.2348, batch acc 0.8296
10:06:36.739   Training iter 100, batch loss 0.2376, batch acc 0.8186
10:06:37.201   Training iter 150, batch loss 0.2339, batch acc 0.8228
10:06:37.686   Training iter 200, batch loss 0.2360, batch acc 0.8208
10:06:38.177   Training iter 250, batch loss 0.2397, batch acc 0.8222
10:06:38.665   Training iter 300, batch loss 0.2366, batch acc 0.8174
10:06:39.140   Training iter 350, batch loss 0.2372, batch acc 0.8166
10:06:39.623   Training iter 400, batch loss 0.2352, batch acc 0.8238
10:06:40.126   Training iter 450, batch loss 0.2348, batch acc 0.8262
10:06:40.643   Training iter 500, batch loss 0.2360, batch acc 0.8258
10:06:41.160   Training iter 550, batch loss 0.2385, batch acc 0.8168
10:06:41.676   Training iter 600, batch loss 0.2361, batch acc 0.8256
10:06:41.678 Training @ 349 epoch...
10:06:42.211   Training iter 50, batch loss 0.2376, batch acc 0.8180
10:06:42.710   Training iter 100, batch loss 0.2369, batch acc 0.8206
10:06:43.218   Training iter 150, batch loss 0.2376, batch acc 0.8222
10:06:43.712   Training iter 200, batch loss 0.2385, batch acc 0.8130
10:06:44.212   Training iter 250, batch loss 0.2359, batch acc 0.8200
10:06:44.714   Training iter 300, batch loss 0.2363, batch acc 0.8208
10:06:45.220   Training iter 350, batch loss 0.2336, batch acc 0.8318
10:06:45.720   Training iter 400, batch loss 0.2369, batch acc 0.8202
10:06:46.219   Training iter 450, batch loss 0.2365, batch acc 0.8252
10:06:46.693   Training iter 500, batch loss 0.2383, batch acc 0.8184
10:06:47.172   Training iter 550, batch loss 0.2348, batch acc 0.8284
10:06:47.647   Training iter 600, batch loss 0.2331, batch acc 0.8268
10:06:47.648 Training @ 350 epoch...
10:06:48.131   Training iter 50, batch loss 0.2377, batch acc 0.8202
10:06:48.612   Training iter 100, batch loss 0.2372, batch acc 0.8164
10:06:49.092   Training iter 150, batch loss 0.2369, batch acc 0.8162
10:06:49.574   Training iter 200, batch loss 0.2379, batch acc 0.8210
10:06:50.057   Training iter 250, batch loss 0.2372, batch acc 0.8226
10:06:50.530   Training iter 300, batch loss 0.2345, batch acc 0.8254
10:06:50.988   Training iter 350, batch loss 0.2365, batch acc 0.8232
10:06:51.442   Training iter 400, batch loss 0.2325, batch acc 0.8374
10:06:51.887   Training iter 450, batch loss 0.2378, batch acc 0.8162
10:06:52.346   Training iter 500, batch loss 0.2357, batch acc 0.8246
10:06:52.800   Training iter 550, batch loss 0.2364, batch acc 0.8162
10:06:53.260   Training iter 600, batch loss 0.2355, batch acc 0.8270
10:06:53.261 Testing @ 350 epoch...
10:06:53.297     Testing, total mean loss 0.23267, total acc 0.83090
10:06:53.297 Training @ 351 epoch...
10:06:53.757   Training iter 50, batch loss 0.2363, batch acc 0.8248
10:06:54.214   Training iter 100, batch loss 0.2347, batch acc 0.8262
10:06:54.679   Training iter 150, batch loss 0.2353, batch acc 0.8198
10:06:55.150   Training iter 200, batch loss 0.2371, batch acc 0.8244
10:06:55.621   Training iter 250, batch loss 0.2364, batch acc 0.8248
10:06:56.077   Training iter 300, batch loss 0.2358, batch acc 0.8186
10:06:56.551   Training iter 350, batch loss 0.2370, batch acc 0.8200
10:06:57.026   Training iter 400, batch loss 0.2360, batch acc 0.8316
10:06:57.505   Training iter 450, batch loss 0.2392, batch acc 0.8098
10:06:57.988   Training iter 500, batch loss 0.2342, batch acc 0.8244
10:06:58.470   Training iter 550, batch loss 0.2379, batch acc 0.8208
10:06:58.956   Training iter 600, batch loss 0.2358, batch acc 0.8200
10:06:58.958 Training @ 352 epoch...
10:06:59.453   Training iter 50, batch loss 0.2356, batch acc 0.8262
10:06:59.944   Training iter 100, batch loss 0.2359, batch acc 0.8244
10:07:00.439   Training iter 150, batch loss 0.2365, batch acc 0.8288
10:07:00.950   Training iter 200, batch loss 0.2378, batch acc 0.8186
10:07:01.498   Training iter 250, batch loss 0.2352, batch acc 0.8240
10:07:02.076   Training iter 300, batch loss 0.2373, batch acc 0.8186
10:07:02.644   Training iter 350, batch loss 0.2373, batch acc 0.8102
10:07:03.203   Training iter 400, batch loss 0.2346, batch acc 0.8250
10:07:03.716   Training iter 450, batch loss 0.2356, batch acc 0.8238
10:07:04.181   Training iter 500, batch loss 0.2353, batch acc 0.8266
10:07:04.640   Training iter 550, batch loss 0.2373, batch acc 0.8184
10:07:05.113   Training iter 600, batch loss 0.2369, batch acc 0.8232
10:07:05.115 Training @ 353 epoch...
10:07:05.601   Training iter 50, batch loss 0.2350, batch acc 0.8246
10:07:06.084   Training iter 100, batch loss 0.2336, batch acc 0.8246
10:07:06.547   Training iter 150, batch loss 0.2361, batch acc 0.8168
10:07:07.007   Training iter 200, batch loss 0.2379, batch acc 0.8178
10:07:07.480   Training iter 250, batch loss 0.2355, batch acc 0.8216
10:07:07.958   Training iter 300, batch loss 0.2349, batch acc 0.8232
10:07:08.428   Training iter 350, batch loss 0.2384, batch acc 0.8182
10:07:08.915   Training iter 400, batch loss 0.2365, batch acc 0.8256
10:07:09.395   Training iter 450, batch loss 0.2359, batch acc 0.8304
10:07:09.896   Training iter 500, batch loss 0.2351, batch acc 0.8210
10:07:10.406   Training iter 550, batch loss 0.2359, batch acc 0.8316
10:07:10.874   Training iter 600, batch loss 0.2402, batch acc 0.8114
10:07:10.876 Training @ 354 epoch...
10:07:11.350   Training iter 50, batch loss 0.2352, batch acc 0.8236
10:07:11.820   Training iter 100, batch loss 0.2328, batch acc 0.8318
10:07:12.320   Training iter 150, batch loss 0.2351, batch acc 0.8272
10:07:12.840   Training iter 200, batch loss 0.2384, batch acc 0.8162
10:07:13.363   Training iter 250, batch loss 0.2375, batch acc 0.8144
10:07:13.875   Training iter 300, batch loss 0.2368, batch acc 0.8236
10:07:14.384   Training iter 350, batch loss 0.2352, batch acc 0.8244
10:07:14.893   Training iter 400, batch loss 0.2362, batch acc 0.8204
10:07:15.416   Training iter 450, batch loss 0.2380, batch acc 0.8210
10:07:15.932   Training iter 500, batch loss 0.2358, batch acc 0.8216
10:07:16.434   Training iter 550, batch loss 0.2384, batch acc 0.8180
10:07:16.940   Training iter 600, batch loss 0.2353, batch acc 0.8266
10:07:16.941 Training @ 355 epoch...
10:07:17.466   Training iter 50, batch loss 0.2363, batch acc 0.8200
10:07:18.006   Training iter 100, batch loss 0.2345, batch acc 0.8360
10:07:18.516   Training iter 150, batch loss 0.2361, batch acc 0.8254
10:07:19.001   Training iter 200, batch loss 0.2366, batch acc 0.8194
10:07:19.481   Training iter 250, batch loss 0.2367, batch acc 0.8158
10:07:19.961   Training iter 300, batch loss 0.2369, batch acc 0.8168
10:07:20.419   Training iter 350, batch loss 0.2352, batch acc 0.8288
10:07:20.862   Training iter 400, batch loss 0.2364, batch acc 0.8194
10:07:21.312   Training iter 450, batch loss 0.2344, batch acc 0.8302
10:07:21.767   Training iter 500, batch loss 0.2360, batch acc 0.8214
10:07:22.212   Training iter 550, batch loss 0.2380, batch acc 0.8186
10:07:22.693   Training iter 600, batch loss 0.2373, batch acc 0.8152
10:07:22.694 Testing @ 355 epoch...
10:07:22.731     Testing, total mean loss 0.23256, total acc 0.83100
10:07:22.731 Training @ 356 epoch...
10:07:23.233   Training iter 50, batch loss 0.2377, batch acc 0.8232
10:07:23.726   Training iter 100, batch loss 0.2378, batch acc 0.8168
10:07:24.236   Training iter 150, batch loss 0.2348, batch acc 0.8280
10:07:24.770   Training iter 200, batch loss 0.2377, batch acc 0.8140
10:07:25.321   Training iter 250, batch loss 0.2353, batch acc 0.8270
10:07:25.839   Training iter 300, batch loss 0.2371, batch acc 0.8194
10:07:26.371   Training iter 350, batch loss 0.2351, batch acc 0.8258
10:07:26.904   Training iter 400, batch loss 0.2358, batch acc 0.8216
10:07:27.462   Training iter 450, batch loss 0.2361, batch acc 0.8208
10:07:28.030   Training iter 500, batch loss 0.2346, batch acc 0.8252
10:07:28.568   Training iter 550, batch loss 0.2346, batch acc 0.8246
10:07:29.129   Training iter 600, batch loss 0.2377, batch acc 0.8224
10:07:29.131 Training @ 357 epoch...
10:07:29.690   Training iter 50, batch loss 0.2330, batch acc 0.8308
10:07:30.251   Training iter 100, batch loss 0.2367, batch acc 0.8188
10:07:30.795   Training iter 150, batch loss 0.2374, batch acc 0.8218
10:07:31.334   Training iter 200, batch loss 0.2374, batch acc 0.8208
10:07:31.879   Training iter 250, batch loss 0.2372, batch acc 0.8180
10:07:32.428   Training iter 300, batch loss 0.2373, batch acc 0.8152
10:07:32.975   Training iter 350, batch loss 0.2329, batch acc 0.8370
10:07:33.534   Training iter 400, batch loss 0.2325, batch acc 0.8308
10:07:34.085   Training iter 450, batch loss 0.2400, batch acc 0.8112
10:07:34.634   Training iter 500, batch loss 0.2361, batch acc 0.8250
10:07:35.155   Training iter 550, batch loss 0.2359, batch acc 0.8198
10:07:35.646   Training iter 600, batch loss 0.2376, batch acc 0.8156
10:07:35.648 Training @ 358 epoch...
10:07:36.142   Training iter 50, batch loss 0.2335, batch acc 0.8246
10:07:36.619   Training iter 100, batch loss 0.2357, batch acc 0.8302
10:07:37.102   Training iter 150, batch loss 0.2373, batch acc 0.8230
10:07:37.595   Training iter 200, batch loss 0.2392, batch acc 0.8164
10:07:38.087   Training iter 250, batch loss 0.2357, batch acc 0.8170
10:07:38.581   Training iter 300, batch loss 0.2384, batch acc 0.8192
10:07:39.044   Training iter 350, batch loss 0.2371, batch acc 0.8182
10:07:39.494   Training iter 400, batch loss 0.2360, batch acc 0.8174
10:07:39.976   Training iter 450, batch loss 0.2337, batch acc 0.8276
10:07:40.448   Training iter 500, batch loss 0.2360, batch acc 0.8252
10:07:40.895   Training iter 550, batch loss 0.2346, batch acc 0.8252
10:07:41.389   Training iter 600, batch loss 0.2365, batch acc 0.8260
10:07:41.390 Training @ 359 epoch...
10:07:41.890   Training iter 50, batch loss 0.2356, batch acc 0.8240
10:07:42.382   Training iter 100, batch loss 0.2365, batch acc 0.8298
10:07:42.873   Training iter 150, batch loss 0.2371, batch acc 0.8238
10:07:43.361   Training iter 200, batch loss 0.2367, batch acc 0.8182
10:07:43.842   Training iter 250, batch loss 0.2333, batch acc 0.8268
10:07:44.348   Training iter 300, batch loss 0.2341, batch acc 0.8278
10:07:44.865   Training iter 350, batch loss 0.2343, batch acc 0.8278
10:07:45.416   Training iter 400, batch loss 0.2357, batch acc 0.8192
10:07:45.949   Training iter 450, batch loss 0.2375, batch acc 0.8148
10:07:46.496   Training iter 500, batch loss 0.2361, batch acc 0.8244
10:07:47.032   Training iter 550, batch loss 0.2387, batch acc 0.8184
10:07:47.570   Training iter 600, batch loss 0.2380, batch acc 0.8122
10:07:47.572 Training @ 360 epoch...
10:07:48.123   Training iter 50, batch loss 0.2352, batch acc 0.8216
10:07:48.649   Training iter 100, batch loss 0.2355, batch acc 0.8292
10:07:49.185   Training iter 150, batch loss 0.2357, batch acc 0.8162
10:07:49.722   Training iter 200, batch loss 0.2357, batch acc 0.8216
10:07:50.265   Training iter 250, batch loss 0.2360, batch acc 0.8186
10:07:50.786   Training iter 300, batch loss 0.2377, batch acc 0.8118
10:07:51.254   Training iter 350, batch loss 0.2385, batch acc 0.8238
10:07:51.725   Training iter 400, batch loss 0.2357, batch acc 0.8216
10:07:52.205   Training iter 450, batch loss 0.2376, batch acc 0.8216
10:07:52.699   Training iter 500, batch loss 0.2357, batch acc 0.8332
10:07:53.168   Training iter 550, batch loss 0.2339, batch acc 0.8272
10:07:53.626   Training iter 600, batch loss 0.2362, batch acc 0.8196
10:07:53.627 Testing @ 360 epoch...
10:07:53.662     Testing, total mean loss 0.23246, total acc 0.83120
10:07:53.662 Training @ 361 epoch...
10:07:54.148   Training iter 50, batch loss 0.2390, batch acc 0.8118
10:07:54.644   Training iter 100, batch loss 0.2373, batch acc 0.8170
10:07:55.139   Training iter 150, batch loss 0.2346, batch acc 0.8246
10:07:55.587   Training iter 200, batch loss 0.2359, batch acc 0.8230
10:07:56.033   Training iter 250, batch loss 0.2373, batch acc 0.8218
10:07:56.481   Training iter 300, batch loss 0.2346, batch acc 0.8306
10:07:56.938   Training iter 350, batch loss 0.2374, batch acc 0.8144
10:07:57.396   Training iter 400, batch loss 0.2353, batch acc 0.8242
10:07:57.868   Training iter 450, batch loss 0.2333, batch acc 0.8306
10:07:58.327   Training iter 500, batch loss 0.2381, batch acc 0.8220
10:07:58.786   Training iter 550, batch loss 0.2347, batch acc 0.8256
10:07:59.245   Training iter 600, batch loss 0.2356, batch acc 0.8234
10:07:59.247 Training @ 362 epoch...
10:07:59.799   Training iter 50, batch loss 0.2357, batch acc 0.8220
10:08:00.356   Training iter 100, batch loss 0.2352, batch acc 0.8314
10:08:00.902   Training iter 150, batch loss 0.2356, batch acc 0.8236
10:08:01.453   Training iter 200, batch loss 0.2386, batch acc 0.8118
10:08:02.010   Training iter 250, batch loss 0.2374, batch acc 0.8116
10:08:02.526   Training iter 300, batch loss 0.2344, batch acc 0.8272
10:08:03.024   Training iter 350, batch loss 0.2362, batch acc 0.8248
10:08:03.501   Training iter 400, batch loss 0.2354, batch acc 0.8272
10:08:03.988   Training iter 450, batch loss 0.2362, batch acc 0.8242
10:08:04.467   Training iter 500, batch loss 0.2365, batch acc 0.8210
10:08:04.954   Training iter 550, batch loss 0.2366, batch acc 0.8198
10:08:05.440   Training iter 600, batch loss 0.2351, batch acc 0.8244
10:08:05.442 Training @ 363 epoch...
10:08:05.929   Training iter 50, batch loss 0.2355, batch acc 0.8256
10:08:06.418   Training iter 100, batch loss 0.2331, batch acc 0.8320
10:08:06.913   Training iter 150, batch loss 0.2367, batch acc 0.8206
10:08:07.452   Training iter 200, batch loss 0.2347, batch acc 0.8324
10:08:07.974   Training iter 250, batch loss 0.2380, batch acc 0.8200
10:08:08.440   Training iter 300, batch loss 0.2349, batch acc 0.8288
10:08:08.879   Training iter 350, batch loss 0.2362, batch acc 0.8158
10:08:09.329   Training iter 400, batch loss 0.2363, batch acc 0.8172
10:08:09.780   Training iter 450, batch loss 0.2378, batch acc 0.8164
10:08:10.230   Training iter 500, batch loss 0.2362, batch acc 0.8200
10:08:10.677   Training iter 550, batch loss 0.2367, batch acc 0.8184
10:08:11.148   Training iter 600, batch loss 0.2365, batch acc 0.8216
10:08:11.150 Training @ 364 epoch...
10:08:11.646   Training iter 50, batch loss 0.2330, batch acc 0.8264
10:08:12.296   Training iter 100, batch loss 0.2328, batch acc 0.8294
10:08:13.224   Training iter 150, batch loss 0.2343, batch acc 0.8376
10:08:13.714   Training iter 200, batch loss 0.2386, batch acc 0.8202
10:08:14.207   Training iter 250, batch loss 0.2338, batch acc 0.8252
10:08:14.701   Training iter 300, batch loss 0.2375, batch acc 0.8144
10:08:15.217   Training iter 350, batch loss 0.2370, batch acc 0.8206
10:08:15.731   Training iter 400, batch loss 0.2345, batch acc 0.8242
10:08:16.234   Training iter 450, batch loss 0.2352, batch acc 0.8206
10:08:16.741   Training iter 500, batch loss 0.2394, batch acc 0.8162
10:08:17.250   Training iter 550, batch loss 0.2381, batch acc 0.8196
10:08:17.771   Training iter 600, batch loss 0.2384, batch acc 0.8150
10:08:17.773 Training @ 365 epoch...
10:08:18.277   Training iter 50, batch loss 0.2355, batch acc 0.8188
10:08:18.783   Training iter 100, batch loss 0.2375, batch acc 0.8160
10:08:19.276   Training iter 150, batch loss 0.2334, batch acc 0.8294
10:08:19.755   Training iter 200, batch loss 0.2380, batch acc 0.8166
10:08:20.238   Training iter 250, batch loss 0.2349, batch acc 0.8278
10:08:20.724   Training iter 300, batch loss 0.2371, batch acc 0.8224
10:08:21.195   Training iter 350, batch loss 0.2375, batch acc 0.8264
10:08:21.668   Training iter 400, batch loss 0.2354, batch acc 0.8206
10:08:22.158   Training iter 450, batch loss 0.2375, batch acc 0.8176
10:08:22.672   Training iter 500, batch loss 0.2355, batch acc 0.8274
10:08:23.164   Training iter 550, batch loss 0.2359, batch acc 0.8230
10:08:23.662   Training iter 600, batch loss 0.2342, batch acc 0.8262
10:08:23.664 Testing @ 365 epoch...
10:08:23.699     Testing, total mean loss 0.23237, total acc 0.83110
10:08:23.699 Training @ 366 epoch...
10:08:24.183   Training iter 50, batch loss 0.2370, batch acc 0.8244
10:08:24.673   Training iter 100, batch loss 0.2343, batch acc 0.8292
10:08:25.179   Training iter 150, batch loss 0.2347, batch acc 0.8274
10:08:25.672   Training iter 200, batch loss 0.2338, batch acc 0.8284
10:08:26.167   Training iter 250, batch loss 0.2390, batch acc 0.8094
10:08:26.659   Training iter 300, batch loss 0.2374, batch acc 0.8144
10:08:27.159   Training iter 350, batch loss 0.2392, batch acc 0.8100
10:08:27.645   Training iter 400, batch loss 0.2352, batch acc 0.8206
10:08:28.132   Training iter 450, batch loss 0.2378, batch acc 0.8202
10:08:28.640   Training iter 500, batch loss 0.2371, batch acc 0.8236
10:08:29.161   Training iter 550, batch loss 0.2335, batch acc 0.8328
10:08:29.656   Training iter 600, batch loss 0.2330, batch acc 0.8310
10:08:29.658 Training @ 367 epoch...
10:08:30.161   Training iter 50, batch loss 0.2351, batch acc 0.8256
10:08:30.660   Training iter 100, batch loss 0.2364, batch acc 0.8202
10:08:31.160   Training iter 150, batch loss 0.2355, batch acc 0.8246
10:08:31.653   Training iter 200, batch loss 0.2378, batch acc 0.8184
10:08:32.139   Training iter 250, batch loss 0.2375, batch acc 0.8170
10:08:32.622   Training iter 300, batch loss 0.2385, batch acc 0.8176
10:08:33.117   Training iter 350, batch loss 0.2353, batch acc 0.8254
10:08:33.619   Training iter 400, batch loss 0.2348, batch acc 0.8286
10:08:34.126   Training iter 450, batch loss 0.2352, batch acc 0.8272
10:08:34.634   Training iter 500, batch loss 0.2353, batch acc 0.8202
10:08:35.133   Training iter 550, batch loss 0.2373, batch acc 0.8198
10:08:35.628   Training iter 600, batch loss 0.2330, batch acc 0.8252
10:08:35.630 Training @ 368 epoch...
10:08:36.130   Training iter 50, batch loss 0.2376, batch acc 0.8112
10:08:36.614   Training iter 100, batch loss 0.2370, batch acc 0.8222
10:08:37.098   Training iter 150, batch loss 0.2331, batch acc 0.8302
10:08:37.564   Training iter 200, batch loss 0.2353, batch acc 0.8272
10:08:38.042   Training iter 250, batch loss 0.2360, batch acc 0.8238
10:08:38.524   Training iter 300, batch loss 0.2378, batch acc 0.8198
10:08:39.011   Training iter 350, batch loss 0.2376, batch acc 0.8182
10:08:39.486   Training iter 400, batch loss 0.2348, batch acc 0.8230
10:08:39.971   Training iter 450, batch loss 0.2354, batch acc 0.8304
10:08:40.466   Training iter 500, batch loss 0.2358, batch acc 0.8244
10:08:40.952   Training iter 550, batch loss 0.2323, batch acc 0.8286
10:08:41.439   Training iter 600, batch loss 0.2388, batch acc 0.8120
10:08:41.441 Training @ 369 epoch...
10:08:41.951   Training iter 50, batch loss 0.2336, batch acc 0.8302
10:08:42.461   Training iter 100, batch loss 0.2361, batch acc 0.8214
10:08:42.950   Training iter 150, batch loss 0.2338, batch acc 0.8316
10:08:43.450   Training iter 200, batch loss 0.2342, batch acc 0.8304
10:08:43.960   Training iter 250, batch loss 0.2361, batch acc 0.8202
10:08:44.488   Training iter 300, batch loss 0.2358, batch acc 0.8232
10:08:44.990   Training iter 350, batch loss 0.2359, batch acc 0.8240
10:08:45.493   Training iter 400, batch loss 0.2386, batch acc 0.8140
10:08:45.977   Training iter 450, batch loss 0.2379, batch acc 0.8202
10:08:46.464   Training iter 500, batch loss 0.2361, batch acc 0.8202
10:08:46.944   Training iter 550, batch loss 0.2371, batch acc 0.8138
10:08:47.417   Training iter 600, batch loss 0.2363, batch acc 0.8206
10:08:47.419 Training @ 370 epoch...
10:08:47.899   Training iter 50, batch loss 0.2323, batch acc 0.8354
10:08:48.411   Training iter 100, batch loss 0.2368, batch acc 0.8148
10:08:48.940   Training iter 150, batch loss 0.2369, batch acc 0.8252
10:08:49.472   Training iter 200, batch loss 0.2361, batch acc 0.8198
10:08:50.011   Training iter 250, batch loss 0.2378, batch acc 0.8212
10:08:50.554   Training iter 300, batch loss 0.2369, batch acc 0.8156
10:08:51.069   Training iter 350, batch loss 0.2341, batch acc 0.8256
10:08:51.590   Training iter 400, batch loss 0.2371, batch acc 0.8244
10:08:52.111   Training iter 450, batch loss 0.2340, batch acc 0.8300
10:08:52.643   Training iter 500, batch loss 0.2374, batch acc 0.8162
10:08:53.187   Training iter 550, batch loss 0.2364, batch acc 0.8212
10:08:53.715   Training iter 600, batch loss 0.2352, batch acc 0.8220
10:08:53.717 Testing @ 370 epoch...
10:08:53.752     Testing, total mean loss 0.23228, total acc 0.83130
10:08:53.752 Training @ 371 epoch...
10:08:54.274   Training iter 50, batch loss 0.2352, batch acc 0.8222
10:08:54.775   Training iter 100, batch loss 0.2368, batch acc 0.8280
10:08:55.281   Training iter 150, batch loss 0.2371, batch acc 0.8218
10:08:55.782   Training iter 200, batch loss 0.2353, batch acc 0.8280
10:08:56.277   Training iter 250, batch loss 0.2365, batch acc 0.8146
10:08:56.767   Training iter 300, batch loss 0.2353, batch acc 0.8254
10:08:57.267   Training iter 350, batch loss 0.2340, batch acc 0.8300
10:08:57.804   Training iter 400, batch loss 0.2385, batch acc 0.8166
10:08:58.369   Training iter 450, batch loss 0.2355, batch acc 0.8200
10:08:58.929   Training iter 500, batch loss 0.2376, batch acc 0.8150
10:08:59.480   Training iter 550, batch loss 0.2356, batch acc 0.8176
10:08:59.962   Training iter 600, batch loss 0.2335, batch acc 0.8318
10:08:59.964 Training @ 372 epoch...
10:09:00.461   Training iter 50, batch loss 0.2342, batch acc 0.8274
10:09:00.980   Training iter 100, batch loss 0.2358, batch acc 0.8206
10:09:01.499   Training iter 150, batch loss 0.2312, batch acc 0.8340
10:09:02.060   Training iter 200, batch loss 0.2390, batch acc 0.8128
10:09:02.600   Training iter 250, batch loss 0.2370, batch acc 0.8214
10:09:03.108   Training iter 300, batch loss 0.2358, batch acc 0.8216
10:09:03.623   Training iter 350, batch loss 0.2354, batch acc 0.8210
10:09:04.140   Training iter 400, batch loss 0.2388, batch acc 0.8246
10:09:04.647   Training iter 450, batch loss 0.2380, batch acc 0.8122
10:09:05.141   Training iter 500, batch loss 0.2330, batch acc 0.8330
10:09:05.659   Training iter 550, batch loss 0.2357, batch acc 0.8232
10:09:06.176   Training iter 600, batch loss 0.2367, batch acc 0.8224
10:09:06.177 Training @ 373 epoch...
10:09:06.699   Training iter 50, batch loss 0.2352, batch acc 0.8248
10:09:07.220   Training iter 100, batch loss 0.2348, batch acc 0.8282
10:09:07.733   Training iter 150, batch loss 0.2370, batch acc 0.8204
10:09:08.236   Training iter 200, batch loss 0.2378, batch acc 0.8136
10:09:08.728   Training iter 250, batch loss 0.2376, batch acc 0.8228
10:09:09.216   Training iter 300, batch loss 0.2353, batch acc 0.8224
10:09:09.717   Training iter 350, batch loss 0.2347, batch acc 0.8290
10:09:10.206   Training iter 400, batch loss 0.2356, batch acc 0.8222
10:09:10.691   Training iter 450, batch loss 0.2356, batch acc 0.8240
10:09:11.136   Training iter 500, batch loss 0.2342, batch acc 0.8208
10:09:11.584   Training iter 550, batch loss 0.2369, batch acc 0.8196
10:09:12.024   Training iter 600, batch loss 0.2360, batch acc 0.8266
10:09:12.026 Training @ 374 epoch...
10:09:12.496   Training iter 50, batch loss 0.2348, batch acc 0.8236
10:09:12.963   Training iter 100, batch loss 0.2370, batch acc 0.8206
10:09:13.423   Training iter 150, batch loss 0.2344, batch acc 0.8248
10:09:13.877   Training iter 200, batch loss 0.2332, batch acc 0.8298
10:09:14.332   Training iter 250, batch loss 0.2364, batch acc 0.8222
10:09:14.805   Training iter 300, batch loss 0.2376, batch acc 0.8216
10:09:15.293   Training iter 350, batch loss 0.2356, batch acc 0.8164
10:09:15.772   Training iter 400, batch loss 0.2379, batch acc 0.8182
10:09:16.261   Training iter 450, batch loss 0.2365, batch acc 0.8238
10:09:16.750   Training iter 500, batch loss 0.2358, batch acc 0.8248
10:09:17.251   Training iter 550, batch loss 0.2368, batch acc 0.8210
10:09:17.744   Training iter 600, batch loss 0.2345, batch acc 0.8236
10:09:17.746 Training @ 375 epoch...
10:09:18.243   Training iter 50, batch loss 0.2362, batch acc 0.8210
10:09:18.725   Training iter 100, batch loss 0.2360, batch acc 0.8240
10:09:19.202   Training iter 150, batch loss 0.2343, batch acc 0.8280
10:09:19.671   Training iter 200, batch loss 0.2349, batch acc 0.8214
10:09:20.151   Training iter 250, batch loss 0.2375, batch acc 0.8226
10:09:20.643   Training iter 300, batch loss 0.2364, batch acc 0.8208
10:09:21.163   Training iter 350, batch loss 0.2354, batch acc 0.8188
10:09:21.705   Training iter 400, batch loss 0.2366, batch acc 0.8146
10:09:22.222   Training iter 450, batch loss 0.2361, batch acc 0.8254
10:09:22.735   Training iter 500, batch loss 0.2375, batch acc 0.8196
10:09:23.272   Training iter 550, batch loss 0.2347, batch acc 0.8288
10:09:23.816   Training iter 600, batch loss 0.2346, batch acc 0.8258
10:09:23.818 Testing @ 375 epoch...
10:09:23.858     Testing, total mean loss 0.23219, total acc 0.83120
10:09:23.858 Training @ 376 epoch...
10:09:24.436   Training iter 50, batch loss 0.2350, batch acc 0.8230
10:09:25.003   Training iter 100, batch loss 0.2357, batch acc 0.8262
10:09:25.538   Training iter 150, batch loss 0.2340, batch acc 0.8246
10:09:26.062   Training iter 200, batch loss 0.2363, batch acc 0.8230
10:09:26.583   Training iter 250, batch loss 0.2346, batch acc 0.8252
10:09:27.072   Training iter 300, batch loss 0.2379, batch acc 0.8206
10:09:27.562   Training iter 350, batch loss 0.2366, batch acc 0.8206
10:09:28.069   Training iter 400, batch loss 0.2338, batch acc 0.8346
10:09:28.579   Training iter 450, batch loss 0.2357, batch acc 0.8166
10:09:29.105   Training iter 500, batch loss 0.2343, batch acc 0.8248
10:09:29.605   Training iter 550, batch loss 0.2385, batch acc 0.8184
10:09:30.112   Training iter 600, batch loss 0.2376, batch acc 0.8132
10:09:30.114 Training @ 377 epoch...
10:09:30.668   Training iter 50, batch loss 0.2372, batch acc 0.8250
10:09:31.219   Training iter 100, batch loss 0.2344, batch acc 0.8224
10:09:31.766   Training iter 150, batch loss 0.2371, batch acc 0.8194
10:09:32.279   Training iter 200, batch loss 0.2373, batch acc 0.8248
10:09:32.754   Training iter 250, batch loss 0.2365, batch acc 0.8206
10:09:33.233   Training iter 300, batch loss 0.2340, batch acc 0.8302
10:09:33.698   Training iter 350, batch loss 0.2363, batch acc 0.8198
10:09:34.172   Training iter 400, batch loss 0.2363, batch acc 0.8212
10:09:34.677   Training iter 450, batch loss 0.2349, batch acc 0.8192
10:09:35.173   Training iter 500, batch loss 0.2363, batch acc 0.8166
10:09:35.651   Training iter 550, batch loss 0.2348, batch acc 0.8316
10:09:36.115   Training iter 600, batch loss 0.2346, batch acc 0.8224
10:09:36.116 Training @ 378 epoch...
10:09:36.581   Training iter 50, batch loss 0.2349, batch acc 0.8262
10:09:37.077   Training iter 100, batch loss 0.2355, batch acc 0.8242
10:09:37.576   Training iter 150, batch loss 0.2376, batch acc 0.8230
10:09:38.086   Training iter 200, batch loss 0.2337, batch acc 0.8276
10:09:38.597   Training iter 250, batch loss 0.2372, batch acc 0.8158
10:09:39.112   Training iter 300, batch loss 0.2331, batch acc 0.8250
10:09:39.625   Training iter 350, batch loss 0.2378, batch acc 0.8164
10:09:40.142   Training iter 400, batch loss 0.2373, batch acc 0.8186
10:09:40.656   Training iter 450, batch loss 0.2357, batch acc 0.8222
10:09:41.177   Training iter 500, batch loss 0.2365, batch acc 0.8196
10:09:41.696   Training iter 550, batch loss 0.2344, batch acc 0.8316
10:09:42.212   Training iter 600, batch loss 0.2359, batch acc 0.8226
10:09:42.214 Training @ 379 epoch...
10:09:42.748   Training iter 50, batch loss 0.2364, batch acc 0.8198
10:09:43.224   Training iter 100, batch loss 0.2373, batch acc 0.8172
10:09:43.685   Training iter 150, batch loss 0.2342, batch acc 0.8244
10:09:44.147   Training iter 200, batch loss 0.2360, batch acc 0.8252
10:09:44.630   Training iter 250, batch loss 0.2361, batch acc 0.8184
10:09:45.098   Training iter 300, batch loss 0.2333, batch acc 0.8316
10:09:45.573   Training iter 350, batch loss 0.2374, batch acc 0.8172
10:09:46.046   Training iter 400, batch loss 0.2367, batch acc 0.8210
10:09:46.515   Training iter 450, batch loss 0.2358, batch acc 0.8226
10:09:46.989   Training iter 500, batch loss 0.2367, batch acc 0.8246
10:09:47.444   Training iter 550, batch loss 0.2332, batch acc 0.8260
10:09:47.908   Training iter 600, batch loss 0.2364, batch acc 0.8260
10:09:47.910 Training @ 380 epoch...
10:09:48.369   Training iter 50, batch loss 0.2360, batch acc 0.8192
10:09:48.835   Training iter 100, batch loss 0.2346, batch acc 0.8200
10:09:49.292   Training iter 150, batch loss 0.2348, batch acc 0.8288
10:09:49.766   Training iter 200, batch loss 0.2345, batch acc 0.8286
10:09:50.241   Training iter 250, batch loss 0.2385, batch acc 0.8218
10:09:50.699   Training iter 300, batch loss 0.2346, batch acc 0.8262
10:09:51.146   Training iter 350, batch loss 0.2372, batch acc 0.8206
10:09:51.604   Training iter 400, batch loss 0.2360, batch acc 0.8268
10:09:52.082   Training iter 450, batch loss 0.2382, batch acc 0.8102
10:09:52.562   Training iter 500, batch loss 0.2329, batch acc 0.8278
10:09:53.101   Training iter 550, batch loss 0.2352, batch acc 0.8212
10:09:53.631   Training iter 600, batch loss 0.2367, batch acc 0.8236
10:09:53.633 Testing @ 380 epoch...
10:09:53.668     Testing, total mean loss 0.23211, total acc 0.83110
10:09:53.668 Training @ 381 epoch...
10:09:54.193   Training iter 50, batch loss 0.2345, batch acc 0.8172
10:09:54.712   Training iter 100, batch loss 0.2376, batch acc 0.8234
10:09:55.254   Training iter 150, batch loss 0.2366, batch acc 0.8144
10:09:55.793   Training iter 200, batch loss 0.2341, batch acc 0.8278
10:09:56.334   Training iter 250, batch loss 0.2375, batch acc 0.8300
10:09:56.874   Training iter 300, batch loss 0.2336, batch acc 0.8252
10:09:57.418   Training iter 350, batch loss 0.2360, batch acc 0.8146
10:09:57.948   Training iter 400, batch loss 0.2353, batch acc 0.8280
10:09:58.493   Training iter 450, batch loss 0.2331, batch acc 0.8240
10:09:59.023   Training iter 500, batch loss 0.2354, batch acc 0.8248
10:09:59.536   Training iter 550, batch loss 0.2369, batch acc 0.8198
10:10:00.048   Training iter 600, batch loss 0.2383, batch acc 0.8234
10:10:00.049 Training @ 382 epoch...
10:10:00.566   Training iter 50, batch loss 0.2352, batch acc 0.8296
10:10:01.068   Training iter 100, batch loss 0.2316, batch acc 0.8286
10:10:01.719   Training iter 150, batch loss 0.2343, batch acc 0.8262
10:10:02.286   Training iter 200, batch loss 0.2367, batch acc 0.8208
10:10:02.789   Training iter 250, batch loss 0.2328, batch acc 0.8304
10:10:03.296   Training iter 300, batch loss 0.2366, batch acc 0.8184
10:10:03.771   Training iter 350, batch loss 0.2385, batch acc 0.8178
10:10:04.268   Training iter 400, batch loss 0.2396, batch acc 0.8128
10:10:04.775   Training iter 450, batch loss 0.2371, batch acc 0.8152
10:10:05.251   Training iter 500, batch loss 0.2358, batch acc 0.8218
10:10:05.722   Training iter 550, batch loss 0.2371, batch acc 0.8200
10:10:06.183   Training iter 600, batch loss 0.2334, batch acc 0.8312
10:10:06.185 Training @ 383 epoch...
10:10:06.658   Training iter 50, batch loss 0.2367, batch acc 0.8174
10:10:07.129   Training iter 100, batch loss 0.2357, batch acc 0.8234
10:10:07.594   Training iter 150, batch loss 0.2354, batch acc 0.8206
10:10:08.079   Training iter 200, batch loss 0.2361, batch acc 0.8200
10:10:08.562   Training iter 250, batch loss 0.2367, batch acc 0.8236
10:10:09.070   Training iter 300, batch loss 0.2319, batch acc 0.8384
10:10:09.588   Training iter 350, batch loss 0.2345, batch acc 0.8252
10:10:10.109   Training iter 400, batch loss 0.2344, batch acc 0.8248
10:10:10.631   Training iter 450, batch loss 0.2383, batch acc 0.8182
10:10:11.165   Training iter 500, batch loss 0.2370, batch acc 0.8186
10:10:11.674   Training iter 550, batch loss 0.2340, batch acc 0.8290
10:10:12.198   Training iter 600, batch loss 0.2381, batch acc 0.8142
10:10:12.200 Training @ 384 epoch...
10:10:12.727   Training iter 50, batch loss 0.2361, batch acc 0.8218
10:10:13.284   Training iter 100, batch loss 0.2356, batch acc 0.8198
10:10:13.843   Training iter 150, batch loss 0.2355, batch acc 0.8212
10:10:14.393   Training iter 200, batch loss 0.2371, batch acc 0.8194
10:10:14.946   Training iter 250, batch loss 0.2388, batch acc 0.8196
10:10:15.481   Training iter 300, batch loss 0.2341, batch acc 0.8204
10:10:15.985   Training iter 350, batch loss 0.2362, batch acc 0.8186
10:10:16.472   Training iter 400, batch loss 0.2353, batch acc 0.8254
10:10:16.966   Training iter 450, batch loss 0.2353, batch acc 0.8268
10:10:17.458   Training iter 500, batch loss 0.2346, batch acc 0.8274
10:10:17.968   Training iter 550, batch loss 0.2356, batch acc 0.8302
10:10:18.472   Training iter 600, batch loss 0.2344, batch acc 0.8244
10:10:18.474 Training @ 385 epoch...
10:10:18.990   Training iter 50, batch loss 0.2352, batch acc 0.8246
10:10:19.503   Training iter 100, batch loss 0.2354, batch acc 0.8250
10:10:19.992   Training iter 150, batch loss 0.2337, batch acc 0.8216
10:10:20.475   Training iter 200, batch loss 0.2361, batch acc 0.8202
10:10:20.938   Training iter 250, batch loss 0.2372, batch acc 0.8152
10:10:21.406   Training iter 300, batch loss 0.2366, batch acc 0.8188
10:10:21.882   Training iter 350, batch loss 0.2353, batch acc 0.8238
10:10:22.359   Training iter 400, batch loss 0.2357, batch acc 0.8238
10:10:22.855   Training iter 450, batch loss 0.2353, batch acc 0.8304
10:10:23.330   Training iter 500, batch loss 0.2357, batch acc 0.8288
10:10:23.803   Training iter 550, batch loss 0.2352, batch acc 0.8238
10:10:24.338   Training iter 600, batch loss 0.2369, batch acc 0.8166
10:10:24.340 Testing @ 385 epoch...
10:10:24.374     Testing, total mean loss 0.23204, total acc 0.83130
10:10:24.374 Training @ 386 epoch...
10:10:24.882   Training iter 50, batch loss 0.2359, batch acc 0.8200
10:10:25.417   Training iter 100, batch loss 0.2349, batch acc 0.8214
10:10:25.931   Training iter 150, batch loss 0.2374, batch acc 0.8198
10:10:26.441   Training iter 200, batch loss 0.2371, batch acc 0.8206
10:10:26.954   Training iter 250, batch loss 0.2360, batch acc 0.8264
10:10:27.467   Training iter 300, batch loss 0.2338, batch acc 0.8286
10:10:28.008   Training iter 350, batch loss 0.2359, batch acc 0.8188
10:10:28.578   Training iter 400, batch loss 0.2354, batch acc 0.8254
10:10:29.149   Training iter 450, batch loss 0.2334, batch acc 0.8254
10:10:29.712   Training iter 500, batch loss 0.2335, batch acc 0.8326
10:10:30.238   Training iter 550, batch loss 0.2379, batch acc 0.8190
10:10:30.758   Training iter 600, batch loss 0.2368, batch acc 0.8170
10:10:30.759 Training @ 387 epoch...
10:10:31.255   Training iter 50, batch loss 0.2362, batch acc 0.8242
10:10:31.749   Training iter 100, batch loss 0.2353, batch acc 0.8254
10:10:32.242   Training iter 150, batch loss 0.2373, batch acc 0.8162
10:10:32.747   Training iter 200, batch loss 0.2362, batch acc 0.8190
10:10:33.248   Training iter 250, batch loss 0.2392, batch acc 0.8198
10:10:33.739   Training iter 300, batch loss 0.2366, batch acc 0.8262
10:10:34.229   Training iter 350, batch loss 0.2344, batch acc 0.8268
10:10:34.718   Training iter 400, batch loss 0.2342, batch acc 0.8226
10:10:35.202   Training iter 450, batch loss 0.2352, batch acc 0.8236
10:10:35.695   Training iter 500, batch loss 0.2379, batch acc 0.8146
10:10:36.185   Training iter 550, batch loss 0.2306, batch acc 0.8350
10:10:36.661   Training iter 600, batch loss 0.2350, batch acc 0.8234
10:10:36.662 Training @ 388 epoch...
10:10:37.160   Training iter 50, batch loss 0.2365, batch acc 0.8210
10:10:37.641   Training iter 100, batch loss 0.2353, batch acc 0.8296
10:10:38.128   Training iter 150, batch loss 0.2356, batch acc 0.8276
10:10:38.603   Training iter 200, batch loss 0.2346, batch acc 0.8268
10:10:39.105   Training iter 250, batch loss 0.2362, batch acc 0.8182
10:10:39.603   Training iter 300, batch loss 0.2387, batch acc 0.8180
10:10:40.102   Training iter 350, batch loss 0.2359, batch acc 0.8194
10:10:40.596   Training iter 400, batch loss 0.2367, batch acc 0.8222
10:10:41.102   Training iter 450, batch loss 0.2345, batch acc 0.8276
10:10:41.625   Training iter 500, batch loss 0.2313, batch acc 0.8294
10:10:42.152   Training iter 550, batch loss 0.2366, batch acc 0.8142
10:10:42.674   Training iter 600, batch loss 0.2359, batch acc 0.8224
10:10:42.676 Training @ 389 epoch...
10:10:43.219   Training iter 50, batch loss 0.2346, batch acc 0.8234
10:10:43.746   Training iter 100, batch loss 0.2360, batch acc 0.8140
10:10:44.266   Training iter 150, batch loss 0.2364, batch acc 0.8238
10:10:44.788   Training iter 200, batch loss 0.2363, batch acc 0.8216
10:10:45.320   Training iter 250, batch loss 0.2347, batch acc 0.8378
10:10:45.852   Training iter 300, batch loss 0.2332, batch acc 0.8274
10:10:46.373   Training iter 350, batch loss 0.2380, batch acc 0.8164
10:10:46.895   Training iter 400, batch loss 0.2337, batch acc 0.8218
10:10:47.406   Training iter 450, batch loss 0.2367, batch acc 0.8190
10:10:47.913   Training iter 500, batch loss 0.2343, batch acc 0.8268
10:10:48.413   Training iter 550, batch loss 0.2365, batch acc 0.8242
10:10:48.905   Training iter 600, batch loss 0.2371, batch acc 0.8174
10:10:48.907 Training @ 390 epoch...
10:10:49.411   Training iter 50, batch loss 0.2359, batch acc 0.8254
10:10:49.906   Training iter 100, batch loss 0.2347, batch acc 0.8244
10:10:50.412   Training iter 150, batch loss 0.2359, batch acc 0.8224
10:10:50.916   Training iter 200, batch loss 0.2351, batch acc 0.8286
10:10:51.415   Training iter 250, batch loss 0.2356, batch acc 0.8156
10:10:51.902   Training iter 300, batch loss 0.2367, batch acc 0.8234
10:10:52.401   Training iter 350, batch loss 0.2365, batch acc 0.8202
10:10:52.911   Training iter 400, batch loss 0.2353, batch acc 0.8232
10:10:53.424   Training iter 450, batch loss 0.2344, batch acc 0.8204
10:10:53.914   Training iter 500, batch loss 0.2350, batch acc 0.8318
10:10:54.415   Training iter 550, batch loss 0.2369, batch acc 0.8140
10:10:54.932   Training iter 600, batch loss 0.2353, batch acc 0.8254
10:10:54.934 Testing @ 390 epoch...
10:10:54.969     Testing, total mean loss 0.23196, total acc 0.83180
10:10:54.969 Training @ 391 epoch...
10:10:55.483   Training iter 50, batch loss 0.2342, batch acc 0.8242
10:10:55.963   Training iter 100, batch loss 0.2371, batch acc 0.8222
10:10:56.443   Training iter 150, batch loss 0.2355, batch acc 0.8286
10:10:56.935   Training iter 200, batch loss 0.2347, batch acc 0.8198
10:10:57.441   Training iter 250, batch loss 0.2368, batch acc 0.8222
10:10:57.970   Training iter 300, batch loss 0.2361, batch acc 0.8282
10:10:58.491   Training iter 350, batch loss 0.2379, batch acc 0.8154
10:10:58.994   Training iter 400, batch loss 0.2333, batch acc 0.8324
10:10:59.499   Training iter 450, batch loss 0.2357, batch acc 0.8194
10:11:00.019   Training iter 500, batch loss 0.2369, batch acc 0.8180
10:11:00.537   Training iter 550, batch loss 0.2362, batch acc 0.8176
10:11:01.058   Training iter 600, batch loss 0.2329, batch acc 0.8306
10:11:01.060 Training @ 392 epoch...
10:11:01.585   Training iter 50, batch loss 0.2366, batch acc 0.8222
10:11:02.118   Training iter 100, batch loss 0.2368, batch acc 0.8232
10:11:02.648   Training iter 150, batch loss 0.2337, batch acc 0.8312
10:11:03.159   Training iter 200, batch loss 0.2348, batch acc 0.8252
10:11:03.655   Training iter 250, batch loss 0.2351, batch acc 0.8246
10:11:04.147   Training iter 300, batch loss 0.2334, batch acc 0.8270
10:11:04.642   Training iter 350, batch loss 0.2379, batch acc 0.8140
10:11:05.133   Training iter 400, batch loss 0.2360, batch acc 0.8140
10:11:05.627   Training iter 450, batch loss 0.2335, batch acc 0.8272
10:11:06.127   Training iter 500, batch loss 0.2366, batch acc 0.8226
10:11:06.596   Training iter 550, batch loss 0.2389, batch acc 0.8136
10:11:07.053   Training iter 600, batch loss 0.2337, batch acc 0.8290
10:11:07.055 Training @ 393 epoch...
10:11:07.529   Training iter 50, batch loss 0.2362, batch acc 0.8236
10:11:07.983   Training iter 100, batch loss 0.2366, batch acc 0.8192
10:11:08.426   Training iter 150, batch loss 0.2360, batch acc 0.8200
10:11:08.874   Training iter 200, batch loss 0.2371, batch acc 0.8166
10:11:09.327   Training iter 250, batch loss 0.2346, batch acc 0.8288
10:11:09.770   Training iter 300, batch loss 0.2355, batch acc 0.8176
10:11:10.231   Training iter 350, batch loss 0.2373, batch acc 0.8206
10:11:10.688   Training iter 400, batch loss 0.2338, batch acc 0.8272
10:11:11.175   Training iter 450, batch loss 0.2353, batch acc 0.8232
10:11:11.630   Training iter 500, batch loss 0.2340, batch acc 0.8284
10:11:12.093   Training iter 550, batch loss 0.2362, batch acc 0.8156
10:11:12.541   Training iter 600, batch loss 0.2344, batch acc 0.8342
10:11:12.543 Training @ 394 epoch...
10:11:13.004   Training iter 50, batch loss 0.2363, batch acc 0.8228
10:11:13.495   Training iter 100, batch loss 0.2361, batch acc 0.8182
10:11:13.996   Training iter 150, batch loss 0.2365, batch acc 0.8236
10:11:14.499   Training iter 200, batch loss 0.2376, batch acc 0.8242
10:11:14.989   Training iter 250, batch loss 0.2338, batch acc 0.8268
10:11:15.482   Training iter 300, batch loss 0.2336, batch acc 0.8224
10:11:15.972   Training iter 350, batch loss 0.2350, batch acc 0.8300
10:11:16.473   Training iter 400, batch loss 0.2386, batch acc 0.8152
10:11:16.981   Training iter 450, batch loss 0.2351, batch acc 0.8244
10:11:17.479   Training iter 500, batch loss 0.2330, batch acc 0.8220
10:11:17.962   Training iter 550, batch loss 0.2360, batch acc 0.8162
10:11:18.461   Training iter 600, batch loss 0.2351, batch acc 0.8302
10:11:18.462 Training @ 395 epoch...
10:11:18.976   Training iter 50, batch loss 0.2347, batch acc 0.8204
10:11:19.464   Training iter 100, batch loss 0.2398, batch acc 0.8070
10:11:19.944   Training iter 150, batch loss 0.2364, batch acc 0.8268
10:11:20.432   Training iter 200, batch loss 0.2322, batch acc 0.8288
10:11:20.888   Training iter 250, batch loss 0.2347, batch acc 0.8262
10:11:21.370   Training iter 300, batch loss 0.2367, batch acc 0.8204
10:11:21.854   Training iter 350, batch loss 0.2354, batch acc 0.8254
10:11:22.313   Training iter 400, batch loss 0.2367, batch acc 0.8154
10:11:22.755   Training iter 450, batch loss 0.2348, batch acc 0.8198
10:11:23.193   Training iter 500, batch loss 0.2341, batch acc 0.8306
10:11:23.626   Training iter 550, batch loss 0.2359, batch acc 0.8252
10:11:24.063   Training iter 600, batch loss 0.2352, batch acc 0.8286
10:11:24.064 Testing @ 395 epoch...
10:11:24.099     Testing, total mean loss 0.23189, total acc 0.83200
10:11:24.099 Training @ 396 epoch...
10:11:24.549   Training iter 50, batch loss 0.2370, batch acc 0.8238
10:11:25.017   Training iter 100, batch loss 0.2355, batch acc 0.8206
10:11:25.493   Training iter 150, batch loss 0.2356, batch acc 0.8186
10:11:25.992   Training iter 200, batch loss 0.2322, batch acc 0.8366
10:11:26.485   Training iter 250, batch loss 0.2364, batch acc 0.8216
10:11:26.969   Training iter 300, batch loss 0.2362, batch acc 0.8194
10:11:27.510   Training iter 350, batch loss 0.2359, batch acc 0.8206
10:11:28.077   Training iter 400, batch loss 0.2357, batch acc 0.8270
10:11:28.639   Training iter 450, batch loss 0.2337, batch acc 0.8248
10:11:29.153   Training iter 500, batch loss 0.2358, batch acc 0.8180
10:11:29.651   Training iter 550, batch loss 0.2343, batch acc 0.8276
10:11:30.153   Training iter 600, batch loss 0.2380, batch acc 0.8192
10:11:30.154 Training @ 397 epoch...
10:11:30.659   Training iter 50, batch loss 0.2374, batch acc 0.8266
10:11:31.171   Training iter 100, batch loss 0.2354, batch acc 0.8230
10:11:31.668   Training iter 150, batch loss 0.2351, batch acc 0.8278
10:11:32.146   Training iter 200, batch loss 0.2331, batch acc 0.8284
10:11:32.626   Training iter 250, batch loss 0.2369, batch acc 0.8138
10:11:33.126   Training iter 300, batch loss 0.2367, batch acc 0.8192
10:11:33.624   Training iter 350, batch loss 0.2309, batch acc 0.8346
10:11:34.122   Training iter 400, batch loss 0.2367, batch acc 0.8168
10:11:34.582   Training iter 450, batch loss 0.2363, batch acc 0.8216
10:11:35.027   Training iter 500, batch loss 0.2362, batch acc 0.8196
10:11:35.488   Training iter 550, batch loss 0.2373, batch acc 0.8196
10:11:35.937   Training iter 600, batch loss 0.2342, batch acc 0.8262
10:11:35.939 Training @ 398 epoch...
10:11:36.406   Training iter 50, batch loss 0.2375, batch acc 0.8102
10:11:36.885   Training iter 100, batch loss 0.2331, batch acc 0.8282
10:11:37.360   Training iter 150, batch loss 0.2354, batch acc 0.8188
10:11:37.814   Training iter 200, batch loss 0.2355, batch acc 0.8182
10:11:38.252   Training iter 250, batch loss 0.2366, batch acc 0.8294
10:11:38.696   Training iter 300, batch loss 0.2341, batch acc 0.8188
10:11:39.141   Training iter 350, batch loss 0.2348, batch acc 0.8256
10:11:39.581   Training iter 400, batch loss 0.2362, batch acc 0.8294
10:11:40.030   Training iter 450, batch loss 0.2375, batch acc 0.8200
10:11:40.509   Training iter 500, batch loss 0.2336, batch acc 0.8308
10:11:40.995   Training iter 550, batch loss 0.2360, batch acc 0.8222
10:11:41.469   Training iter 600, batch loss 0.2357, batch acc 0.8244
10:11:41.470 Training @ 399 epoch...
10:11:41.936   Training iter 50, batch loss 0.2372, batch acc 0.8182
10:11:42.395   Training iter 100, batch loss 0.2366, batch acc 0.8192
10:11:42.905   Training iter 150, batch loss 0.2357, batch acc 0.8194
10:11:43.389   Training iter 200, batch loss 0.2350, batch acc 0.8280
10:11:43.865   Training iter 250, batch loss 0.2356, batch acc 0.8288
10:11:44.341   Training iter 300, batch loss 0.2350, batch acc 0.8154
10:11:44.832   Training iter 350, batch loss 0.2361, batch acc 0.8218
10:11:45.344   Training iter 400, batch loss 0.2354, batch acc 0.8224
10:11:45.861   Training iter 450, batch loss 0.2327, batch acc 0.8296
10:11:46.377   Training iter 500, batch loss 0.2337, batch acc 0.8272
10:11:46.897   Training iter 550, batch loss 0.2355, batch acc 0.8274
10:11:47.411   Training iter 600, batch loss 0.2374, batch acc 0.8200
10:11:47.413 Training @ 400 epoch...
10:11:47.944   Training iter 50, batch loss 0.2352, batch acc 0.8280
10:11:48.525   Training iter 100, batch loss 0.2363, batch acc 0.8210
10:11:49.042   Training iter 150, batch loss 0.2371, batch acc 0.8226
10:11:49.560   Training iter 200, batch loss 0.2364, batch acc 0.8188
10:11:50.095   Training iter 250, batch loss 0.2355, batch acc 0.8238
10:11:50.604   Training iter 300, batch loss 0.2352, batch acc 0.8254
10:11:51.090   Training iter 350, batch loss 0.2374, batch acc 0.8182
10:11:51.552   Training iter 400, batch loss 0.2363, batch acc 0.8210
10:11:52.009   Training iter 450, batch loss 0.2350, batch acc 0.8244
10:11:52.480   Training iter 500, batch loss 0.2365, batch acc 0.8194
10:11:52.936   Training iter 550, batch loss 0.2323, batch acc 0.8288
10:11:53.401   Training iter 600, batch loss 0.2327, batch acc 0.8246
10:11:53.403 Testing @ 400 epoch...
10:11:53.439     Testing, total mean loss 0.23183, total acc 0.83210
10:11:53.439 Plot @ 400 epoch...
10:11:53.439 Training @ 401 epoch...
10:11:53.896   Training iter 50, batch loss 0.2325, batch acc 0.8318
10:11:54.383   Training iter 100, batch loss 0.2360, batch acc 0.8218
10:11:54.871   Training iter 150, batch loss 0.2372, batch acc 0.8136
10:11:55.351   Training iter 200, batch loss 0.2341, batch acc 0.8274
10:11:55.832   Training iter 250, batch loss 0.2353, batch acc 0.8284
10:11:56.314   Training iter 300, batch loss 0.2373, batch acc 0.8188
10:11:56.775   Training iter 350, batch loss 0.2352, batch acc 0.8198
10:11:57.236   Training iter 400, batch loss 0.2358, batch acc 0.8222
10:11:57.713   Training iter 450, batch loss 0.2363, batch acc 0.8202
10:11:58.174   Training iter 500, batch loss 0.2353, batch acc 0.8268
10:11:58.630   Training iter 550, batch loss 0.2342, batch acc 0.8246
10:11:59.075   Training iter 600, batch loss 0.2365, batch acc 0.8206
10:11:59.076 Training @ 402 epoch...
10:11:59.517   Training iter 50, batch loss 0.2345, batch acc 0.8234
10:12:00.011   Training iter 100, batch loss 0.2337, batch acc 0.8318
10:12:00.495   Training iter 150, batch loss 0.2340, batch acc 0.8206
10:12:00.968   Training iter 200, batch loss 0.2350, batch acc 0.8234
10:12:01.472   Training iter 250, batch loss 0.2319, batch acc 0.8334
10:12:01.994   Training iter 300, batch loss 0.2370, batch acc 0.8240
10:12:02.520   Training iter 350, batch loss 0.2354, batch acc 0.8202
10:12:03.046   Training iter 400, batch loss 0.2380, batch acc 0.8192
10:12:03.579   Training iter 450, batch loss 0.2365, batch acc 0.8238
10:12:04.129   Training iter 500, batch loss 0.2364, batch acc 0.8244
10:12:04.666   Training iter 550, batch loss 0.2359, batch acc 0.8210
10:12:05.211   Training iter 600, batch loss 0.2374, batch acc 0.8116
10:12:05.212 Training @ 403 epoch...
10:12:05.761   Training iter 50, batch loss 0.2366, batch acc 0.8136
10:12:06.263   Training iter 100, batch loss 0.2388, batch acc 0.8148
10:12:06.787   Training iter 150, batch loss 0.2354, batch acc 0.8222
10:12:07.311   Training iter 200, batch loss 0.2358, batch acc 0.8198
10:12:07.780   Training iter 250, batch loss 0.2323, batch acc 0.8292
10:12:08.261   Training iter 300, batch loss 0.2370, batch acc 0.8198
10:12:08.723   Training iter 350, batch loss 0.2341, batch acc 0.8286
10:12:09.184   Training iter 400, batch loss 0.2346, batch acc 0.8286
10:12:09.646   Training iter 450, batch loss 0.2339, batch acc 0.8332
10:12:10.109   Training iter 500, batch loss 0.2368, batch acc 0.8200
10:12:10.619   Training iter 550, batch loss 0.2347, batch acc 0.8254
10:12:11.123   Training iter 600, batch loss 0.2353, batch acc 0.8224
10:12:11.125 Training @ 404 epoch...
10:12:11.609   Training iter 50, batch loss 0.2382, batch acc 0.8112
10:12:12.083   Training iter 100, batch loss 0.2351, batch acc 0.8264
10:12:12.540   Training iter 150, batch loss 0.2337, batch acc 0.8316
10:12:12.989   Training iter 200, batch loss 0.2338, batch acc 0.8242
10:12:13.457   Training iter 250, batch loss 0.2374, batch acc 0.8230
10:12:13.901   Training iter 300, batch loss 0.2340, batch acc 0.8248
10:12:14.349   Training iter 350, batch loss 0.2371, batch acc 0.8238
10:12:14.794   Training iter 400, batch loss 0.2369, batch acc 0.8184
10:12:15.249   Training iter 450, batch loss 0.2335, batch acc 0.8240
10:12:15.709   Training iter 500, batch loss 0.2348, batch acc 0.8248
10:12:16.177   Training iter 550, batch loss 0.2356, batch acc 0.8182
10:12:16.645   Training iter 600, batch loss 0.2352, batch acc 0.8258
10:12:16.646 Training @ 405 epoch...
10:12:17.101   Training iter 50, batch loss 0.2378, batch acc 0.8200
10:12:17.587   Training iter 100, batch loss 0.2358, batch acc 0.8194
10:12:18.092   Training iter 150, batch loss 0.2361, batch acc 0.8250
10:12:18.585   Training iter 200, batch loss 0.2366, batch acc 0.8246
10:12:19.067   Training iter 250, batch loss 0.2349, batch acc 0.8232
10:12:19.552   Training iter 300, batch loss 0.2353, batch acc 0.8200
10:12:20.038   Training iter 350, batch loss 0.2356, batch acc 0.8224
10:12:20.525   Training iter 400, batch loss 0.2351, batch acc 0.8238
10:12:21.027   Training iter 450, batch loss 0.2309, batch acc 0.8310
10:12:21.515   Training iter 500, batch loss 0.2356, batch acc 0.8212
10:12:22.008   Training iter 550, batch loss 0.2365, batch acc 0.8218
10:12:22.520   Training iter 600, batch loss 0.2348, batch acc 0.8252
10:12:22.522 Testing @ 405 epoch...
10:12:22.557     Testing, total mean loss 0.23177, total acc 0.83230
10:12:22.557 Training @ 406 epoch...
10:12:23.076   Training iter 50, batch loss 0.2345, batch acc 0.8202
10:12:23.578   Training iter 100, batch loss 0.2351, batch acc 0.8220
10:12:24.068   Training iter 150, batch loss 0.2351, batch acc 0.8204
10:12:24.613   Training iter 200, batch loss 0.2351, batch acc 0.8214
10:12:25.181   Training iter 250, batch loss 0.2359, batch acc 0.8276
10:12:25.757   Training iter 300, batch loss 0.2348, batch acc 0.8260
10:12:26.309   Training iter 350, batch loss 0.2356, batch acc 0.8246
10:12:26.812   Training iter 400, batch loss 0.2336, batch acc 0.8276
10:12:27.337   Training iter 450, batch loss 0.2361, batch acc 0.8236
10:12:27.876   Training iter 500, batch loss 0.2344, batch acc 0.8290
10:12:28.401   Training iter 550, batch loss 0.2375, batch acc 0.8142
10:12:28.908   Training iter 600, batch loss 0.2374, batch acc 0.8188
10:12:28.909 Training @ 407 epoch...
10:12:29.419   Training iter 50, batch loss 0.2350, batch acc 0.8204
10:12:29.934   Training iter 100, batch loss 0.2357, batch acc 0.8240
10:12:30.479   Training iter 150, batch loss 0.2379, batch acc 0.8140
10:12:31.007   Training iter 200, batch loss 0.2349, batch acc 0.8248
10:12:31.523   Training iter 250, batch loss 0.2368, batch acc 0.8272
10:12:32.036   Training iter 300, batch loss 0.2339, batch acc 0.8276
10:12:32.566   Training iter 350, batch loss 0.2363, batch acc 0.8124
10:12:33.131   Training iter 400, batch loss 0.2359, batch acc 0.8190
10:12:33.664   Training iter 450, batch loss 0.2360, batch acc 0.8206
10:12:34.188   Training iter 500, batch loss 0.2351, batch acc 0.8250
10:12:34.715   Training iter 550, batch loss 0.2318, batch acc 0.8352
10:12:35.245   Training iter 600, batch loss 0.2354, batch acc 0.8274
10:12:35.247 Training @ 408 epoch...
10:12:35.764   Training iter 50, batch loss 0.2362, batch acc 0.8226
10:12:36.266   Training iter 100, batch loss 0.2380, batch acc 0.8172
10:12:36.773   Training iter 150, batch loss 0.2326, batch acc 0.8332
10:12:37.267   Training iter 200, batch loss 0.2368, batch acc 0.8206
10:12:37.780   Training iter 250, batch loss 0.2342, batch acc 0.8272
10:12:38.294   Training iter 300, batch loss 0.2325, batch acc 0.8316
10:12:38.803   Training iter 350, batch loss 0.2365, batch acc 0.8202
10:12:39.288   Training iter 400, batch loss 0.2382, batch acc 0.8118
10:12:39.842   Training iter 450, batch loss 0.2331, batch acc 0.8264
10:12:40.354   Training iter 500, batch loss 0.2346, batch acc 0.8218
10:12:40.843   Training iter 550, batch loss 0.2350, batch acc 0.8218
10:12:41.332   Training iter 600, batch loss 0.2371, batch acc 0.8204
10:12:41.334 Training @ 409 epoch...
10:12:41.837   Training iter 50, batch loss 0.2395, batch acc 0.8152
10:12:42.329   Training iter 100, batch loss 0.2344, batch acc 0.8248
10:12:42.836   Training iter 150, batch loss 0.2337, batch acc 0.8262
10:12:43.305   Training iter 200, batch loss 0.2358, batch acc 0.8268
10:12:43.768   Training iter 250, batch loss 0.2384, batch acc 0.8182
10:12:44.265   Training iter 300, batch loss 0.2366, batch acc 0.8190
10:12:44.758   Training iter 350, batch loss 0.2358, batch acc 0.8246
10:12:45.252   Training iter 400, batch loss 0.2333, batch acc 0.8272
10:12:45.744   Training iter 450, batch loss 0.2343, batch acc 0.8282
10:12:46.242   Training iter 500, batch loss 0.2351, batch acc 0.8232
10:12:46.735   Training iter 550, batch loss 0.2339, batch acc 0.8230
10:12:47.217   Training iter 600, batch loss 0.2337, batch acc 0.8240
10:12:47.218 Training @ 410 epoch...
10:12:47.703   Training iter 50, batch loss 0.2364, batch acc 0.8306
10:12:48.207   Training iter 100, batch loss 0.2367, batch acc 0.8176
10:12:48.681   Training iter 150, batch loss 0.2312, batch acc 0.8308
10:12:49.187   Training iter 200, batch loss 0.2391, batch acc 0.8116
10:12:49.701   Training iter 250, batch loss 0.2353, batch acc 0.8312
10:12:50.223   Training iter 300, batch loss 0.2346, batch acc 0.8260
10:12:50.759   Training iter 350, batch loss 0.2343, batch acc 0.8222
10:12:51.296   Training iter 400, batch loss 0.2354, batch acc 0.8234
10:12:51.827   Training iter 450, batch loss 0.2348, batch acc 0.8210
10:12:52.355   Training iter 500, batch loss 0.2353, batch acc 0.8214
10:12:52.886   Training iter 550, batch loss 0.2354, batch acc 0.8238
10:12:53.416   Training iter 600, batch loss 0.2357, batch acc 0.8176
10:12:53.418 Testing @ 410 epoch...
10:12:53.453     Testing, total mean loss 0.23170, total acc 0.83230
10:12:53.453 Training @ 411 epoch...
10:12:53.967   Training iter 50, batch loss 0.2346, batch acc 0.8266
10:12:54.485   Training iter 100, batch loss 0.2353, batch acc 0.8248
10:12:54.997   Training iter 150, batch loss 0.2352, batch acc 0.8338
10:12:55.544   Training iter 200, batch loss 0.2375, batch acc 0.8154
10:12:56.011   Training iter 250, batch loss 0.2351, batch acc 0.8160
10:12:56.472   Training iter 300, batch loss 0.2337, batch acc 0.8226
10:12:56.962   Training iter 350, batch loss 0.2337, batch acc 0.8212
10:12:57.454   Training iter 400, batch loss 0.2357, batch acc 0.8372
10:12:57.954   Training iter 450, batch loss 0.2343, batch acc 0.8258
10:12:58.454   Training iter 500, batch loss 0.2392, batch acc 0.8174
10:12:58.973   Training iter 550, batch loss 0.2366, batch acc 0.8140
10:12:59.492   Training iter 600, batch loss 0.2332, batch acc 0.8218
10:12:59.494 Training @ 412 epoch...
10:13:00.095   Training iter 50, batch loss 0.2330, batch acc 0.8326
10:13:00.583   Training iter 100, batch loss 0.2347, batch acc 0.8186
10:13:01.079   Training iter 150, batch loss 0.2381, batch acc 0.8182
10:13:01.585   Training iter 200, batch loss 0.2350, batch acc 0.8292
10:13:02.067   Training iter 250, batch loss 0.2362, batch acc 0.8156
10:13:02.537   Training iter 300, batch loss 0.2355, batch acc 0.8220
10:13:02.975   Training iter 350, batch loss 0.2316, batch acc 0.8280
10:13:03.425   Training iter 400, batch loss 0.2363, batch acc 0.8182
10:13:03.865   Training iter 450, batch loss 0.2355, batch acc 0.8260
10:13:04.301   Training iter 500, batch loss 0.2343, batch acc 0.8302
10:13:04.765   Training iter 550, batch loss 0.2379, batch acc 0.8200
10:13:05.231   Training iter 600, batch loss 0.2360, batch acc 0.8194
10:13:05.233 Training @ 413 epoch...
10:13:05.713   Training iter 50, batch loss 0.2351, batch acc 0.8210
10:13:06.205   Training iter 100, batch loss 0.2321, batch acc 0.8320
10:13:06.684   Training iter 150, batch loss 0.2344, batch acc 0.8266
10:13:07.181   Training iter 200, batch loss 0.2341, batch acc 0.8284
10:13:07.677   Training iter 250, batch loss 0.2357, batch acc 0.8210
10:13:08.186   Training iter 300, batch loss 0.2362, batch acc 0.8182
10:13:08.691   Training iter 350, batch loss 0.2348, batch acc 0.8236
10:13:09.184   Training iter 400, batch loss 0.2379, batch acc 0.8166
10:13:09.673   Training iter 450, batch loss 0.2356, batch acc 0.8210
10:13:10.144   Training iter 500, batch loss 0.2364, batch acc 0.8218
10:13:10.640   Training iter 550, batch loss 0.2356, batch acc 0.8200
10:13:11.158   Training iter 600, batch loss 0.2361, batch acc 0.8266
10:13:11.159 Training @ 414 epoch...
10:13:11.662   Training iter 50, batch loss 0.2348, batch acc 0.8244
10:13:12.150   Training iter 100, batch loss 0.2358, batch acc 0.8248
10:13:12.634   Training iter 150, batch loss 0.2348, batch acc 0.8266
10:13:13.105   Training iter 200, batch loss 0.2358, batch acc 0.8200
10:13:13.549   Training iter 250, batch loss 0.2355, batch acc 0.8246
10:13:13.979   Training iter 300, batch loss 0.2337, batch acc 0.8302
10:13:14.420   Training iter 350, batch loss 0.2360, batch acc 0.8192
10:13:14.860   Training iter 400, batch loss 0.2321, batch acc 0.8308
10:13:15.321   Training iter 450, batch loss 0.2359, batch acc 0.8216
10:13:15.797   Training iter 500, batch loss 0.2352, batch acc 0.8208
10:13:16.277   Training iter 550, batch loss 0.2373, batch acc 0.8164
10:13:16.760   Training iter 600, batch loss 0.2369, batch acc 0.8178
10:13:16.761 Training @ 415 epoch...
10:13:17.246   Training iter 50, batch loss 0.2350, batch acc 0.8276
10:13:17.722   Training iter 100, batch loss 0.2373, batch acc 0.8164
10:13:18.205   Training iter 150, batch loss 0.2349, batch acc 0.8302
10:13:18.736   Training iter 200, batch loss 0.2343, batch acc 0.8282
10:13:19.431   Training iter 250, batch loss 0.2320, batch acc 0.8276
10:13:20.070   Training iter 300, batch loss 0.2361, batch acc 0.8186
10:13:20.578   Training iter 350, batch loss 0.2385, batch acc 0.8164
10:13:21.026   Training iter 400, batch loss 0.2355, batch acc 0.8192
10:13:21.459   Training iter 450, batch loss 0.2360, batch acc 0.8190
10:13:21.916   Training iter 500, batch loss 0.2349, batch acc 0.8256
10:13:22.393   Training iter 550, batch loss 0.2364, batch acc 0.8270
10:13:22.844   Training iter 600, batch loss 0.2325, batch acc 0.8216
10:13:22.846 Testing @ 415 epoch...
10:13:22.881     Testing, total mean loss 0.23165, total acc 0.83240
10:13:22.881 Training @ 416 epoch...
10:13:23.359   Training iter 50, batch loss 0.2340, batch acc 0.8210
10:13:23.834   Training iter 100, batch loss 0.2333, batch acc 0.8290
10:13:24.324   Training iter 150, batch loss 0.2363, batch acc 0.8242
10:13:24.833   Training iter 200, batch loss 0.2352, batch acc 0.8222
10:13:25.346   Training iter 250, batch loss 0.2383, batch acc 0.8086
10:13:25.847   Training iter 300, batch loss 0.2343, batch acc 0.8204
10:13:26.343   Training iter 350, batch loss 0.2363, batch acc 0.8242
10:13:26.842   Training iter 400, batch loss 0.2325, batch acc 0.8302
10:13:27.342   Training iter 450, batch loss 0.2353, batch acc 0.8244
10:13:27.825   Training iter 500, batch loss 0.2357, batch acc 0.8262
10:13:28.312   Training iter 550, batch loss 0.2370, batch acc 0.8210
10:13:28.793   Training iter 600, batch loss 0.2353, batch acc 0.8260
10:13:28.795 Training @ 417 epoch...
10:13:29.309   Training iter 50, batch loss 0.2341, batch acc 0.8274
10:13:29.795   Training iter 100, batch loss 0.2342, batch acc 0.8276
10:13:30.291   Training iter 150, batch loss 0.2365, batch acc 0.8120
10:13:30.764   Training iter 200, batch loss 0.2354, batch acc 0.8210
10:13:31.229   Training iter 250, batch loss 0.2366, batch acc 0.8238
10:13:31.700   Training iter 300, batch loss 0.2353, batch acc 0.8224
10:13:32.168   Training iter 350, batch loss 0.2350, batch acc 0.8234
10:13:32.635   Training iter 400, batch loss 0.2353, batch acc 0.8184
10:13:33.120   Training iter 450, batch loss 0.2337, batch acc 0.8288
10:13:33.589   Training iter 500, batch loss 0.2366, batch acc 0.8188
10:13:34.074   Training iter 550, batch loss 0.2354, batch acc 0.8264
10:13:34.545   Training iter 600, batch loss 0.2353, batch acc 0.8288
10:13:34.547 Training @ 418 epoch...
10:13:35.025   Training iter 50, batch loss 0.2353, batch acc 0.8288
10:13:35.500   Training iter 100, batch loss 0.2325, batch acc 0.8286
10:13:35.967   Training iter 150, batch loss 0.2350, batch acc 0.8160
10:13:36.440   Training iter 200, batch loss 0.2375, batch acc 0.8152
10:13:36.923   Training iter 250, batch loss 0.2336, batch acc 0.8220
10:13:37.431   Training iter 300, batch loss 0.2345, batch acc 0.8354
10:13:37.960   Training iter 350, batch loss 0.2347, batch acc 0.8312
10:13:38.499   Training iter 400, batch loss 0.2364, batch acc 0.8228
10:13:39.027   Training iter 450, batch loss 0.2338, batch acc 0.8262
10:13:39.534   Training iter 500, batch loss 0.2368, batch acc 0.8186
10:13:40.050   Training iter 550, batch loss 0.2356, batch acc 0.8184
10:13:40.566   Training iter 600, batch loss 0.2376, batch acc 0.8174
10:13:40.568 Training @ 419 epoch...
10:13:41.092   Training iter 50, batch loss 0.2341, batch acc 0.8236
10:13:41.604   Training iter 100, batch loss 0.2323, batch acc 0.8374
10:13:42.125   Training iter 150, batch loss 0.2368, batch acc 0.8186
10:13:42.637   Training iter 200, batch loss 0.2342, batch acc 0.8188
10:13:43.167   Training iter 250, batch loss 0.2369, batch acc 0.8164
10:13:43.656   Training iter 300, batch loss 0.2359, batch acc 0.8172
10:13:44.123   Training iter 350, batch loss 0.2352, batch acc 0.8228
10:13:44.594   Training iter 400, batch loss 0.2336, batch acc 0.8360
10:13:45.069   Training iter 450, batch loss 0.2359, batch acc 0.8208
10:13:45.543   Training iter 500, batch loss 0.2358, batch acc 0.8208
10:13:46.015   Training iter 550, batch loss 0.2372, batch acc 0.8216
10:13:46.485   Training iter 600, batch loss 0.2352, batch acc 0.8250
10:13:46.487 Training @ 420 epoch...
10:13:46.971   Training iter 50, batch loss 0.2332, batch acc 0.8246
10:13:47.437   Training iter 100, batch loss 0.2345, batch acc 0.8250
10:13:47.906   Training iter 150, batch loss 0.2384, batch acc 0.8158
10:13:48.378   Training iter 200, batch loss 0.2348, batch acc 0.8264
10:13:48.855   Training iter 250, batch loss 0.2341, batch acc 0.8250
10:13:49.336   Training iter 300, batch loss 0.2339, batch acc 0.8262
10:13:49.812   Training iter 350, batch loss 0.2344, batch acc 0.8262
10:13:50.281   Training iter 400, batch loss 0.2376, batch acc 0.8136
10:13:50.720   Training iter 450, batch loss 0.2348, batch acc 0.8256
10:13:51.169   Training iter 500, batch loss 0.2357, batch acc 0.8262
10:13:51.630   Training iter 550, batch loss 0.2339, batch acc 0.8292
10:13:52.092   Training iter 600, batch loss 0.2379, batch acc 0.8128
10:13:52.093 Testing @ 420 epoch...
10:13:52.128     Testing, total mean loss 0.23160, total acc 0.83250
10:13:52.128 Training @ 421 epoch...
10:13:52.602   Training iter 50, batch loss 0.2355, batch acc 0.8254
10:13:53.059   Training iter 100, batch loss 0.2318, batch acc 0.8332
10:13:53.540   Training iter 150, batch loss 0.2328, batch acc 0.8302
10:13:54.028   Training iter 200, batch loss 0.2345, batch acc 0.8202
10:13:54.504   Training iter 250, batch loss 0.2356, batch acc 0.8244
10:13:54.978   Training iter 300, batch loss 0.2365, batch acc 0.8134
10:13:55.448   Training iter 350, batch loss 0.2359, batch acc 0.8222
10:13:55.918   Training iter 400, batch loss 0.2347, batch acc 0.8262
10:13:56.413   Training iter 450, batch loss 0.2365, batch acc 0.8194
10:13:56.885   Training iter 500, batch loss 0.2367, batch acc 0.8208
10:13:57.379   Training iter 550, batch loss 0.2344, batch acc 0.8230
10:13:57.892   Training iter 600, batch loss 0.2380, batch acc 0.8190
10:13:57.894 Training @ 422 epoch...
10:13:58.417   Training iter 50, batch loss 0.2334, batch acc 0.8248
10:13:58.912   Training iter 100, batch loss 0.2367, batch acc 0.8244
10:13:59.383   Training iter 150, batch loss 0.2354, batch acc 0.8210
10:13:59.825   Training iter 200, batch loss 0.2363, batch acc 0.8220
10:14:00.284   Training iter 250, batch loss 0.2332, batch acc 0.8316
10:14:00.743   Training iter 300, batch loss 0.2367, batch acc 0.8192
10:14:01.201   Training iter 350, batch loss 0.2345, batch acc 0.8200
10:14:01.709   Training iter 400, batch loss 0.2379, batch acc 0.8180
10:14:02.248   Training iter 450, batch loss 0.2314, batch acc 0.8328
10:14:02.778   Training iter 500, batch loss 0.2366, batch acc 0.8166
10:14:03.310   Training iter 550, batch loss 0.2351, batch acc 0.8216
10:14:03.839   Training iter 600, batch loss 0.2357, batch acc 0.8300
10:14:03.841 Training @ 423 epoch...
10:14:04.381   Training iter 50, batch loss 0.2348, batch acc 0.8206
10:14:04.885   Training iter 100, batch loss 0.2365, batch acc 0.8196
10:14:05.415   Training iter 150, batch loss 0.2351, batch acc 0.8222
10:14:05.902   Training iter 200, batch loss 0.2364, batch acc 0.8232
10:14:06.417   Training iter 250, batch loss 0.2301, batch acc 0.8414
10:14:06.908   Training iter 300, batch loss 0.2330, batch acc 0.8354
10:14:07.366   Training iter 350, batch loss 0.2361, batch acc 0.8230
10:14:07.831   Training iter 400, batch loss 0.2370, batch acc 0.8124
10:14:08.322   Training iter 450, batch loss 0.2357, batch acc 0.8184
10:14:08.789   Training iter 500, batch loss 0.2357, batch acc 0.8210
10:14:09.253   Training iter 550, batch loss 0.2368, batch acc 0.8152
10:14:09.747   Training iter 600, batch loss 0.2355, batch acc 0.8264
10:14:09.748 Training @ 424 epoch...
10:14:10.276   Training iter 50, batch loss 0.2326, batch acc 0.8316
10:14:10.790   Training iter 100, batch loss 0.2350, batch acc 0.8244
10:14:11.300   Training iter 150, batch loss 0.2369, batch acc 0.8236
10:14:11.811   Training iter 200, batch loss 0.2344, batch acc 0.8286
10:14:12.326   Training iter 250, batch loss 0.2378, batch acc 0.8164
10:14:12.874   Training iter 300, batch loss 0.2333, batch acc 0.8284
10:14:13.412   Training iter 350, batch loss 0.2347, batch acc 0.8240
10:14:13.957   Training iter 400, batch loss 0.2386, batch acc 0.8100
10:14:14.483   Training iter 450, batch loss 0.2335, batch acc 0.8210
10:14:15.038   Training iter 500, batch loss 0.2356, batch acc 0.8264
10:14:15.562   Training iter 550, batch loss 0.2356, batch acc 0.8158
10:14:16.114   Training iter 600, batch loss 0.2345, batch acc 0.8296
10:14:16.116 Training @ 425 epoch...
10:14:16.687   Training iter 50, batch loss 0.2352, batch acc 0.8256
10:14:17.265   Training iter 100, batch loss 0.2359, batch acc 0.8218
10:14:17.788   Training iter 150, batch loss 0.2341, batch acc 0.8254
10:14:18.277   Training iter 200, batch loss 0.2368, batch acc 0.8190
10:14:18.764   Training iter 250, batch loss 0.2344, batch acc 0.8234
10:14:19.278   Training iter 300, batch loss 0.2324, batch acc 0.8328
10:14:19.805   Training iter 350, batch loss 0.2360, batch acc 0.8206
10:14:20.326   Training iter 400, batch loss 0.2344, batch acc 0.8178
10:14:20.855   Training iter 450, batch loss 0.2337, batch acc 0.8264
10:14:21.370   Training iter 500, batch loss 0.2365, batch acc 0.8182
10:14:21.875   Training iter 550, batch loss 0.2366, batch acc 0.8236
10:14:22.398   Training iter 600, batch loss 0.2364, batch acc 0.8254
10:14:22.400 Testing @ 425 epoch...
10:14:22.438     Testing, total mean loss 0.23154, total acc 0.83250
10:14:22.438 Training @ 426 epoch...
10:14:22.960   Training iter 50, batch loss 0.2359, batch acc 0.8236
10:14:23.484   Training iter 100, batch loss 0.2351, batch acc 0.8216
10:14:24.008   Training iter 150, batch loss 0.2348, batch acc 0.8228
10:14:24.528   Training iter 200, batch loss 0.2362, batch acc 0.8236
10:14:25.038   Training iter 250, batch loss 0.2322, batch acc 0.8344
10:14:25.561   Training iter 300, batch loss 0.2345, batch acc 0.8264
10:14:26.081   Training iter 350, batch loss 0.2373, batch acc 0.8264
10:14:26.594   Training iter 400, batch loss 0.2364, batch acc 0.8162
10:14:27.105   Training iter 450, batch loss 0.2360, batch acc 0.8226
10:14:27.614   Training iter 500, batch loss 0.2353, batch acc 0.8230
10:14:28.102   Training iter 550, batch loss 0.2324, batch acc 0.8214
10:14:28.592   Training iter 600, batch loss 0.2360, batch acc 0.8186
10:14:28.593 Training @ 427 epoch...
10:14:29.089   Training iter 50, batch loss 0.2369, batch acc 0.8128
10:14:29.552   Training iter 100, batch loss 0.2334, batch acc 0.8214
10:14:29.996   Training iter 150, batch loss 0.2337, batch acc 0.8286
10:14:30.438   Training iter 200, batch loss 0.2338, batch acc 0.8330
10:14:30.885   Training iter 250, batch loss 0.2345, batch acc 0.8242
10:14:31.321   Training iter 300, batch loss 0.2353, batch acc 0.8262
10:14:31.766   Training iter 350, batch loss 0.2375, batch acc 0.8216
10:14:32.212   Training iter 400, batch loss 0.2336, batch acc 0.8266
10:14:32.677   Training iter 450, batch loss 0.2346, batch acc 0.8188
10:14:33.179   Training iter 500, batch loss 0.2354, batch acc 0.8270
10:14:33.663   Training iter 550, batch loss 0.2354, batch acc 0.8250
10:14:34.121   Training iter 600, batch loss 0.2382, batch acc 0.8124
10:14:34.123 Training @ 428 epoch...
10:14:34.594   Training iter 50, batch loss 0.2347, batch acc 0.8224
10:14:35.081   Training iter 100, batch loss 0.2331, batch acc 0.8320
10:14:35.566   Training iter 150, batch loss 0.2337, batch acc 0.8314
10:14:36.051   Training iter 200, batch loss 0.2343, batch acc 0.8280
10:14:36.535   Training iter 250, batch loss 0.2365, batch acc 0.8188
10:14:37.028   Training iter 300, batch loss 0.2356, batch acc 0.8218
10:14:37.529   Training iter 350, batch loss 0.2376, batch acc 0.8174
10:14:38.034   Training iter 400, batch loss 0.2372, batch acc 0.8156
10:14:38.539   Training iter 450, batch loss 0.2333, batch acc 0.8296
10:14:39.034   Training iter 500, batch loss 0.2340, batch acc 0.8238
10:14:39.531   Training iter 550, batch loss 0.2372, batch acc 0.8166
10:14:40.032   Training iter 600, batch loss 0.2348, batch acc 0.8218
10:14:40.034 Training @ 429 epoch...
10:14:40.546   Training iter 50, batch loss 0.2356, batch acc 0.8272
10:14:41.073   Training iter 100, batch loss 0.2354, batch acc 0.8278
10:14:41.603   Training iter 150, batch loss 0.2335, batch acc 0.8304
10:14:42.168   Training iter 200, batch loss 0.2365, batch acc 0.8222
10:14:42.686   Training iter 250, batch loss 0.2355, batch acc 0.8150
10:14:43.210   Training iter 300, batch loss 0.2326, batch acc 0.8304
10:14:43.725   Training iter 350, batch loss 0.2313, batch acc 0.8286
10:14:44.241   Training iter 400, batch loss 0.2380, batch acc 0.8168
10:14:44.775   Training iter 450, batch loss 0.2357, batch acc 0.8188
10:14:45.299   Training iter 500, batch loss 0.2352, batch acc 0.8252
10:14:45.806   Training iter 550, batch loss 0.2379, batch acc 0.8172
10:14:46.319   Training iter 600, batch loss 0.2348, batch acc 0.8218
10:14:46.321 Training @ 430 epoch...
10:14:46.862   Training iter 50, batch loss 0.2336, batch acc 0.8284
10:14:47.393   Training iter 100, batch loss 0.2345, batch acc 0.8196
10:14:47.898   Training iter 150, batch loss 0.2366, batch acc 0.8174
10:14:48.407   Training iter 200, batch loss 0.2376, batch acc 0.8202
10:14:48.908   Training iter 250, batch loss 0.2351, batch acc 0.8230
10:14:49.405   Training iter 300, batch loss 0.2345, batch acc 0.8274
10:14:49.916   Training iter 350, batch loss 0.2351, batch acc 0.8216
10:14:50.430   Training iter 400, batch loss 0.2326, batch acc 0.8302
10:14:50.948   Training iter 450, batch loss 0.2348, batch acc 0.8270
10:14:51.450   Training iter 500, batch loss 0.2352, batch acc 0.8238
10:14:51.953   Training iter 550, batch loss 0.2353, batch acc 0.8200
10:14:52.460   Training iter 600, batch loss 0.2369, batch acc 0.8214
10:14:52.462 Testing @ 430 epoch...
10:14:52.497     Testing, total mean loss 0.23149, total acc 0.83250
10:14:52.497 Training @ 431 epoch...
10:14:53.021   Training iter 50, batch loss 0.2356, batch acc 0.8222
10:14:53.582   Training iter 100, batch loss 0.2364, batch acc 0.8186
10:14:54.136   Training iter 150, batch loss 0.2337, batch acc 0.8278
10:14:54.686   Training iter 200, batch loss 0.2362, batch acc 0.8204
10:14:55.204   Training iter 250, batch loss 0.2341, batch acc 0.8266
10:14:55.716   Training iter 300, batch loss 0.2326, batch acc 0.8250
10:14:56.232   Training iter 350, batch loss 0.2354, batch acc 0.8230
10:14:56.734   Training iter 400, batch loss 0.2326, batch acc 0.8278
10:14:57.241   Training iter 450, batch loss 0.2380, batch acc 0.8200
10:14:57.744   Training iter 500, batch loss 0.2359, batch acc 0.8202
10:14:58.231   Training iter 550, batch loss 0.2340, batch acc 0.8226
10:14:58.731   Training iter 600, batch loss 0.2372, batch acc 0.8264
10:14:58.733 Training @ 432 epoch...
10:14:59.252   Training iter 50, batch loss 0.2365, batch acc 0.8190
10:14:59.767   Training iter 100, batch loss 0.2365, batch acc 0.8208
10:15:00.272   Training iter 150, batch loss 0.2342, batch acc 0.8280
10:15:00.797   Training iter 200, batch loss 0.2357, batch acc 0.8154
10:15:01.334   Training iter 250, batch loss 0.2372, batch acc 0.8208
10:15:01.860   Training iter 300, batch loss 0.2371, batch acc 0.8162
10:15:02.349   Training iter 350, batch loss 0.2329, batch acc 0.8356
10:15:02.844   Training iter 400, batch loss 0.2362, batch acc 0.8214
10:15:03.308   Training iter 450, batch loss 0.2326, batch acc 0.8292
10:15:03.771   Training iter 500, batch loss 0.2341, batch acc 0.8252
10:15:04.239   Training iter 550, batch loss 0.2338, batch acc 0.8242
10:15:04.733   Training iter 600, batch loss 0.2348, batch acc 0.8260
10:15:04.735 Training @ 433 epoch...
10:15:05.248   Training iter 50, batch loss 0.2371, batch acc 0.8210
10:15:05.742   Training iter 100, batch loss 0.2359, batch acc 0.8286
10:15:06.236   Training iter 150, batch loss 0.2337, batch acc 0.8220
10:15:06.745   Training iter 200, batch loss 0.2381, batch acc 0.8152
10:15:07.258   Training iter 250, batch loss 0.2319, batch acc 0.8310
10:15:07.766   Training iter 300, batch loss 0.2355, batch acc 0.8216
10:15:08.264   Training iter 350, batch loss 0.2340, batch acc 0.8244
10:15:08.765   Training iter 400, batch loss 0.2368, batch acc 0.8244
10:15:09.256   Training iter 450, batch loss 0.2350, batch acc 0.8220
10:15:09.756   Training iter 500, batch loss 0.2362, batch acc 0.8210
10:15:10.259   Training iter 550, batch loss 0.2330, batch acc 0.8244
10:15:10.754   Training iter 600, batch loss 0.2344, batch acc 0.8244
10:15:10.755 Training @ 434 epoch...
10:15:11.246   Training iter 50, batch loss 0.2354, batch acc 0.8252
10:15:11.779   Training iter 100, batch loss 0.2334, batch acc 0.8252
10:15:12.270   Training iter 150, batch loss 0.2358, batch acc 0.8234
10:15:12.744   Training iter 200, batch loss 0.2318, batch acc 0.8302
10:15:13.216   Training iter 250, batch loss 0.2362, batch acc 0.8192
10:15:13.670   Training iter 300, batch loss 0.2341, batch acc 0.8248
10:15:14.124   Training iter 350, batch loss 0.2364, batch acc 0.8184
10:15:14.575   Training iter 400, batch loss 0.2373, batch acc 0.8172
10:15:15.046   Training iter 450, batch loss 0.2354, batch acc 0.8256
10:15:15.531   Training iter 500, batch loss 0.2351, batch acc 0.8252
10:15:16.009   Training iter 550, batch loss 0.2376, batch acc 0.8194
10:15:16.477   Training iter 600, batch loss 0.2328, batch acc 0.8262
10:15:16.478 Training @ 435 epoch...
10:15:16.946   Training iter 50, batch loss 0.2345, batch acc 0.8248
10:15:17.418   Training iter 100, batch loss 0.2352, batch acc 0.8192
10:15:17.916   Training iter 150, batch loss 0.2367, batch acc 0.8220
10:15:18.415   Training iter 200, batch loss 0.2342, batch acc 0.8220
10:15:18.922   Training iter 250, batch loss 0.2356, batch acc 0.8294
10:15:19.416   Training iter 300, batch loss 0.2365, batch acc 0.8184
10:15:19.911   Training iter 350, batch loss 0.2349, batch acc 0.8242
10:15:20.409   Training iter 400, batch loss 0.2350, batch acc 0.8244
10:15:20.907   Training iter 450, batch loss 0.2354, batch acc 0.8170
10:15:21.398   Training iter 500, batch loss 0.2350, batch acc 0.8250
10:15:21.903   Training iter 550, batch loss 0.2330, batch acc 0.8324
10:15:22.426   Training iter 600, batch loss 0.2354, batch acc 0.8224
10:15:22.428 Testing @ 435 epoch...
10:15:22.463     Testing, total mean loss 0.23145, total acc 0.83240
10:15:22.463 Training @ 436 epoch...
10:15:22.964   Training iter 50, batch loss 0.2356, batch acc 0.8248
10:15:23.457   Training iter 100, batch loss 0.2332, batch acc 0.8260
10:15:23.942   Training iter 150, batch loss 0.2368, batch acc 0.8190
10:15:24.427   Training iter 200, batch loss 0.2340, batch acc 0.8298
10:15:24.898   Training iter 250, batch loss 0.2368, batch acc 0.8162
10:15:25.378   Training iter 300, batch loss 0.2346, batch acc 0.8260
10:15:25.851   Training iter 350, batch loss 0.2352, batch acc 0.8222
10:15:26.324   Training iter 400, batch loss 0.2341, batch acc 0.8250
10:15:26.784   Training iter 450, batch loss 0.2357, batch acc 0.8224
10:15:27.254   Training iter 500, batch loss 0.2362, batch acc 0.8214
10:15:27.733   Training iter 550, batch loss 0.2338, batch acc 0.8284
10:15:28.224   Training iter 600, batch loss 0.2353, batch acc 0.8212
10:15:28.226 Training @ 437 epoch...
10:15:28.725   Training iter 50, batch loss 0.2351, batch acc 0.8224
10:15:29.224   Training iter 100, batch loss 0.2366, batch acc 0.8234
10:15:29.728   Training iter 150, batch loss 0.2339, batch acc 0.8240
10:15:30.247   Training iter 200, batch loss 0.2360, batch acc 0.8242
10:15:30.783   Training iter 250, batch loss 0.2333, batch acc 0.8266
10:15:31.307   Training iter 300, batch loss 0.2363, batch acc 0.8206
10:15:31.798   Training iter 350, batch loss 0.2354, batch acc 0.8226
10:15:32.278   Training iter 400, batch loss 0.2361, batch acc 0.8224
10:15:32.787   Training iter 450, batch loss 0.2371, batch acc 0.8136
10:15:33.301   Training iter 500, batch loss 0.2317, batch acc 0.8312
10:15:33.811   Training iter 550, batch loss 0.2337, batch acc 0.8284
10:15:34.310   Training iter 600, batch loss 0.2359, batch acc 0.8232
10:15:34.312 Training @ 438 epoch...
10:15:34.830   Training iter 50, batch loss 0.2337, batch acc 0.8302
10:15:35.349   Training iter 100, batch loss 0.2357, batch acc 0.8200
10:15:35.858   Training iter 150, batch loss 0.2330, batch acc 0.8276
10:15:36.370   Training iter 200, batch loss 0.2374, batch acc 0.8160
10:15:36.884   Training iter 250, batch loss 0.2334, batch acc 0.8298
10:15:37.401   Training iter 300, batch loss 0.2358, batch acc 0.8238
10:15:37.924   Training iter 350, batch loss 0.2363, batch acc 0.8196
10:15:38.453   Training iter 400, batch loss 0.2357, batch acc 0.8200
10:15:38.970   Training iter 450, batch loss 0.2355, batch acc 0.8188
10:15:39.502   Training iter 500, batch loss 0.2358, batch acc 0.8264
10:15:40.019   Training iter 550, batch loss 0.2341, batch acc 0.8308
10:15:40.548   Training iter 600, batch loss 0.2345, batch acc 0.8206
10:15:40.550 Training @ 439 epoch...
10:15:41.073   Training iter 50, batch loss 0.2384, batch acc 0.8144
10:15:41.588   Training iter 100, batch loss 0.2351, batch acc 0.8246
10:15:42.087   Training iter 150, batch loss 0.2346, batch acc 0.8270
10:15:42.595   Training iter 200, batch loss 0.2383, batch acc 0.8148
10:15:43.106   Training iter 250, batch loss 0.2346, batch acc 0.8220
10:15:43.626   Training iter 300, batch loss 0.2360, batch acc 0.8196
10:15:44.123   Training iter 350, batch loss 0.2357, batch acc 0.8162
10:15:44.630   Training iter 400, batch loss 0.2341, batch acc 0.8252
10:15:45.116   Training iter 450, batch loss 0.2331, batch acc 0.8264
10:15:45.594   Training iter 500, batch loss 0.2331, batch acc 0.8316
10:15:46.072   Training iter 550, batch loss 0.2328, batch acc 0.8334
10:15:46.555   Training iter 600, batch loss 0.2351, batch acc 0.8286
10:15:46.557 Training @ 440 epoch...
10:15:47.056   Training iter 50, batch loss 0.2366, batch acc 0.8248
10:15:47.562   Training iter 100, batch loss 0.2357, batch acc 0.8210
10:15:48.071   Training iter 150, batch loss 0.2364, batch acc 0.8240
10:15:48.576   Training iter 200, batch loss 0.2351, batch acc 0.8212
10:15:49.082   Training iter 250, batch loss 0.2343, batch acc 0.8252
10:15:49.582   Training iter 300, batch loss 0.2362, batch acc 0.8272
10:15:50.096   Training iter 350, batch loss 0.2348, batch acc 0.8280
10:15:50.607   Training iter 400, batch loss 0.2321, batch acc 0.8280
10:15:51.117   Training iter 450, batch loss 0.2336, batch acc 0.8244
10:15:51.608   Training iter 500, batch loss 0.2354, batch acc 0.8224
10:15:52.122   Training iter 550, batch loss 0.2349, batch acc 0.8160
10:15:52.638   Training iter 600, batch loss 0.2356, batch acc 0.8196
10:15:52.640 Testing @ 440 epoch...
10:15:52.675     Testing, total mean loss 0.23140, total acc 0.83240
10:15:52.675 Training @ 441 epoch...
10:15:53.185   Training iter 50, batch loss 0.2370, batch acc 0.8198
10:15:53.664   Training iter 100, batch loss 0.2346, batch acc 0.8188
10:15:54.113   Training iter 150, batch loss 0.2349, batch acc 0.8298
10:15:54.601   Training iter 200, batch loss 0.2355, batch acc 0.8210
10:15:55.059   Training iter 250, batch loss 0.2346, batch acc 0.8270
10:15:55.508   Training iter 300, batch loss 0.2337, batch acc 0.8262
10:15:55.962   Training iter 350, batch loss 0.2359, batch acc 0.8184
10:15:56.450   Training iter 400, batch loss 0.2327, batch acc 0.8252
10:15:56.898   Training iter 450, batch loss 0.2329, batch acc 0.8286
10:15:57.373   Training iter 500, batch loss 0.2367, batch acc 0.8222
10:15:57.858   Training iter 550, batch loss 0.2362, batch acc 0.8218
10:15:58.308   Training iter 600, batch loss 0.2361, batch acc 0.8248
10:15:58.310 Training @ 442 epoch...
10:15:58.770   Training iter 50, batch loss 0.2350, batch acc 0.8254
10:15:59.235   Training iter 100, batch loss 0.2360, batch acc 0.8174
10:15:59.690   Training iter 150, batch loss 0.2360, batch acc 0.8244
10:16:00.136   Training iter 200, batch loss 0.2343, batch acc 0.8258
10:16:00.617   Training iter 250, batch loss 0.2363, batch acc 0.8182
10:16:01.114   Training iter 300, batch loss 0.2350, batch acc 0.8280
10:16:01.647   Training iter 350, batch loss 0.2333, batch acc 0.8272
10:16:02.183   Training iter 400, batch loss 0.2354, batch acc 0.8188
10:16:02.722   Training iter 450, batch loss 0.2372, batch acc 0.8218
10:16:03.261   Training iter 500, batch loss 0.2343, batch acc 0.8234
10:16:03.785   Training iter 550, batch loss 0.2337, batch acc 0.8304
10:16:04.310   Training iter 600, batch loss 0.2341, batch acc 0.8246
10:16:04.312 Training @ 443 epoch...
10:16:04.850   Training iter 50, batch loss 0.2320, batch acc 0.8284
10:16:05.382   Training iter 100, batch loss 0.2367, batch acc 0.8136
10:16:05.911   Training iter 150, batch loss 0.2358, batch acc 0.8212
10:16:06.423   Training iter 200, batch loss 0.2344, batch acc 0.8282
10:16:06.920   Training iter 250, batch loss 0.2351, batch acc 0.8218
10:16:07.388   Training iter 300, batch loss 0.2364, batch acc 0.8196
10:16:07.865   Training iter 350, batch loss 0.2363, batch acc 0.8202
10:16:08.332   Training iter 400, batch loss 0.2344, batch acc 0.8276
10:16:08.809   Training iter 450, batch loss 0.2366, batch acc 0.8210
10:16:09.270   Training iter 500, batch loss 0.2351, batch acc 0.8202
10:16:09.730   Training iter 550, batch loss 0.2334, batch acc 0.8312
10:16:10.220   Training iter 600, batch loss 0.2342, batch acc 0.8284
10:16:10.222 Training @ 444 epoch...
10:16:10.721   Training iter 50, batch loss 0.2364, batch acc 0.8256
10:16:11.240   Training iter 100, batch loss 0.2372, batch acc 0.8226
10:16:11.728   Training iter 150, batch loss 0.2330, batch acc 0.8250
10:16:12.274   Training iter 200, batch loss 0.2348, batch acc 0.8240
10:16:12.796   Training iter 250, batch loss 0.2348, batch acc 0.8242
10:16:13.318   Training iter 300, batch loss 0.2343, batch acc 0.8250
10:16:13.826   Training iter 350, batch loss 0.2367, batch acc 0.8228
10:16:14.339   Training iter 400, batch loss 0.2360, batch acc 0.8182
10:16:14.865   Training iter 450, batch loss 0.2345, batch acc 0.8218
10:16:15.392   Training iter 500, batch loss 0.2330, batch acc 0.8302
10:16:15.904   Training iter 550, batch loss 0.2344, batch acc 0.8266
10:16:16.420   Training iter 600, batch loss 0.2353, batch acc 0.8202
10:16:16.422 Training @ 445 epoch...
10:16:16.929   Training iter 50, batch loss 0.2338, batch acc 0.8264
10:16:17.446   Training iter 100, batch loss 0.2342, batch acc 0.8316
10:16:17.970   Training iter 150, batch loss 0.2334, batch acc 0.8270
10:16:18.499   Training iter 200, batch loss 0.2356, batch acc 0.8238
10:16:19.027   Training iter 250, batch loss 0.2352, batch acc 0.8246
10:16:19.555   Training iter 300, batch loss 0.2328, batch acc 0.8278
10:16:20.043   Training iter 350, batch loss 0.2373, batch acc 0.8194
10:16:20.533   Training iter 400, batch loss 0.2393, batch acc 0.8126
10:16:21.034   Training iter 450, batch loss 0.2334, batch acc 0.8262
10:16:21.526   Training iter 500, batch loss 0.2354, batch acc 0.8206
10:16:22.044   Training iter 550, batch loss 0.2361, batch acc 0.8154
10:16:22.552   Training iter 600, batch loss 0.2338, batch acc 0.8272
10:16:22.554 Testing @ 445 epoch...
10:16:22.589     Testing, total mean loss 0.23136, total acc 0.83220
10:16:22.589 Training @ 446 epoch...
10:16:23.077   Training iter 50, batch loss 0.2332, batch acc 0.8226
10:16:23.545   Training iter 100, batch loss 0.2342, batch acc 0.8254
10:16:24.020   Training iter 150, batch loss 0.2375, batch acc 0.8188
10:16:24.480   Training iter 200, batch loss 0.2371, batch acc 0.8140
10:16:24.958   Training iter 250, batch loss 0.2332, batch acc 0.8346
10:16:25.433   Training iter 300, batch loss 0.2344, batch acc 0.8288
10:16:25.894   Training iter 350, batch loss 0.2340, batch acc 0.8262
10:16:26.364   Training iter 400, batch loss 0.2380, batch acc 0.8176
10:16:26.838   Training iter 450, batch loss 0.2338, batch acc 0.8256
10:16:27.328   Training iter 500, batch loss 0.2353, batch acc 0.8224
10:16:27.816   Training iter 550, batch loss 0.2344, batch acc 0.8246
10:16:28.325   Training iter 600, batch loss 0.2349, batch acc 0.8242
10:16:28.327 Training @ 447 epoch...
10:16:28.811   Training iter 50, batch loss 0.2353, batch acc 0.8284
10:16:29.290   Training iter 100, batch loss 0.2324, batch acc 0.8350
10:16:29.754   Training iter 150, batch loss 0.2341, batch acc 0.8248
10:16:30.211   Training iter 200, batch loss 0.2350, batch acc 0.8298
10:16:30.672   Training iter 250, batch loss 0.2358, batch acc 0.8130
10:16:31.186   Training iter 300, batch loss 0.2346, batch acc 0.8212
10:16:31.690   Training iter 350, batch loss 0.2340, batch acc 0.8316
10:16:32.211   Training iter 400, batch loss 0.2369, batch acc 0.8158
10:16:32.711   Training iter 450, batch loss 0.2350, batch acc 0.8256
10:16:33.241   Training iter 500, batch loss 0.2339, batch acc 0.8200
10:16:33.796   Training iter 550, batch loss 0.2366, batch acc 0.8166
10:16:34.356   Training iter 600, batch loss 0.2364, batch acc 0.8212
10:16:34.358 Training @ 448 epoch...
10:16:34.893   Training iter 50, batch loss 0.2375, batch acc 0.8180
10:16:35.431   Training iter 100, batch loss 0.2334, batch acc 0.8312
10:16:35.967   Training iter 150, batch loss 0.2339, batch acc 0.8142
10:16:36.492   Training iter 200, batch loss 0.2344, batch acc 0.8252
10:16:37.025   Training iter 250, batch loss 0.2340, batch acc 0.8256
10:16:37.537   Training iter 300, batch loss 0.2368, batch acc 0.8246
10:16:38.040   Training iter 350, batch loss 0.2359, batch acc 0.8198
10:16:38.555   Training iter 400, batch loss 0.2340, batch acc 0.8268
10:16:39.065   Training iter 450, batch loss 0.2345, batch acc 0.8258
10:16:39.528   Training iter 500, batch loss 0.2358, batch acc 0.8250
10:16:40.002   Training iter 550, batch loss 0.2354, batch acc 0.8180
10:16:40.478   Training iter 600, batch loss 0.2345, batch acc 0.8298
10:16:40.479 Training @ 449 epoch...
10:16:40.995   Training iter 50, batch loss 0.2369, batch acc 0.8214
10:16:41.509   Training iter 100, batch loss 0.2363, batch acc 0.8182
10:16:42.006   Training iter 150, batch loss 0.2328, batch acc 0.8308
10:16:42.516   Training iter 200, batch loss 0.2333, batch acc 0.8314
10:16:43.027   Training iter 250, batch loss 0.2374, batch acc 0.8236
10:16:43.512   Training iter 300, batch loss 0.2362, batch acc 0.8148
10:16:44.023   Training iter 350, batch loss 0.2356, batch acc 0.8218
10:16:44.516   Training iter 400, batch loss 0.2372, batch acc 0.8180
10:16:45.030   Training iter 450, batch loss 0.2337, batch acc 0.8286
10:16:45.647   Training iter 500, batch loss 0.2331, batch acc 0.8268
10:16:46.214   Training iter 550, batch loss 0.2334, batch acc 0.8268
10:16:46.746   Training iter 600, batch loss 0.2339, batch acc 0.8236
10:16:46.748 Training @ 450 epoch...
10:16:47.283   Training iter 50, batch loss 0.2352, batch acc 0.8258
10:16:47.766   Training iter 100, batch loss 0.2368, batch acc 0.8120
10:16:48.250   Training iter 150, batch loss 0.2347, batch acc 0.8264
10:16:48.732   Training iter 200, batch loss 0.2343, batch acc 0.8276
10:16:49.219   Training iter 250, batch loss 0.2329, batch acc 0.8290
10:16:49.729   Training iter 300, batch loss 0.2344, batch acc 0.8262
10:16:50.247   Training iter 350, batch loss 0.2347, batch acc 0.8196
10:16:50.766   Training iter 400, batch loss 0.2340, batch acc 0.8178
10:16:51.279   Training iter 450, batch loss 0.2362, batch acc 0.8210
10:16:51.797   Training iter 500, batch loss 0.2363, batch acc 0.8240
10:16:52.318   Training iter 550, batch loss 0.2336, batch acc 0.8280
10:16:52.836   Training iter 600, batch loss 0.2367, batch acc 0.8262
10:16:52.838 Testing @ 450 epoch...
10:16:52.872     Testing, total mean loss 0.23132, total acc 0.83240
10:16:52.873 Training @ 451 epoch...
10:16:53.400   Training iter 50, batch loss 0.2343, batch acc 0.8256
10:16:53.910   Training iter 100, batch loss 0.2338, batch acc 0.8388
10:16:54.420   Training iter 150, batch loss 0.2344, batch acc 0.8198
10:16:54.932   Training iter 200, batch loss 0.2330, batch acc 0.8200
10:16:55.423   Training iter 250, batch loss 0.2350, batch acc 0.8246
10:16:55.915   Training iter 300, batch loss 0.2346, batch acc 0.8268
10:16:56.421   Training iter 350, batch loss 0.2329, batch acc 0.8296
10:16:56.914   Training iter 400, batch loss 0.2371, batch acc 0.8180
10:16:57.419   Training iter 450, batch loss 0.2369, batch acc 0.8188
10:16:57.925   Training iter 500, batch loss 0.2342, batch acc 0.8298
10:16:58.424   Training iter 550, batch loss 0.2366, batch acc 0.8124
10:16:58.929   Training iter 600, batch loss 0.2371, batch acc 0.8176
10:16:58.931 Training @ 452 epoch...
10:16:59.435   Training iter 50, batch loss 0.2365, batch acc 0.8216
10:16:59.934   Training iter 100, batch loss 0.2361, batch acc 0.8164
10:17:00.437   Training iter 150, batch loss 0.2354, batch acc 0.8264
10:17:00.948   Training iter 200, batch loss 0.2345, batch acc 0.8254
10:17:01.464   Training iter 250, batch loss 0.2333, batch acc 0.8262
10:17:01.998   Training iter 300, batch loss 0.2337, batch acc 0.8282
10:17:02.510   Training iter 350, batch loss 0.2335, batch acc 0.8270
10:17:03.002   Training iter 400, batch loss 0.2362, batch acc 0.8246
10:17:03.482   Training iter 450, batch loss 0.2344, batch acc 0.8204
10:17:03.953   Training iter 500, batch loss 0.2348, batch acc 0.8238
10:17:04.423   Training iter 550, batch loss 0.2360, batch acc 0.8214
10:17:04.914   Training iter 600, batch loss 0.2353, batch acc 0.8236
10:17:04.916 Training @ 453 epoch...
10:17:05.447   Training iter 50, batch loss 0.2367, batch acc 0.8226
10:17:05.973   Training iter 100, batch loss 0.2345, batch acc 0.8298
10:17:06.502   Training iter 150, batch loss 0.2354, batch acc 0.8202
10:17:07.051   Training iter 200, batch loss 0.2336, batch acc 0.8270
10:17:07.556   Training iter 250, batch loss 0.2342, batch acc 0.8224
10:17:08.057   Training iter 300, batch loss 0.2374, batch acc 0.8160
10:17:08.550   Training iter 350, batch loss 0.2328, batch acc 0.8300
10:17:09.035   Training iter 400, batch loss 0.2329, batch acc 0.8326
10:17:09.509   Training iter 450, batch loss 0.2357, batch acc 0.8190
10:17:09.984   Training iter 500, batch loss 0.2356, batch acc 0.8246
10:17:10.477   Training iter 550, batch loss 0.2333, batch acc 0.8242
10:17:10.972   Training iter 600, batch loss 0.2372, batch acc 0.8162
10:17:10.973 Training @ 454 epoch...
10:17:11.475   Training iter 50, batch loss 0.2332, batch acc 0.8298
10:17:11.960   Training iter 100, batch loss 0.2370, batch acc 0.8190
10:17:12.463   Training iter 150, batch loss 0.2345, batch acc 0.8268
10:17:12.953   Training iter 200, batch loss 0.2356, batch acc 0.8186
10:17:13.443   Training iter 250, batch loss 0.2349, batch acc 0.8294
10:17:13.920   Training iter 300, batch loss 0.2343, batch acc 0.8246
10:17:14.405   Training iter 350, batch loss 0.2366, batch acc 0.8206
10:17:14.857   Training iter 400, batch loss 0.2362, batch acc 0.8232
10:17:15.309   Training iter 450, batch loss 0.2331, batch acc 0.8334
10:17:15.766   Training iter 500, batch loss 0.2329, batch acc 0.8274
10:17:16.219   Training iter 550, batch loss 0.2354, batch acc 0.8166
10:17:16.693   Training iter 600, batch loss 0.2358, batch acc 0.8130
10:17:16.694 Training @ 455 epoch...
10:17:17.196   Training iter 50, batch loss 0.2350, batch acc 0.8274
10:17:17.666   Training iter 100, batch loss 0.2340, batch acc 0.8242
10:17:18.126   Training iter 150, batch loss 0.2356, batch acc 0.8270
10:17:18.623   Training iter 200, batch loss 0.2361, batch acc 0.8172
10:17:19.105   Training iter 250, batch loss 0.2348, batch acc 0.8186
10:17:19.576   Training iter 300, batch loss 0.2351, batch acc 0.8256
10:17:20.056   Training iter 350, batch loss 0.2344, batch acc 0.8220
10:17:20.568   Training iter 400, batch loss 0.2346, batch acc 0.8250
10:17:21.072   Training iter 450, batch loss 0.2335, batch acc 0.8266
10:17:21.566   Training iter 500, batch loss 0.2366, batch acc 0.8208
10:17:22.076   Training iter 550, batch loss 0.2339, batch acc 0.8340
10:17:22.592   Training iter 600, batch loss 0.2359, batch acc 0.8172
10:17:22.594 Testing @ 455 epoch...
10:17:22.629     Testing, total mean loss 0.23128, total acc 0.83220
10:17:22.629 Training @ 456 epoch...
10:17:23.161   Training iter 50, batch loss 0.2345, batch acc 0.8226
10:17:23.669   Training iter 100, batch loss 0.2337, batch acc 0.8280
10:17:24.154   Training iter 150, batch loss 0.2347, batch acc 0.8260
10:17:24.669   Training iter 200, batch loss 0.2359, batch acc 0.8208
10:17:25.209   Training iter 250, batch loss 0.2342, batch acc 0.8314
10:17:25.742   Training iter 300, batch loss 0.2355, batch acc 0.8178
10:17:26.287   Training iter 350, batch loss 0.2348, batch acc 0.8284
10:17:26.797   Training iter 400, batch loss 0.2339, batch acc 0.8264
10:17:27.309   Training iter 450, batch loss 0.2372, batch acc 0.8172
10:17:27.792   Training iter 500, batch loss 0.2339, batch acc 0.8252
10:17:28.292   Training iter 550, batch loss 0.2348, batch acc 0.8212
10:17:28.794   Training iter 600, batch loss 0.2363, batch acc 0.8182
10:17:28.796 Training @ 457 epoch...
10:17:29.301   Training iter 50, batch loss 0.2371, batch acc 0.8216
10:17:29.781   Training iter 100, batch loss 0.2358, batch acc 0.8236
10:17:30.261   Training iter 150, batch loss 0.2349, batch acc 0.8218
10:17:30.736   Training iter 200, batch loss 0.2323, batch acc 0.8294
10:17:31.210   Training iter 250, batch loss 0.2350, batch acc 0.8244
10:17:31.675   Training iter 300, batch loss 0.2356, batch acc 0.8210
10:17:32.142   Training iter 350, batch loss 0.2340, batch acc 0.8294
10:17:32.626   Training iter 400, batch loss 0.2363, batch acc 0.8270
10:17:33.120   Training iter 450, batch loss 0.2363, batch acc 0.8222
10:17:33.607   Training iter 500, batch loss 0.2336, batch acc 0.8244
10:17:34.090   Training iter 550, batch loss 0.2332, batch acc 0.8232
10:17:34.575   Training iter 600, batch loss 0.2351, batch acc 0.8184
10:17:34.576 Training @ 458 epoch...
10:17:35.063   Training iter 50, batch loss 0.2331, batch acc 0.8276
10:17:35.556   Training iter 100, batch loss 0.2331, batch acc 0.8296
10:17:36.043   Training iter 150, batch loss 0.2346, batch acc 0.8200
10:17:36.502   Training iter 200, batch loss 0.2365, batch acc 0.8204
10:17:36.952   Training iter 250, batch loss 0.2350, batch acc 0.8238
10:17:37.415   Training iter 300, batch loss 0.2339, batch acc 0.8264
10:17:37.881   Training iter 350, batch loss 0.2333, batch acc 0.8294
10:17:38.370   Training iter 400, batch loss 0.2381, batch acc 0.8176
10:17:38.814   Training iter 450, batch loss 0.2364, batch acc 0.8226
10:17:39.254   Training iter 500, batch loss 0.2344, batch acc 0.8232
10:17:39.714   Training iter 550, batch loss 0.2338, batch acc 0.8216
10:17:40.176   Training iter 600, batch loss 0.2368, batch acc 0.8218
10:17:40.178 Training @ 459 epoch...
10:17:40.630   Training iter 50, batch loss 0.2337, batch acc 0.8208
10:17:41.076   Training iter 100, batch loss 0.2351, batch acc 0.8258
10:17:41.520   Training iter 150, batch loss 0.2354, batch acc 0.8276
10:17:41.965   Training iter 200, batch loss 0.2364, batch acc 0.8182
10:17:42.414   Training iter 250, batch loss 0.2351, batch acc 0.8254
10:17:42.909   Training iter 300, batch loss 0.2336, batch acc 0.8314
10:17:43.622   Training iter 350, batch loss 0.2345, batch acc 0.8240
10:17:44.342   Training iter 400, batch loss 0.2356, batch acc 0.8202
10:17:44.949   Training iter 450, batch loss 0.2341, batch acc 0.8244
10:17:45.434   Training iter 500, batch loss 0.2350, batch acc 0.8254
10:17:45.928   Training iter 550, batch loss 0.2314, batch acc 0.8320
10:17:46.413   Training iter 600, batch loss 0.2390, batch acc 0.8088
10:17:46.415 Training @ 460 epoch...
10:17:46.933   Training iter 50, batch loss 0.2355, batch acc 0.8160
10:17:47.436   Training iter 100, batch loss 0.2361, batch acc 0.8184
10:17:47.942   Training iter 150, batch loss 0.2345, batch acc 0.8204
10:17:48.438   Training iter 200, batch loss 0.2326, batch acc 0.8236
10:17:48.927   Training iter 250, batch loss 0.2352, batch acc 0.8252
10:17:49.413   Training iter 300, batch loss 0.2365, batch acc 0.8234
10:17:49.905   Training iter 350, batch loss 0.2356, batch acc 0.8206
10:17:50.429   Training iter 400, batch loss 0.2377, batch acc 0.8200
10:17:50.937   Training iter 450, batch loss 0.2337, batch acc 0.8290
10:17:51.444   Training iter 500, batch loss 0.2341, batch acc 0.8288
10:17:51.936   Training iter 550, batch loss 0.2340, batch acc 0.8344
10:17:52.444   Training iter 600, batch loss 0.2335, batch acc 0.8262
10:17:52.446 Testing @ 460 epoch...
10:17:52.481     Testing, total mean loss 0.23125, total acc 0.83210
10:17:52.481 Training @ 461 epoch...
10:17:52.992   Training iter 50, batch loss 0.2332, batch acc 0.8250
10:17:53.527   Training iter 100, batch loss 0.2352, batch acc 0.8200
10:17:54.068   Training iter 150, batch loss 0.2362, batch acc 0.8234
10:17:54.603   Training iter 200, batch loss 0.2345, batch acc 0.8274
10:17:55.153   Training iter 250, batch loss 0.2366, batch acc 0.8216
10:17:55.693   Training iter 300, batch loss 0.2340, batch acc 0.8200
10:17:56.224   Training iter 350, batch loss 0.2346, batch acc 0.8264
10:17:56.740   Training iter 400, batch loss 0.2332, batch acc 0.8234
10:17:57.258   Training iter 450, batch loss 0.2348, batch acc 0.8216
10:17:57.780   Training iter 500, batch loss 0.2334, batch acc 0.8304
10:17:58.306   Training iter 550, batch loss 0.2339, batch acc 0.8356
10:17:58.832   Training iter 600, batch loss 0.2392, batch acc 0.8112
10:17:58.834 Training @ 462 epoch...
10:17:59.360   Training iter 50, batch loss 0.2372, batch acc 0.8212
10:17:59.858   Training iter 100, batch loss 0.2377, batch acc 0.8162
10:18:00.358   Training iter 150, batch loss 0.2357, batch acc 0.8176
10:18:00.886   Training iter 200, batch loss 0.2348, batch acc 0.8248
10:18:01.417   Training iter 250, batch loss 0.2339, batch acc 0.8228
10:18:01.940   Training iter 300, batch loss 0.2349, batch acc 0.8260
10:18:02.487   Training iter 350, batch loss 0.2316, batch acc 0.8322
10:18:03.037   Training iter 400, batch loss 0.2362, batch acc 0.8166
10:18:03.576   Training iter 450, batch loss 0.2343, batch acc 0.8232
10:18:04.102   Training iter 500, batch loss 0.2345, batch acc 0.8228
10:18:04.634   Training iter 550, batch loss 0.2342, batch acc 0.8246
10:18:05.163   Training iter 600, batch loss 0.2338, batch acc 0.8370
10:18:05.164 Training @ 463 epoch...
10:18:05.699   Training iter 50, batch loss 0.2365, batch acc 0.8266
10:18:06.224   Training iter 100, batch loss 0.2375, batch acc 0.8196
10:18:06.725   Training iter 150, batch loss 0.2329, batch acc 0.8230
10:18:07.206   Training iter 200, batch loss 0.2359, batch acc 0.8212
10:18:07.689   Training iter 250, batch loss 0.2356, batch acc 0.8232
10:18:08.181   Training iter 300, batch loss 0.2327, batch acc 0.8308
10:18:08.684   Training iter 350, batch loss 0.2337, batch acc 0.8222
10:18:09.189   Training iter 400, batch loss 0.2331, batch acc 0.8322
10:18:09.706   Training iter 450, batch loss 0.2350, batch acc 0.8254
10:18:10.224   Training iter 500, batch loss 0.2352, batch acc 0.8224
10:18:10.756   Training iter 550, batch loss 0.2343, batch acc 0.8234
10:18:11.273   Training iter 600, batch loss 0.2362, batch acc 0.8162
10:18:11.274 Training @ 464 epoch...
10:18:11.801   Training iter 50, batch loss 0.2301, batch acc 0.8356
10:18:12.338   Training iter 100, batch loss 0.2363, batch acc 0.8170
10:18:12.889   Training iter 150, batch loss 0.2368, batch acc 0.8200
10:18:13.428   Training iter 200, batch loss 0.2354, batch acc 0.8222
10:18:13.961   Training iter 250, batch loss 0.2349, batch acc 0.8172
10:18:14.493   Training iter 300, batch loss 0.2343, batch acc 0.8328
10:18:15.028   Training iter 350, batch loss 0.2341, batch acc 0.8236
10:18:15.548   Training iter 400, batch loss 0.2365, batch acc 0.8218
10:18:16.046   Training iter 450, batch loss 0.2356, batch acc 0.8208
10:18:16.547   Training iter 500, batch loss 0.2360, batch acc 0.8216
10:18:17.046   Training iter 550, batch loss 0.2322, batch acc 0.8286
10:18:17.554   Training iter 600, batch loss 0.2364, batch acc 0.8238
10:18:17.556 Training @ 465 epoch...
10:18:18.063   Training iter 50, batch loss 0.2357, batch acc 0.8216
10:18:18.558   Training iter 100, batch loss 0.2346, batch acc 0.8208
10:18:19.060   Training iter 150, batch loss 0.2336, batch acc 0.8246
10:18:19.571   Training iter 200, batch loss 0.2316, batch acc 0.8324
10:18:20.072   Training iter 250, batch loss 0.2360, batch acc 0.8264
10:18:20.585   Training iter 300, batch loss 0.2353, batch acc 0.8188
10:18:21.090   Training iter 350, batch loss 0.2334, batch acc 0.8284
10:18:21.584   Training iter 400, batch loss 0.2365, batch acc 0.8192
10:18:22.108   Training iter 450, batch loss 0.2334, batch acc 0.8254
10:18:22.629   Training iter 500, batch loss 0.2351, batch acc 0.8192
10:18:23.160   Training iter 550, batch loss 0.2376, batch acc 0.8240
10:18:23.693   Training iter 600, batch loss 0.2356, batch acc 0.8216
10:18:23.695 Testing @ 465 epoch...
10:18:23.730     Testing, total mean loss 0.23122, total acc 0.83220
10:18:23.730 Training @ 466 epoch...
10:18:24.261   Training iter 50, batch loss 0.2360, batch acc 0.8222
10:18:24.785   Training iter 100, batch loss 0.2362, batch acc 0.8242
10:18:25.307   Training iter 150, batch loss 0.2328, batch acc 0.8310
10:18:25.830   Training iter 200, batch loss 0.2346, batch acc 0.8268
10:18:26.366   Training iter 250, batch loss 0.2335, batch acc 0.8264
10:18:26.896   Training iter 300, batch loss 0.2363, batch acc 0.8202
10:18:27.428   Training iter 350, batch loss 0.2323, batch acc 0.8282
10:18:27.950   Training iter 400, batch loss 0.2365, batch acc 0.8170
10:18:28.474   Training iter 450, batch loss 0.2350, batch acc 0.8192
10:18:29.009   Training iter 500, batch loss 0.2360, batch acc 0.8198
10:18:29.558   Training iter 550, batch loss 0.2338, batch acc 0.8298
10:18:30.098   Training iter 600, batch loss 0.2354, batch acc 0.8232
10:18:30.099 Training @ 467 epoch...
10:18:30.648   Training iter 50, batch loss 0.2340, batch acc 0.8336
10:18:31.187   Training iter 100, batch loss 0.2355, batch acc 0.8236
10:18:31.726   Training iter 150, batch loss 0.2359, batch acc 0.8196
10:18:32.306   Training iter 200, batch loss 0.2349, batch acc 0.8224
10:18:32.794   Training iter 250, batch loss 0.2350, batch acc 0.8270
10:18:33.269   Training iter 300, batch loss 0.2351, batch acc 0.8272
10:18:33.741   Training iter 350, batch loss 0.2346, batch acc 0.8280
10:18:34.228   Training iter 400, batch loss 0.2340, batch acc 0.8248
10:18:34.719   Training iter 450, batch loss 0.2331, batch acc 0.8262
10:18:35.201   Training iter 500, batch loss 0.2369, batch acc 0.8150
10:18:35.680   Training iter 550, batch loss 0.2356, batch acc 0.8136
10:18:36.167   Training iter 600, batch loss 0.2338, batch acc 0.8250
10:18:36.168 Training @ 468 epoch...
10:18:36.635   Training iter 50, batch loss 0.2342, batch acc 0.8318
10:18:37.105   Training iter 100, batch loss 0.2343, batch acc 0.8230
10:18:37.591   Training iter 150, batch loss 0.2333, batch acc 0.8238
10:18:38.060   Training iter 200, batch loss 0.2351, batch acc 0.8230
10:18:38.523   Training iter 250, batch loss 0.2382, batch acc 0.8182
10:18:38.994   Training iter 300, batch loss 0.2337, batch acc 0.8184
10:18:39.470   Training iter 350, batch loss 0.2318, batch acc 0.8308
10:18:39.949   Training iter 400, batch loss 0.2341, batch acc 0.8292
10:18:40.417   Training iter 450, batch loss 0.2359, batch acc 0.8238
10:18:40.877   Training iter 500, batch loss 0.2379, batch acc 0.8174
10:18:41.338   Training iter 550, batch loss 0.2353, batch acc 0.8200
10:18:41.886   Training iter 600, batch loss 0.2343, batch acc 0.8274
10:18:41.888 Training @ 469 epoch...
10:18:42.407   Training iter 50, batch loss 0.2345, batch acc 0.8270
10:18:42.930   Training iter 100, batch loss 0.2385, batch acc 0.8182
10:18:43.455   Training iter 150, batch loss 0.2358, batch acc 0.8232
10:18:43.971   Training iter 200, batch loss 0.2351, batch acc 0.8220
10:18:44.500   Training iter 250, batch loss 0.2344, batch acc 0.8244
10:18:45.035   Training iter 300, batch loss 0.2330, batch acc 0.8292
10:18:45.561   Training iter 350, batch loss 0.2357, batch acc 0.8210
10:18:46.092   Training iter 400, batch loss 0.2341, batch acc 0.8238
10:18:46.629   Training iter 450, batch loss 0.2334, batch acc 0.8268
10:18:47.142   Training iter 500, batch loss 0.2337, batch acc 0.8248
10:18:47.658   Training iter 550, batch loss 0.2362, batch acc 0.8214
10:18:48.170   Training iter 600, batch loss 0.2335, batch acc 0.8250
10:18:48.172 Training @ 470 epoch...
10:18:48.684   Training iter 50, batch loss 0.2338, batch acc 0.8266
10:18:49.171   Training iter 100, batch loss 0.2347, batch acc 0.8254
10:18:49.646   Training iter 150, batch loss 0.2339, batch acc 0.8250
10:18:50.114   Training iter 200, batch loss 0.2352, batch acc 0.8200
10:18:50.602   Training iter 250, batch loss 0.2359, batch acc 0.8166
10:18:51.091   Training iter 300, batch loss 0.2378, batch acc 0.8148
10:18:51.566   Training iter 350, batch loss 0.2341, batch acc 0.8244
10:18:52.056   Training iter 400, batch loss 0.2338, batch acc 0.8362
10:18:52.570   Training iter 450, batch loss 0.2337, batch acc 0.8306
10:18:53.071   Training iter 500, batch loss 0.2360, batch acc 0.8184
10:18:53.588   Training iter 550, batch loss 0.2364, batch acc 0.8194
10:18:54.096   Training iter 600, batch loss 0.2329, batch acc 0.8300
10:18:54.098 Testing @ 470 epoch...
10:18:54.133     Testing, total mean loss 0.23118, total acc 0.83240
10:18:54.133 Training @ 471 epoch...
10:18:54.640   Training iter 50, batch loss 0.2324, batch acc 0.8352
10:18:55.150   Training iter 100, batch loss 0.2342, batch acc 0.8272
10:18:55.646   Training iter 150, batch loss 0.2350, batch acc 0.8216
10:18:56.173   Training iter 200, batch loss 0.2348, batch acc 0.8222
10:18:56.685   Training iter 250, batch loss 0.2354, batch acc 0.8252
10:18:57.178   Training iter 300, batch loss 0.2329, batch acc 0.8236
10:18:57.664   Training iter 350, batch loss 0.2381, batch acc 0.8172
10:18:58.148   Training iter 400, batch loss 0.2387, batch acc 0.8120
10:18:58.654   Training iter 450, batch loss 0.2341, batch acc 0.8220
10:18:59.154   Training iter 500, batch loss 0.2324, batch acc 0.8326
10:18:59.653   Training iter 550, batch loss 0.2344, batch acc 0.8272
10:19:00.145   Training iter 600, batch loss 0.2355, batch acc 0.8176
10:19:00.147 Training @ 472 epoch...
10:19:00.651   Training iter 50, batch loss 0.2350, batch acc 0.8242
10:19:01.165   Training iter 100, batch loss 0.2349, batch acc 0.8242
10:19:01.698   Training iter 150, batch loss 0.2353, batch acc 0.8218
10:19:02.251   Training iter 200, batch loss 0.2358, batch acc 0.8228
10:19:02.757   Training iter 250, batch loss 0.2365, batch acc 0.8268
10:19:03.254   Training iter 300, batch loss 0.2322, batch acc 0.8242
10:19:03.739   Training iter 350, batch loss 0.2340, batch acc 0.8244
10:19:04.243   Training iter 400, batch loss 0.2355, batch acc 0.8260
10:19:04.758   Training iter 450, batch loss 0.2351, batch acc 0.8244
10:19:05.258   Training iter 500, batch loss 0.2352, batch acc 0.8194
10:19:05.751   Training iter 550, batch loss 0.2353, batch acc 0.8218
10:19:06.232   Training iter 600, batch loss 0.2330, batch acc 0.8270
10:19:06.234 Training @ 473 epoch...
10:19:06.751   Training iter 50, batch loss 0.2337, batch acc 0.8260
10:19:07.280   Training iter 100, batch loss 0.2340, batch acc 0.8262
10:19:07.780   Training iter 150, batch loss 0.2348, batch acc 0.8192
10:19:08.264   Training iter 200, batch loss 0.2346, batch acc 0.8330
10:19:08.727   Training iter 250, batch loss 0.2356, batch acc 0.8160
10:19:09.193   Training iter 300, batch loss 0.2361, batch acc 0.8164
10:19:09.666   Training iter 350, batch loss 0.2341, batch acc 0.8206
10:19:10.154   Training iter 400, batch loss 0.2324, batch acc 0.8296
10:19:10.650   Training iter 450, batch loss 0.2377, batch acc 0.8188
10:19:11.163   Training iter 500, batch loss 0.2339, batch acc 0.8246
10:19:11.671   Training iter 550, batch loss 0.2352, batch acc 0.8262
10:19:12.180   Training iter 600, batch loss 0.2357, batch acc 0.8290
10:19:12.182 Training @ 474 epoch...
10:19:12.702   Training iter 50, batch loss 0.2321, batch acc 0.8300
10:19:13.214   Training iter 100, batch loss 0.2351, batch acc 0.8214
10:19:13.716   Training iter 150, batch loss 0.2371, batch acc 0.8212
10:19:14.242   Training iter 200, batch loss 0.2371, batch acc 0.8180
10:19:14.787   Training iter 250, batch loss 0.2341, batch acc 0.8270
10:19:15.327   Training iter 300, batch loss 0.2337, batch acc 0.8254
10:19:15.850   Training iter 350, batch loss 0.2353, batch acc 0.8184
10:19:16.370   Training iter 400, batch loss 0.2352, batch acc 0.8244
10:19:16.892   Training iter 450, batch loss 0.2354, batch acc 0.8214
10:19:17.422   Training iter 500, batch loss 0.2333, batch acc 0.8296
10:19:17.954   Training iter 550, batch loss 0.2360, batch acc 0.8264
10:19:18.498   Training iter 600, batch loss 0.2334, batch acc 0.8268
10:19:18.500 Training @ 475 epoch...
10:19:19.034   Training iter 50, batch loss 0.2352, batch acc 0.8184
10:19:19.529   Training iter 100, batch loss 0.2363, batch acc 0.8186
10:19:20.040   Training iter 150, batch loss 0.2364, batch acc 0.8232
10:19:20.530   Training iter 200, batch loss 0.2319, batch acc 0.8232
10:19:21.018   Training iter 250, batch loss 0.2320, batch acc 0.8284
10:19:21.499   Training iter 300, batch loss 0.2353, batch acc 0.8238
10:19:21.993   Training iter 350, batch loss 0.2353, batch acc 0.8210
10:19:22.516   Training iter 400, batch loss 0.2338, batch acc 0.8292
10:19:23.032   Training iter 450, batch loss 0.2345, batch acc 0.8226
10:19:23.531   Training iter 500, batch loss 0.2346, batch acc 0.8294
10:19:24.036   Training iter 550, batch loss 0.2361, batch acc 0.8232
10:19:24.572   Training iter 600, batch loss 0.2363, batch acc 0.8250
10:19:24.574 Testing @ 475 epoch...
10:19:24.609     Testing, total mean loss 0.23115, total acc 0.83230
10:19:24.609 Training @ 476 epoch...
10:19:25.145   Training iter 50, batch loss 0.2345, batch acc 0.8234
10:19:25.675   Training iter 100, batch loss 0.2341, batch acc 0.8374
10:19:26.192   Training iter 150, batch loss 0.2371, batch acc 0.8184
10:19:26.683   Training iter 200, batch loss 0.2311, batch acc 0.8290
10:19:27.177   Training iter 250, batch loss 0.2355, batch acc 0.8190
10:19:27.675   Training iter 300, batch loss 0.2353, batch acc 0.8266
10:19:28.170   Training iter 350, batch loss 0.2378, batch acc 0.8230
10:19:28.669   Training iter 400, batch loss 0.2334, batch acc 0.8234
10:19:29.169   Training iter 450, batch loss 0.2358, batch acc 0.8188
10:19:29.667   Training iter 500, batch loss 0.2348, batch acc 0.8202
10:19:30.194   Training iter 550, batch loss 0.2351, batch acc 0.8214
10:19:30.723   Training iter 600, batch loss 0.2332, batch acc 0.8250
10:19:30.724 Training @ 477 epoch...
10:19:31.268   Training iter 50, batch loss 0.2366, batch acc 0.8146
10:19:31.803   Training iter 100, batch loss 0.2341, batch acc 0.8226
10:19:32.337   Training iter 150, batch loss 0.2336, batch acc 0.8308
10:19:32.873   Training iter 200, batch loss 0.2322, batch acc 0.8310
10:19:33.384   Training iter 250, batch loss 0.2341, batch acc 0.8232
10:19:33.881   Training iter 300, batch loss 0.2373, batch acc 0.8144
10:19:34.373   Training iter 350, batch loss 0.2357, batch acc 0.8206
10:19:34.895   Training iter 400, batch loss 0.2339, batch acc 0.8230
10:19:35.414   Training iter 450, batch loss 0.2342, batch acc 0.8252
10:19:35.930   Training iter 500, batch loss 0.2353, batch acc 0.8324
10:19:36.408   Training iter 550, batch loss 0.2339, batch acc 0.8262
10:19:36.886   Training iter 600, batch loss 0.2368, batch acc 0.8226
10:19:36.888 Training @ 478 epoch...
10:19:37.372   Training iter 50, batch loss 0.2363, batch acc 0.8232
10:19:37.852   Training iter 100, batch loss 0.2356, batch acc 0.8198
10:19:38.331   Training iter 150, batch loss 0.2339, batch acc 0.8250
10:19:38.838   Training iter 200, batch loss 0.2318, batch acc 0.8274
10:19:39.362   Training iter 250, batch loss 0.2370, batch acc 0.8172
10:19:39.906   Training iter 300, batch loss 0.2348, batch acc 0.8300
10:19:40.559   Training iter 350, batch loss 0.2376, batch acc 0.8190
10:19:41.289   Training iter 400, batch loss 0.2325, batch acc 0.8292
10:19:41.990   Training iter 450, batch loss 0.2334, batch acc 0.8282
10:19:42.521   Training iter 500, batch loss 0.2360, batch acc 0.8248
10:19:43.050   Training iter 550, batch loss 0.2346, batch acc 0.8246
10:19:43.626   Training iter 600, batch loss 0.2341, batch acc 0.8166
10:19:43.628 Training @ 479 epoch...
10:19:44.197   Training iter 50, batch loss 0.2330, batch acc 0.8292
10:19:44.730   Training iter 100, batch loss 0.2338, batch acc 0.8304
10:19:45.172   Training iter 150, batch loss 0.2362, batch acc 0.8186
10:19:45.613   Training iter 200, batch loss 0.2361, batch acc 0.8192
10:19:46.080   Training iter 250, batch loss 0.2342, batch acc 0.8202
10:19:46.604   Training iter 300, batch loss 0.2360, batch acc 0.8240
10:19:47.120   Training iter 350, batch loss 0.2369, batch acc 0.8150
10:19:47.632   Training iter 400, batch loss 0.2323, batch acc 0.8294
10:19:48.138   Training iter 450, batch loss 0.2343, batch acc 0.8232
10:19:48.606   Training iter 500, batch loss 0.2350, batch acc 0.8246
10:19:49.075   Training iter 550, batch loss 0.2365, batch acc 0.8206
10:19:49.540   Training iter 600, batch loss 0.2333, batch acc 0.8316
10:19:49.542 Training @ 480 epoch...
10:19:50.026   Training iter 50, batch loss 0.2351, batch acc 0.8242
10:19:50.522   Training iter 100, batch loss 0.2393, batch acc 0.8122
10:19:51.034   Training iter 150, batch loss 0.2342, batch acc 0.8240
10:19:51.552   Training iter 200, batch loss 0.2306, batch acc 0.8450
10:19:52.052   Training iter 250, batch loss 0.2344, batch acc 0.8198
10:19:52.538   Training iter 300, batch loss 0.2355, batch acc 0.8226
10:19:52.983   Training iter 350, batch loss 0.2361, batch acc 0.8172
10:19:53.412   Training iter 400, batch loss 0.2340, batch acc 0.8270
10:19:53.879   Training iter 450, batch loss 0.2339, batch acc 0.8246
10:19:54.360   Training iter 500, batch loss 0.2361, batch acc 0.8206
10:19:54.844   Training iter 550, batch loss 0.2345, batch acc 0.8232
10:19:55.321   Training iter 600, batch loss 0.2337, batch acc 0.8278
10:19:55.322 Testing @ 480 epoch...
10:19:55.358     Testing, total mean loss 0.23112, total acc 0.83240
10:19:55.358 Training @ 481 epoch...
10:19:55.874   Training iter 50, batch loss 0.2330, batch acc 0.8372
10:19:56.359   Training iter 100, batch loss 0.2353, batch acc 0.8242
10:19:56.828   Training iter 150, batch loss 0.2345, batch acc 0.8192
10:19:57.278   Training iter 200, batch loss 0.2356, batch acc 0.8230
10:19:57.743   Training iter 250, batch loss 0.2366, batch acc 0.8232
10:19:58.215   Training iter 300, batch loss 0.2359, batch acc 0.8200
10:19:58.676   Training iter 350, batch loss 0.2358, batch acc 0.8218
10:19:59.127   Training iter 400, batch loss 0.2343, batch acc 0.8200
10:19:59.585   Training iter 450, batch loss 0.2325, batch acc 0.8288
10:20:00.050   Training iter 500, batch loss 0.2335, batch acc 0.8230
10:20:00.502   Training iter 550, batch loss 0.2359, batch acc 0.8268
10:20:00.996   Training iter 600, batch loss 0.2342, batch acc 0.8190
10:20:00.998 Training @ 482 epoch...
10:20:01.547   Training iter 50, batch loss 0.2343, batch acc 0.8264
10:20:02.085   Training iter 100, batch loss 0.2372, batch acc 0.8164
10:20:02.593   Training iter 150, batch loss 0.2370, batch acc 0.8196
10:20:03.097   Training iter 200, batch loss 0.2344, batch acc 0.8212
10:20:03.599   Training iter 250, batch loss 0.2350, batch acc 0.8232
10:20:04.124   Training iter 300, batch loss 0.2357, batch acc 0.8254
10:20:04.632   Training iter 350, batch loss 0.2354, batch acc 0.8202
10:20:05.145   Training iter 400, batch loss 0.2362, batch acc 0.8224
10:20:05.658   Training iter 450, batch loss 0.2332, batch acc 0.8244
10:20:06.166   Training iter 500, batch loss 0.2317, batch acc 0.8322
10:20:06.643   Training iter 550, batch loss 0.2337, batch acc 0.8284
10:20:07.089   Training iter 600, batch loss 0.2335, batch acc 0.8284
10:20:07.090 Training @ 483 epoch...
10:20:07.558   Training iter 50, batch loss 0.2341, batch acc 0.8276
10:20:08.035   Training iter 100, batch loss 0.2351, batch acc 0.8262
10:20:08.557   Training iter 150, batch loss 0.2356, batch acc 0.8184
10:20:09.055   Training iter 200, batch loss 0.2368, batch acc 0.8222
10:20:09.520   Training iter 250, batch loss 0.2345, batch acc 0.8208
10:20:09.976   Training iter 300, batch loss 0.2365, batch acc 0.8214
10:20:10.462   Training iter 350, batch loss 0.2310, batch acc 0.8342
10:20:10.919   Training iter 400, batch loss 0.2359, batch acc 0.8200
10:20:11.389   Training iter 450, batch loss 0.2355, batch acc 0.8178
10:20:11.865   Training iter 500, batch loss 0.2325, batch acc 0.8322
10:20:12.348   Training iter 550, batch loss 0.2353, batch acc 0.8208
10:20:12.903   Training iter 600, batch loss 0.2341, batch acc 0.8260
10:20:12.904 Training @ 484 epoch...
10:20:13.562   Training iter 50, batch loss 0.2351, batch acc 0.8188
10:20:14.130   Training iter 100, batch loss 0.2343, batch acc 0.8266
10:20:14.650   Training iter 150, batch loss 0.2358, batch acc 0.8226
10:20:15.161   Training iter 200, batch loss 0.2315, batch acc 0.8302
10:20:15.677   Training iter 250, batch loss 0.2324, batch acc 0.8306
10:20:16.191   Training iter 300, batch loss 0.2356, batch acc 0.8188
10:20:16.679   Training iter 350, batch loss 0.2346, batch acc 0.8266
10:20:17.233   Training iter 400, batch loss 0.2355, batch acc 0.8196
10:20:17.724   Training iter 450, batch loss 0.2366, batch acc 0.8238
10:20:18.221   Training iter 500, batch loss 0.2325, batch acc 0.8308
10:20:18.703   Training iter 550, batch loss 0.2370, batch acc 0.8200
10:20:19.182   Training iter 600, batch loss 0.2361, batch acc 0.8172
10:20:19.184 Training @ 485 epoch...
10:20:19.665   Training iter 50, batch loss 0.2359, batch acc 0.8142
10:20:20.145   Training iter 100, batch loss 0.2350, batch acc 0.8272
10:20:20.625   Training iter 150, batch loss 0.2327, batch acc 0.8320
10:20:21.113   Training iter 200, batch loss 0.2361, batch acc 0.8194
10:20:21.590   Training iter 250, batch loss 0.2353, batch acc 0.8234
10:20:22.075   Training iter 300, batch loss 0.2325, batch acc 0.8306
10:20:22.554   Training iter 350, batch loss 0.2339, batch acc 0.8320
10:20:23.035   Training iter 400, batch loss 0.2339, batch acc 0.8204
10:20:23.520   Training iter 450, batch loss 0.2340, batch acc 0.8278
10:20:24.023   Training iter 500, batch loss 0.2338, batch acc 0.8224
10:20:24.524   Training iter 550, batch loss 0.2393, batch acc 0.8162
10:20:25.026   Training iter 600, batch loss 0.2348, batch acc 0.8214
10:20:25.028 Testing @ 485 epoch...
10:20:25.063     Testing, total mean loss 0.23109, total acc 0.83230
10:20:25.063 Training @ 486 epoch...
10:20:25.570   Training iter 50, batch loss 0.2345, batch acc 0.8192
10:20:26.069   Training iter 100, batch loss 0.2354, batch acc 0.8192
10:20:26.567   Training iter 150, batch loss 0.2350, batch acc 0.8182
10:20:27.063   Training iter 200, batch loss 0.2331, batch acc 0.8322
10:20:27.563   Training iter 250, batch loss 0.2354, batch acc 0.8262
10:20:28.070   Training iter 300, batch loss 0.2344, batch acc 0.8266
10:20:28.576   Training iter 350, batch loss 0.2355, batch acc 0.8218
10:20:29.091   Training iter 400, batch loss 0.2348, batch acc 0.8230
10:20:29.598   Training iter 450, batch loss 0.2361, batch acc 0.8310
10:20:30.114   Training iter 500, batch loss 0.2322, batch acc 0.8284
10:20:30.604   Training iter 550, batch loss 0.2355, batch acc 0.8208
10:20:31.107   Training iter 600, batch loss 0.2349, batch acc 0.8228
10:20:31.109 Training @ 487 epoch...
10:20:31.616   Training iter 50, batch loss 0.2319, batch acc 0.8308
10:20:32.116   Training iter 100, batch loss 0.2339, batch acc 0.8280
10:20:32.695   Training iter 150, batch loss 0.2327, batch acc 0.8304
10:20:33.195   Training iter 200, batch loss 0.2368, batch acc 0.8196
10:20:33.687   Training iter 250, batch loss 0.2353, batch acc 0.8218
10:20:34.187   Training iter 300, batch loss 0.2345, batch acc 0.8302
10:20:34.679   Training iter 350, batch loss 0.2362, batch acc 0.8190
10:20:35.141   Training iter 400, batch loss 0.2366, batch acc 0.8218
10:20:35.607   Training iter 450, batch loss 0.2354, batch acc 0.8198
10:20:36.071   Training iter 500, batch loss 0.2358, batch acc 0.8172
10:20:36.530   Training iter 550, batch loss 0.2323, batch acc 0.8268
10:20:36.998   Training iter 600, batch loss 0.2355, batch acc 0.8220
10:20:37.001 Training @ 488 epoch...
10:20:37.473   Training iter 50, batch loss 0.2353, batch acc 0.8218
10:20:37.945   Training iter 100, batch loss 0.2324, batch acc 0.8290
10:20:38.406   Training iter 150, batch loss 0.2366, batch acc 0.8138
10:20:38.872   Training iter 200, batch loss 0.2342, batch acc 0.8242
10:20:39.344   Training iter 250, batch loss 0.2339, batch acc 0.8274
10:20:39.802   Training iter 300, batch loss 0.2324, batch acc 0.8286
10:20:40.284   Training iter 350, batch loss 0.2354, batch acc 0.8242
10:20:40.758   Training iter 400, batch loss 0.2341, batch acc 0.8262
10:20:41.224   Training iter 450, batch loss 0.2360, batch acc 0.8200
10:20:41.705   Training iter 500, batch loss 0.2354, batch acc 0.8286
10:20:42.200   Training iter 550, batch loss 0.2361, batch acc 0.8216
10:20:42.687   Training iter 600, batch loss 0.2350, batch acc 0.8224
10:20:42.689 Training @ 489 epoch...
10:20:43.178   Training iter 50, batch loss 0.2365, batch acc 0.8132
10:20:43.641   Training iter 100, batch loss 0.2328, batch acc 0.8230
10:20:44.123   Training iter 150, batch loss 0.2387, batch acc 0.8148
10:20:44.593   Training iter 200, batch loss 0.2375, batch acc 0.8142
10:20:45.087   Training iter 250, batch loss 0.2342, batch acc 0.8292
10:20:45.597   Training iter 300, batch loss 0.2314, batch acc 0.8408
10:20:46.117   Training iter 350, batch loss 0.2340, batch acc 0.8262
10:20:46.626   Training iter 400, batch loss 0.2348, batch acc 0.8214
10:20:47.113   Training iter 450, batch loss 0.2339, batch acc 0.8280
10:20:47.613   Training iter 500, batch loss 0.2354, batch acc 0.8268
10:20:48.101   Training iter 550, batch loss 0.2345, batch acc 0.8212
10:20:48.607   Training iter 600, batch loss 0.2330, batch acc 0.8268
10:20:48.609 Training @ 490 epoch...
10:20:49.093   Training iter 50, batch loss 0.2339, batch acc 0.8258
10:20:49.577   Training iter 100, batch loss 0.2323, batch acc 0.8342
10:20:50.075   Training iter 150, batch loss 0.2327, batch acc 0.8280
10:20:50.581   Training iter 200, batch loss 0.2355, batch acc 0.8246
10:20:51.080   Training iter 250, batch loss 0.2357, batch acc 0.8226
10:20:51.565   Training iter 300, batch loss 0.2355, batch acc 0.8264
10:20:52.060   Training iter 350, batch loss 0.2363, batch acc 0.8212
10:20:52.558   Training iter 400, batch loss 0.2344, batch acc 0.8238
10:20:53.038   Training iter 450, batch loss 0.2376, batch acc 0.8144
10:20:53.507   Training iter 500, batch loss 0.2343, batch acc 0.8216
10:20:53.987   Training iter 550, batch loss 0.2322, batch acc 0.8270
10:20:54.480   Training iter 600, batch loss 0.2362, batch acc 0.8200
10:20:54.481 Testing @ 490 epoch...
10:20:54.518     Testing, total mean loss 0.23106, total acc 0.83230
10:20:54.518 Training @ 491 epoch...
10:20:55.020   Training iter 50, batch loss 0.2334, batch acc 0.8260
10:20:55.528   Training iter 100, batch loss 0.2349, batch acc 0.8216
10:20:56.018   Training iter 150, batch loss 0.2336, batch acc 0.8272
10:20:56.523   Training iter 200, batch loss 0.2377, batch acc 0.8156
10:20:57.020   Training iter 250, batch loss 0.2346, batch acc 0.8280
10:20:57.522   Training iter 300, batch loss 0.2361, batch acc 0.8152
10:20:58.026   Training iter 350, batch loss 0.2312, batch acc 0.8316
10:20:58.522   Training iter 400, batch loss 0.2345, batch acc 0.8258
10:20:59.038   Training iter 450, batch loss 0.2369, batch acc 0.8182
10:20:59.534   Training iter 500, batch loss 0.2336, batch acc 0.8286
10:21:00.039   Training iter 550, batch loss 0.2354, batch acc 0.8194
10:21:00.542   Training iter 600, batch loss 0.2348, batch acc 0.8292
10:21:00.544 Training @ 492 epoch...
10:21:01.055   Training iter 50, batch loss 0.2341, batch acc 0.8214
10:21:01.588   Training iter 100, batch loss 0.2350, batch acc 0.8278
10:21:02.089   Training iter 150, batch loss 0.2341, batch acc 0.8226
10:21:02.626   Training iter 200, batch loss 0.2358, batch acc 0.8232
10:21:03.161   Training iter 250, batch loss 0.2334, batch acc 0.8214
10:21:03.657   Training iter 300, batch loss 0.2356, batch acc 0.8218
10:21:04.153   Training iter 350, batch loss 0.2348, batch acc 0.8282
10:21:04.648   Training iter 400, batch loss 0.2349, batch acc 0.8240
10:21:05.168   Training iter 450, batch loss 0.2362, batch acc 0.8208
10:21:05.674   Training iter 500, batch loss 0.2349, batch acc 0.8262
10:21:06.195   Training iter 550, batch loss 0.2343, batch acc 0.8220
10:21:06.738   Training iter 600, batch loss 0.2335, batch acc 0.8298
10:21:06.740 Training @ 493 epoch...
10:21:07.291   Training iter 50, batch loss 0.2319, batch acc 0.8354
10:21:07.837   Training iter 100, batch loss 0.2361, batch acc 0.8228
10:21:08.344   Training iter 150, batch loss 0.2355, batch acc 0.8208
10:21:08.856   Training iter 200, batch loss 0.2335, batch acc 0.8274
10:21:09.379   Training iter 250, batch loss 0.2353, batch acc 0.8306
10:21:09.924   Training iter 300, batch loss 0.2347, batch acc 0.8262
10:21:10.480   Training iter 350, batch loss 0.2354, batch acc 0.8200
10:21:11.044   Training iter 400, batch loss 0.2369, batch acc 0.8164
10:21:11.598   Training iter 450, batch loss 0.2338, batch acc 0.8230
10:21:12.160   Training iter 500, batch loss 0.2348, batch acc 0.8142
10:21:12.670   Training iter 550, batch loss 0.2344, batch acc 0.8278
10:21:13.194   Training iter 600, batch loss 0.2343, batch acc 0.8222
10:21:13.196 Training @ 494 epoch...
10:21:13.709   Training iter 50, batch loss 0.2326, batch acc 0.8200
10:21:14.216   Training iter 100, batch loss 0.2343, batch acc 0.8252
10:21:14.709   Training iter 150, batch loss 0.2372, batch acc 0.8104
10:21:15.199   Training iter 200, batch loss 0.2349, batch acc 0.8222
10:21:15.677   Training iter 250, batch loss 0.2354, batch acc 0.8254
10:21:16.153   Training iter 300, batch loss 0.2357, batch acc 0.8178
10:21:16.634   Training iter 350, batch loss 0.2341, batch acc 0.8256
10:21:17.116   Training iter 400, batch loss 0.2324, batch acc 0.8344
10:21:17.603   Training iter 450, batch loss 0.2378, batch acc 0.8218
10:21:18.110   Training iter 500, batch loss 0.2359, batch acc 0.8252
10:21:18.607   Training iter 550, batch loss 0.2328, batch acc 0.8322
10:21:19.104   Training iter 600, batch loss 0.2334, batch acc 0.8290
10:21:19.106 Training @ 495 epoch...
10:21:19.593   Training iter 50, batch loss 0.2359, batch acc 0.8200
10:21:20.099   Training iter 100, batch loss 0.2360, batch acc 0.8186
10:21:20.601   Training iter 150, batch loss 0.2328, batch acc 0.8270
10:21:21.088   Training iter 200, batch loss 0.2348, batch acc 0.8270
10:21:21.583   Training iter 250, batch loss 0.2350, batch acc 0.8214
10:21:22.109   Training iter 300, batch loss 0.2351, batch acc 0.8262
10:21:22.659   Training iter 350, batch loss 0.2334, batch acc 0.8290
10:21:23.202   Training iter 400, batch loss 0.2342, batch acc 0.8290
10:21:23.743   Training iter 450, batch loss 0.2355, batch acc 0.8226
10:21:24.284   Training iter 500, batch loss 0.2327, batch acc 0.8238
10:21:24.777   Training iter 550, batch loss 0.2367, batch acc 0.8170
10:21:25.293   Training iter 600, batch loss 0.2342, batch acc 0.8266
10:21:25.295 Testing @ 495 epoch...
10:21:25.331     Testing, total mean loss 0.23104, total acc 0.83240
10:21:25.331 Training @ 496 epoch...
10:21:25.819   Training iter 50, batch loss 0.2337, batch acc 0.8286
10:21:26.321   Training iter 100, batch loss 0.2328, batch acc 0.8324
10:21:26.816   Training iter 150, batch loss 0.2345, batch acc 0.8308
10:21:27.331   Training iter 200, batch loss 0.2374, batch acc 0.8120
10:21:27.843   Training iter 250, batch loss 0.2333, batch acc 0.8254
10:21:28.329   Training iter 300, batch loss 0.2352, batch acc 0.8156
10:21:28.800   Training iter 350, batch loss 0.2342, batch acc 0.8222
10:21:29.278   Training iter 400, batch loss 0.2354, batch acc 0.8230
10:21:29.751   Training iter 450, batch loss 0.2349, batch acc 0.8272
10:21:30.253   Training iter 500, batch loss 0.2349, batch acc 0.8264
10:21:30.727   Training iter 550, batch loss 0.2360, batch acc 0.8170
10:21:31.177   Training iter 600, batch loss 0.2340, batch acc 0.8258
10:21:31.179 Training @ 497 epoch...
10:21:31.645   Training iter 50, batch loss 0.2360, batch acc 0.8226
10:21:32.101   Training iter 100, batch loss 0.2332, batch acc 0.8244
10:21:32.563   Training iter 150, batch loss 0.2354, batch acc 0.8238
10:21:33.036   Training iter 200, batch loss 0.2336, batch acc 0.8284
10:21:33.505   Training iter 250, batch loss 0.2336, batch acc 0.8276
10:21:34.005   Training iter 300, batch loss 0.2384, batch acc 0.8142
10:21:34.495   Training iter 350, batch loss 0.2333, batch acc 0.8292
10:21:34.990   Training iter 400, batch loss 0.2331, batch acc 0.8306
10:21:35.505   Training iter 450, batch loss 0.2369, batch acc 0.8182
10:21:35.995   Training iter 500, batch loss 0.2347, batch acc 0.8242
10:21:36.481   Training iter 550, batch loss 0.2347, batch acc 0.8214
10:21:36.968   Training iter 600, batch loss 0.2335, batch acc 0.8244
10:21:36.970 Training @ 498 epoch...
10:21:37.472   Training iter 50, batch loss 0.2368, batch acc 0.8148
10:21:38.026   Training iter 100, batch loss 0.2347, batch acc 0.8276
10:21:38.574   Training iter 150, batch loss 0.2327, batch acc 0.8306
10:21:39.142   Training iter 200, batch loss 0.2357, batch acc 0.8164
10:21:39.679   Training iter 250, batch loss 0.2313, batch acc 0.8278
10:21:40.218   Training iter 300, batch loss 0.2332, batch acc 0.8280
10:21:40.760   Training iter 350, batch loss 0.2331, batch acc 0.8306
10:21:41.262   Training iter 400, batch loss 0.2329, batch acc 0.8308
10:21:41.755   Training iter 450, batch loss 0.2344, batch acc 0.8258
10:21:42.252   Training iter 500, batch loss 0.2376, batch acc 0.8170
10:21:42.756   Training iter 550, batch loss 0.2376, batch acc 0.8158
10:21:43.266   Training iter 600, batch loss 0.2362, batch acc 0.8222
10:21:43.267 Training @ 499 epoch...
10:21:43.775   Training iter 50, batch loss 0.2331, batch acc 0.8220
10:21:44.286   Training iter 100, batch loss 0.2376, batch acc 0.8092
10:21:44.756   Training iter 150, batch loss 0.2344, batch acc 0.8326
10:21:45.226   Training iter 200, batch loss 0.2341, batch acc 0.8240
10:21:45.695   Training iter 250, batch loss 0.2351, batch acc 0.8230
10:21:46.182   Training iter 300, batch loss 0.2344, batch acc 0.8240
10:21:46.650   Training iter 350, batch loss 0.2334, batch acc 0.8282
10:21:47.133   Training iter 400, batch loss 0.2354, batch acc 0.8216
10:21:47.609   Training iter 450, batch loss 0.2317, batch acc 0.8308
10:21:48.095   Training iter 500, batch loss 0.2330, batch acc 0.8312
10:21:48.553   Training iter 550, batch loss 0.2369, batch acc 0.8222
10:21:49.014   Training iter 600, batch loss 0.2370, batch acc 0.8156
======================================================
10:21:49.016 Testing @ final epoch...
10:21:49.052     Testing, total mean loss 0.23102, total acc 0.83250
training time: 3048 seconds
