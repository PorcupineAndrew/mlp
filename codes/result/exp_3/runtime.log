======================================================
learning_rate: 0.001
weight_decay: 0.01
momentum: 0.01
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Relu
loss: Softmax
result dir: ./result/exp_3
======================================================
11:13:44.786 Training @ 0 epoch...
11:13:45.361   Training iter 50, batch loss 2.3048, batch acc 0.1134
11:13:45.954   Training iter 100, batch loss 2.2839, batch acc 0.2076
11:13:46.531   Training iter 150, batch loss 2.2659, batch acc 0.2710
11:13:47.053   Training iter 200, batch loss 2.2442, batch acc 0.3136
11:13:47.603   Training iter 250, batch loss 2.2225, batch acc 0.3584
11:13:48.176   Training iter 300, batch loss 2.2091, batch acc 0.3580
11:13:48.752   Training iter 350, batch loss 2.1917, batch acc 0.3746
11:13:49.308   Training iter 400, batch loss 2.1774, batch acc 0.3818
11:13:49.855   Training iter 450, batch loss 2.1510, batch acc 0.4126
11:13:50.416   Training iter 500, batch loss 2.1229, batch acc 0.4596
11:13:50.995   Training iter 550, batch loss 2.1046, batch acc 0.4732
11:13:51.545   Training iter 600, batch loss 2.0816, batch acc 0.4668
11:13:51.547 Testing @ 0 epoch...
11:13:51.593     Testing, total mean loss 2.05797, total acc 0.49250
11:13:51.593 Plot @ 0 epoch...
11:13:51.593 Training @ 1 epoch...
11:13:52.133   Training iter 50, batch loss 2.0570, batch acc 0.4836
11:13:52.667   Training iter 100, batch loss 2.0255, batch acc 0.4928
11:13:53.210   Training iter 150, batch loss 2.0121, batch acc 0.4714
11:13:53.756   Training iter 200, batch loss 1.9993, batch acc 0.4764
11:13:54.282   Training iter 250, batch loss 1.9793, batch acc 0.4826
11:13:54.803   Training iter 300, batch loss 1.9625, batch acc 0.4966
11:13:55.356   Training iter 350, batch loss 1.9367, batch acc 0.5006
11:13:55.889   Training iter 400, batch loss 1.9104, batch acc 0.5054
11:13:56.434   Training iter 450, batch loss 1.8982, batch acc 0.5060
11:13:56.959   Training iter 500, batch loss 1.8865, batch acc 0.5010
11:13:57.499   Training iter 550, batch loss 1.8744, batch acc 0.5036
11:13:58.027   Training iter 600, batch loss 1.8619, batch acc 0.5088
11:13:58.029 Training @ 2 epoch...
11:13:58.604   Training iter 50, batch loss 1.8423, batch acc 0.5262
11:13:59.196   Training iter 100, batch loss 1.8352, batch acc 0.5160
11:13:59.767   Training iter 150, batch loss 1.8177, batch acc 0.5198
11:14:00.381   Training iter 200, batch loss 1.7995, batch acc 0.5314
11:14:00.968   Training iter 250, batch loss 1.7908, batch acc 0.5288
11:14:01.574   Training iter 300, batch loss 1.7844, batch acc 0.5342
11:14:02.173   Training iter 350, batch loss 1.7651, batch acc 0.5370
11:14:02.760   Training iter 400, batch loss 1.7679, batch acc 0.5258
11:14:03.376   Training iter 450, batch loss 1.7586, batch acc 0.5314
11:14:03.972   Training iter 500, batch loss 1.7268, batch acc 0.5472
11:14:04.573   Training iter 550, batch loss 1.7326, batch acc 0.5328
11:14:05.180   Training iter 600, batch loss 1.7292, batch acc 0.5408
11:14:05.182 Training @ 3 epoch...
11:14:05.801   Training iter 50, batch loss 1.6809, batch acc 0.5650
11:14:06.424   Training iter 100, batch loss 1.7117, batch acc 0.5400
11:14:07.061   Training iter 150, batch loss 1.7062, batch acc 0.5460
11:14:07.692   Training iter 200, batch loss 1.6882, batch acc 0.5496
11:14:08.313   Training iter 250, batch loss 1.6782, batch acc 0.5528
11:14:08.920   Training iter 300, batch loss 1.6669, batch acc 0.5558
11:14:09.513   Training iter 350, batch loss 1.6544, batch acc 0.5578
11:14:10.113   Training iter 400, batch loss 1.6659, batch acc 0.5504
11:14:10.708   Training iter 450, batch loss 1.6524, batch acc 0.5528
11:14:11.285   Training iter 500, batch loss 1.6291, batch acc 0.5680
11:14:11.830   Training iter 550, batch loss 1.6335, batch acc 0.5634
11:14:12.362   Training iter 600, batch loss 1.6313, batch acc 0.5584
11:14:12.364 Training @ 4 epoch...
11:14:12.911   Training iter 50, batch loss 1.6330, batch acc 0.5502
11:14:13.409   Training iter 100, batch loss 1.6225, batch acc 0.5524
11:14:13.918   Training iter 150, batch loss 1.6264, batch acc 0.5596
11:14:14.426   Training iter 200, batch loss 1.6107, batch acc 0.5654
11:14:14.939   Training iter 250, batch loss 1.5832, batch acc 0.5786
11:14:15.460   Training iter 300, batch loss 1.5947, batch acc 0.5706
11:14:15.954   Training iter 350, batch loss 1.5870, batch acc 0.5676
11:14:16.437   Training iter 400, batch loss 1.5633, batch acc 0.5746
11:14:16.922   Training iter 450, batch loss 1.5771, batch acc 0.5710
11:14:17.397   Training iter 500, batch loss 1.5474, batch acc 0.5844
11:14:17.878   Training iter 550, batch loss 1.5551, batch acc 0.5774
11:14:18.367   Training iter 600, batch loss 1.5429, batch acc 0.5754
11:14:18.368 Training @ 5 epoch...
11:14:18.857   Training iter 50, batch loss 1.5661, batch acc 0.5626
11:14:19.330   Training iter 100, batch loss 1.5243, batch acc 0.5828
11:14:19.803   Training iter 150, batch loss 1.5489, batch acc 0.5674
11:14:20.280   Training iter 200, batch loss 1.5474, batch acc 0.5724
11:14:20.757   Training iter 250, batch loss 1.5408, batch acc 0.5696
11:14:21.244   Training iter 300, batch loss 1.5276, batch acc 0.5808
11:14:21.765   Training iter 350, batch loss 1.5308, batch acc 0.5808
11:14:22.282   Training iter 400, batch loss 1.5240, batch acc 0.5762
11:14:22.793   Training iter 450, batch loss 1.5037, batch acc 0.5824
11:14:23.320   Training iter 500, batch loss 1.5093, batch acc 0.5774
11:14:23.837   Training iter 550, batch loss 1.5075, batch acc 0.5866
11:14:24.338   Training iter 600, batch loss 1.4984, batch acc 0.5820
11:14:24.340 Testing @ 5 epoch...
11:14:24.381     Testing, total mean loss 1.47752, total acc 0.59400
11:14:24.381 Training @ 6 epoch...
11:14:24.895   Training iter 50, batch loss 1.5121, batch acc 0.5822
11:14:25.407   Training iter 100, batch loss 1.4916, batch acc 0.5868
11:14:25.918   Training iter 150, batch loss 1.5075, batch acc 0.5744
11:14:26.435   Training iter 200, batch loss 1.4863, batch acc 0.5794
11:14:26.940   Training iter 250, batch loss 1.4577, batch acc 0.5990
11:14:27.457   Training iter 300, batch loss 1.4912, batch acc 0.5776
11:14:27.984   Training iter 350, batch loss 1.4643, batch acc 0.5894
11:14:28.515   Training iter 400, batch loss 1.4744, batch acc 0.5794
11:14:29.036   Training iter 450, batch loss 1.4796, batch acc 0.5768
11:14:29.551   Training iter 500, batch loss 1.4531, batch acc 0.5942
11:14:30.076   Training iter 550, batch loss 1.4761, batch acc 0.5762
11:14:30.595   Training iter 600, batch loss 1.4783, batch acc 0.5778
11:14:30.596 Training @ 7 epoch...
11:14:31.117   Training iter 50, batch loss 1.4397, batch acc 0.5932
11:14:31.638   Training iter 100, batch loss 1.4578, batch acc 0.5874
11:14:32.146   Training iter 150, batch loss 1.4513, batch acc 0.5866
11:14:32.657   Training iter 200, batch loss 1.4587, batch acc 0.5766
11:14:33.166   Training iter 250, batch loss 1.4534, batch acc 0.5816
11:14:33.673   Training iter 300, batch loss 1.4463, batch acc 0.5872
11:14:34.201   Training iter 350, batch loss 1.4562, batch acc 0.5780
11:14:34.721   Training iter 400, batch loss 1.4368, batch acc 0.5944
11:14:35.238   Training iter 450, batch loss 1.4261, batch acc 0.5928
11:14:35.747   Training iter 500, batch loss 1.4293, batch acc 0.5944
11:14:36.265   Training iter 550, batch loss 1.4282, batch acc 0.5884
11:14:36.789   Training iter 600, batch loss 1.4412, batch acc 0.5890
11:14:36.791 Training @ 8 epoch...
11:14:37.324   Training iter 50, batch loss 1.4465, batch acc 0.5820
11:14:37.868   Training iter 100, batch loss 1.4347, batch acc 0.5838
11:14:38.406   Training iter 150, batch loss 1.4394, batch acc 0.5830
11:14:38.928   Training iter 200, batch loss 1.4355, batch acc 0.5870
11:14:39.445   Training iter 250, batch loss 1.4136, batch acc 0.5946
11:14:39.973   Training iter 300, batch loss 1.4233, batch acc 0.5942
11:14:40.509   Training iter 350, batch loss 1.3894, batch acc 0.6010
11:14:41.046   Training iter 400, batch loss 1.3952, batch acc 0.5980
11:14:41.588   Training iter 450, batch loss 1.4009, batch acc 0.5942
11:14:42.104   Training iter 500, batch loss 1.4137, batch acc 0.5860
11:14:42.642   Training iter 550, batch loss 1.3837, batch acc 0.6026
11:14:43.196   Training iter 600, batch loss 1.3795, batch acc 0.6012
11:14:43.198 Training @ 9 epoch...
11:14:43.748   Training iter 50, batch loss 1.4069, batch acc 0.5850
11:14:44.287   Training iter 100, batch loss 1.4047, batch acc 0.5908
11:14:44.807   Training iter 150, batch loss 1.3838, batch acc 0.6030
11:14:45.335   Training iter 200, batch loss 1.3992, batch acc 0.5948
11:14:45.864   Training iter 250, batch loss 1.3920, batch acc 0.5912
11:14:46.395   Training iter 300, batch loss 1.3902, batch acc 0.5922
11:14:46.906   Training iter 350, batch loss 1.3667, batch acc 0.6044
11:14:47.430   Training iter 400, batch loss 1.3662, batch acc 0.6028
11:14:47.956   Training iter 450, batch loss 1.4000, batch acc 0.5900
11:14:48.490   Training iter 500, batch loss 1.3812, batch acc 0.5982
11:14:49.027   Training iter 550, batch loss 1.3731, batch acc 0.6046
11:14:49.550   Training iter 600, batch loss 1.3807, batch acc 0.5958
11:14:49.552 Training @ 10 epoch...
11:14:50.083   Training iter 50, batch loss 1.3838, batch acc 0.5946
11:14:50.611   Training iter 100, batch loss 1.3736, batch acc 0.5986
11:14:51.124   Training iter 150, batch loss 1.3594, batch acc 0.6070
11:14:51.649   Training iter 200, batch loss 1.3806, batch acc 0.5918
11:14:52.185   Training iter 250, batch loss 1.3605, batch acc 0.5974
11:14:52.721   Training iter 300, batch loss 1.3582, batch acc 0.6012
11:14:53.256   Training iter 350, batch loss 1.3678, batch acc 0.6034
11:14:53.797   Training iter 400, batch loss 1.3751, batch acc 0.5904
11:14:54.334   Training iter 450, batch loss 1.3622, batch acc 0.6016
11:14:54.850   Training iter 500, batch loss 1.3425, batch acc 0.6078
11:14:55.379   Training iter 550, batch loss 1.3522, batch acc 0.6020
11:14:55.901   Training iter 600, batch loss 1.3636, batch acc 0.5988
11:14:55.903 Testing @ 10 epoch...
11:14:55.944     Testing, total mean loss 1.32713, total acc 0.61040
11:14:55.944 Training @ 11 epoch...
11:14:56.484   Training iter 50, batch loss 1.3405, batch acc 0.6052
11:14:57.022   Training iter 100, batch loss 1.3354, batch acc 0.6088
11:14:57.595   Training iter 150, batch loss 1.3263, batch acc 0.6148
11:14:58.179   Training iter 200, batch loss 1.3366, batch acc 0.6050
11:14:58.759   Training iter 250, batch loss 1.3571, batch acc 0.5918
11:14:59.317   Training iter 300, batch loss 1.3448, batch acc 0.6022
11:14:59.833   Training iter 350, batch loss 1.3556, batch acc 0.5978
11:15:00.354   Training iter 400, batch loss 1.3581, batch acc 0.5950
11:15:00.868   Training iter 450, batch loss 1.3661, batch acc 0.5950
11:15:01.402   Training iter 500, batch loss 1.3558, batch acc 0.5948
11:15:01.942   Training iter 550, batch loss 1.3396, batch acc 0.6128
11:15:02.492   Training iter 600, batch loss 1.3337, batch acc 0.6076
11:15:02.494 Training @ 12 epoch...
11:15:03.048   Training iter 50, batch loss 1.3476, batch acc 0.5990
11:15:03.594   Training iter 100, batch loss 1.3237, batch acc 0.6108
11:15:04.131   Training iter 150, batch loss 1.3134, batch acc 0.6166
11:15:04.654   Training iter 200, batch loss 1.3453, batch acc 0.5986
11:15:05.184   Training iter 250, batch loss 1.3427, batch acc 0.5984
11:15:05.679   Training iter 300, batch loss 1.3275, batch acc 0.6090
11:15:06.202   Training iter 350, batch loss 1.3450, batch acc 0.5998
11:15:06.727   Training iter 400, batch loss 1.3155, batch acc 0.6060
11:15:07.257   Training iter 450, batch loss 1.3390, batch acc 0.6008
11:15:07.772   Training iter 500, batch loss 1.3212, batch acc 0.6116
11:15:08.285   Training iter 550, batch loss 1.3230, batch acc 0.6078
11:15:08.805   Training iter 600, batch loss 1.3042, batch acc 0.6164
11:15:08.807 Training @ 13 epoch...
11:15:09.346   Training iter 50, batch loss 1.3243, batch acc 0.6060
11:15:09.878   Training iter 100, batch loss 1.3227, batch acc 0.6042
11:15:10.442   Training iter 150, batch loss 1.3236, batch acc 0.5996
11:15:10.989   Training iter 200, batch loss 1.3252, batch acc 0.6066
11:15:11.531   Training iter 250, batch loss 1.3256, batch acc 0.6138
11:15:12.043   Training iter 300, batch loss 1.3044, batch acc 0.6088
11:15:12.558   Training iter 350, batch loss 1.3098, batch acc 0.6106
11:15:13.077   Training iter 400, batch loss 1.3138, batch acc 0.6074
11:15:13.596   Training iter 450, batch loss 1.3058, batch acc 0.6152
11:15:14.145   Training iter 500, batch loss 1.3095, batch acc 0.6088
11:15:14.675   Training iter 550, batch loss 1.3209, batch acc 0.6078
11:15:15.194   Training iter 600, batch loss 1.2851, batch acc 0.6174
11:15:15.196 Training @ 14 epoch...
11:15:15.715   Training iter 50, batch loss 1.3230, batch acc 0.6030
11:15:16.165   Training iter 100, batch loss 1.3109, batch acc 0.6124
11:15:16.638   Training iter 150, batch loss 1.3061, batch acc 0.6128
11:15:17.089   Training iter 200, batch loss 1.2942, batch acc 0.6110
11:15:17.556   Training iter 250, batch loss 1.2886, batch acc 0.6182
11:15:18.040   Training iter 300, batch loss 1.3204, batch acc 0.6100
11:15:18.526   Training iter 350, batch loss 1.3007, batch acc 0.6106
11:15:19.042   Training iter 400, batch loss 1.3087, batch acc 0.6040
11:15:19.537   Training iter 450, batch loss 1.2795, batch acc 0.6186
11:15:20.051   Training iter 500, batch loss 1.2991, batch acc 0.6096
11:15:20.554   Training iter 550, batch loss 1.2760, batch acc 0.6154
11:15:21.065   Training iter 600, batch loss 1.3041, batch acc 0.6090
11:15:21.067 Training @ 15 epoch...
11:15:21.582   Training iter 50, batch loss 1.2966, batch acc 0.6112
11:15:22.077   Training iter 100, batch loss 1.2757, batch acc 0.6224
11:15:22.592   Training iter 150, batch loss 1.2733, batch acc 0.6204
11:15:23.098   Training iter 200, batch loss 1.2899, batch acc 0.6118
11:15:23.615   Training iter 250, batch loss 1.3053, batch acc 0.6070
11:15:24.116   Training iter 300, batch loss 1.2898, batch acc 0.6100
11:15:24.598   Training iter 350, batch loss 1.2871, batch acc 0.6180
11:15:25.131   Training iter 400, batch loss 1.2584, batch acc 0.6264
11:15:25.658   Training iter 450, batch loss 1.2918, batch acc 0.6136
11:15:26.230   Training iter 500, batch loss 1.2940, batch acc 0.6112
11:15:26.754   Training iter 550, batch loss 1.3115, batch acc 0.6006
11:15:27.308   Training iter 600, batch loss 1.2950, batch acc 0.6132
11:15:27.310 Testing @ 15 epoch...
11:15:27.353     Testing, total mean loss 1.25477, total acc 0.62500
11:15:27.353 Training @ 16 epoch...
11:15:27.938   Training iter 50, batch loss 1.2696, batch acc 0.6198
11:15:28.443   Training iter 100, batch loss 1.2827, batch acc 0.6158
11:15:28.946   Training iter 150, batch loss 1.2728, batch acc 0.6180
11:15:29.466   Training iter 200, batch loss 1.2731, batch acc 0.6158
11:15:29.979   Training iter 250, batch loss 1.2695, batch acc 0.6194
11:15:30.502   Training iter 300, batch loss 1.2930, batch acc 0.6128
11:15:31.029   Training iter 350, batch loss 1.2579, batch acc 0.6254
11:15:31.553   Training iter 400, batch loss 1.2690, batch acc 0.6212
11:15:32.062   Training iter 450, batch loss 1.2919, batch acc 0.6098
11:15:32.540   Training iter 500, batch loss 1.2929, batch acc 0.6100
11:15:33.015   Training iter 550, batch loss 1.2833, batch acc 0.6122
11:15:33.526   Training iter 600, batch loss 1.2847, batch acc 0.6140
11:15:33.528 Training @ 17 epoch...
11:15:34.100   Training iter 50, batch loss 1.2631, batch acc 0.6222
11:15:34.595   Training iter 100, batch loss 1.2980, batch acc 0.6024
11:15:35.055   Training iter 150, batch loss 1.2856, batch acc 0.6116
11:15:35.524   Training iter 200, batch loss 1.2838, batch acc 0.6160
11:15:36.015   Training iter 250, batch loss 1.2695, batch acc 0.6160
11:15:36.496   Training iter 300, batch loss 1.2514, batch acc 0.6250
11:15:36.978   Training iter 350, batch loss 1.2642, batch acc 0.6214
11:15:37.476   Training iter 400, batch loss 1.2579, batch acc 0.6166
11:15:38.015   Training iter 450, batch loss 1.2419, batch acc 0.6332
11:15:38.524   Training iter 500, batch loss 1.2782, batch acc 0.6178
11:15:39.024   Training iter 550, batch loss 1.2587, batch acc 0.6202
11:15:39.511   Training iter 600, batch loss 1.2716, batch acc 0.6176
11:15:39.513 Training @ 18 epoch...
11:15:40.017   Training iter 50, batch loss 1.2844, batch acc 0.6108
11:15:40.510   Training iter 100, batch loss 1.2873, batch acc 0.6084
11:15:41.025   Training iter 150, batch loss 1.2522, batch acc 0.6252
11:15:41.533   Training iter 200, batch loss 1.2910, batch acc 0.6142
11:15:42.072   Training iter 250, batch loss 1.2356, batch acc 0.6346
11:15:42.609   Training iter 300, batch loss 1.2531, batch acc 0.6242
11:15:43.141   Training iter 350, batch loss 1.2826, batch acc 0.6076
11:15:43.674   Training iter 400, batch loss 1.2418, batch acc 0.6298
11:15:44.198   Training iter 450, batch loss 1.2512, batch acc 0.6248
11:15:44.735   Training iter 500, batch loss 1.2287, batch acc 0.6308
11:15:45.273   Training iter 550, batch loss 1.2492, batch acc 0.6174
11:15:45.787   Training iter 600, batch loss 1.2597, batch acc 0.6162
11:15:45.789 Training @ 19 epoch...
11:15:46.338   Training iter 50, batch loss 1.2572, batch acc 0.6206
11:15:46.871   Training iter 100, batch loss 1.2631, batch acc 0.6196
11:15:47.399   Training iter 150, batch loss 1.2451, batch acc 0.6280
11:15:47.920   Training iter 200, batch loss 1.2468, batch acc 0.6226
11:15:48.441   Training iter 250, batch loss 1.2622, batch acc 0.6110
11:15:48.954   Training iter 300, batch loss 1.2601, batch acc 0.6180
11:15:49.446   Training iter 350, batch loss 1.2590, batch acc 0.6222
11:15:49.947   Training iter 400, batch loss 1.2496, batch acc 0.6236
11:15:50.452   Training iter 450, batch loss 1.2450, batch acc 0.6228
11:15:50.996   Training iter 500, batch loss 1.2475, batch acc 0.6284
11:15:51.558   Training iter 550, batch loss 1.2439, batch acc 0.6288
11:15:52.141   Training iter 600, batch loss 1.2388, batch acc 0.6224
11:15:52.143 Training @ 20 epoch...
11:15:52.694   Training iter 50, batch loss 1.2345, batch acc 0.6324
11:15:53.215   Training iter 100, batch loss 1.2474, batch acc 0.6204
11:15:53.688   Training iter 150, batch loss 1.2646, batch acc 0.6154
11:15:54.160   Training iter 200, batch loss 1.2312, batch acc 0.6288
11:15:54.633   Training iter 250, batch loss 1.2532, batch acc 0.6208
11:15:55.109   Training iter 300, batch loss 1.2455, batch acc 0.6174
11:15:55.575   Training iter 350, batch loss 1.2452, batch acc 0.6248
11:15:56.043   Training iter 400, batch loss 1.2227, batch acc 0.6344
11:15:56.513   Training iter 450, batch loss 1.2506, batch acc 0.6220
11:15:56.982   Training iter 500, batch loss 1.2394, batch acc 0.6292
11:15:57.470   Training iter 550, batch loss 1.2541, batch acc 0.6178
11:15:57.937   Training iter 600, batch loss 1.2395, batch acc 0.6258
11:15:57.939 Testing @ 20 epoch...
11:15:57.981     Testing, total mean loss 1.21117, total acc 0.63470
11:15:57.981 Training @ 21 epoch...
11:15:58.498   Training iter 50, batch loss 1.2077, batch acc 0.6344
11:15:59.008   Training iter 100, batch loss 1.2512, batch acc 0.6208
11:15:59.528   Training iter 150, batch loss 1.2435, batch acc 0.6234
11:16:00.065   Training iter 200, batch loss 1.2535, batch acc 0.6192
11:16:00.603   Training iter 250, batch loss 1.2294, batch acc 0.6286
11:16:01.153   Training iter 300, batch loss 1.2496, batch acc 0.6204
11:16:01.784   Training iter 350, batch loss 1.2314, batch acc 0.6266
11:16:02.375   Training iter 400, batch loss 1.2461, batch acc 0.6230
11:16:02.887   Training iter 450, batch loss 1.2333, batch acc 0.6274
11:16:03.423   Training iter 500, batch loss 1.2385, batch acc 0.6232
11:16:03.942   Training iter 550, batch loss 1.2267, batch acc 0.6342
11:16:04.464   Training iter 600, batch loss 1.2339, batch acc 0.6266
11:16:04.466 Training @ 22 epoch...
11:16:04.954   Training iter 50, batch loss 1.2371, batch acc 0.6252
11:16:05.428   Training iter 100, batch loss 1.2463, batch acc 0.6242
11:16:05.910   Training iter 150, batch loss 1.2265, batch acc 0.6248
11:16:06.420   Training iter 200, batch loss 1.2448, batch acc 0.6206
11:16:06.924   Training iter 250, batch loss 1.2190, batch acc 0.6316
11:16:07.438   Training iter 300, batch loss 1.2349, batch acc 0.6238
11:16:07.947   Training iter 350, batch loss 1.2256, batch acc 0.6226
11:16:08.462   Training iter 400, batch loss 1.2316, batch acc 0.6294
11:16:08.984   Training iter 450, batch loss 1.2323, batch acc 0.6298
11:16:09.492   Training iter 500, batch loss 1.2262, batch acc 0.6314
11:16:10.004   Training iter 550, batch loss 1.2307, batch acc 0.6266
11:16:10.514   Training iter 600, batch loss 1.2123, batch acc 0.6372
11:16:10.516 Training @ 23 epoch...
11:16:11.038   Training iter 50, batch loss 1.2328, batch acc 0.6238
11:16:11.540   Training iter 100, batch loss 1.2433, batch acc 0.6230
11:16:12.048   Training iter 150, batch loss 1.2163, batch acc 0.6354
11:16:12.566   Training iter 200, batch loss 1.2447, batch acc 0.6202
11:16:13.098   Training iter 250, batch loss 1.2326, batch acc 0.6234
11:16:13.631   Training iter 300, batch loss 1.2202, batch acc 0.6308
11:16:14.169   Training iter 350, batch loss 1.2033, batch acc 0.6360
11:16:14.693   Training iter 400, batch loss 1.2219, batch acc 0.6286
11:16:15.213   Training iter 450, batch loss 1.1983, batch acc 0.6382
11:16:15.720   Training iter 500, batch loss 1.2203, batch acc 0.6376
11:16:16.249   Training iter 550, batch loss 1.2232, batch acc 0.6300
11:16:16.793   Training iter 600, batch loss 1.2388, batch acc 0.6176
11:16:16.795 Training @ 24 epoch...
11:16:17.284   Training iter 50, batch loss 1.2245, batch acc 0.6230
11:16:17.767   Training iter 100, batch loss 1.2165, batch acc 0.6336
11:16:18.260   Training iter 150, batch loss 1.2333, batch acc 0.6202
11:16:18.747   Training iter 200, batch loss 1.2351, batch acc 0.6292
11:16:19.226   Training iter 250, batch loss 1.1923, batch acc 0.6456
11:16:19.704   Training iter 300, batch loss 1.2329, batch acc 0.6158
11:16:20.193   Training iter 350, batch loss 1.1964, batch acc 0.6392
11:16:20.668   Training iter 400, batch loss 1.2161, batch acc 0.6304
11:16:21.149   Training iter 450, batch loss 1.2137, batch acc 0.6388
11:16:21.659   Training iter 500, batch loss 1.2323, batch acc 0.6236
11:16:22.193   Training iter 550, batch loss 1.2238, batch acc 0.6306
11:16:22.717   Training iter 600, batch loss 1.2114, batch acc 0.6300
11:16:22.718 Training @ 25 epoch...
11:16:23.237   Training iter 50, batch loss 1.2126, batch acc 0.6286
11:16:23.744   Training iter 100, batch loss 1.1882, batch acc 0.6398
11:16:24.250   Training iter 150, batch loss 1.1975, batch acc 0.6392
11:16:24.767   Training iter 200, batch loss 1.2461, batch acc 0.6162
11:16:25.292   Training iter 250, batch loss 1.2149, batch acc 0.6336
11:16:25.816   Training iter 300, batch loss 1.1963, batch acc 0.6388
11:16:26.339   Training iter 350, batch loss 1.2340, batch acc 0.6226
11:16:26.861   Training iter 400, batch loss 1.2121, batch acc 0.6322
11:16:27.389   Training iter 450, batch loss 1.2178, batch acc 0.6326
11:16:27.925   Training iter 500, batch loss 1.2326, batch acc 0.6268
11:16:28.453   Training iter 550, batch loss 1.2055, batch acc 0.6334
11:16:28.978   Training iter 600, batch loss 1.2077, batch acc 0.6368
11:16:28.979 Testing @ 25 epoch...
11:16:29.022     Testing, total mean loss 1.18184, total acc 0.64230
11:16:29.022 Training @ 26 epoch...
11:16:29.556   Training iter 50, batch loss 1.1947, batch acc 0.6362
11:16:30.069   Training iter 100, batch loss 1.2066, batch acc 0.6402
11:16:30.569   Training iter 150, batch loss 1.2211, batch acc 0.6274
11:16:31.094   Training iter 200, batch loss 1.1921, batch acc 0.6392
11:16:31.625   Training iter 250, batch loss 1.1870, batch acc 0.6434
11:16:32.165   Training iter 300, batch loss 1.2190, batch acc 0.6284
11:16:32.698   Training iter 350, batch loss 1.2016, batch acc 0.6394
11:16:33.237   Training iter 400, batch loss 1.2202, batch acc 0.6264
11:16:33.776   Training iter 450, batch loss 1.2151, batch acc 0.6266
11:16:34.323   Training iter 500, batch loss 1.2295, batch acc 0.6292
11:16:34.868   Training iter 550, batch loss 1.2242, batch acc 0.6256
11:16:35.412   Training iter 600, batch loss 1.1956, batch acc 0.6322
11:16:35.414 Training @ 27 epoch...
11:16:35.998   Training iter 50, batch loss 1.2210, batch acc 0.6264
11:16:36.585   Training iter 100, batch loss 1.2085, batch acc 0.6308
11:16:37.104   Training iter 150, batch loss 1.2115, batch acc 0.6364
11:16:37.630   Training iter 200, batch loss 1.2062, batch acc 0.6366
11:16:38.153   Training iter 250, batch loss 1.2023, batch acc 0.6348
11:16:38.693   Training iter 300, batch loss 1.2256, batch acc 0.6174
11:16:39.232   Training iter 350, batch loss 1.2109, batch acc 0.6352
11:16:39.758   Training iter 400, batch loss 1.2133, batch acc 0.6378
11:16:40.320   Training iter 450, batch loss 1.2043, batch acc 0.6298
11:16:40.857   Training iter 500, batch loss 1.1665, batch acc 0.6460
11:16:41.446   Training iter 550, batch loss 1.1729, batch acc 0.6430
11:16:42.006   Training iter 600, batch loss 1.2080, batch acc 0.6364
11:16:42.008 Training @ 28 epoch...
11:16:42.547   Training iter 50, batch loss 1.2104, batch acc 0.6304
11:16:43.089   Training iter 100, batch loss 1.1621, batch acc 0.6432
11:16:43.625   Training iter 150, batch loss 1.2228, batch acc 0.6226
11:16:44.150   Training iter 200, batch loss 1.2028, batch acc 0.6346
11:16:44.686   Training iter 250, batch loss 1.1828, batch acc 0.6408
11:16:45.205   Training iter 300, batch loss 1.2223, batch acc 0.6240
11:16:45.726   Training iter 350, batch loss 1.1825, batch acc 0.6466
11:16:46.262   Training iter 400, batch loss 1.2125, batch acc 0.6314
11:16:46.846   Training iter 450, batch loss 1.2171, batch acc 0.6302
11:16:47.425   Training iter 500, batch loss 1.1990, batch acc 0.6336
11:16:47.993   Training iter 550, batch loss 1.2102, batch acc 0.6314
11:16:48.519   Training iter 600, batch loss 1.1744, batch acc 0.6508
11:16:48.521 Training @ 29 epoch...
11:16:49.024   Training iter 50, batch loss 1.2148, batch acc 0.6262
11:16:49.509   Training iter 100, batch loss 1.2057, batch acc 0.6388
11:16:50.007   Training iter 150, batch loss 1.1874, batch acc 0.6416
11:16:50.544   Training iter 200, batch loss 1.2076, batch acc 0.6234
11:16:51.093   Training iter 250, batch loss 1.1774, batch acc 0.6476
11:16:51.636   Training iter 300, batch loss 1.1926, batch acc 0.6366
11:16:52.173   Training iter 350, batch loss 1.2117, batch acc 0.6360
11:16:52.720   Training iter 400, batch loss 1.1807, batch acc 0.6376
11:16:53.229   Training iter 450, batch loss 1.2032, batch acc 0.6330
11:16:53.746   Training iter 500, batch loss 1.1991, batch acc 0.6338
11:16:54.235   Training iter 550, batch loss 1.1817, batch acc 0.6436
11:16:54.717   Training iter 600, batch loss 1.1879, batch acc 0.6374
11:16:54.719 Training @ 30 epoch...
11:16:55.217   Training iter 50, batch loss 1.1852, batch acc 0.6392
11:16:55.701   Training iter 100, batch loss 1.2062, batch acc 0.6298
11:16:56.196   Training iter 150, batch loss 1.1950, batch acc 0.6356
11:16:56.710   Training iter 200, batch loss 1.2056, batch acc 0.6316
11:16:57.242   Training iter 250, batch loss 1.1866, batch acc 0.6464
11:16:57.750   Training iter 300, batch loss 1.1653, batch acc 0.6490
11:16:58.243   Training iter 350, batch loss 1.1922, batch acc 0.6364
11:16:58.738   Training iter 400, batch loss 1.2152, batch acc 0.6346
11:16:59.226   Training iter 450, batch loss 1.1735, batch acc 0.6452
11:16:59.718   Training iter 500, batch loss 1.1792, batch acc 0.6332
11:17:00.219   Training iter 550, batch loss 1.1917, batch acc 0.6348
11:17:00.714   Training iter 600, batch loss 1.2076, batch acc 0.6326
11:17:00.716 Testing @ 30 epoch...
11:17:00.756     Testing, total mean loss 1.16038, total acc 0.64850
11:17:00.757 Training @ 31 epoch...
11:17:01.279   Training iter 50, batch loss 1.2030, batch acc 0.6314
11:17:01.835   Training iter 100, batch loss 1.1830, batch acc 0.6412
11:17:02.375   Training iter 150, batch loss 1.1738, batch acc 0.6480
11:17:02.908   Training iter 200, batch loss 1.1706, batch acc 0.6458
11:17:03.475   Training iter 250, batch loss 1.1645, batch acc 0.6440
11:17:04.036   Training iter 300, batch loss 1.2143, batch acc 0.6276
11:17:04.592   Training iter 350, batch loss 1.1901, batch acc 0.6378
11:17:05.146   Training iter 400, batch loss 1.1795, batch acc 0.6494
11:17:05.696   Training iter 450, batch loss 1.1938, batch acc 0.6350
11:17:06.246   Training iter 500, batch loss 1.2123, batch acc 0.6316
11:17:06.810   Training iter 550, batch loss 1.1891, batch acc 0.6356
11:17:07.364   Training iter 600, batch loss 1.1852, batch acc 0.6356
11:17:07.366 Training @ 32 epoch...
11:17:07.909   Training iter 50, batch loss 1.1836, batch acc 0.6446
11:17:08.438   Training iter 100, batch loss 1.1887, batch acc 0.6354
11:17:08.953   Training iter 150, batch loss 1.1943, batch acc 0.6320
11:17:09.445   Training iter 200, batch loss 1.1775, batch acc 0.6476
11:17:09.936   Training iter 250, batch loss 1.1790, batch acc 0.6424
11:17:10.452   Training iter 300, batch loss 1.1777, batch acc 0.6402
11:17:10.957   Training iter 350, batch loss 1.1844, batch acc 0.6466
11:17:11.487   Training iter 400, batch loss 1.2052, batch acc 0.6308
11:17:12.003   Training iter 450, batch loss 1.1861, batch acc 0.6418
11:17:12.538   Training iter 500, batch loss 1.1820, batch acc 0.6356
11:17:13.077   Training iter 550, batch loss 1.2216, batch acc 0.6204
11:17:13.597   Training iter 600, batch loss 1.1375, batch acc 0.6606
11:17:13.599 Training @ 33 epoch...
11:17:14.126   Training iter 50, batch loss 1.1635, batch acc 0.6484
11:17:14.649   Training iter 100, batch loss 1.2099, batch acc 0.6322
11:17:15.150   Training iter 150, batch loss 1.1914, batch acc 0.6372
11:17:15.635   Training iter 200, batch loss 1.1884, batch acc 0.6392
11:17:16.124   Training iter 250, batch loss 1.1768, batch acc 0.6474
11:17:16.604   Training iter 300, batch loss 1.1796, batch acc 0.6444
11:17:17.081   Training iter 350, batch loss 1.1830, batch acc 0.6374
11:17:17.567   Training iter 400, batch loss 1.1763, batch acc 0.6450
11:17:18.050   Training iter 450, batch loss 1.1849, batch acc 0.6336
11:17:18.546   Training iter 500, batch loss 1.1694, batch acc 0.6456
11:17:19.046   Training iter 550, batch loss 1.1797, batch acc 0.6374
11:17:19.555   Training iter 600, batch loss 1.1751, batch acc 0.6452
11:17:19.556 Training @ 34 epoch...
11:17:20.069   Training iter 50, batch loss 1.1846, batch acc 0.6380
11:17:20.585   Training iter 100, batch loss 1.1787, batch acc 0.6380
11:17:21.091   Training iter 150, batch loss 1.1818, batch acc 0.6438
11:17:21.598   Training iter 200, batch loss 1.1727, batch acc 0.6472
11:17:22.116   Training iter 250, batch loss 1.1847, batch acc 0.6420
11:17:22.633   Training iter 300, batch loss 1.1989, batch acc 0.6370
11:17:23.145   Training iter 350, batch loss 1.1688, batch acc 0.6440
11:17:23.677   Training iter 400, batch loss 1.1830, batch acc 0.6432
11:17:24.188   Training iter 450, batch loss 1.1614, batch acc 0.6520
11:17:24.721   Training iter 500, batch loss 1.1667, batch acc 0.6424
11:17:25.249   Training iter 550, batch loss 1.1765, batch acc 0.6392
11:17:25.777   Training iter 600, batch loss 1.1823, batch acc 0.6400
11:17:25.778 Training @ 35 epoch...
11:17:26.308   Training iter 50, batch loss 1.1834, batch acc 0.6426
11:17:26.827   Training iter 100, batch loss 1.1912, batch acc 0.6352
11:17:27.354   Training iter 150, batch loss 1.1821, batch acc 0.6432
11:17:27.850   Training iter 200, batch loss 1.1597, batch acc 0.6524
11:17:28.333   Training iter 250, batch loss 1.1584, batch acc 0.6486
11:17:28.834   Training iter 300, batch loss 1.1668, batch acc 0.6456
11:17:29.332   Training iter 350, batch loss 1.1746, batch acc 0.6406
11:17:29.820   Training iter 400, batch loss 1.1730, batch acc 0.6426
11:17:30.326   Training iter 450, batch loss 1.1644, batch acc 0.6492
11:17:30.827   Training iter 500, batch loss 1.1821, batch acc 0.6368
11:17:31.337   Training iter 550, batch loss 1.1765, batch acc 0.6400
11:17:31.842   Training iter 600, batch loss 1.1923, batch acc 0.6354
11:17:31.843 Testing @ 35 epoch...
11:17:31.884     Testing, total mean loss 1.14413, total acc 0.65510
11:17:31.884 Training @ 36 epoch...
11:17:32.396   Training iter 50, batch loss 1.1759, batch acc 0.6386
11:17:32.888   Training iter 100, batch loss 1.1701, batch acc 0.6462
11:17:33.377   Training iter 150, batch loss 1.1739, batch acc 0.6490
11:17:33.873   Training iter 200, batch loss 1.1726, batch acc 0.6384
11:17:34.362   Training iter 250, batch loss 1.1725, batch acc 0.6440
11:17:34.878   Training iter 300, batch loss 1.1866, batch acc 0.6348
11:17:35.417   Training iter 350, batch loss 1.1606, batch acc 0.6482
11:17:35.936   Training iter 400, batch loss 1.1994, batch acc 0.6390
11:17:36.456   Training iter 450, batch loss 1.1592, batch acc 0.6512
11:17:36.953   Training iter 500, batch loss 1.1732, batch acc 0.6410
11:17:37.472   Training iter 550, batch loss 1.1748, batch acc 0.6438
11:17:37.986   Training iter 600, batch loss 1.1514, batch acc 0.6486
11:17:37.988 Training @ 37 epoch...
11:17:38.505   Training iter 50, batch loss 1.1577, batch acc 0.6496
11:17:38.991   Training iter 100, batch loss 1.1540, batch acc 0.6562
11:17:39.504   Training iter 150, batch loss 1.1983, batch acc 0.6334
11:17:40.024   Training iter 200, batch loss 1.1689, batch acc 0.6456
11:17:40.549   Training iter 250, batch loss 1.1962, batch acc 0.6340
11:17:41.053   Training iter 300, batch loss 1.1508, batch acc 0.6506
11:17:41.541   Training iter 350, batch loss 1.1876, batch acc 0.6366
11:17:42.045   Training iter 400, batch loss 1.1552, batch acc 0.6480
11:17:42.546   Training iter 450, batch loss 1.1761, batch acc 0.6400
11:17:43.056   Training iter 500, batch loss 1.1613, batch acc 0.6424
11:17:43.610   Training iter 550, batch loss 1.1711, batch acc 0.6414
11:17:44.182   Training iter 600, batch loss 1.1602, batch acc 0.6552
11:17:44.183 Training @ 38 epoch...
11:17:44.802   Training iter 50, batch loss 1.1475, batch acc 0.6558
11:17:45.326   Training iter 100, batch loss 1.1549, batch acc 0.6504
11:17:45.842   Training iter 150, batch loss 1.1879, batch acc 0.6382
11:17:46.364   Training iter 200, batch loss 1.1421, batch acc 0.6508
11:17:46.881   Training iter 250, batch loss 1.1899, batch acc 0.6344
11:17:47.406   Training iter 300, batch loss 1.1671, batch acc 0.6430
11:17:47.937   Training iter 350, batch loss 1.1677, batch acc 0.6444
11:17:48.447   Training iter 400, batch loss 1.1468, batch acc 0.6508
11:17:48.957   Training iter 450, batch loss 1.1717, batch acc 0.6518
11:17:49.462   Training iter 500, batch loss 1.1850, batch acc 0.6382
11:17:49.977   Training iter 550, batch loss 1.1685, batch acc 0.6492
11:17:50.502   Training iter 600, batch loss 1.1768, batch acc 0.6394
11:17:50.504 Training @ 39 epoch...
11:17:51.030   Training iter 50, batch loss 1.1672, batch acc 0.6444
11:17:51.680   Training iter 100, batch loss 1.1436, batch acc 0.6518
11:17:52.288   Training iter 150, batch loss 1.1511, batch acc 0.6458
11:17:52.835   Training iter 200, batch loss 1.1980, batch acc 0.6368
11:17:53.389   Training iter 250, batch loss 1.1936, batch acc 0.6330
11:17:53.932   Training iter 300, batch loss 1.1574, batch acc 0.6538
11:17:54.480   Training iter 350, batch loss 1.1739, batch acc 0.6478
11:17:55.041   Training iter 400, batch loss 1.1382, batch acc 0.6518
11:17:55.621   Training iter 450, batch loss 1.1384, batch acc 0.6580
11:17:56.217   Training iter 500, batch loss 1.1967, batch acc 0.6292
11:17:56.803   Training iter 550, batch loss 1.1720, batch acc 0.6428
11:17:57.383   Training iter 600, batch loss 1.1465, batch acc 0.6566
11:17:57.384 Training @ 40 epoch...
11:17:57.953   Training iter 50, batch loss 1.1486, batch acc 0.6532
11:17:58.538   Training iter 100, batch loss 1.1584, batch acc 0.6466
11:17:59.109   Training iter 150, batch loss 1.1564, batch acc 0.6450
11:17:59.674   Training iter 200, batch loss 1.1481, batch acc 0.6508
11:18:00.234   Training iter 250, batch loss 1.1626, batch acc 0.6466
11:18:00.770   Training iter 300, batch loss 1.1647, batch acc 0.6462
11:18:01.320   Training iter 350, batch loss 1.1761, batch acc 0.6438
11:18:01.947   Training iter 400, batch loss 1.1866, batch acc 0.6402
11:18:02.508   Training iter 450, batch loss 1.1810, batch acc 0.6358
11:18:03.079   Training iter 500, batch loss 1.1578, batch acc 0.6486
11:18:03.661   Training iter 550, batch loss 1.1829, batch acc 0.6384
11:18:04.246   Training iter 600, batch loss 1.1245, batch acc 0.6662
11:18:04.248 Testing @ 40 epoch...
11:18:04.296     Testing, total mean loss 1.13128, total acc 0.65860
11:18:04.296 Training @ 41 epoch...
11:18:04.898   Training iter 50, batch loss 1.1394, batch acc 0.6552
11:18:05.510   Training iter 100, batch loss 1.1599, batch acc 0.6494
11:18:06.107   Training iter 150, batch loss 1.1553, batch acc 0.6516
11:18:06.727   Training iter 200, batch loss 1.1817, batch acc 0.6392
11:18:07.333   Training iter 250, batch loss 1.1671, batch acc 0.6432
11:18:07.932   Training iter 300, batch loss 1.1520, batch acc 0.6470
11:18:08.547   Training iter 350, batch loss 1.1480, batch acc 0.6556
11:18:09.165   Training iter 400, batch loss 1.1715, batch acc 0.6366
11:18:09.776   Training iter 450, batch loss 1.1572, batch acc 0.6494
11:18:10.368   Training iter 500, batch loss 1.1398, batch acc 0.6524
11:18:10.955   Training iter 550, batch loss 1.1693, batch acc 0.6476
11:18:11.517   Training iter 600, batch loss 1.1792, batch acc 0.6400
11:18:11.519 Training @ 42 epoch...
11:18:12.111   Training iter 50, batch loss 1.1602, batch acc 0.6454
11:18:12.685   Training iter 100, batch loss 1.1445, batch acc 0.6548
11:18:13.248   Training iter 150, batch loss 1.1635, batch acc 0.6492
11:18:13.785   Training iter 200, batch loss 1.1679, batch acc 0.6486
11:18:14.332   Training iter 250, batch loss 1.1589, batch acc 0.6490
11:18:14.879   Training iter 300, batch loss 1.1759, batch acc 0.6382
11:18:15.423   Training iter 350, batch loss 1.1554, batch acc 0.6518
11:18:15.969   Training iter 400, batch loss 1.1374, batch acc 0.6546
11:18:16.485   Training iter 450, batch loss 1.1560, batch acc 0.6474
11:18:16.995   Training iter 500, batch loss 1.1553, batch acc 0.6484
11:18:17.496   Training iter 550, batch loss 1.1719, batch acc 0.6422
11:18:17.980   Training iter 600, batch loss 1.1472, batch acc 0.6500
11:18:17.981 Training @ 43 epoch...
11:18:18.470   Training iter 50, batch loss 1.1414, batch acc 0.6504
11:18:18.954   Training iter 100, batch loss 1.1596, batch acc 0.6446
11:18:19.446   Training iter 150, batch loss 1.1278, batch acc 0.6552
11:18:19.922   Training iter 200, batch loss 1.1795, batch acc 0.6456
11:18:20.404   Training iter 250, batch loss 1.1430, batch acc 0.6544
11:18:20.876   Training iter 300, batch loss 1.1602, batch acc 0.6516
11:18:21.350   Training iter 350, batch loss 1.1715, batch acc 0.6460
11:18:21.855   Training iter 400, batch loss 1.1331, batch acc 0.6598
11:18:22.369   Training iter 450, batch loss 1.1705, batch acc 0.6420
11:18:22.880   Training iter 500, batch loss 1.1639, batch acc 0.6470
11:18:23.406   Training iter 550, batch loss 1.1680, batch acc 0.6418
11:18:23.941   Training iter 600, batch loss 1.1505, batch acc 0.6500
11:18:23.943 Training @ 44 epoch...
11:18:24.485   Training iter 50, batch loss 1.1721, batch acc 0.6426
11:18:25.020   Training iter 100, batch loss 1.1393, batch acc 0.6560
11:18:25.570   Training iter 150, batch loss 1.1519, batch acc 0.6492
11:18:26.098   Training iter 200, batch loss 1.1454, batch acc 0.6498
11:18:26.604   Training iter 250, batch loss 1.1565, batch acc 0.6508
11:18:27.107   Training iter 300, batch loss 1.1688, batch acc 0.6384
11:18:27.608   Training iter 350, batch loss 1.1505, batch acc 0.6530
11:18:28.124   Training iter 400, batch loss 1.1587, batch acc 0.6484
11:18:28.645   Training iter 450, batch loss 1.1787, batch acc 0.6472
11:18:29.162   Training iter 500, batch loss 1.1734, batch acc 0.6390
11:18:29.672   Training iter 550, batch loss 1.1152, batch acc 0.6648
11:18:30.199   Training iter 600, batch loss 1.1342, batch acc 0.6586
11:18:30.200 Training @ 45 epoch...
11:18:30.739   Training iter 50, batch loss 1.1503, batch acc 0.6458
11:18:31.269   Training iter 100, batch loss 1.1753, batch acc 0.6404
11:18:31.792   Training iter 150, batch loss 1.1354, batch acc 0.6608
11:18:32.301   Training iter 200, batch loss 1.1311, batch acc 0.6572
11:18:32.806   Training iter 250, batch loss 1.1358, batch acc 0.6546
11:18:33.333   Training iter 300, batch loss 1.1696, batch acc 0.6464
11:18:33.839   Training iter 350, batch loss 1.1582, batch acc 0.6474
11:18:34.357   Training iter 400, batch loss 1.1624, batch acc 0.6494
11:18:34.870   Training iter 450, batch loss 1.1392, batch acc 0.6570
11:18:35.385   Training iter 500, batch loss 1.1402, batch acc 0.6550
11:18:35.890   Training iter 550, batch loss 1.1619, batch acc 0.6472
11:18:36.401   Training iter 600, batch loss 1.1621, batch acc 0.6412
11:18:36.403 Testing @ 45 epoch...
11:18:36.444     Testing, total mean loss 1.12098, total acc 0.66080
11:18:36.444 Training @ 46 epoch...
11:18:36.959   Training iter 50, batch loss 1.1513, batch acc 0.6514
11:18:37.483   Training iter 100, batch loss 1.1363, batch acc 0.6534
11:18:38.030   Training iter 150, batch loss 1.1356, batch acc 0.6568
11:18:38.577   Training iter 200, batch loss 1.1573, batch acc 0.6492
11:18:39.115   Training iter 250, batch loss 1.1443, batch acc 0.6596
11:18:39.668   Training iter 300, batch loss 1.1699, batch acc 0.6450
11:18:40.237   Training iter 350, batch loss 1.1639, batch acc 0.6446
11:18:40.801   Training iter 400, batch loss 1.1490, batch acc 0.6508
11:18:41.368   Training iter 450, batch loss 1.1531, batch acc 0.6474
11:18:41.934   Training iter 500, batch loss 1.1258, batch acc 0.6522
11:18:42.481   Training iter 550, batch loss 1.1606, batch acc 0.6430
11:18:43.046   Training iter 600, batch loss 1.1518, batch acc 0.6568
11:18:43.048 Training @ 47 epoch...
11:18:43.607   Training iter 50, batch loss 1.1575, batch acc 0.6484
11:18:44.142   Training iter 100, batch loss 1.1756, batch acc 0.6398
11:18:44.685   Training iter 150, batch loss 1.1694, batch acc 0.6386
11:18:45.223   Training iter 200, batch loss 1.1687, batch acc 0.6508
11:18:45.758   Training iter 250, batch loss 1.1239, batch acc 0.6616
11:18:46.269   Training iter 300, batch loss 1.1279, batch acc 0.6628
11:18:46.782   Training iter 350, batch loss 1.1263, batch acc 0.6642
11:18:47.285   Training iter 400, batch loss 1.1387, batch acc 0.6556
11:18:47.783   Training iter 450, batch loss 1.1400, batch acc 0.6544
11:18:48.283   Training iter 500, batch loss 1.1597, batch acc 0.6402
11:18:48.803   Training iter 550, batch loss 1.1514, batch acc 0.6488
11:18:49.339   Training iter 600, batch loss 1.1384, batch acc 0.6540
11:18:49.342 Training @ 48 epoch...
11:18:49.921   Training iter 50, batch loss 1.1290, batch acc 0.6614
11:18:50.404   Training iter 100, batch loss 1.1632, batch acc 0.6444
11:18:50.884   Training iter 150, batch loss 1.1198, batch acc 0.6568
11:18:51.356   Training iter 200, batch loss 1.1579, batch acc 0.6430
11:18:51.842   Training iter 250, batch loss 1.1488, batch acc 0.6520
11:18:52.337   Training iter 300, batch loss 1.1611, batch acc 0.6444
11:18:52.825   Training iter 350, batch loss 1.1491, batch acc 0.6544
11:18:53.320   Training iter 400, batch loss 1.1381, batch acc 0.6536
11:18:53.816   Training iter 450, batch loss 1.1335, batch acc 0.6622
11:18:54.311   Training iter 500, batch loss 1.1402, batch acc 0.6560
11:18:54.796   Training iter 550, batch loss 1.1625, batch acc 0.6434
11:18:55.281   Training iter 600, batch loss 1.1536, batch acc 0.6564
11:18:55.283 Training @ 49 epoch...
11:18:55.767   Training iter 50, batch loss 1.1459, batch acc 0.6518
11:18:56.258   Training iter 100, batch loss 1.1476, batch acc 0.6546
11:18:56.762   Training iter 150, batch loss 1.1542, batch acc 0.6490
11:18:57.263   Training iter 200, batch loss 1.1394, batch acc 0.6490
11:18:57.767   Training iter 250, batch loss 1.1466, batch acc 0.6464
11:18:58.282   Training iter 300, batch loss 1.1333, batch acc 0.6554
11:18:58.817   Training iter 350, batch loss 1.1390, batch acc 0.6594
11:18:59.414   Training iter 400, batch loss 1.1268, batch acc 0.6642
11:18:59.959   Training iter 450, batch loss 1.1783, batch acc 0.6368
11:19:00.516   Training iter 500, batch loss 1.1419, batch acc 0.6568
11:19:01.052   Training iter 550, batch loss 1.1488, batch acc 0.6476
11:19:01.605   Training iter 600, batch loss 1.1344, batch acc 0.6578
11:19:01.606 Training @ 50 epoch...
11:19:02.185   Training iter 50, batch loss 1.1291, batch acc 0.6632
11:19:02.740   Training iter 100, batch loss 1.1675, batch acc 0.6450
11:19:03.284   Training iter 150, batch loss 1.1207, batch acc 0.6632
11:19:03.833   Training iter 200, batch loss 1.1478, batch acc 0.6504
11:19:04.382   Training iter 250, batch loss 1.1296, batch acc 0.6578
11:19:04.922   Training iter 300, batch loss 1.1128, batch acc 0.6618
11:19:05.466   Training iter 350, batch loss 1.1567, batch acc 0.6454
11:19:06.005   Training iter 400, batch loss 1.2030, batch acc 0.6382
11:19:06.550   Training iter 450, batch loss 1.1465, batch acc 0.6512
11:19:07.099   Training iter 500, batch loss 1.1054, batch acc 0.6636
11:19:07.603   Training iter 550, batch loss 1.1634, batch acc 0.6440
11:19:08.089   Training iter 600, batch loss 1.1347, batch acc 0.6548
11:19:08.090 Testing @ 50 epoch...
11:19:08.131     Testing, total mean loss 1.11247, total acc 0.66350
11:19:08.131 Training @ 51 epoch...
11:19:08.628   Training iter 50, batch loss 1.1363, batch acc 0.6578
11:19:09.100   Training iter 100, batch loss 1.1310, batch acc 0.6602
11:19:09.592   Training iter 150, batch loss 1.1178, batch acc 0.6676
11:19:10.075   Training iter 200, batch loss 1.1458, batch acc 0.6468
11:19:10.567   Training iter 250, batch loss 1.1367, batch acc 0.6550
11:19:11.050   Training iter 300, batch loss 1.1778, batch acc 0.6358
11:19:11.543   Training iter 350, batch loss 1.1314, batch acc 0.6532
11:19:12.046   Training iter 400, batch loss 1.1516, batch acc 0.6522
11:19:12.541   Training iter 450, batch loss 1.1005, batch acc 0.6624
11:19:13.046   Training iter 500, batch loss 1.1474, batch acc 0.6538
11:19:13.576   Training iter 550, batch loss 1.1858, batch acc 0.6354
11:19:14.146   Training iter 600, batch loss 1.1363, batch acc 0.6610
11:19:14.147 Training @ 52 epoch...
11:19:14.715   Training iter 50, batch loss 1.1476, batch acc 0.6502
11:19:15.266   Training iter 100, batch loss 1.1597, batch acc 0.6444
11:19:15.787   Training iter 150, batch loss 1.1294, batch acc 0.6560
11:19:16.311   Training iter 200, batch loss 1.1305, batch acc 0.6602
11:19:16.824   Training iter 250, batch loss 1.1728, batch acc 0.6392
11:19:17.351   Training iter 300, batch loss 1.1287, batch acc 0.6604
11:19:17.878   Training iter 350, batch loss 1.1184, batch acc 0.6662
11:19:18.383   Training iter 400, batch loss 1.1547, batch acc 0.6460
11:19:18.877   Training iter 450, batch loss 1.1345, batch acc 0.6522
11:19:19.364   Training iter 500, batch loss 1.1325, batch acc 0.6516
11:19:19.843   Training iter 550, batch loss 1.1358, batch acc 0.6610
11:19:20.339   Training iter 600, batch loss 1.1355, batch acc 0.6572
11:19:20.341 Training @ 53 epoch...
11:19:20.835   Training iter 50, batch loss 1.1317, batch acc 0.6620
11:19:21.326   Training iter 100, batch loss 1.1375, batch acc 0.6584
11:19:21.807   Training iter 150, batch loss 1.1648, batch acc 0.6408
11:19:22.288   Training iter 200, batch loss 1.1308, batch acc 0.6590
11:19:22.792   Training iter 250, batch loss 1.1535, batch acc 0.6480
11:19:23.292   Training iter 300, batch loss 1.1689, batch acc 0.6424
11:19:23.790   Training iter 350, batch loss 1.1020, batch acc 0.6684
11:19:24.293   Training iter 400, batch loss 1.1491, batch acc 0.6522
11:19:24.797   Training iter 450, batch loss 1.1346, batch acc 0.6544
11:19:25.315   Training iter 500, batch loss 1.1367, batch acc 0.6542
11:19:25.814   Training iter 550, batch loss 1.1328, batch acc 0.6568
11:19:26.321   Training iter 600, batch loss 1.1206, batch acc 0.6614
11:19:26.323 Training @ 54 epoch...
11:19:26.836   Training iter 50, batch loss 1.1535, batch acc 0.6496
11:19:27.356   Training iter 100, batch loss 1.1570, batch acc 0.6472
11:19:27.873   Training iter 150, batch loss 1.1553, batch acc 0.6500
11:19:28.426   Training iter 200, batch loss 1.1035, batch acc 0.6660
11:19:28.989   Training iter 250, batch loss 1.1161, batch acc 0.6612
11:19:29.550   Training iter 300, batch loss 1.1466, batch acc 0.6498
11:19:30.109   Training iter 350, batch loss 1.1413, batch acc 0.6510
11:19:30.658   Training iter 400, batch loss 1.1492, batch acc 0.6512
11:19:31.203   Training iter 450, batch loss 1.1320, batch acc 0.6610
11:19:31.745   Training iter 500, batch loss 1.1170, batch acc 0.6620
11:19:32.280   Training iter 550, batch loss 1.1545, batch acc 0.6454
11:19:32.824   Training iter 600, batch loss 1.1196, batch acc 0.6630
11:19:32.826 Training @ 55 epoch...
11:19:33.383   Training iter 50, batch loss 1.1561, batch acc 0.6510
11:19:33.934   Training iter 100, batch loss 1.1375, batch acc 0.6546
11:19:34.471   Training iter 150, batch loss 1.1349, batch acc 0.6536
11:19:35.006   Training iter 200, batch loss 1.1231, batch acc 0.6598
11:19:35.549   Training iter 250, batch loss 1.1410, batch acc 0.6522
11:19:36.076   Training iter 300, batch loss 1.1623, batch acc 0.6462
11:19:36.589   Training iter 350, batch loss 1.1385, batch acc 0.6524
11:19:37.096   Training iter 400, batch loss 1.1171, batch acc 0.6586
11:19:37.597   Training iter 450, batch loss 1.1284, batch acc 0.6594
11:19:38.089   Training iter 500, batch loss 1.1295, batch acc 0.6564
11:19:38.584   Training iter 550, batch loss 1.1100, batch acc 0.6700
11:19:39.082   Training iter 600, batch loss 1.1509, batch acc 0.6484
11:19:39.084 Testing @ 55 epoch...
11:19:39.126     Testing, total mean loss 1.10534, total acc 0.66650
11:19:39.126 Training @ 56 epoch...
11:19:39.634   Training iter 50, batch loss 1.1148, batch acc 0.6616
11:19:40.142   Training iter 100, batch loss 1.1346, batch acc 0.6540
11:19:40.642   Training iter 150, batch loss 1.1691, batch acc 0.6480
11:19:41.126   Training iter 200, batch loss 1.1372, batch acc 0.6516
11:19:41.611   Training iter 250, batch loss 1.1174, batch acc 0.6594
11:19:42.090   Training iter 300, batch loss 1.1479, batch acc 0.6562
11:19:42.586   Training iter 350, batch loss 1.1455, batch acc 0.6444
11:19:43.079   Training iter 400, batch loss 1.1386, batch acc 0.6524
11:19:43.613   Training iter 450, batch loss 1.1370, batch acc 0.6602
11:19:44.133   Training iter 500, batch loss 1.1240, batch acc 0.6604
11:19:44.648   Training iter 550, batch loss 1.1085, batch acc 0.6672
11:19:45.163   Training iter 600, batch loss 1.1391, batch acc 0.6564
11:19:45.164 Training @ 57 epoch...
11:19:45.687   Training iter 50, batch loss 1.1583, batch acc 0.6470
11:19:46.196   Training iter 100, batch loss 1.1310, batch acc 0.6528
11:19:46.725   Training iter 150, batch loss 1.1160, batch acc 0.6668
11:19:47.220   Training iter 200, batch loss 1.1284, batch acc 0.6598
11:19:47.723   Training iter 250, batch loss 1.1493, batch acc 0.6448
11:19:48.213   Training iter 300, batch loss 1.1265, batch acc 0.6652
11:19:48.693   Training iter 350, batch loss 1.1376, batch acc 0.6548
11:19:49.172   Training iter 400, batch loss 1.1278, batch acc 0.6560
11:19:49.655   Training iter 450, batch loss 1.1137, batch acc 0.6654
11:19:50.144   Training iter 500, batch loss 1.1347, batch acc 0.6572
11:19:50.624   Training iter 550, batch loss 1.1438, batch acc 0.6532
11:19:51.097   Training iter 600, batch loss 1.1310, batch acc 0.6526
11:19:51.099 Training @ 58 epoch...
11:19:51.586   Training iter 50, batch loss 1.1325, batch acc 0.6596
11:19:52.072   Training iter 100, batch loss 1.1319, batch acc 0.6580
11:19:52.564   Training iter 150, batch loss 1.1317, batch acc 0.6548
11:19:53.038   Training iter 200, batch loss 1.1244, batch acc 0.6644
11:19:53.510   Training iter 250, batch loss 1.1264, batch acc 0.6510
11:19:54.004   Training iter 300, batch loss 1.1243, batch acc 0.6600
11:19:54.499   Training iter 350, batch loss 1.1259, batch acc 0.6628
11:19:55.015   Training iter 400, batch loss 1.1520, batch acc 0.6444
11:19:55.527   Training iter 450, batch loss 1.1253, batch acc 0.6574
11:19:56.020   Training iter 500, batch loss 1.1515, batch acc 0.6486
11:19:56.522   Training iter 550, batch loss 1.1253, batch acc 0.6574
11:19:57.084   Training iter 600, batch loss 1.1321, batch acc 0.6612
11:19:57.086 Training @ 59 epoch...
11:19:57.666   Training iter 50, batch loss 1.1248, batch acc 0.6616
11:19:58.185   Training iter 100, batch loss 1.1362, batch acc 0.6578
11:19:58.720   Training iter 150, batch loss 1.1250, batch acc 0.6620
11:19:59.246   Training iter 200, batch loss 1.1495, batch acc 0.6504
11:19:59.763   Training iter 250, batch loss 1.1233, batch acc 0.6600
11:20:00.315   Training iter 300, batch loss 1.1308, batch acc 0.6530
11:20:00.886   Training iter 350, batch loss 1.1211, batch acc 0.6608
11:20:01.481   Training iter 400, batch loss 1.1511, batch acc 0.6528
11:20:02.055   Training iter 450, batch loss 1.1210, batch acc 0.6576
11:20:02.612   Training iter 500, batch loss 1.1492, batch acc 0.6510
11:20:03.167   Training iter 550, batch loss 1.1105, batch acc 0.6654
11:20:03.711   Training iter 600, batch loss 1.1264, batch acc 0.6562
11:20:03.712 Training @ 60 epoch...
11:20:04.238   Training iter 50, batch loss 1.1490, batch acc 0.6526
11:20:04.754   Training iter 100, batch loss 1.1186, batch acc 0.6628
11:20:05.256   Training iter 150, batch loss 1.1077, batch acc 0.6666
11:20:05.781   Training iter 200, batch loss 1.1312, batch acc 0.6576
11:20:06.320   Training iter 250, batch loss 1.1359, batch acc 0.6592
11:20:06.826   Training iter 300, batch loss 1.1229, batch acc 0.6566
11:20:07.324   Training iter 350, batch loss 1.1407, batch acc 0.6548
11:20:07.809   Training iter 400, batch loss 1.1347, batch acc 0.6550
11:20:08.325   Training iter 450, batch loss 1.1124, batch acc 0.6640
11:20:08.841   Training iter 500, batch loss 1.1388, batch acc 0.6506
11:20:09.356   Training iter 550, batch loss 1.1273, batch acc 0.6574
11:20:09.872   Training iter 600, batch loss 1.1358, batch acc 0.6542
11:20:09.873 Testing @ 60 epoch...
11:20:09.915     Testing, total mean loss 1.09933, total acc 0.66700
11:20:09.916 Training @ 61 epoch...
11:20:10.438   Training iter 50, batch loss 1.1549, batch acc 0.6482
11:20:10.953   Training iter 100, batch loss 1.1185, batch acc 0.6600
11:20:11.463   Training iter 150, batch loss 1.1141, batch acc 0.6610
11:20:11.972   Training iter 200, batch loss 1.1302, batch acc 0.6600
11:20:12.525   Training iter 250, batch loss 1.1436, batch acc 0.6468
11:20:13.104   Training iter 300, batch loss 1.1312, batch acc 0.6636
11:20:13.670   Training iter 350, batch loss 1.1295, batch acc 0.6570
11:20:14.241   Training iter 400, batch loss 1.1243, batch acc 0.6580
11:20:14.764   Training iter 450, batch loss 1.1301, batch acc 0.6612
11:20:15.303   Training iter 500, batch loss 1.1243, batch acc 0.6610
11:20:15.808   Training iter 550, batch loss 1.1275, batch acc 0.6552
11:20:16.320   Training iter 600, batch loss 1.1135, batch acc 0.6642
11:20:16.322 Training @ 62 epoch...
11:20:16.846   Training iter 50, batch loss 1.1348, batch acc 0.6598
11:20:17.377   Training iter 100, batch loss 1.1249, batch acc 0.6616
11:20:17.869   Training iter 150, batch loss 1.1364, batch acc 0.6494
11:20:18.380   Training iter 200, batch loss 1.1257, batch acc 0.6606
11:20:18.882   Training iter 250, batch loss 1.1152, batch acc 0.6668
11:20:19.388   Training iter 300, batch loss 1.1346, batch acc 0.6520
11:20:19.899   Training iter 350, batch loss 1.1427, batch acc 0.6434
11:20:20.432   Training iter 400, batch loss 1.1384, batch acc 0.6544
11:20:20.961   Training iter 450, batch loss 1.1099, batch acc 0.6664
11:20:21.483   Training iter 500, batch loss 1.1319, batch acc 0.6594
11:20:21.976   Training iter 550, batch loss 1.1136, batch acc 0.6692
11:20:22.488   Training iter 600, batch loss 1.1203, batch acc 0.6598
11:20:22.490 Training @ 63 epoch...
11:20:23.003   Training iter 50, batch loss 1.1356, batch acc 0.6568
11:20:23.517   Training iter 100, batch loss 1.1336, batch acc 0.6544
11:20:24.033   Training iter 150, batch loss 1.1153, batch acc 0.6640
11:20:24.533   Training iter 200, batch loss 1.1337, batch acc 0.6614
11:20:25.022   Training iter 250, batch loss 1.1388, batch acc 0.6504
11:20:25.507   Training iter 300, batch loss 1.1326, batch acc 0.6594
11:20:26.005   Training iter 350, batch loss 1.1016, batch acc 0.6634
11:20:26.518   Training iter 400, batch loss 1.1378, batch acc 0.6596
11:20:27.035   Training iter 450, batch loss 1.1453, batch acc 0.6530
11:20:27.564   Training iter 500, batch loss 1.1102, batch acc 0.6626
11:20:28.088   Training iter 550, batch loss 1.1300, batch acc 0.6538
11:20:28.610   Training iter 600, batch loss 1.1010, batch acc 0.6646
11:20:28.612 Training @ 64 epoch...
11:20:29.135   Training iter 50, batch loss 1.1253, batch acc 0.6598
11:20:29.659   Training iter 100, batch loss 1.0949, batch acc 0.6756
11:20:30.181   Training iter 150, batch loss 1.1220, batch acc 0.6578
11:20:30.721   Training iter 200, batch loss 1.1332, batch acc 0.6526
11:20:31.249   Training iter 250, batch loss 1.1301, batch acc 0.6574
11:20:31.738   Training iter 300, batch loss 1.1243, batch acc 0.6564
11:20:32.239   Training iter 350, batch loss 1.1340, batch acc 0.6564
11:20:32.759   Training iter 400, batch loss 1.1355, batch acc 0.6534
11:20:33.285   Training iter 450, batch loss 1.1037, batch acc 0.6742
11:20:33.783   Training iter 500, batch loss 1.1496, batch acc 0.6566
11:20:34.288   Training iter 550, batch loss 1.1467, batch acc 0.6454
11:20:34.804   Training iter 600, batch loss 1.1040, batch acc 0.6654
11:20:34.805 Training @ 65 epoch...
11:20:35.348   Training iter 50, batch loss 1.1318, batch acc 0.6566
11:20:35.884   Training iter 100, batch loss 1.1096, batch acc 0.6612
11:20:36.423   Training iter 150, batch loss 1.1228, batch acc 0.6608
11:20:36.965   Training iter 200, batch loss 1.1456, batch acc 0.6526
11:20:37.523   Training iter 250, batch loss 1.1186, batch acc 0.6622
11:20:38.068   Training iter 300, batch loss 1.1332, batch acc 0.6508
11:20:38.585   Training iter 350, batch loss 1.1133, batch acc 0.6690
11:20:39.074   Training iter 400, batch loss 1.1274, batch acc 0.6542
11:20:39.538   Training iter 450, batch loss 1.1204, batch acc 0.6606
11:20:40.014   Training iter 500, batch loss 1.1136, batch acc 0.6654
11:20:40.499   Training iter 550, batch loss 1.1474, batch acc 0.6556
11:20:40.972   Training iter 600, batch loss 1.1072, batch acc 0.6636
11:20:40.974 Testing @ 65 epoch...
11:20:41.016     Testing, total mean loss 1.09404, total acc 0.66960
11:20:41.016 Training @ 66 epoch...
11:20:41.498   Training iter 50, batch loss 1.1376, batch acc 0.6586
11:20:41.977   Training iter 100, batch loss 1.1057, batch acc 0.6618
11:20:42.467   Training iter 150, batch loss 1.0915, batch acc 0.6708
11:20:42.952   Training iter 200, batch loss 1.1094, batch acc 0.6664
11:20:43.412   Training iter 250, batch loss 1.1402, batch acc 0.6492
11:20:43.879   Training iter 300, batch loss 1.1287, batch acc 0.6608
11:20:44.357   Training iter 350, batch loss 1.1092, batch acc 0.6668
11:20:44.852   Training iter 400, batch loss 1.1304, batch acc 0.6556
11:20:45.361   Training iter 450, batch loss 1.1179, batch acc 0.6628
11:20:45.857   Training iter 500, batch loss 1.1483, batch acc 0.6496
11:20:46.352   Training iter 550, batch loss 1.1178, batch acc 0.6634
11:20:46.850   Training iter 600, batch loss 1.1425, batch acc 0.6542
11:20:46.852 Training @ 67 epoch...
11:20:47.376   Training iter 50, batch loss 1.1182, batch acc 0.6548
11:20:47.885   Training iter 100, batch loss 1.1133, batch acc 0.6668
11:20:48.458   Training iter 150, batch loss 1.1356, batch acc 0.6586
11:20:48.996   Training iter 200, batch loss 1.1416, batch acc 0.6480
11:20:49.558   Training iter 250, batch loss 1.1113, batch acc 0.6630
11:20:50.117   Training iter 300, batch loss 1.1287, batch acc 0.6572
11:20:50.678   Training iter 350, batch loss 1.1222, batch acc 0.6628
11:20:51.204   Training iter 400, batch loss 1.1095, batch acc 0.6644
11:20:51.739   Training iter 450, batch loss 1.1344, batch acc 0.6530
11:20:52.259   Training iter 500, batch loss 1.1333, batch acc 0.6648
11:20:52.803   Training iter 550, batch loss 1.1197, batch acc 0.6626
11:20:53.368   Training iter 600, batch loss 1.1005, batch acc 0.6674
11:20:53.370 Training @ 68 epoch...
11:20:53.931   Training iter 50, batch loss 1.1235, batch acc 0.6564
11:20:54.454   Training iter 100, batch loss 1.1249, batch acc 0.6572
11:20:54.973   Training iter 150, batch loss 1.1148, batch acc 0.6610
11:20:55.515   Training iter 200, batch loss 1.1077, batch acc 0.6750
11:20:56.023   Training iter 250, batch loss 1.1120, batch acc 0.6616
11:20:56.541   Training iter 300, batch loss 1.1167, batch acc 0.6656
11:20:57.062   Training iter 350, batch loss 1.1113, batch acc 0.6674
11:20:57.593   Training iter 400, batch loss 1.1208, batch acc 0.6600
11:20:58.120   Training iter 450, batch loss 1.1428, batch acc 0.6518
11:20:58.646   Training iter 500, batch loss 1.1323, batch acc 0.6576
11:20:59.173   Training iter 550, batch loss 1.1071, batch acc 0.6654
11:20:59.703   Training iter 600, batch loss 1.1431, batch acc 0.6480
11:20:59.705 Training @ 69 epoch...
11:21:00.240   Training iter 50, batch loss 1.1279, batch acc 0.6526
11:21:00.779   Training iter 100, batch loss 1.1246, batch acc 0.6606
11:21:01.315   Training iter 150, batch loss 1.1386, batch acc 0.6506
11:21:01.886   Training iter 200, batch loss 1.1099, batch acc 0.6674
11:21:02.447   Training iter 250, batch loss 1.1100, batch acc 0.6682
11:21:02.992   Training iter 300, batch loss 1.0733, batch acc 0.6828
11:21:03.529   Training iter 350, batch loss 1.1489, batch acc 0.6454
11:21:04.038   Training iter 400, batch loss 1.1125, batch acc 0.6672
11:21:04.561   Training iter 450, batch loss 1.1217, batch acc 0.6550
11:21:05.086   Training iter 500, batch loss 1.1273, batch acc 0.6546
11:21:05.570   Training iter 550, batch loss 1.1302, batch acc 0.6536
11:21:06.065   Training iter 600, batch loss 1.1211, batch acc 0.6686
11:21:06.066 Training @ 70 epoch...
11:21:06.604   Training iter 50, batch loss 1.1284, batch acc 0.6600
11:21:07.147   Training iter 100, batch loss 1.1036, batch acc 0.6620
11:21:07.691   Training iter 150, batch loss 1.1504, batch acc 0.6510
11:21:08.227   Training iter 200, batch loss 1.1153, batch acc 0.6652
11:21:08.757   Training iter 250, batch loss 1.1330, batch acc 0.6538
11:21:09.276   Training iter 300, batch loss 1.1102, batch acc 0.6678
11:21:09.843   Training iter 350, batch loss 1.1167, batch acc 0.6576
11:21:10.410   Training iter 400, batch loss 1.1047, batch acc 0.6652
11:21:10.960   Training iter 450, batch loss 1.1200, batch acc 0.6616
11:21:11.492   Training iter 500, batch loss 1.1188, batch acc 0.6588
11:21:12.011   Training iter 550, batch loss 1.1236, batch acc 0.6648
11:21:12.503   Training iter 600, batch loss 1.1110, batch acc 0.6636
11:21:12.505 Testing @ 70 epoch...
11:21:12.547     Testing, total mean loss 1.08963, total acc 0.67100
11:21:12.547 Training @ 71 epoch...
11:21:13.061   Training iter 50, batch loss 1.0985, batch acc 0.6718
11:21:13.595   Training iter 100, batch loss 1.1195, batch acc 0.6578
11:21:14.132   Training iter 150, batch loss 1.1130, batch acc 0.6606
11:21:14.653   Training iter 200, batch loss 1.1256, batch acc 0.6552
11:21:15.178   Training iter 250, batch loss 1.1044, batch acc 0.6674
11:21:15.688   Training iter 300, batch loss 1.1343, batch acc 0.6614
11:21:16.201   Training iter 350, batch loss 1.1424, batch acc 0.6464
11:21:16.713   Training iter 400, batch loss 1.1104, batch acc 0.6654
11:21:17.212   Training iter 450, batch loss 1.1169, batch acc 0.6654
11:21:17.728   Training iter 500, batch loss 1.1190, batch acc 0.6650
11:21:18.252   Training iter 550, batch loss 1.1211, batch acc 0.6634
11:21:18.778   Training iter 600, batch loss 1.1204, batch acc 0.6576
11:21:18.780 Training @ 72 epoch...
11:21:19.311   Training iter 50, batch loss 1.1019, batch acc 0.6662
11:21:19.834   Training iter 100, batch loss 1.1362, batch acc 0.6534
11:21:20.367   Training iter 150, batch loss 1.1075, batch acc 0.6664
11:21:20.885   Training iter 200, batch loss 1.1144, batch acc 0.6630
11:21:21.406   Training iter 250, batch loss 1.1277, batch acc 0.6630
11:21:21.939   Training iter 300, batch loss 1.1117, batch acc 0.6610
11:21:22.467   Training iter 350, batch loss 1.1177, batch acc 0.6592
11:21:22.998   Training iter 400, batch loss 1.1100, batch acc 0.6664
11:21:23.536   Training iter 450, batch loss 1.1260, batch acc 0.6572
11:21:24.061   Training iter 500, batch loss 1.1019, batch acc 0.6696
11:21:24.585   Training iter 550, batch loss 1.1368, batch acc 0.6546
11:21:25.113   Training iter 600, batch loss 1.1239, batch acc 0.6592
11:21:25.115 Training @ 73 epoch...
11:21:25.707   Training iter 50, batch loss 1.1208, batch acc 0.6640
11:21:26.258   Training iter 100, batch loss 1.1318, batch acc 0.6594
11:21:26.787   Training iter 150, batch loss 1.1133, batch acc 0.6582
11:21:27.316   Training iter 200, batch loss 1.1095, batch acc 0.6644
11:21:27.905   Training iter 250, batch loss 1.1410, batch acc 0.6540
11:21:28.442   Training iter 300, batch loss 1.1052, batch acc 0.6670
11:21:28.964   Training iter 350, batch loss 1.1218, batch acc 0.6614
11:21:29.475   Training iter 400, batch loss 1.1034, batch acc 0.6658
11:21:30.011   Training iter 450, batch loss 1.1183, batch acc 0.6580
11:21:30.558   Training iter 500, batch loss 1.1178, batch acc 0.6594
11:21:31.120   Training iter 550, batch loss 1.1219, batch acc 0.6676
11:21:31.660   Training iter 600, batch loss 1.1011, batch acc 0.6674
11:21:31.662 Training @ 74 epoch...
11:21:32.174   Training iter 50, batch loss 1.0906, batch acc 0.6730
11:21:32.696   Training iter 100, batch loss 1.1065, batch acc 0.6630
11:21:33.221   Training iter 150, batch loss 1.0903, batch acc 0.6676
11:21:33.732   Training iter 200, batch loss 1.1136, batch acc 0.6662
11:21:34.238   Training iter 250, batch loss 1.1459, batch acc 0.6470
11:21:34.758   Training iter 300, batch loss 1.1045, batch acc 0.6696
11:21:35.296   Training iter 350, batch loss 1.1115, batch acc 0.6646
11:21:35.833   Training iter 400, batch loss 1.1169, batch acc 0.6628
11:21:36.382   Training iter 450, batch loss 1.1375, batch acc 0.6530
11:21:36.935   Training iter 500, batch loss 1.1267, batch acc 0.6552
11:21:37.509   Training iter 550, batch loss 1.1219, batch acc 0.6652
11:21:38.066   Training iter 600, batch loss 1.1307, batch acc 0.6598
11:21:38.068 Training @ 75 epoch...
11:21:38.624   Training iter 50, batch loss 1.1166, batch acc 0.6686
11:21:39.160   Training iter 100, batch loss 1.1188, batch acc 0.6530
11:21:39.724   Training iter 150, batch loss 1.0996, batch acc 0.6640
11:21:40.299   Training iter 200, batch loss 1.1027, batch acc 0.6626
11:21:40.871   Training iter 250, batch loss 1.1286, batch acc 0.6582
11:21:41.444   Training iter 300, batch loss 1.1142, batch acc 0.6588
11:21:41.998   Training iter 350, batch loss 1.1179, batch acc 0.6666
11:21:42.535   Training iter 400, batch loss 1.1272, batch acc 0.6574
11:21:43.065   Training iter 450, batch loss 1.1332, batch acc 0.6608
11:21:43.608   Training iter 500, batch loss 1.0833, batch acc 0.6808
11:21:44.144   Training iter 550, batch loss 1.1208, batch acc 0.6594
11:21:44.674   Training iter 600, batch loss 1.1244, batch acc 0.6604
11:21:44.675 Testing @ 75 epoch...
11:21:44.716     Testing, total mean loss 1.08570, total acc 0.67230
11:21:44.716 Training @ 76 epoch...
11:21:45.250   Training iter 50, batch loss 1.1030, batch acc 0.6670
11:21:45.764   Training iter 100, batch loss 1.1066, batch acc 0.6692
11:21:46.289   Training iter 150, batch loss 1.1004, batch acc 0.6610
11:21:46.795   Training iter 200, batch loss 1.1488, batch acc 0.6524
11:21:47.306   Training iter 250, batch loss 1.1007, batch acc 0.6690
11:21:47.840   Training iter 300, batch loss 1.1123, batch acc 0.6600
11:21:48.404   Training iter 350, batch loss 1.1223, batch acc 0.6668
11:21:48.960   Training iter 400, batch loss 1.0816, batch acc 0.6724
11:21:49.502   Training iter 450, batch loss 1.1270, batch acc 0.6584
11:21:49.980   Training iter 500, batch loss 1.1085, batch acc 0.6638
11:21:50.462   Training iter 550, batch loss 1.1417, batch acc 0.6516
11:21:50.963   Training iter 600, batch loss 1.1254, batch acc 0.6612
11:21:50.964 Training @ 77 epoch...
11:21:51.484   Training iter 50, batch loss 1.1243, batch acc 0.6606
11:21:51.955   Training iter 100, batch loss 1.1257, batch acc 0.6554
11:21:52.426   Training iter 150, batch loss 1.1242, batch acc 0.6574
11:21:52.907   Training iter 200, batch loss 1.0984, batch acc 0.6630
11:21:53.370   Training iter 250, batch loss 1.1395, batch acc 0.6538
11:21:53.838   Training iter 300, batch loss 1.1072, batch acc 0.6658
11:21:54.311   Training iter 350, batch loss 1.1071, batch acc 0.6666
11:21:54.786   Training iter 400, batch loss 1.1066, batch acc 0.6694
11:21:55.261   Training iter 450, batch loss 1.1310, batch acc 0.6592
11:21:55.735   Training iter 500, batch loss 1.1029, batch acc 0.6654
11:21:56.218   Training iter 550, batch loss 1.0991, batch acc 0.6748
11:21:56.688   Training iter 600, batch loss 1.1035, batch acc 0.6648
11:21:56.690 Training @ 78 epoch...
11:21:57.179   Training iter 50, batch loss 1.1248, batch acc 0.6630
11:21:57.675   Training iter 100, batch loss 1.1144, batch acc 0.6678
11:21:58.172   Training iter 150, batch loss 1.1132, batch acc 0.6662
11:21:58.669   Training iter 200, batch loss 1.1152, batch acc 0.6546
11:21:59.188   Training iter 250, batch loss 1.1241, batch acc 0.6592
11:21:59.709   Training iter 300, batch loss 1.1245, batch acc 0.6584
11:22:00.253   Training iter 350, batch loss 1.1052, batch acc 0.6738
11:22:00.786   Training iter 400, batch loss 1.1028, batch acc 0.6698
11:22:01.318   Training iter 450, batch loss 1.1032, batch acc 0.6582
11:22:01.990   Training iter 500, batch loss 1.1032, batch acc 0.6642
11:22:02.551   Training iter 550, batch loss 1.1160, batch acc 0.6638
11:22:03.112   Training iter 600, batch loss 1.1144, batch acc 0.6610
11:22:03.114 Training @ 79 epoch...
11:22:03.687   Training iter 50, batch loss 1.1014, batch acc 0.6670
11:22:04.257   Training iter 100, batch loss 1.1119, batch acc 0.6638
11:22:04.812   Training iter 150, batch loss 1.0838, batch acc 0.6772
11:22:05.375   Training iter 200, batch loss 1.1223, batch acc 0.6618
11:22:05.941   Training iter 250, batch loss 1.0865, batch acc 0.6796
11:22:06.526   Training iter 300, batch loss 1.1135, batch acc 0.6694
11:22:07.079   Training iter 350, batch loss 1.1132, batch acc 0.6608
11:22:07.618   Training iter 400, batch loss 1.1006, batch acc 0.6662
11:22:08.165   Training iter 450, batch loss 1.1349, batch acc 0.6532
11:22:08.719   Training iter 500, batch loss 1.1234, batch acc 0.6558
11:22:09.327   Training iter 550, batch loss 1.1269, batch acc 0.6558
11:22:09.895   Training iter 600, batch loss 1.1346, batch acc 0.6548
11:22:09.897 Training @ 80 epoch...
11:22:10.457   Training iter 50, batch loss 1.0964, batch acc 0.6672
11:22:10.977   Training iter 100, batch loss 1.1254, batch acc 0.6508
11:22:11.500   Training iter 150, batch loss 1.1022, batch acc 0.6726
11:22:12.010   Training iter 200, batch loss 1.0976, batch acc 0.6682
11:22:12.541   Training iter 250, batch loss 1.1289, batch acc 0.6566
11:22:13.072   Training iter 300, batch loss 1.1206, batch acc 0.6614
11:22:13.584   Training iter 350, batch loss 1.1021, batch acc 0.6680
11:22:14.094   Training iter 400, batch loss 1.1199, batch acc 0.6620
11:22:14.585   Training iter 450, batch loss 1.1193, batch acc 0.6638
11:22:15.095   Training iter 500, batch loss 1.0915, batch acc 0.6744
11:22:15.592   Training iter 550, batch loss 1.1155, batch acc 0.6598
11:22:16.112   Training iter 600, batch loss 1.1254, batch acc 0.6608
11:22:16.114 Testing @ 80 epoch...
11:22:16.156     Testing, total mean loss 1.08228, total acc 0.67400
11:22:16.157 Training @ 81 epoch...
11:22:16.686   Training iter 50, batch loss 1.1097, batch acc 0.6628
11:22:17.209   Training iter 100, batch loss 1.1117, batch acc 0.6674
11:22:17.747   Training iter 150, batch loss 1.1285, batch acc 0.6554
11:22:18.295   Training iter 200, batch loss 1.1034, batch acc 0.6674
11:22:18.833   Training iter 250, batch loss 1.1111, batch acc 0.6624
11:22:19.361   Training iter 300, batch loss 1.1043, batch acc 0.6668
11:22:19.863   Training iter 350, batch loss 1.1230, batch acc 0.6606
11:22:20.357   Training iter 400, batch loss 1.1096, batch acc 0.6636
11:22:20.836   Training iter 450, batch loss 1.1157, batch acc 0.6674
11:22:21.323   Training iter 500, batch loss 1.1025, batch acc 0.6652
11:22:21.814   Training iter 550, batch loss 1.1290, batch acc 0.6560
11:22:22.315   Training iter 600, batch loss 1.0886, batch acc 0.6734
11:22:22.317 Training @ 82 epoch...
11:22:22.812   Training iter 50, batch loss 1.1018, batch acc 0.6662
11:22:23.297   Training iter 100, batch loss 1.1093, batch acc 0.6578
11:22:23.779   Training iter 150, batch loss 1.1198, batch acc 0.6632
11:22:24.271   Training iter 200, batch loss 1.0824, batch acc 0.6774
11:22:24.785   Training iter 250, batch loss 1.1348, batch acc 0.6578
11:22:25.305   Training iter 300, batch loss 1.1081, batch acc 0.6640
11:22:25.835   Training iter 350, batch loss 1.0864, batch acc 0.6724
11:22:26.365   Training iter 400, batch loss 1.1336, batch acc 0.6570
11:22:26.896   Training iter 450, batch loss 1.1410, batch acc 0.6544
11:22:27.436   Training iter 500, batch loss 1.0935, batch acc 0.6738
11:22:27.987   Training iter 550, batch loss 1.1025, batch acc 0.6658
11:22:28.531   Training iter 600, batch loss 1.1158, batch acc 0.6592
11:22:28.533 Training @ 83 epoch...
11:22:29.078   Training iter 50, batch loss 1.1131, batch acc 0.6668
11:22:29.617   Training iter 100, batch loss 1.1140, batch acc 0.6638
11:22:30.154   Training iter 150, batch loss 1.1150, batch acc 0.6594
11:22:30.669   Training iter 200, batch loss 1.1065, batch acc 0.6672
11:22:31.195   Training iter 250, batch loss 1.1122, batch acc 0.6606
11:22:31.728   Training iter 300, batch loss 1.0939, batch acc 0.6724
11:22:32.244   Training iter 350, batch loss 1.1309, batch acc 0.6566
11:22:32.757   Training iter 400, batch loss 1.1157, batch acc 0.6606
11:22:33.280   Training iter 450, batch loss 1.0934, batch acc 0.6698
11:22:33.788   Training iter 500, batch loss 1.0938, batch acc 0.6724
11:22:34.282   Training iter 550, batch loss 1.1233, batch acc 0.6592
11:22:34.795   Training iter 600, batch loss 1.1100, batch acc 0.6670
11:22:34.796 Training @ 84 epoch...
11:22:35.326   Training iter 50, batch loss 1.1094, batch acc 0.6658
11:22:35.840   Training iter 100, batch loss 1.1205, batch acc 0.6606
11:22:36.361   Training iter 150, batch loss 1.1232, batch acc 0.6550
11:22:36.866   Training iter 200, batch loss 1.0960, batch acc 0.6746
11:22:37.371   Training iter 250, batch loss 1.1264, batch acc 0.6546
11:22:37.870   Training iter 300, batch loss 1.0841, batch acc 0.6712
11:22:38.381   Training iter 350, batch loss 1.0834, batch acc 0.6776
11:22:38.876   Training iter 400, batch loss 1.1243, batch acc 0.6554
11:22:39.372   Training iter 450, batch loss 1.0849, batch acc 0.6748
11:22:39.832   Training iter 500, batch loss 1.1086, batch acc 0.6694
11:22:40.305   Training iter 550, batch loss 1.1190, batch acc 0.6588
11:22:40.811   Training iter 600, batch loss 1.1345, batch acc 0.6538
11:22:40.813 Training @ 85 epoch...
11:22:41.314   Training iter 50, batch loss 1.1027, batch acc 0.6666
11:22:41.835   Training iter 100, batch loss 1.1296, batch acc 0.6606
11:22:42.375   Training iter 150, batch loss 1.0596, batch acc 0.6824
11:22:42.934   Training iter 200, batch loss 1.1278, batch acc 0.6602
11:22:43.478   Training iter 250, batch loss 1.0904, batch acc 0.6672
11:22:44.008   Training iter 300, batch loss 1.1271, batch acc 0.6560
11:22:44.520   Training iter 350, batch loss 1.1162, batch acc 0.6608
11:22:45.038   Training iter 400, batch loss 1.1187, batch acc 0.6626
11:22:45.583   Training iter 450, batch loss 1.1203, batch acc 0.6654
11:22:46.092   Training iter 500, batch loss 1.1069, batch acc 0.6638
11:22:46.574   Training iter 550, batch loss 1.1125, batch acc 0.6672
11:22:47.046   Training iter 600, batch loss 1.0954, batch acc 0.6632
11:22:47.048 Testing @ 85 epoch...
11:22:47.090     Testing, total mean loss 1.07935, total acc 0.67490
11:22:47.091 Training @ 86 epoch...
11:22:47.568   Training iter 50, batch loss 1.1308, batch acc 0.6550
11:22:48.050   Training iter 100, batch loss 1.1207, batch acc 0.6554
11:22:48.536   Training iter 150, batch loss 1.0635, batch acc 0.6852
11:22:49.015   Training iter 200, batch loss 1.1410, batch acc 0.6568
11:22:49.477   Training iter 250, batch loss 1.0778, batch acc 0.6788
11:22:49.984   Training iter 300, batch loss 1.1180, batch acc 0.6568
11:22:50.510   Training iter 350, batch loss 1.1086, batch acc 0.6674
11:22:51.031   Training iter 400, batch loss 1.1026, batch acc 0.6698
11:22:51.550   Training iter 450, batch loss 1.0966, batch acc 0.6738
11:22:52.067   Training iter 500, batch loss 1.1304, batch acc 0.6486
11:22:52.573   Training iter 550, batch loss 1.1061, batch acc 0.6688
11:22:53.072   Training iter 600, batch loss 1.1045, batch acc 0.6622
11:22:53.074 Training @ 87 epoch...
11:22:53.599   Training iter 50, batch loss 1.1345, batch acc 0.6564
11:22:54.115   Training iter 100, batch loss 1.0956, batch acc 0.6760
11:22:54.625   Training iter 150, batch loss 1.0842, batch acc 0.6744
11:22:55.142   Training iter 200, batch loss 1.1268, batch acc 0.6576
11:22:55.666   Training iter 250, batch loss 1.0947, batch acc 0.6650
11:22:56.194   Training iter 300, batch loss 1.1143, batch acc 0.6632
11:22:56.741   Training iter 350, batch loss 1.1225, batch acc 0.6604
11:22:57.315   Training iter 400, batch loss 1.1117, batch acc 0.6646
11:22:57.879   Training iter 450, batch loss 1.1187, batch acc 0.6604
11:22:58.438   Training iter 500, batch loss 1.0847, batch acc 0.6708
11:22:58.991   Training iter 550, batch loss 1.1078, batch acc 0.6680
11:22:59.577   Training iter 600, batch loss 1.0977, batch acc 0.6678
11:22:59.578 Training @ 88 epoch...
11:23:00.181   Training iter 50, batch loss 1.1145, batch acc 0.6630
11:23:00.764   Training iter 100, batch loss 1.1011, batch acc 0.6670
11:23:01.337   Training iter 150, batch loss 1.1005, batch acc 0.6756
11:23:01.889   Training iter 200, batch loss 1.1187, batch acc 0.6622
11:23:02.451   Training iter 250, batch loss 1.1289, batch acc 0.6548
11:23:02.985   Training iter 300, batch loss 1.1097, batch acc 0.6574
11:23:03.520   Training iter 350, batch loss 1.0996, batch acc 0.6694
11:23:04.009   Training iter 400, batch loss 1.0756, batch acc 0.6702
11:23:04.502   Training iter 450, batch loss 1.1202, batch acc 0.6666
11:23:05.022   Training iter 500, batch loss 1.0992, batch acc 0.6660
11:23:05.538   Training iter 550, batch loss 1.1162, batch acc 0.6616
11:23:06.053   Training iter 600, batch loss 1.1026, batch acc 0.6700
11:23:06.055 Training @ 89 epoch...
11:23:06.584   Training iter 50, batch loss 1.0969, batch acc 0.6680
11:23:07.111   Training iter 100, batch loss 1.1076, batch acc 0.6624
11:23:07.607   Training iter 150, batch loss 1.0889, batch acc 0.6754
11:23:08.077   Training iter 200, batch loss 1.1083, batch acc 0.6654
11:23:08.564   Training iter 250, batch loss 1.1246, batch acc 0.6596
11:23:09.039   Training iter 300, batch loss 1.1154, batch acc 0.6618
11:23:09.500   Training iter 350, batch loss 1.0921, batch acc 0.6728
11:23:09.966   Training iter 400, batch loss 1.1070, batch acc 0.6616
11:23:10.459   Training iter 450, batch loss 1.1040, batch acc 0.6690
11:23:10.911   Training iter 500, batch loss 1.1196, batch acc 0.6562
11:23:11.383   Training iter 550, batch loss 1.1232, batch acc 0.6582
11:23:11.839   Training iter 600, batch loss 1.0929, batch acc 0.6746
11:23:11.841 Training @ 90 epoch...
11:23:12.316   Training iter 50, batch loss 1.1041, batch acc 0.6674
11:23:12.822   Training iter 100, batch loss 1.1161, batch acc 0.6608
11:23:13.331   Training iter 150, batch loss 1.1042, batch acc 0.6666
11:23:13.842   Training iter 200, batch loss 1.1258, batch acc 0.6590
11:23:14.332   Training iter 250, batch loss 1.1140, batch acc 0.6686
11:23:14.833   Training iter 300, batch loss 1.0864, batch acc 0.6678
11:23:15.348   Training iter 350, batch loss 1.1258, batch acc 0.6544
11:23:15.855   Training iter 400, batch loss 1.1110, batch acc 0.6626
11:23:16.378   Training iter 450, batch loss 1.0929, batch acc 0.6664
11:23:16.898   Training iter 500, batch loss 1.1218, batch acc 0.6548
11:23:17.429   Training iter 550, batch loss 1.0754, batch acc 0.6832
11:23:17.957   Training iter 600, batch loss 1.0965, batch acc 0.6812
11:23:17.958 Testing @ 90 epoch...
11:23:18.002     Testing, total mean loss 1.07666, total acc 0.67600
11:23:18.002 Training @ 91 epoch...
11:23:18.530   Training iter 50, batch loss 1.0714, batch acc 0.6772
11:23:19.021   Training iter 100, batch loss 1.0856, batch acc 0.6720
11:23:19.516   Training iter 150, batch loss 1.0935, batch acc 0.6742
11:23:20.021   Training iter 200, batch loss 1.1272, batch acc 0.6530
11:23:20.531   Training iter 250, batch loss 1.1078, batch acc 0.6660
11:23:21.033   Training iter 300, batch loss 1.0949, batch acc 0.6714
11:23:21.525   Training iter 350, batch loss 1.1227, batch acc 0.6664
11:23:22.039   Training iter 400, batch loss 1.1024, batch acc 0.6668
11:23:22.549   Training iter 450, batch loss 1.1220, batch acc 0.6576
11:23:23.071   Training iter 500, batch loss 1.1225, batch acc 0.6572
11:23:23.584   Training iter 550, batch loss 1.1156, batch acc 0.6602
11:23:24.094   Training iter 600, batch loss 1.1020, batch acc 0.6680
11:23:24.096 Training @ 92 epoch...
11:23:24.611   Training iter 50, batch loss 1.1105, batch acc 0.6670
11:23:25.128   Training iter 100, batch loss 1.0875, batch acc 0.6780
11:23:25.641   Training iter 150, batch loss 1.1123, batch acc 0.6604
11:23:26.143   Training iter 200, batch loss 1.0732, batch acc 0.6798
11:23:26.650   Training iter 250, batch loss 1.0999, batch acc 0.6720
11:23:27.206   Training iter 300, batch loss 1.1091, batch acc 0.6636
11:23:27.711   Training iter 350, batch loss 1.0997, batch acc 0.6626
11:23:28.236   Training iter 400, batch loss 1.1325, batch acc 0.6554
11:23:28.766   Training iter 450, batch loss 1.1069, batch acc 0.6622
11:23:29.308   Training iter 500, batch loss 1.1212, batch acc 0.6560
11:23:29.855   Training iter 550, batch loss 1.0966, batch acc 0.6676
11:23:30.408   Training iter 600, batch loss 1.1121, batch acc 0.6670
11:23:30.410 Training @ 93 epoch...
11:23:30.955   Training iter 50, batch loss 1.1018, batch acc 0.6702
11:23:31.487   Training iter 100, batch loss 1.1021, batch acc 0.6666
11:23:32.017   Training iter 150, batch loss 1.1028, batch acc 0.6638
11:23:32.551   Training iter 200, batch loss 1.0757, batch acc 0.6794
11:23:33.088   Training iter 250, batch loss 1.0789, batch acc 0.6806
11:23:33.622   Training iter 300, batch loss 1.1124, batch acc 0.6604
11:23:34.146   Training iter 350, batch loss 1.0944, batch acc 0.6688
11:23:34.673   Training iter 400, batch loss 1.1221, batch acc 0.6524
11:23:35.177   Training iter 450, batch loss 1.1173, batch acc 0.6620
11:23:35.671   Training iter 500, batch loss 1.1097, batch acc 0.6630
11:23:36.165   Training iter 550, batch loss 1.1203, batch acc 0.6682
11:23:36.664   Training iter 600, batch loss 1.1180, batch acc 0.6606
11:23:36.665 Training @ 94 epoch...
11:23:37.192   Training iter 50, batch loss 1.0896, batch acc 0.6728
11:23:37.736   Training iter 100, batch loss 1.1120, batch acc 0.6660
11:23:38.269   Training iter 150, batch loss 1.1170, batch acc 0.6636
11:23:38.790   Training iter 200, batch loss 1.0901, batch acc 0.6690
11:23:39.310   Training iter 250, batch loss 1.1134, batch acc 0.6620
11:23:39.800   Training iter 300, batch loss 1.0984, batch acc 0.6702
11:23:40.326   Training iter 350, batch loss 1.1214, batch acc 0.6570
11:23:40.851   Training iter 400, batch loss 1.1088, batch acc 0.6700
11:23:41.357   Training iter 450, batch loss 1.1066, batch acc 0.6652
11:23:41.870   Training iter 500, batch loss 1.0783, batch acc 0.6706
11:23:42.377   Training iter 550, batch loss 1.1244, batch acc 0.6610
11:23:42.874   Training iter 600, batch loss 1.0900, batch acc 0.6696
11:23:42.876 Training @ 95 epoch...
11:23:43.381   Training iter 50, batch loss 1.1148, batch acc 0.6634
11:23:43.882   Training iter 100, batch loss 1.1135, batch acc 0.6646
11:23:44.379   Training iter 150, batch loss 1.1149, batch acc 0.6598
11:23:44.890   Training iter 200, batch loss 1.0930, batch acc 0.6726
11:23:45.411   Training iter 250, batch loss 1.1041, batch acc 0.6632
11:23:45.927   Training iter 300, batch loss 1.1006, batch acc 0.6698
11:23:46.466   Training iter 350, batch loss 1.1039, batch acc 0.6672
11:23:46.996   Training iter 400, batch loss 1.0904, batch acc 0.6678
11:23:47.531   Training iter 450, batch loss 1.1614, batch acc 0.6454
11:23:48.052   Training iter 500, batch loss 1.0905, batch acc 0.6690
11:23:48.601   Training iter 550, batch loss 1.0651, batch acc 0.6818
11:23:49.152   Training iter 600, batch loss 1.0917, batch acc 0.6734
11:23:49.154 Testing @ 95 epoch...
11:23:49.196     Testing, total mean loss 1.07434, total acc 0.67620
11:23:49.196 Training @ 96 epoch...
11:23:49.753   Training iter 50, batch loss 1.1125, batch acc 0.6600
11:23:50.306   Training iter 100, batch loss 1.0993, batch acc 0.6640
11:23:50.838   Training iter 150, batch loss 1.1085, batch acc 0.6758
11:23:51.346   Training iter 200, batch loss 1.1105, batch acc 0.6536
11:23:51.858   Training iter 250, batch loss 1.0959, batch acc 0.6662
11:23:52.374   Training iter 300, batch loss 1.1232, batch acc 0.6558
11:23:52.896   Training iter 350, batch loss 1.1200, batch acc 0.6652
11:23:53.423   Training iter 400, batch loss 1.0690, batch acc 0.6800
11:23:53.936   Training iter 450, batch loss 1.0965, batch acc 0.6782
11:23:54.438   Training iter 500, batch loss 1.0779, batch acc 0.6742
11:23:54.955   Training iter 550, batch loss 1.0957, batch acc 0.6636
11:23:55.470   Training iter 600, batch loss 1.1296, batch acc 0.6626
11:23:55.471 Training @ 97 epoch...
11:23:55.969   Training iter 50, batch loss 1.0760, batch acc 0.6752
11:23:56.484   Training iter 100, batch loss 1.0953, batch acc 0.6682
11:23:56.992   Training iter 150, batch loss 1.0936, batch acc 0.6688
11:23:57.532   Training iter 200, batch loss 1.1097, batch acc 0.6632
11:23:58.118   Training iter 250, batch loss 1.0863, batch acc 0.6702
11:23:58.695   Training iter 300, batch loss 1.0957, batch acc 0.6702
11:23:59.282   Training iter 350, batch loss 1.1037, batch acc 0.6668
11:23:59.827   Training iter 400, batch loss 1.1131, batch acc 0.6642
11:24:00.370   Training iter 450, batch loss 1.1114, batch acc 0.6678
11:24:00.908   Training iter 500, batch loss 1.1258, batch acc 0.6662
11:24:01.464   Training iter 550, batch loss 1.1056, batch acc 0.6572
11:24:02.006   Training iter 600, batch loss 1.1171, batch acc 0.6640
11:24:02.007 Training @ 98 epoch...
11:24:02.543   Training iter 50, batch loss 1.0888, batch acc 0.6742
11:24:03.092   Training iter 100, batch loss 1.1272, batch acc 0.6566
11:24:03.608   Training iter 150, batch loss 1.0927, batch acc 0.6718
11:24:04.144   Training iter 200, batch loss 1.0913, batch acc 0.6702
11:24:04.669   Training iter 250, batch loss 1.1057, batch acc 0.6666
11:24:05.178   Training iter 300, batch loss 1.0996, batch acc 0.6684
11:24:05.684   Training iter 350, batch loss 1.1099, batch acc 0.6642
11:24:06.201   Training iter 400, batch loss 1.0961, batch acc 0.6686
11:24:06.708   Training iter 450, batch loss 1.1099, batch acc 0.6622
11:24:07.225   Training iter 500, batch loss 1.1055, batch acc 0.6684
11:24:07.742   Training iter 550, batch loss 1.1136, batch acc 0.6610
11:24:08.235   Training iter 600, batch loss 1.0875, batch acc 0.6734
11:24:08.236 Training @ 99 epoch...
11:24:08.755   Training iter 50, batch loss 1.1047, batch acc 0.6662
11:24:09.264   Training iter 100, batch loss 1.1092, batch acc 0.6632
11:24:09.775   Training iter 150, batch loss 1.1033, batch acc 0.6614
11:24:10.311   Training iter 200, batch loss 1.0981, batch acc 0.6628
11:24:10.820   Training iter 250, batch loss 1.1209, batch acc 0.6622
11:24:11.348   Training iter 300, batch loss 1.1222, batch acc 0.6624
11:24:11.860   Training iter 350, batch loss 1.0901, batch acc 0.6784
11:24:12.389   Training iter 400, batch loss 1.1177, batch acc 0.6616
11:24:12.897   Training iter 450, batch loss 1.0813, batch acc 0.6774
11:24:13.407   Training iter 500, batch loss 1.0925, batch acc 0.6674
11:24:13.924   Training iter 550, batch loss 1.0839, batch acc 0.6732
11:24:14.420   Training iter 600, batch loss 1.0986, batch acc 0.6710
11:24:14.421 Training @ 100 epoch...
11:24:14.922   Training iter 50, batch loss 1.1154, batch acc 0.6640
11:24:15.417   Training iter 100, batch loss 1.1115, batch acc 0.6670
11:24:15.916   Training iter 150, batch loss 1.0899, batch acc 0.6672
11:24:16.413   Training iter 200, batch loss 1.1081, batch acc 0.6626
11:24:16.910   Training iter 250, batch loss 1.0970, batch acc 0.6694
11:24:17.404   Training iter 300, batch loss 1.0944, batch acc 0.6660
11:24:17.896   Training iter 350, batch loss 1.1346, batch acc 0.6524
11:24:18.407   Training iter 400, batch loss 1.0938, batch acc 0.6774
11:24:18.917   Training iter 450, batch loss 1.1009, batch acc 0.6696
11:24:19.430   Training iter 500, batch loss 1.1156, batch acc 0.6600
11:24:19.937   Training iter 550, batch loss 1.0788, batch acc 0.6726
11:24:20.445   Training iter 600, batch loss 1.0776, batch acc 0.6826
11:24:20.446 Testing @ 100 epoch...
11:24:20.487     Testing, total mean loss 1.07210, total acc 0.67830
11:24:20.487 Plot @ 100 epoch...
11:24:20.487 Training @ 101 epoch...
11:24:21.013   Training iter 50, batch loss 1.0907, batch acc 0.6706
11:24:21.536   Training iter 100, batch loss 1.1032, batch acc 0.6654
11:24:22.063   Training iter 150, batch loss 1.1231, batch acc 0.6634
11:24:22.583   Training iter 200, batch loss 1.0988, batch acc 0.6666
11:24:23.106   Training iter 250, batch loss 1.0975, batch acc 0.6672
11:24:23.632   Training iter 300, batch loss 1.0949, batch acc 0.6652
11:24:24.153   Training iter 350, batch loss 1.0807, batch acc 0.6760
11:24:24.679   Training iter 400, batch loss 1.1079, batch acc 0.6662
11:24:25.202   Training iter 450, batch loss 1.1116, batch acc 0.6678
11:24:25.682   Training iter 500, batch loss 1.0857, batch acc 0.6742
11:24:26.144   Training iter 550, batch loss 1.1079, batch acc 0.6630
11:24:26.603   Training iter 600, batch loss 1.1104, batch acc 0.6650
11:24:26.605 Training @ 102 epoch...
11:24:27.097   Training iter 50, batch loss 1.1098, batch acc 0.6724
11:24:27.627   Training iter 100, batch loss 1.0856, batch acc 0.6788
11:24:28.188   Training iter 150, batch loss 1.1107, batch acc 0.6640
11:24:28.728   Training iter 200, batch loss 1.1026, batch acc 0.6680
11:24:29.268   Training iter 250, batch loss 1.0915, batch acc 0.6686
11:24:29.814   Training iter 300, batch loss 1.0852, batch acc 0.6644
11:24:30.390   Training iter 350, batch loss 1.0778, batch acc 0.6754
11:24:30.948   Training iter 400, batch loss 1.1283, batch acc 0.6616
11:24:31.505   Training iter 450, batch loss 1.1001, batch acc 0.6634
11:24:32.085   Training iter 500, batch loss 1.1102, batch acc 0.6662
11:24:32.596   Training iter 550, batch loss 1.0959, batch acc 0.6704
11:24:33.102   Training iter 600, batch loss 1.1099, batch acc 0.6600
11:24:33.104 Training @ 103 epoch...
11:24:33.642   Training iter 50, batch loss 1.1116, batch acc 0.6616
11:24:34.148   Training iter 100, batch loss 1.0789, batch acc 0.6764
11:24:34.667   Training iter 150, batch loss 1.0909, batch acc 0.6700
11:24:35.186   Training iter 200, batch loss 1.1028, batch acc 0.6634
11:24:35.711   Training iter 250, batch loss 1.1175, batch acc 0.6636
11:24:36.203   Training iter 300, batch loss 1.0900, batch acc 0.6752
11:24:36.693   Training iter 350, batch loss 1.1187, batch acc 0.6616
11:24:37.197   Training iter 400, batch loss 1.0887, batch acc 0.6626
11:24:37.707   Training iter 450, batch loss 1.0857, batch acc 0.6784
11:24:38.217   Training iter 500, batch loss 1.1129, batch acc 0.6634
11:24:38.696   Training iter 550, batch loss 1.1250, batch acc 0.6606
11:24:39.157   Training iter 600, batch loss 1.0801, batch acc 0.6770
11:24:39.158 Training @ 104 epoch...
11:24:39.640   Training iter 50, batch loss 1.1325, batch acc 0.6618
11:24:40.142   Training iter 100, batch loss 1.0939, batch acc 0.6670
11:24:40.642   Training iter 150, batch loss 1.0956, batch acc 0.6674
11:24:41.119   Training iter 200, batch loss 1.0839, batch acc 0.6682
11:24:41.602   Training iter 250, batch loss 1.0869, batch acc 0.6702
11:24:42.098   Training iter 300, batch loss 1.1014, batch acc 0.6648
11:24:42.592   Training iter 350, batch loss 1.1024, batch acc 0.6726
11:24:43.105   Training iter 400, batch loss 1.1026, batch acc 0.6734
11:24:43.592   Training iter 450, batch loss 1.0988, batch acc 0.6682
11:24:44.089   Training iter 500, batch loss 1.0776, batch acc 0.6722
11:24:44.567   Training iter 550, batch loss 1.0994, batch acc 0.6700
11:24:45.040   Training iter 600, batch loss 1.1231, batch acc 0.6622
11:24:45.042 Training @ 105 epoch...
11:24:45.511   Training iter 50, batch loss 1.1148, batch acc 0.6670
11:24:45.981   Training iter 100, batch loss 1.0966, batch acc 0.6672
11:24:46.456   Training iter 150, batch loss 1.0905, batch acc 0.6690
11:24:46.924   Training iter 200, batch loss 1.1224, batch acc 0.6590
11:24:47.399   Training iter 250, batch loss 1.0814, batch acc 0.6718
11:24:47.868   Training iter 300, batch loss 1.0854, batch acc 0.6686
11:24:48.361   Training iter 350, batch loss 1.0942, batch acc 0.6710
11:24:48.876   Training iter 400, batch loss 1.0890, batch acc 0.6708
11:24:49.402   Training iter 450, batch loss 1.1135, batch acc 0.6614
11:24:49.944   Training iter 500, batch loss 1.1031, batch acc 0.6698
11:24:50.492   Training iter 550, batch loss 1.1138, batch acc 0.6652
11:24:51.041   Training iter 600, batch loss 1.0888, batch acc 0.6736
11:24:51.043 Testing @ 105 epoch...
11:24:51.085     Testing, total mean loss 1.07023, total acc 0.67860
11:24:51.085 Training @ 106 epoch...
11:24:51.638   Training iter 50, batch loss 1.0958, batch acc 0.6690
11:24:52.199   Training iter 100, batch loss 1.1187, batch acc 0.6688
11:24:52.755   Training iter 150, batch loss 1.0879, batch acc 0.6734
11:24:53.326   Training iter 200, batch loss 1.0701, batch acc 0.6808
11:24:53.889   Training iter 250, batch loss 1.0995, batch acc 0.6742
11:24:54.420   Training iter 300, batch loss 1.0940, batch acc 0.6640
11:24:54.937   Training iter 350, batch loss 1.0984, batch acc 0.6654
11:24:55.418   Training iter 400, batch loss 1.1003, batch acc 0.6648
11:24:55.935   Training iter 450, batch loss 1.1198, batch acc 0.6592
11:24:56.466   Training iter 500, batch loss 1.1094, batch acc 0.6650
11:24:56.982   Training iter 550, batch loss 1.0771, batch acc 0.6734
11:24:57.503   Training iter 600, batch loss 1.1181, batch acc 0.6640
11:24:57.504 Training @ 107 epoch...
11:24:58.044   Training iter 50, batch loss 1.1036, batch acc 0.6680
11:24:58.575   Training iter 100, batch loss 1.0832, batch acc 0.6794
11:24:59.114   Training iter 150, batch loss 1.0987, batch acc 0.6654
11:24:59.657   Training iter 200, batch loss 1.0955, batch acc 0.6660
11:25:00.204   Training iter 250, batch loss 1.1211, batch acc 0.6624
11:25:00.736   Training iter 300, batch loss 1.1029, batch acc 0.6680
11:25:01.271   Training iter 350, batch loss 1.0933, batch acc 0.6690
11:25:01.815   Training iter 400, batch loss 1.1026, batch acc 0.6666
11:25:02.333   Training iter 450, batch loss 1.1038, batch acc 0.6686
11:25:02.850   Training iter 500, batch loss 1.0953, batch acc 0.6656
11:25:03.360   Training iter 550, batch loss 1.0876, batch acc 0.6736
11:25:03.881   Training iter 600, batch loss 1.0967, batch acc 0.6660
11:25:03.883 Training @ 108 epoch...
11:25:04.402   Training iter 50, batch loss 1.1207, batch acc 0.6550
11:25:04.917   Training iter 100, batch loss 1.0981, batch acc 0.6670
11:25:05.411   Training iter 150, batch loss 1.1055, batch acc 0.6668
11:25:05.932   Training iter 200, batch loss 1.1092, batch acc 0.6654
11:25:06.455   Training iter 250, batch loss 1.0766, batch acc 0.6768
11:25:07.004   Training iter 300, batch loss 1.1081, batch acc 0.6616
11:25:07.559   Training iter 350, batch loss 1.0861, batch acc 0.6752
11:25:08.081   Training iter 400, batch loss 1.0965, batch acc 0.6674
11:25:08.582   Training iter 450, batch loss 1.0952, batch acc 0.6674
11:25:09.092   Training iter 500, batch loss 1.0899, batch acc 0.6758
11:25:09.604   Training iter 550, batch loss 1.0917, batch acc 0.6704
11:25:10.138   Training iter 600, batch loss 1.1025, batch acc 0.6694
11:25:10.140 Training @ 109 epoch...
11:25:10.669   Training iter 50, batch loss 1.1017, batch acc 0.6674
11:25:11.181   Training iter 100, batch loss 1.0903, batch acc 0.6722
11:25:11.688   Training iter 150, batch loss 1.0913, batch acc 0.6654
11:25:12.178   Training iter 200, batch loss 1.1153, batch acc 0.6610
11:25:12.661   Training iter 250, batch loss 1.1122, batch acc 0.6668
11:25:13.120   Training iter 300, batch loss 1.1177, batch acc 0.6620
11:25:13.610   Training iter 350, batch loss 1.1093, batch acc 0.6630
11:25:14.082   Training iter 400, batch loss 1.1017, batch acc 0.6656
11:25:14.582   Training iter 450, batch loss 1.0892, batch acc 0.6758
11:25:15.112   Training iter 500, batch loss 1.0927, batch acc 0.6676
11:25:15.649   Training iter 550, batch loss 1.0691, batch acc 0.6800
11:25:16.165   Training iter 600, batch loss 1.0856, batch acc 0.6748
11:25:16.167 Training @ 110 epoch...
11:25:16.709   Training iter 50, batch loss 1.0815, batch acc 0.6750
11:25:17.216   Training iter 100, batch loss 1.1222, batch acc 0.6554
11:25:17.736   Training iter 150, batch loss 1.0885, batch acc 0.6728
11:25:18.264   Training iter 200, batch loss 1.1047, batch acc 0.6652
11:25:18.784   Training iter 250, batch loss 1.0870, batch acc 0.6712
11:25:19.292   Training iter 300, batch loss 1.0803, batch acc 0.6790
11:25:19.811   Training iter 350, batch loss 1.1070, batch acc 0.6680
11:25:20.348   Training iter 400, batch loss 1.1070, batch acc 0.6636
11:25:20.896   Training iter 450, batch loss 1.0926, batch acc 0.6678
11:25:21.459   Training iter 500, batch loss 1.1165, batch acc 0.6632
11:25:22.018   Training iter 550, batch loss 1.1078, batch acc 0.6666
11:25:22.562   Training iter 600, batch loss 1.0763, batch acc 0.6804
11:25:22.565 Testing @ 110 epoch...
11:25:22.606     Testing, total mean loss 1.06842, total acc 0.67780
11:25:22.606 Training @ 111 epoch...
11:25:23.163   Training iter 50, batch loss 1.1128, batch acc 0.6630
11:25:23.716   Training iter 100, batch loss 1.0935, batch acc 0.6702
11:25:24.271   Training iter 150, batch loss 1.1156, batch acc 0.6594
11:25:24.811   Training iter 200, batch loss 1.1020, batch acc 0.6710
11:25:25.358   Training iter 250, batch loss 1.1032, batch acc 0.6698
11:25:25.897   Training iter 300, batch loss 1.1023, batch acc 0.6672
11:25:26.461   Training iter 350, batch loss 1.0678, batch acc 0.6786
11:25:27.002   Training iter 400, batch loss 1.0842, batch acc 0.6684
11:25:27.643   Training iter 450, batch loss 1.0968, batch acc 0.6708
11:25:28.218   Training iter 500, batch loss 1.1093, batch acc 0.6642
11:25:28.787   Training iter 550, batch loss 1.0920, batch acc 0.6706
11:25:29.307   Training iter 600, batch loss 1.0880, batch acc 0.6722
11:25:29.309 Training @ 112 epoch...
11:25:29.841   Training iter 50, batch loss 1.0801, batch acc 0.6746
11:25:30.415   Training iter 100, batch loss 1.0695, batch acc 0.6720
11:25:31.008   Training iter 150, batch loss 1.0796, batch acc 0.6782
11:25:31.597   Training iter 200, batch loss 1.0994, batch acc 0.6656
11:25:32.175   Training iter 250, batch loss 1.0841, batch acc 0.6720
11:25:32.725   Training iter 300, batch loss 1.1014, batch acc 0.6726
11:25:33.243   Training iter 350, batch loss 1.1178, batch acc 0.6646
11:25:33.737   Training iter 400, batch loss 1.1076, batch acc 0.6592
11:25:34.210   Training iter 450, batch loss 1.0908, batch acc 0.6694
11:25:34.702   Training iter 500, batch loss 1.0946, batch acc 0.6790
11:25:35.199   Training iter 550, batch loss 1.1136, batch acc 0.6632
11:25:35.699   Training iter 600, batch loss 1.1252, batch acc 0.6542
11:25:35.701 Training @ 113 epoch...
11:25:36.193   Training iter 50, batch loss 1.1006, batch acc 0.6660
11:25:36.679   Training iter 100, batch loss 1.1058, batch acc 0.6652
11:25:37.203   Training iter 150, batch loss 1.0742, batch acc 0.6758
11:25:37.748   Training iter 200, batch loss 1.1170, batch acc 0.6626
11:25:38.288   Training iter 250, batch loss 1.0874, batch acc 0.6778
11:25:38.827   Training iter 300, batch loss 1.0921, batch acc 0.6692
11:25:39.363   Training iter 350, batch loss 1.0761, batch acc 0.6736
11:25:39.898   Training iter 400, batch loss 1.1136, batch acc 0.6622
11:25:40.420   Training iter 450, batch loss 1.1107, batch acc 0.6646
11:25:40.922   Training iter 500, batch loss 1.0931, batch acc 0.6674
11:25:41.411   Training iter 550, batch loss 1.0833, batch acc 0.6726
11:25:41.906   Training iter 600, batch loss 1.1060, batch acc 0.6692
11:25:41.908 Training @ 114 epoch...
11:25:42.429   Training iter 50, batch loss 1.1126, batch acc 0.6634
11:25:42.936   Training iter 100, batch loss 1.0872, batch acc 0.6690
11:25:43.400   Training iter 150, batch loss 1.1018, batch acc 0.6652
11:25:43.892   Training iter 200, batch loss 1.1188, batch acc 0.6636
11:25:44.414   Training iter 250, batch loss 1.0965, batch acc 0.6672
11:25:44.968   Training iter 300, batch loss 1.0939, batch acc 0.6704
11:25:45.523   Training iter 350, batch loss 1.0978, batch acc 0.6674
11:25:46.010   Training iter 400, batch loss 1.0950, batch acc 0.6732
11:25:46.500   Training iter 450, batch loss 1.0838, batch acc 0.6770
11:25:46.986   Training iter 500, batch loss 1.0978, batch acc 0.6742
11:25:47.476   Training iter 550, batch loss 1.0965, batch acc 0.6678
11:25:47.952   Training iter 600, batch loss 1.0741, batch acc 0.6750
11:25:47.953 Training @ 115 epoch...
11:25:48.450   Training iter 50, batch loss 1.0959, batch acc 0.6712
11:25:48.936   Training iter 100, batch loss 1.1224, batch acc 0.6604
11:25:49.422   Training iter 150, batch loss 1.0900, batch acc 0.6790
11:25:49.915   Training iter 200, batch loss 1.0895, batch acc 0.6732
11:25:50.410   Training iter 250, batch loss 1.0807, batch acc 0.6710
11:25:50.892   Training iter 300, batch loss 1.0878, batch acc 0.6748
11:25:51.352   Training iter 350, batch loss 1.0916, batch acc 0.6706
11:25:51.845   Training iter 400, batch loss 1.1164, batch acc 0.6610
11:25:52.369   Training iter 450, batch loss 1.0995, batch acc 0.6658
11:25:52.892   Training iter 500, batch loss 1.1045, batch acc 0.6624
11:25:53.443   Training iter 550, batch loss 1.0892, batch acc 0.6768
11:25:53.987   Training iter 600, batch loss 1.0844, batch acc 0.6662
11:25:53.989 Testing @ 115 epoch...
11:25:54.032     Testing, total mean loss 1.06697, total acc 0.67830
11:25:54.032 Training @ 116 epoch...
11:25:54.573   Training iter 50, batch loss 1.0876, batch acc 0.6838
11:25:55.114   Training iter 100, batch loss 1.1077, batch acc 0.6636
11:25:55.661   Training iter 150, batch loss 1.1146, batch acc 0.6640
11:25:56.196   Training iter 200, batch loss 1.0857, batch acc 0.6738
11:25:56.730   Training iter 250, batch loss 1.1029, batch acc 0.6646
11:25:57.263   Training iter 300, batch loss 1.0931, batch acc 0.6722
11:25:57.805   Training iter 350, batch loss 1.0830, batch acc 0.6694
11:25:58.359   Training iter 400, batch loss 1.0821, batch acc 0.6692
11:25:58.910   Training iter 450, batch loss 1.0866, batch acc 0.6724
11:25:59.424   Training iter 500, batch loss 1.0995, batch acc 0.6700
11:25:59.931   Training iter 550, batch loss 1.1160, batch acc 0.6592
11:26:00.461   Training iter 600, batch loss 1.0897, batch acc 0.6696
11:26:00.463 Training @ 117 epoch...
11:26:00.984   Training iter 50, batch loss 1.0871, batch acc 0.6750
11:26:01.527   Training iter 100, batch loss 1.1028, batch acc 0.6632
11:26:02.088   Training iter 150, batch loss 1.0863, batch acc 0.6768
11:26:02.633   Training iter 200, batch loss 1.0911, batch acc 0.6696
11:26:03.171   Training iter 250, batch loss 1.1029, batch acc 0.6616
11:26:03.707   Training iter 300, batch loss 1.0767, batch acc 0.6790
11:26:04.239   Training iter 350, batch loss 1.1106, batch acc 0.6612
11:26:04.782   Training iter 400, batch loss 1.0998, batch acc 0.6710
11:26:05.328   Training iter 450, batch loss 1.0907, batch acc 0.6690
11:26:05.867   Training iter 500, batch loss 1.0910, batch acc 0.6694
11:26:06.420   Training iter 550, batch loss 1.0967, batch acc 0.6700
11:26:06.970   Training iter 600, batch loss 1.1089, batch acc 0.6684
11:26:06.972 Training @ 118 epoch...
11:26:07.505   Training iter 50, batch loss 1.0820, batch acc 0.6742
11:26:08.029   Training iter 100, batch loss 1.0941, batch acc 0.6666
11:26:08.555   Training iter 150, batch loss 1.0998, batch acc 0.6638
11:26:09.092   Training iter 200, batch loss 1.1044, batch acc 0.6646
11:26:09.638   Training iter 250, batch loss 1.0844, batch acc 0.6742
11:26:10.183   Training iter 300, batch loss 1.0909, batch acc 0.6692
11:26:10.733   Training iter 350, batch loss 1.1051, batch acc 0.6626
11:26:11.267   Training iter 400, batch loss 1.1133, batch acc 0.6708
11:26:11.809   Training iter 450, batch loss 1.0779, batch acc 0.6774
11:26:12.350   Training iter 500, batch loss 1.0981, batch acc 0.6696
11:26:12.909   Training iter 550, batch loss 1.1017, batch acc 0.6694
11:26:13.463   Training iter 600, batch loss 1.0895, batch acc 0.6716
11:26:13.465 Training @ 119 epoch...
11:26:14.023   Training iter 50, batch loss 1.0543, batch acc 0.6814
11:26:14.579   Training iter 100, batch loss 1.0846, batch acc 0.6740
11:26:15.122   Training iter 150, batch loss 1.1064, batch acc 0.6624
11:26:15.685   Training iter 200, batch loss 1.1145, batch acc 0.6640
11:26:16.183   Training iter 250, batch loss 1.0951, batch acc 0.6756
11:26:16.682   Training iter 300, batch loss 1.0749, batch acc 0.6796
11:26:17.174   Training iter 350, batch loss 1.0880, batch acc 0.6696
11:26:17.684   Training iter 400, batch loss 1.1138, batch acc 0.6622
11:26:18.185   Training iter 450, batch loss 1.1010, batch acc 0.6680
11:26:18.720   Training iter 500, batch loss 1.1088, batch acc 0.6684
11:26:19.277   Training iter 550, batch loss 1.1098, batch acc 0.6666
11:26:19.827   Training iter 600, batch loss 1.0866, batch acc 0.6666
11:26:19.828 Training @ 120 epoch...
11:26:20.377   Training iter 50, batch loss 1.0916, batch acc 0.6656
11:26:20.925   Training iter 100, batch loss 1.0872, batch acc 0.6720
11:26:21.453   Training iter 150, batch loss 1.0820, batch acc 0.6762
11:26:21.983   Training iter 200, batch loss 1.1037, batch acc 0.6636
11:26:22.506   Training iter 250, batch loss 1.0988, batch acc 0.6684
11:26:23.019   Training iter 300, batch loss 1.0840, batch acc 0.6710
11:26:23.576   Training iter 350, batch loss 1.1117, batch acc 0.6606
11:26:24.143   Training iter 400, batch loss 1.1008, batch acc 0.6632
11:26:24.717   Training iter 450, batch loss 1.0852, batch acc 0.6778
11:26:25.282   Training iter 500, batch loss 1.1081, batch acc 0.6722
11:26:25.833   Training iter 550, batch loss 1.0892, batch acc 0.6720
11:26:26.374   Training iter 600, batch loss 1.0919, batch acc 0.6732
11:26:26.376 Testing @ 120 epoch...
11:26:26.417     Testing, total mean loss 1.06559, total acc 0.67970
11:26:26.417 Training @ 121 epoch...
11:26:26.965   Training iter 50, batch loss 1.1036, batch acc 0.6620
11:26:27.517   Training iter 100, batch loss 1.0953, batch acc 0.6734
11:26:28.080   Training iter 150, batch loss 1.1106, batch acc 0.6602
11:26:28.616   Training iter 200, batch loss 1.0719, batch acc 0.6774
11:26:29.149   Training iter 250, batch loss 1.1312, batch acc 0.6536
11:26:29.690   Training iter 300, batch loss 1.0888, batch acc 0.6722
11:26:30.222   Training iter 350, batch loss 1.1169, batch acc 0.6586
11:26:30.754   Training iter 400, batch loss 1.0886, batch acc 0.6782
11:26:31.282   Training iter 450, batch loss 1.0954, batch acc 0.6668
11:26:31.802   Training iter 500, batch loss 1.0862, batch acc 0.6748
11:26:32.317   Training iter 550, batch loss 1.0709, batch acc 0.6816
11:26:32.825   Training iter 600, batch loss 1.0713, batch acc 0.6806
11:26:32.827 Training @ 122 epoch...
11:26:33.344   Training iter 50, batch loss 1.0750, batch acc 0.6778
11:26:33.855   Training iter 100, batch loss 1.1219, batch acc 0.6632
11:26:34.367   Training iter 150, batch loss 1.0871, batch acc 0.6762
11:26:34.876   Training iter 200, batch loss 1.0897, batch acc 0.6686
11:26:35.394   Training iter 250, batch loss 1.1190, batch acc 0.6618
11:26:35.911   Training iter 300, batch loss 1.0874, batch acc 0.6748
11:26:36.418   Training iter 350, batch loss 1.0891, batch acc 0.6672
11:26:36.932   Training iter 400, batch loss 1.1144, batch acc 0.6630
11:26:37.461   Training iter 450, batch loss 1.0793, batch acc 0.6732
11:26:37.992   Training iter 500, batch loss 1.0804, batch acc 0.6758
11:26:38.523   Training iter 550, batch loss 1.0981, batch acc 0.6640
11:26:39.041   Training iter 600, batch loss 1.0862, batch acc 0.6768
11:26:39.043 Training @ 123 epoch...
11:26:39.554   Training iter 50, batch loss 1.0907, batch acc 0.6730
11:26:40.070   Training iter 100, batch loss 1.0787, batch acc 0.6830
11:26:40.582   Training iter 150, batch loss 1.0978, batch acc 0.6630
11:26:41.093   Training iter 200, batch loss 1.0915, batch acc 0.6692
11:26:41.601   Training iter 250, batch loss 1.1188, batch acc 0.6606
11:26:42.120   Training iter 300, batch loss 1.0655, batch acc 0.6804
11:26:42.634   Training iter 350, batch loss 1.1403, batch acc 0.6572
11:26:43.168   Training iter 400, batch loss 1.0935, batch acc 0.6684
11:26:43.665   Training iter 450, batch loss 1.0818, batch acc 0.6724
11:26:44.135   Training iter 500, batch loss 1.1005, batch acc 0.6666
11:26:44.616   Training iter 550, batch loss 1.0699, batch acc 0.6794
11:26:45.115   Training iter 600, batch loss 1.0954, batch acc 0.6660
11:26:45.117 Training @ 124 epoch...
11:26:45.618   Training iter 50, batch loss 1.1133, batch acc 0.6588
11:26:46.136   Training iter 100, batch loss 1.0949, batch acc 0.6702
11:26:46.676   Training iter 150, batch loss 1.0940, batch acc 0.6654
11:26:47.202   Training iter 200, batch loss 1.0717, batch acc 0.6782
11:26:47.726   Training iter 250, batch loss 1.1101, batch acc 0.6674
11:26:48.254   Training iter 300, batch loss 1.0607, batch acc 0.6870
11:26:48.757   Training iter 350, batch loss 1.1072, batch acc 0.6646
11:26:49.243   Training iter 400, batch loss 1.1100, batch acc 0.6608
11:26:49.729   Training iter 450, batch loss 1.0546, batch acc 0.6864
11:26:50.208   Training iter 500, batch loss 1.0935, batch acc 0.6680
11:26:50.678   Training iter 550, batch loss 1.1058, batch acc 0.6644
11:26:51.147   Training iter 600, batch loss 1.1054, batch acc 0.6718
11:26:51.148 Training @ 125 epoch...
11:26:51.615   Training iter 50, batch loss 1.1022, batch acc 0.6688
11:26:52.081   Training iter 100, batch loss 1.0719, batch acc 0.6758
11:26:52.565   Training iter 150, batch loss 1.0805, batch acc 0.6724
11:26:53.053   Training iter 200, batch loss 1.0724, batch acc 0.6724
11:26:53.552   Training iter 250, batch loss 1.0942, batch acc 0.6742
11:26:54.043   Training iter 300, batch loss 1.0621, batch acc 0.6852
11:26:54.522   Training iter 350, batch loss 1.1059, batch acc 0.6664
11:26:55.049   Training iter 400, batch loss 1.0876, batch acc 0.6660
11:26:55.578   Training iter 450, batch loss 1.1174, batch acc 0.6600
11:26:56.104   Training iter 500, batch loss 1.0998, batch acc 0.6674
11:26:56.626   Training iter 550, batch loss 1.0962, batch acc 0.6744
11:26:57.197   Training iter 600, batch loss 1.1278, batch acc 0.6590
11:26:57.199 Testing @ 125 epoch...
11:26:57.241     Testing, total mean loss 1.06424, total acc 0.67970
11:26:57.241 Training @ 126 epoch...
11:26:57.781   Training iter 50, batch loss 1.0815, batch acc 0.6810
11:26:58.296   Training iter 100, batch loss 1.0999, batch acc 0.6720
11:26:58.810   Training iter 150, batch loss 1.1130, batch acc 0.6638
11:26:59.295   Training iter 200, batch loss 1.1097, batch acc 0.6656
11:26:59.788   Training iter 250, batch loss 1.1130, batch acc 0.6606
11:27:00.272   Training iter 300, batch loss 1.1010, batch acc 0.6622
11:27:00.770   Training iter 350, batch loss 1.0754, batch acc 0.6746
11:27:01.285   Training iter 400, batch loss 1.0878, batch acc 0.6718
11:27:01.844   Training iter 450, batch loss 1.0752, batch acc 0.6776
11:27:02.394   Training iter 500, batch loss 1.1100, batch acc 0.6660
11:27:02.910   Training iter 550, batch loss 1.0712, batch acc 0.6720
11:27:03.438   Training iter 600, batch loss 1.0773, batch acc 0.6790
11:27:03.439 Training @ 127 epoch...
11:27:03.925   Training iter 50, batch loss 1.1224, batch acc 0.6568
11:27:04.413   Training iter 100, batch loss 1.0967, batch acc 0.6678
11:27:04.892   Training iter 150, batch loss 1.0740, batch acc 0.6838
11:27:05.359   Training iter 200, batch loss 1.0978, batch acc 0.6700
11:27:05.869   Training iter 250, batch loss 1.0990, batch acc 0.6708
11:27:06.364   Training iter 300, batch loss 1.0556, batch acc 0.6866
11:27:06.861   Training iter 350, batch loss 1.1160, batch acc 0.6574
11:27:07.329   Training iter 400, batch loss 1.0946, batch acc 0.6692
11:27:07.791   Training iter 450, batch loss 1.0806, batch acc 0.6730
11:27:08.276   Training iter 500, batch loss 1.1180, batch acc 0.6620
11:27:08.758   Training iter 550, batch loss 1.1025, batch acc 0.6618
11:27:09.218   Training iter 600, batch loss 1.0549, batch acc 0.6882
11:27:09.219 Training @ 128 epoch...
11:27:09.689   Training iter 50, batch loss 1.1116, batch acc 0.6640
11:27:10.156   Training iter 100, batch loss 1.0954, batch acc 0.6688
11:27:10.628   Training iter 150, batch loss 1.0922, batch acc 0.6692
11:27:11.080   Training iter 200, batch loss 1.0709, batch acc 0.6754
11:27:11.540   Training iter 250, batch loss 1.1063, batch acc 0.6616
11:27:12.007   Training iter 300, batch loss 1.0750, batch acc 0.6768
11:27:12.475   Training iter 350, batch loss 1.0865, batch acc 0.6746
11:27:12.949   Training iter 400, batch loss 1.0972, batch acc 0.6682
11:27:13.459   Training iter 450, batch loss 1.0705, batch acc 0.6782
11:27:13.990   Training iter 500, batch loss 1.0995, batch acc 0.6680
11:27:14.521   Training iter 550, batch loss 1.1098, batch acc 0.6700
11:27:15.069   Training iter 600, batch loss 1.0938, batch acc 0.6706
11:27:15.071 Training @ 129 epoch...
11:27:15.625   Training iter 50, batch loss 1.0919, batch acc 0.6680
11:27:16.166   Training iter 100, batch loss 1.0831, batch acc 0.6750
11:27:16.745   Training iter 150, batch loss 1.0923, batch acc 0.6756
11:27:17.328   Training iter 200, batch loss 1.0775, batch acc 0.6758
11:27:17.902   Training iter 250, batch loss 1.0836, batch acc 0.6770
11:27:18.446   Training iter 300, batch loss 1.0885, batch acc 0.6734
11:27:18.959   Training iter 350, batch loss 1.1018, batch acc 0.6664
11:27:19.481   Training iter 400, batch loss 1.0757, batch acc 0.6746
11:27:19.978   Training iter 450, batch loss 1.0976, batch acc 0.6654
11:27:20.488   Training iter 500, batch loss 1.1096, batch acc 0.6592
11:27:20.994   Training iter 550, batch loss 1.0921, batch acc 0.6704
11:27:21.452   Training iter 600, batch loss 1.1120, batch acc 0.6644
11:27:21.454 Training @ 130 epoch...
11:27:21.933   Training iter 50, batch loss 1.1063, batch acc 0.6692
11:27:22.427   Training iter 100, batch loss 1.0939, batch acc 0.6674
11:27:22.912   Training iter 150, batch loss 1.1020, batch acc 0.6712
11:27:23.397   Training iter 200, batch loss 1.1089, batch acc 0.6698
11:27:23.857   Training iter 250, batch loss 1.0895, batch acc 0.6754
11:27:24.323   Training iter 300, batch loss 1.0880, batch acc 0.6662
11:27:24.801   Training iter 350, batch loss 1.0784, batch acc 0.6786
11:27:25.285   Training iter 400, batch loss 1.0940, batch acc 0.6704
11:27:25.758   Training iter 450, batch loss 1.0935, batch acc 0.6678
11:27:26.233   Training iter 500, batch loss 1.0850, batch acc 0.6690
11:27:26.704   Training iter 550, batch loss 1.0708, batch acc 0.6822
11:27:27.186   Training iter 600, batch loss 1.0927, batch acc 0.6626
11:27:27.188 Testing @ 130 epoch...
11:27:27.229     Testing, total mean loss 1.06315, total acc 0.67990
11:27:27.229 Training @ 131 epoch...
11:27:27.734   Training iter 50, batch loss 1.0937, batch acc 0.6644
11:27:28.245   Training iter 100, batch loss 1.0775, batch acc 0.6784
11:27:28.722   Training iter 150, batch loss 1.1129, batch acc 0.6700
11:27:29.192   Training iter 200, batch loss 1.1001, batch acc 0.6676
11:27:29.660   Training iter 250, batch loss 1.0802, batch acc 0.6692
11:27:30.135   Training iter 300, batch loss 1.1059, batch acc 0.6626
11:27:30.608   Training iter 350, batch loss 1.0801, batch acc 0.6726
11:27:31.073   Training iter 400, batch loss 1.0537, batch acc 0.6892
11:27:31.536   Training iter 450, batch loss 1.1009, batch acc 0.6724
11:27:32.006   Training iter 500, batch loss 1.0946, batch acc 0.6654
11:27:32.561   Training iter 550, batch loss 1.0906, batch acc 0.6716
11:27:33.056   Training iter 600, batch loss 1.1100, batch acc 0.6644
11:27:33.058 Training @ 132 epoch...
11:27:33.547   Training iter 50, batch loss 1.0752, batch acc 0.6742
11:27:34.044   Training iter 100, batch loss 1.0964, batch acc 0.6694
11:27:34.557   Training iter 150, batch loss 1.1013, batch acc 0.6676
11:27:35.090   Training iter 200, batch loss 1.0829, batch acc 0.6730
11:27:35.633   Training iter 250, batch loss 1.0781, batch acc 0.6790
11:27:36.152   Training iter 300, batch loss 1.1031, batch acc 0.6690
11:27:36.659   Training iter 350, batch loss 1.1295, batch acc 0.6602
11:27:37.183   Training iter 400, batch loss 1.1003, batch acc 0.6656
11:27:37.712   Training iter 450, batch loss 1.0533, batch acc 0.6838
11:27:38.235   Training iter 500, batch loss 1.0854, batch acc 0.6660
11:27:38.755   Training iter 550, batch loss 1.0892, batch acc 0.6758
11:27:39.271   Training iter 600, batch loss 1.1030, batch acc 0.6670
11:27:39.273 Training @ 133 epoch...
11:27:39.802   Training iter 50, batch loss 1.1016, batch acc 0.6656
11:27:40.347   Training iter 100, batch loss 1.0866, batch acc 0.6706
11:27:40.862   Training iter 150, batch loss 1.0881, batch acc 0.6804
11:27:41.369   Training iter 200, batch loss 1.0927, batch acc 0.6740
11:27:41.861   Training iter 250, batch loss 1.0923, batch acc 0.6688
11:27:42.361   Training iter 300, batch loss 1.0897, batch acc 0.6654
11:27:42.871   Training iter 350, batch loss 1.0784, batch acc 0.6750
11:27:43.399   Training iter 400, batch loss 1.1052, batch acc 0.6666
11:27:43.908   Training iter 450, batch loss 1.0698, batch acc 0.6778
11:27:44.411   Training iter 500, batch loss 1.0908, batch acc 0.6698
11:27:44.914   Training iter 550, batch loss 1.0969, batch acc 0.6680
11:27:45.415   Training iter 600, batch loss 1.1026, batch acc 0.6716
11:27:45.417 Training @ 134 epoch...
11:27:45.934   Training iter 50, batch loss 1.0783, batch acc 0.6688
11:27:46.490   Training iter 100, batch loss 1.1056, batch acc 0.6652
11:27:47.045   Training iter 150, batch loss 1.0700, batch acc 0.6778
11:27:47.588   Training iter 200, batch loss 1.1204, batch acc 0.6610
11:27:48.160   Training iter 250, batch loss 1.1108, batch acc 0.6570
11:27:48.732   Training iter 300, batch loss 1.0770, batch acc 0.6766
11:27:49.298   Training iter 350, batch loss 1.1069, batch acc 0.6686
11:27:49.843   Training iter 400, batch loss 1.0749, batch acc 0.6744
11:27:50.389   Training iter 450, batch loss 1.1119, batch acc 0.6644
11:27:50.927   Training iter 500, batch loss 1.0746, batch acc 0.6834
11:27:51.462   Training iter 550, batch loss 1.0796, batch acc 0.6800
11:27:51.996   Training iter 600, batch loss 1.0821, batch acc 0.6730
11:27:51.998 Training @ 135 epoch...
11:27:52.516   Training iter 50, batch loss 1.0815, batch acc 0.6734
11:27:53.027   Training iter 100, batch loss 1.1098, batch acc 0.6622
11:27:53.549   Training iter 150, batch loss 1.0870, batch acc 0.6690
11:27:54.055   Training iter 200, batch loss 1.1035, batch acc 0.6638
11:27:54.549   Training iter 250, batch loss 1.1005, batch acc 0.6738
11:27:55.071   Training iter 300, batch loss 1.0939, batch acc 0.6668
11:27:55.610   Training iter 350, batch loss 1.0868, batch acc 0.6754
11:27:56.132   Training iter 400, batch loss 1.0703, batch acc 0.6752
11:27:56.641   Training iter 450, batch loss 1.1012, batch acc 0.6752
11:27:57.157   Training iter 500, batch loss 1.0783, batch acc 0.6754
11:27:57.670   Training iter 550, batch loss 1.0835, batch acc 0.6728
11:27:58.167   Training iter 600, batch loss 1.0932, batch acc 0.6710
11:27:58.169 Testing @ 135 epoch...
11:27:58.211     Testing, total mean loss 1.06207, total acc 0.68020
11:27:58.211 Training @ 136 epoch...
11:27:58.702   Training iter 50, batch loss 1.0816, batch acc 0.6692
11:27:59.188   Training iter 100, batch loss 1.0761, batch acc 0.6776
11:27:59.691   Training iter 150, batch loss 1.0793, batch acc 0.6770
11:28:00.227   Training iter 200, batch loss 1.1060, batch acc 0.6662
11:28:00.749   Training iter 250, batch loss 1.0950, batch acc 0.6658
11:28:01.281   Training iter 300, batch loss 1.1044, batch acc 0.6674
11:28:01.794   Training iter 350, batch loss 1.1008, batch acc 0.6702
11:28:02.349   Training iter 400, batch loss 1.0733, batch acc 0.6780
11:28:02.897   Training iter 450, batch loss 1.1088, batch acc 0.6678
11:28:03.470   Training iter 500, batch loss 1.0665, batch acc 0.6778
11:28:04.074   Training iter 550, batch loss 1.0817, batch acc 0.6702
11:28:04.686   Training iter 600, batch loss 1.1134, batch acc 0.6684
11:28:04.688 Training @ 137 epoch...
11:28:05.263   Training iter 50, batch loss 1.0980, batch acc 0.6660
11:28:05.814   Training iter 100, batch loss 1.0886, batch acc 0.6722
11:28:06.357   Training iter 150, batch loss 1.0818, batch acc 0.6756
11:28:06.938   Training iter 200, batch loss 1.1037, batch acc 0.6650
11:28:07.495   Training iter 250, batch loss 1.1145, batch acc 0.6590
11:28:08.018   Training iter 300, batch loss 1.0835, batch acc 0.6784
11:28:08.484   Training iter 350, batch loss 1.0684, batch acc 0.6766
11:28:08.923   Training iter 400, batch loss 1.0850, batch acc 0.6762
11:28:09.362   Training iter 450, batch loss 1.0935, batch acc 0.6710
11:28:09.804   Training iter 500, batch loss 1.0952, batch acc 0.6688
11:28:10.256   Training iter 550, batch loss 1.0823, batch acc 0.6726
11:28:10.710   Training iter 600, batch loss 1.0902, batch acc 0.6766
11:28:10.711 Training @ 138 epoch...
11:28:11.176   Training iter 50, batch loss 1.0870, batch acc 0.6748
11:28:11.635   Training iter 100, batch loss 1.1302, batch acc 0.6562
11:28:12.096   Training iter 150, batch loss 1.0583, batch acc 0.6774
11:28:12.586   Training iter 200, batch loss 1.0832, batch acc 0.6738
11:28:13.072   Training iter 250, batch loss 1.0808, batch acc 0.6746
11:28:13.548   Training iter 300, batch loss 1.0813, batch acc 0.6722
11:28:14.057   Training iter 350, batch loss 1.1034, batch acc 0.6672
11:28:14.676   Training iter 400, batch loss 1.0894, batch acc 0.6744
11:28:15.308   Training iter 450, batch loss 1.1069, batch acc 0.6692
11:28:15.887   Training iter 500, batch loss 1.0907, batch acc 0.6742
11:28:16.416   Training iter 550, batch loss 1.0903, batch acc 0.6706
11:28:16.921   Training iter 600, batch loss 1.0805, batch acc 0.6708
11:28:16.923 Training @ 139 epoch...
11:28:17.415   Training iter 50, batch loss 1.1077, batch acc 0.6670
11:28:17.906   Training iter 100, batch loss 1.0681, batch acc 0.6870
11:28:18.437   Training iter 150, batch loss 1.0869, batch acc 0.6690
11:28:18.992   Training iter 200, batch loss 1.1257, batch acc 0.6578
11:28:19.537   Training iter 250, batch loss 1.0879, batch acc 0.6782
11:28:20.095   Training iter 300, batch loss 1.0878, batch acc 0.6738
11:28:20.654   Training iter 350, batch loss 1.0678, batch acc 0.6754
11:28:21.221   Training iter 400, batch loss 1.1150, batch acc 0.6610
11:28:21.787   Training iter 450, batch loss 1.0846, batch acc 0.6670
11:28:22.368   Training iter 500, batch loss 1.0830, batch acc 0.6730
11:28:22.915   Training iter 550, batch loss 1.1014, batch acc 0.6666
11:28:23.463   Training iter 600, batch loss 1.0635, batch acc 0.6818
11:28:23.465 Training @ 140 epoch...
11:28:24.004   Training iter 50, batch loss 1.0749, batch acc 0.6724
11:28:24.538   Training iter 100, batch loss 1.1017, batch acc 0.6714
11:28:25.032   Training iter 150, batch loss 1.1124, batch acc 0.6636
11:28:25.533   Training iter 200, batch loss 1.0597, batch acc 0.6854
11:28:26.033   Training iter 250, batch loss 1.0998, batch acc 0.6690
11:28:26.494   Training iter 300, batch loss 1.0950, batch acc 0.6660
11:28:26.950   Training iter 350, batch loss 1.0908, batch acc 0.6718
11:28:27.421   Training iter 400, batch loss 1.1034, batch acc 0.6610
11:28:27.905   Training iter 450, batch loss 1.0744, batch acc 0.6804
11:28:28.404   Training iter 500, batch loss 1.0896, batch acc 0.6716
11:28:28.899   Training iter 550, batch loss 1.1079, batch acc 0.6630
11:28:29.412   Training iter 600, batch loss 1.0678, batch acc 0.6844
11:28:29.414 Testing @ 140 epoch...
11:28:29.456     Testing, total mean loss 1.06108, total acc 0.68040
11:28:29.456 Training @ 141 epoch...
11:28:29.965   Training iter 50, batch loss 1.0922, batch acc 0.6790
11:28:30.501   Training iter 100, batch loss 1.0947, batch acc 0.6678
11:28:31.042   Training iter 150, batch loss 1.1117, batch acc 0.6610
11:28:31.601   Training iter 200, batch loss 1.0679, batch acc 0.6746
11:28:32.164   Training iter 250, batch loss 1.0875, batch acc 0.6674
11:28:32.661   Training iter 300, batch loss 1.1066, batch acc 0.6650
11:28:33.164   Training iter 350, batch loss 1.0820, batch acc 0.6786
11:28:33.661   Training iter 400, batch loss 1.0725, batch acc 0.6828
11:28:34.157   Training iter 450, batch loss 1.0866, batch acc 0.6730
11:28:34.666   Training iter 500, batch loss 1.0820, batch acc 0.6718
11:28:35.188   Training iter 550, batch loss 1.0853, batch acc 0.6738
11:28:35.711   Training iter 600, batch loss 1.1057, batch acc 0.6652
11:28:35.713 Training @ 142 epoch...
11:28:36.215   Training iter 50, batch loss 1.0808, batch acc 0.6734
11:28:36.725   Training iter 100, batch loss 1.0809, batch acc 0.6788
11:28:37.234   Training iter 150, batch loss 1.0733, batch acc 0.6790
11:28:37.745   Training iter 200, batch loss 1.1031, batch acc 0.6688
11:28:38.270   Training iter 250, batch loss 1.0846, batch acc 0.6768
11:28:38.802   Training iter 300, batch loss 1.0904, batch acc 0.6724
11:28:39.338   Training iter 350, batch loss 1.1143, batch acc 0.6570
11:28:39.863   Training iter 400, batch loss 1.0987, batch acc 0.6674
11:28:40.381   Training iter 450, batch loss 1.0799, batch acc 0.6730
11:28:40.862   Training iter 500, batch loss 1.0631, batch acc 0.6850
11:28:41.345   Training iter 550, batch loss 1.1121, batch acc 0.6620
11:28:41.847   Training iter 600, batch loss 1.0914, batch acc 0.6676
11:28:41.849 Training @ 143 epoch...
11:28:42.368   Training iter 50, batch loss 1.0936, batch acc 0.6726
11:28:42.878   Training iter 100, batch loss 1.1019, batch acc 0.6662
11:28:43.382   Training iter 150, batch loss 1.0680, batch acc 0.6806
11:28:43.883   Training iter 200, batch loss 1.0701, batch acc 0.6774
11:28:44.386   Training iter 250, batch loss 1.0762, batch acc 0.6744
11:28:44.887   Training iter 300, batch loss 1.0972, batch acc 0.6660
11:28:45.393   Training iter 350, batch loss 1.0995, batch acc 0.6684
11:28:45.897   Training iter 400, batch loss 1.0739, batch acc 0.6764
11:28:46.406   Training iter 450, batch loss 1.1034, batch acc 0.6636
11:28:46.916   Training iter 500, batch loss 1.0703, batch acc 0.6798
11:28:47.436   Training iter 550, batch loss 1.1062, batch acc 0.6660
11:28:47.952   Training iter 600, batch loss 1.1100, batch acc 0.6666
11:28:47.954 Training @ 144 epoch...
11:28:48.493   Training iter 50, batch loss 1.0828, batch acc 0.6732
11:28:49.004   Training iter 100, batch loss 1.0914, batch acc 0.6714
11:28:49.518   Training iter 150, batch loss 1.1030, batch acc 0.6698
11:28:50.030   Training iter 200, batch loss 1.1037, batch acc 0.6672
11:28:50.544   Training iter 250, batch loss 1.0889, batch acc 0.6720
11:28:51.088   Training iter 300, batch loss 1.1041, batch acc 0.6688
11:28:51.623   Training iter 350, batch loss 1.0528, batch acc 0.6864
11:28:52.180   Training iter 400, batch loss 1.1062, batch acc 0.6646
11:28:52.721   Training iter 450, batch loss 1.0975, batch acc 0.6688
11:28:53.279   Training iter 500, batch loss 1.0839, batch acc 0.6760
11:28:53.838   Training iter 550, batch loss 1.0540, batch acc 0.6822
11:28:54.380   Training iter 600, batch loss 1.0997, batch acc 0.6638
11:28:54.382 Training @ 145 epoch...
11:28:54.924   Training iter 50, batch loss 1.1011, batch acc 0.6680
11:28:55.470   Training iter 100, batch loss 1.0764, batch acc 0.6744
11:28:55.996   Training iter 150, batch loss 1.0811, batch acc 0.6746
11:28:56.528   Training iter 200, batch loss 1.0939, batch acc 0.6654
11:28:57.063   Training iter 250, batch loss 1.0928, batch acc 0.6742
11:28:57.575   Training iter 300, batch loss 1.0854, batch acc 0.6748
11:28:58.098   Training iter 350, batch loss 1.0699, batch acc 0.6788
11:28:58.618   Training iter 400, batch loss 1.0988, batch acc 0.6660
11:28:59.139   Training iter 450, batch loss 1.0944, batch acc 0.6654
11:28:59.647   Training iter 500, batch loss 1.0584, batch acc 0.6812
11:29:00.163   Training iter 550, batch loss 1.1224, batch acc 0.6660
11:29:00.688   Training iter 600, batch loss 1.0910, batch acc 0.6738
11:29:00.690 Testing @ 145 epoch...
11:29:00.731     Testing, total mean loss 1.06018, total acc 0.68130
11:29:00.731 Training @ 146 epoch...
11:29:01.271   Training iter 50, batch loss 1.0996, batch acc 0.6690
11:29:01.812   Training iter 100, batch loss 1.0854, batch acc 0.6692
11:29:02.366   Training iter 150, batch loss 1.0894, batch acc 0.6690
11:29:02.883   Training iter 200, batch loss 1.0716, batch acc 0.6810
11:29:03.388   Training iter 250, batch loss 1.0707, batch acc 0.6786
11:29:03.901   Training iter 300, batch loss 1.0842, batch acc 0.6724
11:29:04.447   Training iter 350, batch loss 1.0847, batch acc 0.6756
11:29:04.972   Training iter 400, batch loss 1.1104, batch acc 0.6608
11:29:05.485   Training iter 450, batch loss 1.0822, batch acc 0.6802
11:29:06.007   Training iter 500, batch loss 1.1253, batch acc 0.6652
11:29:06.544   Training iter 550, batch loss 1.0964, batch acc 0.6648
11:29:07.098   Training iter 600, batch loss 1.0639, batch acc 0.6782
11:29:07.100 Training @ 147 epoch...
11:29:07.625   Training iter 50, batch loss 1.1039, batch acc 0.6646
11:29:08.149   Training iter 100, batch loss 1.1154, batch acc 0.6632
11:29:08.658   Training iter 150, batch loss 1.0788, batch acc 0.6786
11:29:09.121   Training iter 200, batch loss 1.0703, batch acc 0.6790
11:29:09.578   Training iter 250, batch loss 1.0905, batch acc 0.6726
11:29:10.076   Training iter 300, batch loss 1.1038, batch acc 0.6696
11:29:10.585   Training iter 350, batch loss 1.0912, batch acc 0.6662
11:29:11.083   Training iter 400, batch loss 1.0579, batch acc 0.6870
11:29:11.576   Training iter 450, batch loss 1.0813, batch acc 0.6736
11:29:12.082   Training iter 500, batch loss 1.1169, batch acc 0.6618
11:29:12.579   Training iter 550, batch loss 1.0811, batch acc 0.6754
11:29:13.145   Training iter 600, batch loss 1.0703, batch acc 0.6752
11:29:13.147 Training @ 148 epoch...
11:29:13.748   Training iter 50, batch loss 1.0837, batch acc 0.6728
11:29:14.328   Training iter 100, batch loss 1.0890, batch acc 0.6688
11:29:14.843   Training iter 150, batch loss 1.0957, batch acc 0.6676
11:29:15.343   Training iter 200, batch loss 1.1148, batch acc 0.6594
11:29:15.852   Training iter 250, batch loss 1.0816, batch acc 0.6714
11:29:16.337   Training iter 300, batch loss 1.0970, batch acc 0.6688
11:29:16.813   Training iter 350, batch loss 1.0602, batch acc 0.6840
11:29:17.292   Training iter 400, batch loss 1.0902, batch acc 0.6762
11:29:17.782   Training iter 450, batch loss 1.0926, batch acc 0.6726
11:29:18.263   Training iter 500, batch loss 1.0878, batch acc 0.6748
11:29:18.733   Training iter 550, batch loss 1.0808, batch acc 0.6754
11:29:19.200   Training iter 600, batch loss 1.0859, batch acc 0.6744
11:29:19.202 Training @ 149 epoch...
11:29:19.697   Training iter 50, batch loss 1.0791, batch acc 0.6780
11:29:20.174   Training iter 100, batch loss 1.0709, batch acc 0.6798
11:29:20.690   Training iter 150, batch loss 1.1146, batch acc 0.6634
11:29:21.212   Training iter 200, batch loss 1.1058, batch acc 0.6700
11:29:21.732   Training iter 250, batch loss 1.0814, batch acc 0.6756
11:29:22.263   Training iter 300, batch loss 1.0580, batch acc 0.6846
11:29:22.794   Training iter 350, batch loss 1.0691, batch acc 0.6780
11:29:23.288   Training iter 400, batch loss 1.0998, batch acc 0.6644
11:29:23.746   Training iter 450, batch loss 1.1022, batch acc 0.6622
11:29:24.265   Training iter 500, batch loss 1.0817, batch acc 0.6710
11:29:24.762   Training iter 550, batch loss 1.0963, batch acc 0.6666
11:29:25.258   Training iter 600, batch loss 1.0982, batch acc 0.6708
11:29:25.260 Training @ 150 epoch...
11:29:25.762   Training iter 50, batch loss 1.1072, batch acc 0.6616
11:29:26.267   Training iter 100, batch loss 1.1010, batch acc 0.6652
11:29:26.795   Training iter 150, batch loss 1.0894, batch acc 0.6692
11:29:27.317   Training iter 200, batch loss 1.0796, batch acc 0.6798
11:29:27.853   Training iter 250, batch loss 1.0985, batch acc 0.6714
11:29:28.425   Training iter 300, batch loss 1.0931, batch acc 0.6716
11:29:28.984   Training iter 350, batch loss 1.0824, batch acc 0.6714
11:29:29.498   Training iter 400, batch loss 1.1022, batch acc 0.6650
11:29:30.052   Training iter 450, batch loss 1.0584, batch acc 0.6818
11:29:30.610   Training iter 500, batch loss 1.0710, batch acc 0.6824
11:29:31.157   Training iter 550, batch loss 1.0868, batch acc 0.6696
11:29:31.688   Training iter 600, batch loss 1.0855, batch acc 0.6768
11:29:31.689 Testing @ 150 epoch...
11:29:31.734     Testing, total mean loss 1.05926, total acc 0.68090
11:29:31.734 Training @ 151 epoch...
11:29:32.277   Training iter 50, batch loss 1.1011, batch acc 0.6678
11:29:32.805   Training iter 100, batch loss 1.0827, batch acc 0.6752
11:29:33.335   Training iter 150, batch loss 1.0836, batch acc 0.6758
11:29:33.851   Training iter 200, batch loss 1.0955, batch acc 0.6642
11:29:34.373   Training iter 250, batch loss 1.0753, batch acc 0.6806
11:29:34.902   Training iter 300, batch loss 1.1105, batch acc 0.6632
11:29:35.427   Training iter 350, batch loss 1.0911, batch acc 0.6708
11:29:35.953   Training iter 400, batch loss 1.1012, batch acc 0.6650
11:29:36.457   Training iter 450, batch loss 1.0763, batch acc 0.6752
11:29:36.970   Training iter 500, batch loss 1.1035, batch acc 0.6698
11:29:37.508   Training iter 550, batch loss 1.0646, batch acc 0.6754
11:29:38.058   Training iter 600, batch loss 1.0680, batch acc 0.6850
11:29:38.060 Training @ 152 epoch...
11:29:38.591   Training iter 50, batch loss 1.0784, batch acc 0.6708
11:29:39.116   Training iter 100, batch loss 1.0724, batch acc 0.6818
11:29:39.645   Training iter 150, batch loss 1.1051, batch acc 0.6684
11:29:40.161   Training iter 200, batch loss 1.1018, batch acc 0.6726
11:29:40.682   Training iter 250, batch loss 1.0997, batch acc 0.6686
11:29:41.194   Training iter 300, batch loss 1.0653, batch acc 0.6808
11:29:41.717   Training iter 350, batch loss 1.0818, batch acc 0.6748
11:29:42.245   Training iter 400, batch loss 1.0744, batch acc 0.6764
11:29:42.730   Training iter 450, batch loss 1.1042, batch acc 0.6640
11:29:43.236   Training iter 500, batch loss 1.0854, batch acc 0.6674
11:29:43.745   Training iter 550, batch loss 1.0839, batch acc 0.6744
11:29:44.272   Training iter 600, batch loss 1.0989, batch acc 0.6692
11:29:44.273 Training @ 153 epoch...
11:29:44.778   Training iter 50, batch loss 1.0773, batch acc 0.6724
11:29:45.282   Training iter 100, batch loss 1.0768, batch acc 0.6792
11:29:45.787   Training iter 150, batch loss 1.1138, batch acc 0.6644
11:29:46.319   Training iter 200, batch loss 1.0869, batch acc 0.6694
11:29:46.854   Training iter 250, batch loss 1.1221, batch acc 0.6650
11:29:47.398   Training iter 300, batch loss 1.0727, batch acc 0.6736
11:29:47.875   Training iter 350, batch loss 1.0823, batch acc 0.6750
11:29:48.350   Training iter 400, batch loss 1.0891, batch acc 0.6712
11:29:48.828   Training iter 450, batch loss 1.0656, batch acc 0.6798
11:29:49.300   Training iter 500, batch loss 1.0903, batch acc 0.6748
11:29:49.778   Training iter 550, batch loss 1.0787, batch acc 0.6744
11:29:50.269   Training iter 600, batch loss 1.0937, batch acc 0.6728
11:29:50.270 Training @ 154 epoch...
11:29:50.744   Training iter 50, batch loss 1.1137, batch acc 0.6610
11:29:51.221   Training iter 100, batch loss 1.0760, batch acc 0.6694
11:29:51.694   Training iter 150, batch loss 1.0706, batch acc 0.6716
11:29:52.180   Training iter 200, batch loss 1.0947, batch acc 0.6702
11:29:52.667   Training iter 250, batch loss 1.0876, batch acc 0.6762
11:29:53.140   Training iter 300, batch loss 1.0803, batch acc 0.6730
11:29:53.638   Training iter 350, batch loss 1.0674, batch acc 0.6856
11:29:54.152   Training iter 400, batch loss 1.1203, batch acc 0.6650
11:29:54.668   Training iter 450, batch loss 1.0800, batch acc 0.6796
11:29:55.193   Training iter 500, batch loss 1.0922, batch acc 0.6710
11:29:55.717   Training iter 550, batch loss 1.0772, batch acc 0.6760
11:29:56.215   Training iter 600, batch loss 1.0875, batch acc 0.6716
11:29:56.217 Training @ 155 epoch...
11:29:56.729   Training iter 50, batch loss 1.0861, batch acc 0.6678
11:29:57.244   Training iter 100, batch loss 1.0863, batch acc 0.6734
11:29:57.747   Training iter 150, batch loss 1.0776, batch acc 0.6768
11:29:58.263   Training iter 200, batch loss 1.1065, batch acc 0.6686
11:29:58.777   Training iter 250, batch loss 1.1177, batch acc 0.6554
11:29:59.306   Training iter 300, batch loss 1.0679, batch acc 0.6800
11:29:59.791   Training iter 350, batch loss 1.0780, batch acc 0.6762
11:30:00.320   Training iter 400, batch loss 1.0769, batch acc 0.6734
11:30:00.841   Training iter 450, batch loss 1.1048, batch acc 0.6670
11:30:01.378   Training iter 500, batch loss 1.0861, batch acc 0.6730
11:30:01.933   Training iter 550, batch loss 1.0831, batch acc 0.6768
11:30:02.455   Training iter 600, batch loss 1.0746, batch acc 0.6822
11:30:02.456 Testing @ 155 epoch...
11:30:02.498     Testing, total mean loss 1.05866, total acc 0.68060
11:30:02.498 Training @ 156 epoch...
11:30:03.045   Training iter 50, batch loss 1.1005, batch acc 0.6658
11:30:03.567   Training iter 100, batch loss 1.0799, batch acc 0.6752
11:30:04.155   Training iter 150, batch loss 1.0929, batch acc 0.6710
11:30:04.730   Training iter 200, batch loss 1.0530, batch acc 0.6824
11:30:05.297   Training iter 250, batch loss 1.1010, batch acc 0.6658
11:30:05.864   Training iter 300, batch loss 1.1040, batch acc 0.6682
11:30:06.444   Training iter 350, batch loss 1.1067, batch acc 0.6662
11:30:06.999   Training iter 400, batch loss 1.0691, batch acc 0.6834
11:30:07.569   Training iter 450, batch loss 1.0688, batch acc 0.6830
11:30:08.096   Training iter 500, batch loss 1.1174, batch acc 0.6572
11:30:08.623   Training iter 550, batch loss 1.0837, batch acc 0.6730
11:30:09.128   Training iter 600, batch loss 1.0668, batch acc 0.6796
11:30:09.130 Training @ 157 epoch...
11:30:09.632   Training iter 50, batch loss 1.0941, batch acc 0.6702
11:30:10.180   Training iter 100, batch loss 1.0871, batch acc 0.6776
11:30:10.731   Training iter 150, batch loss 1.0626, batch acc 0.6828
11:30:11.290   Training iter 200, batch loss 1.0685, batch acc 0.6772
11:30:11.889   Training iter 250, batch loss 1.0947, batch acc 0.6718
11:30:12.470   Training iter 300, batch loss 1.0718, batch acc 0.6794
11:30:13.038   Training iter 350, batch loss 1.1015, batch acc 0.6612
11:30:13.611   Training iter 400, batch loss 1.0859, batch acc 0.6744
11:30:14.181   Training iter 450, batch loss 1.0971, batch acc 0.6668
11:30:14.743   Training iter 500, batch loss 1.0861, batch acc 0.6760
11:30:15.291   Training iter 550, batch loss 1.0891, batch acc 0.6674
11:30:15.835   Training iter 600, batch loss 1.1035, batch acc 0.6674
11:30:15.836 Training @ 158 epoch...
11:30:16.393   Training iter 50, batch loss 1.0815, batch acc 0.6744
11:30:17.006   Training iter 100, batch loss 1.0849, batch acc 0.6708
11:30:17.518   Training iter 150, batch loss 1.1217, batch acc 0.6564
11:30:18.024   Training iter 200, batch loss 1.0838, batch acc 0.6798
11:30:18.560   Training iter 250, batch loss 1.0846, batch acc 0.6748
11:30:19.070   Training iter 300, batch loss 1.0737, batch acc 0.6790
11:30:19.594   Training iter 350, batch loss 1.0756, batch acc 0.6740
11:30:20.116   Training iter 400, batch loss 1.0758, batch acc 0.6776
11:30:20.640   Training iter 450, batch loss 1.0907, batch acc 0.6614
11:30:21.192   Training iter 500, batch loss 1.1172, batch acc 0.6674
11:30:21.755   Training iter 550, batch loss 1.0852, batch acc 0.6746
11:30:22.252   Training iter 600, batch loss 1.0652, batch acc 0.6816
11:30:22.254 Training @ 159 epoch...
11:30:22.769   Training iter 50, batch loss 1.0924, batch acc 0.6736
11:30:23.282   Training iter 100, batch loss 1.0687, batch acc 0.6772
11:30:23.779   Training iter 150, batch loss 1.0528, batch acc 0.6814
11:30:24.287   Training iter 200, batch loss 1.1017, batch acc 0.6750
11:30:24.794   Training iter 250, batch loss 1.0829, batch acc 0.6716
11:30:25.322   Training iter 300, batch loss 1.0943, batch acc 0.6724
11:30:25.835   Training iter 350, batch loss 1.0937, batch acc 0.6670
11:30:26.347   Training iter 400, batch loss 1.0789, batch acc 0.6768
11:30:26.858   Training iter 450, batch loss 1.0844, batch acc 0.6730
11:30:27.380   Training iter 500, batch loss 1.0875, batch acc 0.6728
11:30:27.889   Training iter 550, batch loss 1.0963, batch acc 0.6684
11:30:28.437   Training iter 600, batch loss 1.1049, batch acc 0.6666
11:30:28.440 Training @ 160 epoch...
11:30:29.189   Training iter 50, batch loss 1.0989, batch acc 0.6700
11:30:29.931   Training iter 100, batch loss 1.0881, batch acc 0.6752
11:30:30.608   Training iter 150, batch loss 1.0880, batch acc 0.6690
11:30:31.178   Training iter 200, batch loss 1.0708, batch acc 0.6772
11:30:31.744   Training iter 250, batch loss 1.0637, batch acc 0.6798
11:30:32.330   Training iter 300, batch loss 1.0803, batch acc 0.6700
11:30:32.878   Training iter 350, batch loss 1.0981, batch acc 0.6694
11:30:33.433   Training iter 400, batch loss 1.0619, batch acc 0.6876
11:30:33.964   Training iter 450, batch loss 1.0829, batch acc 0.6756
11:30:34.486   Training iter 500, batch loss 1.0913, batch acc 0.6702
11:30:35.013   Training iter 550, batch loss 1.1073, batch acc 0.6574
11:30:35.534   Training iter 600, batch loss 1.1052, batch acc 0.6702
11:30:35.536 Testing @ 160 epoch...
11:30:35.577     Testing, total mean loss 1.05790, total acc 0.68150
11:30:35.577 Training @ 161 epoch...
11:30:36.101   Training iter 50, batch loss 1.0932, batch acc 0.6732
11:30:36.621   Training iter 100, batch loss 1.1278, batch acc 0.6604
11:30:37.129   Training iter 150, batch loss 1.0608, batch acc 0.6796
11:30:37.637   Training iter 200, batch loss 1.1001, batch acc 0.6688
11:30:38.174   Training iter 250, batch loss 1.0669, batch acc 0.6818
11:30:38.702   Training iter 300, batch loss 1.0853, batch acc 0.6702
11:30:39.234   Training iter 350, batch loss 1.0855, batch acc 0.6714
11:30:39.780   Training iter 400, batch loss 1.0813, batch acc 0.6756
11:30:40.330   Training iter 450, batch loss 1.0683, batch acc 0.6784
11:30:40.869   Training iter 500, batch loss 1.1032, batch acc 0.6716
11:30:41.407   Training iter 550, batch loss 1.0765, batch acc 0.6750
11:30:41.927   Training iter 600, batch loss 1.0860, batch acc 0.6712
11:30:41.929 Training @ 162 epoch...
11:30:42.466   Training iter 50, batch loss 1.1038, batch acc 0.6678
11:30:43.002   Training iter 100, batch loss 1.0729, batch acc 0.6782
11:30:43.579   Training iter 150, batch loss 1.0988, batch acc 0.6688
11:30:44.178   Training iter 200, batch loss 1.0747, batch acc 0.6672
11:30:44.764   Training iter 250, batch loss 1.0655, batch acc 0.6830
11:30:45.337   Training iter 300, batch loss 1.0949, batch acc 0.6670
11:30:45.870   Training iter 350, batch loss 1.0851, batch acc 0.6820
11:30:46.414   Training iter 400, batch loss 1.0812, batch acc 0.6730
11:30:46.987   Training iter 450, batch loss 1.1003, batch acc 0.6624
11:30:47.562   Training iter 500, batch loss 1.0945, batch acc 0.6744
11:30:48.175   Training iter 550, batch loss 1.0846, batch acc 0.6708
11:30:48.725   Training iter 600, batch loss 1.0771, batch acc 0.6760
11:30:48.727 Training @ 163 epoch...
11:30:49.265   Training iter 50, batch loss 1.0933, batch acc 0.6754
11:30:49.819   Training iter 100, batch loss 1.1014, batch acc 0.6682
11:30:50.361   Training iter 150, batch loss 1.0939, batch acc 0.6690
11:30:50.882   Training iter 200, batch loss 1.0920, batch acc 0.6730
11:30:51.390   Training iter 250, batch loss 1.0767, batch acc 0.6828
11:30:51.905   Training iter 300, batch loss 1.0777, batch acc 0.6744
11:30:52.405   Training iter 350, batch loss 1.0844, batch acc 0.6692
11:30:52.910   Training iter 400, batch loss 1.0697, batch acc 0.6774
11:30:53.425   Training iter 450, batch loss 1.0881, batch acc 0.6652
11:30:53.943   Training iter 500, batch loss 1.0850, batch acc 0.6722
11:30:54.483   Training iter 550, batch loss 1.0868, batch acc 0.6722
11:30:55.006   Training iter 600, batch loss 1.0828, batch acc 0.6766
11:30:55.008 Training @ 164 epoch...
11:30:55.543   Training iter 50, batch loss 1.0965, batch acc 0.6634
11:30:56.072   Training iter 100, batch loss 1.0580, batch acc 0.6832
11:30:56.598   Training iter 150, batch loss 1.1182, batch acc 0.6608
11:30:57.137   Training iter 200, batch loss 1.1009, batch acc 0.6660
11:30:57.675   Training iter 250, batch loss 1.0421, batch acc 0.6906
11:30:58.217   Training iter 300, batch loss 1.0661, batch acc 0.6830
11:30:58.745   Training iter 350, batch loss 1.1116, batch acc 0.6682
11:30:59.284   Training iter 400, batch loss 1.1112, batch acc 0.6660
11:30:59.815   Training iter 450, batch loss 1.0743, batch acc 0.6762
11:31:00.353   Training iter 500, batch loss 1.0919, batch acc 0.6634
11:31:00.934   Training iter 550, batch loss 1.0684, batch acc 0.6802
11:31:01.534   Training iter 600, batch loss 1.0905, batch acc 0.6748
11:31:01.536 Training @ 165 epoch...
11:31:02.149   Training iter 50, batch loss 1.0686, batch acc 0.6836
11:31:02.721   Training iter 100, batch loss 1.0938, batch acc 0.6692
11:31:03.287   Training iter 150, batch loss 1.1101, batch acc 0.6598
11:31:03.848   Training iter 200, batch loss 1.1009, batch acc 0.6692
11:31:04.408   Training iter 250, batch loss 1.0730, batch acc 0.6714
11:31:04.989   Training iter 300, batch loss 1.0838, batch acc 0.6794
11:31:05.566   Training iter 350, batch loss 1.0759, batch acc 0.6750
11:31:06.130   Training iter 400, batch loss 1.0822, batch acc 0.6752
11:31:06.676   Training iter 450, batch loss 1.1007, batch acc 0.6636
11:31:07.231   Training iter 500, batch loss 1.0843, batch acc 0.6678
11:31:07.759   Training iter 550, batch loss 1.0753, batch acc 0.6798
11:31:08.275   Training iter 600, batch loss 1.0800, batch acc 0.6802
11:31:08.277 Testing @ 165 epoch...
11:31:08.318     Testing, total mean loss 1.05719, total acc 0.68180
11:31:08.318 Training @ 166 epoch...
11:31:08.853   Training iter 50, batch loss 1.0854, batch acc 0.6732
11:31:09.367   Training iter 100, batch loss 1.0807, batch acc 0.6758
11:31:09.878   Training iter 150, batch loss 1.0728, batch acc 0.6772
11:31:10.384   Training iter 200, batch loss 1.0771, batch acc 0.6706
11:31:10.894   Training iter 250, batch loss 1.0976, batch acc 0.6682
11:31:11.397   Training iter 300, batch loss 1.1079, batch acc 0.6690
11:31:11.920   Training iter 350, batch loss 1.0865, batch acc 0.6728
11:31:12.444   Training iter 400, batch loss 1.0837, batch acc 0.6796
11:31:12.973   Training iter 450, batch loss 1.0777, batch acc 0.6698
11:31:13.502   Training iter 500, batch loss 1.0831, batch acc 0.6708
11:31:14.019   Training iter 550, batch loss 1.1014, batch acc 0.6710
11:31:14.529   Training iter 600, batch loss 1.0733, batch acc 0.6782
11:31:14.530 Training @ 167 epoch...
11:31:15.048   Training iter 50, batch loss 1.1206, batch acc 0.6556
11:31:15.562   Training iter 100, batch loss 1.1023, batch acc 0.6642
11:31:16.083   Training iter 150, batch loss 1.0588, batch acc 0.6874
11:31:16.623   Training iter 200, batch loss 1.1057, batch acc 0.6670
11:31:17.154   Training iter 250, batch loss 1.1083, batch acc 0.6644
11:31:17.693   Training iter 300, batch loss 1.0813, batch acc 0.6746
11:31:18.185   Training iter 350, batch loss 1.0674, batch acc 0.6806
11:31:18.668   Training iter 400, batch loss 1.0902, batch acc 0.6664
11:31:19.169   Training iter 450, batch loss 1.0894, batch acc 0.6734
11:31:19.682   Training iter 500, batch loss 1.0756, batch acc 0.6822
11:31:20.199   Training iter 550, batch loss 1.0621, batch acc 0.6800
11:31:20.730   Training iter 600, batch loss 1.0637, batch acc 0.6820
11:31:20.732 Training @ 168 epoch...
11:31:21.284   Training iter 50, batch loss 1.0919, batch acc 0.6710
11:31:21.829   Training iter 100, batch loss 1.0830, batch acc 0.6732
11:31:22.369   Training iter 150, batch loss 1.1060, batch acc 0.6606
11:31:22.883   Training iter 200, batch loss 1.0660, batch acc 0.6786
11:31:23.397   Training iter 250, batch loss 1.0822, batch acc 0.6716
11:31:23.933   Training iter 300, batch loss 1.0534, batch acc 0.6842
11:31:24.469   Training iter 350, batch loss 1.0825, batch acc 0.6744
11:31:24.994   Training iter 400, batch loss 1.0816, batch acc 0.6764
11:31:25.524   Training iter 450, batch loss 1.1056, batch acc 0.6668
11:31:26.040   Training iter 500, batch loss 1.0963, batch acc 0.6698
11:31:26.563   Training iter 550, batch loss 1.0965, batch acc 0.6716
11:31:27.085   Training iter 600, batch loss 1.0790, batch acc 0.6776
11:31:27.087 Training @ 169 epoch...
11:31:27.623   Training iter 50, batch loss 1.0839, batch acc 0.6724
11:31:28.175   Training iter 100, batch loss 1.0858, batch acc 0.6686
11:31:28.699   Training iter 150, batch loss 1.0823, batch acc 0.6742
11:31:29.218   Training iter 200, batch loss 1.0985, batch acc 0.6716
11:31:29.742   Training iter 250, batch loss 1.0705, batch acc 0.6788
11:31:30.292   Training iter 300, batch loss 1.0937, batch acc 0.6694
11:31:30.844   Training iter 350, batch loss 1.0727, batch acc 0.6754
11:31:31.378   Training iter 400, batch loss 1.0950, batch acc 0.6646
11:31:31.921   Training iter 450, batch loss 1.0814, batch acc 0.6812
11:31:32.458   Training iter 500, batch loss 1.1054, batch acc 0.6648
11:31:32.985   Training iter 550, batch loss 1.0677, batch acc 0.6842
11:31:33.515   Training iter 600, batch loss 1.0854, batch acc 0.6722
11:31:33.516 Training @ 170 epoch...
11:31:34.069   Training iter 50, batch loss 1.0866, batch acc 0.6674
11:31:34.627   Training iter 100, batch loss 1.0720, batch acc 0.6754
11:31:35.176   Training iter 150, batch loss 1.0880, batch acc 0.6704
11:31:35.720   Training iter 200, batch loss 1.0904, batch acc 0.6746
11:31:36.251   Training iter 250, batch loss 1.0758, batch acc 0.6760
11:31:36.761   Training iter 300, batch loss 1.0765, batch acc 0.6786
11:31:37.272   Training iter 350, batch loss 1.0831, batch acc 0.6740
11:31:37.797   Training iter 400, batch loss 1.0834, batch acc 0.6808
11:31:38.329   Training iter 450, batch loss 1.0927, batch acc 0.6692
11:31:38.834   Training iter 500, batch loss 1.0995, batch acc 0.6634
11:31:39.349   Training iter 550, batch loss 1.0856, batch acc 0.6738
11:31:39.861   Training iter 600, batch loss 1.0869, batch acc 0.6772
11:31:39.862 Testing @ 170 epoch...
11:31:39.904     Testing, total mean loss 1.05682, total acc 0.68180
11:31:39.904 Training @ 171 epoch...
11:31:40.445   Training iter 50, batch loss 1.0966, batch acc 0.6668
11:31:41.011   Training iter 100, batch loss 1.0792, batch acc 0.6742
11:31:41.567   Training iter 150, batch loss 1.0825, batch acc 0.6730
11:31:42.152   Training iter 200, batch loss 1.1116, batch acc 0.6646
11:31:42.665   Training iter 250, batch loss 1.0755, batch acc 0.6836
11:31:43.167   Training iter 300, batch loss 1.0715, batch acc 0.6822
11:31:43.645   Training iter 350, batch loss 1.0982, batch acc 0.6648
11:31:44.132   Training iter 400, batch loss 1.0946, batch acc 0.6734
11:31:44.636   Training iter 450, batch loss 1.0759, batch acc 0.6846
11:31:45.169   Training iter 500, batch loss 1.0819, batch acc 0.6678
11:31:45.695   Training iter 550, batch loss 1.0781, batch acc 0.6662
11:31:46.213   Training iter 600, batch loss 1.0737, batch acc 0.6784
11:31:46.215 Training @ 172 epoch...
11:31:46.720   Training iter 50, batch loss 1.1017, batch acc 0.6676
11:31:47.232   Training iter 100, batch loss 1.0788, batch acc 0.6782
11:31:47.734   Training iter 150, batch loss 1.0829, batch acc 0.6758
11:31:48.239   Training iter 200, batch loss 1.0309, batch acc 0.6854
11:31:48.765   Training iter 250, batch loss 1.0943, batch acc 0.6636
11:31:49.295   Training iter 300, batch loss 1.0994, batch acc 0.6636
11:31:49.841   Training iter 350, batch loss 1.1100, batch acc 0.6696
11:31:50.372   Training iter 400, batch loss 1.0932, batch acc 0.6736
11:31:50.894   Training iter 450, batch loss 1.0726, batch acc 0.6784
11:31:51.410   Training iter 500, batch loss 1.1049, batch acc 0.6644
11:31:51.938   Training iter 550, batch loss 1.0613, batch acc 0.6838
11:31:52.464   Training iter 600, batch loss 1.0880, batch acc 0.6750
11:31:52.466 Training @ 173 epoch...
11:31:52.982   Training iter 50, batch loss 1.0761, batch acc 0.6696
11:31:53.479   Training iter 100, batch loss 1.0814, batch acc 0.6686
11:31:53.982   Training iter 150, batch loss 1.0810, batch acc 0.6710
11:31:54.490   Training iter 200, batch loss 1.0822, batch acc 0.6786
11:31:55.010   Training iter 250, batch loss 1.0866, batch acc 0.6716
11:31:55.539   Training iter 300, batch loss 1.0995, batch acc 0.6684
11:31:56.050   Training iter 350, batch loss 1.1150, batch acc 0.6616
11:31:56.553   Training iter 400, batch loss 1.0636, batch acc 0.6868
11:31:57.062   Training iter 450, batch loss 1.0756, batch acc 0.6818
11:31:57.579   Training iter 500, batch loss 1.0965, batch acc 0.6676
11:31:58.082   Training iter 550, batch loss 1.1073, batch acc 0.6642
11:31:58.581   Training iter 600, batch loss 1.0514, batch acc 0.6898
11:31:58.583 Training @ 174 epoch...
11:31:59.084   Training iter 50, batch loss 1.1003, batch acc 0.6692
11:31:59.580   Training iter 100, batch loss 1.0985, batch acc 0.6690
11:32:00.097   Training iter 150, batch loss 1.0780, batch acc 0.6786
11:32:00.606   Training iter 200, batch loss 1.0845, batch acc 0.6688
11:32:01.126   Training iter 250, batch loss 1.0765, batch acc 0.6760
11:32:01.656   Training iter 300, batch loss 1.0949, batch acc 0.6732
11:32:02.182   Training iter 350, batch loss 1.0954, batch acc 0.6726
11:32:02.681   Training iter 400, batch loss 1.0677, batch acc 0.6824
11:32:03.173   Training iter 450, batch loss 1.0791, batch acc 0.6750
11:32:03.652   Training iter 500, batch loss 1.0799, batch acc 0.6720
11:32:04.152   Training iter 550, batch loss 1.0731, batch acc 0.6756
11:32:04.660   Training iter 600, batch loss 1.0874, batch acc 0.6702
11:32:04.662 Training @ 175 epoch...
11:32:05.167   Training iter 50, batch loss 1.0814, batch acc 0.6778
11:32:05.675   Training iter 100, batch loss 1.0611, batch acc 0.6808
11:32:06.206   Training iter 150, batch loss 1.0840, batch acc 0.6774
11:32:06.709   Training iter 200, batch loss 1.0950, batch acc 0.6576
11:32:07.232   Training iter 250, batch loss 1.0993, batch acc 0.6636
11:32:07.786   Training iter 300, batch loss 1.0730, batch acc 0.6770
11:32:08.335   Training iter 350, batch loss 1.0930, batch acc 0.6726
11:32:08.878   Training iter 400, batch loss 1.0756, batch acc 0.6790
11:32:09.421   Training iter 450, batch loss 1.0861, batch acc 0.6712
11:32:09.973   Training iter 500, batch loss 1.1108, batch acc 0.6594
11:32:10.508   Training iter 550, batch loss 1.0810, batch acc 0.6830
11:32:11.026   Training iter 600, batch loss 1.0734, batch acc 0.6788
11:32:11.028 Testing @ 175 epoch...
11:32:11.069     Testing, total mean loss 1.05613, total acc 0.68220
11:32:11.069 Training @ 176 epoch...
11:32:11.600   Training iter 50, batch loss 1.0729, batch acc 0.6798
11:32:12.118   Training iter 100, batch loss 1.0977, batch acc 0.6654
11:32:12.611   Training iter 150, batch loss 1.0857, batch acc 0.6722
11:32:13.101   Training iter 200, batch loss 1.0985, batch acc 0.6686
11:32:13.589   Training iter 250, batch loss 1.0871, batch acc 0.6754
11:32:14.092   Training iter 300, batch loss 1.1035, batch acc 0.6674
11:32:14.593   Training iter 350, batch loss 1.0761, batch acc 0.6774
11:32:15.089   Training iter 400, batch loss 1.0618, batch acc 0.6828
11:32:15.589   Training iter 450, batch loss 1.0860, batch acc 0.6748
11:32:16.108   Training iter 500, batch loss 1.0904, batch acc 0.6686
11:32:16.619   Training iter 550, batch loss 1.0756, batch acc 0.6784
11:32:17.124   Training iter 600, batch loss 1.0772, batch acc 0.6750
11:32:17.125 Training @ 177 epoch...
11:32:17.654   Training iter 50, batch loss 1.0941, batch acc 0.6692
11:32:18.195   Training iter 100, batch loss 1.0757, batch acc 0.6726
11:32:18.735   Training iter 150, batch loss 1.0627, batch acc 0.6850
11:32:19.249   Training iter 200, batch loss 1.0685, batch acc 0.6766
11:32:19.770   Training iter 250, batch loss 1.0886, batch acc 0.6778
11:32:20.486   Training iter 300, batch loss 1.1049, batch acc 0.6648
11:32:21.218   Training iter 350, batch loss 1.1034, batch acc 0.6636
11:32:21.923   Training iter 400, batch loss 1.0682, batch acc 0.6806
11:32:22.512   Training iter 450, batch loss 1.0843, batch acc 0.6754
11:32:23.052   Training iter 500, batch loss 1.0914, batch acc 0.6684
11:32:23.590   Training iter 550, batch loss 1.0676, batch acc 0.6776
11:32:24.145   Training iter 600, batch loss 1.1015, batch acc 0.6682
11:32:24.146 Training @ 178 epoch...
11:32:24.714   Training iter 50, batch loss 1.0977, batch acc 0.6646
11:32:25.275   Training iter 100, batch loss 1.0771, batch acc 0.6792
11:32:25.838   Training iter 150, batch loss 1.0784, batch acc 0.6742
11:32:26.400   Training iter 200, batch loss 1.0785, batch acc 0.6816
11:32:26.943   Training iter 250, batch loss 1.1212, batch acc 0.6594
11:32:27.488   Training iter 300, batch loss 1.0699, batch acc 0.6792
11:32:28.033   Training iter 350, batch loss 1.0913, batch acc 0.6710
11:32:28.588   Training iter 400, batch loss 1.0799, batch acc 0.6732
11:32:29.152   Training iter 450, batch loss 1.0885, batch acc 0.6674
11:32:29.716   Training iter 500, batch loss 1.0661, batch acc 0.6884
11:32:30.246   Training iter 550, batch loss 1.0757, batch acc 0.6776
11:32:30.749   Training iter 600, batch loss 1.0853, batch acc 0.6658
11:32:30.750 Training @ 179 epoch...
11:32:31.287   Training iter 50, batch loss 1.0804, batch acc 0.6684
11:32:31.793   Training iter 100, batch loss 1.0696, batch acc 0.6798
11:32:32.325   Training iter 150, batch loss 1.1063, batch acc 0.6666
11:32:32.850   Training iter 200, batch loss 1.1068, batch acc 0.6650
11:32:33.375   Training iter 250, batch loss 1.0858, batch acc 0.6692
11:32:33.887   Training iter 300, batch loss 1.0873, batch acc 0.6774
11:32:34.416   Training iter 350, batch loss 1.1004, batch acc 0.6662
11:32:34.930   Training iter 400, batch loss 1.0545, batch acc 0.6792
11:32:35.448   Training iter 450, batch loss 1.0846, batch acc 0.6802
11:32:35.969   Training iter 500, batch loss 1.0724, batch acc 0.6800
11:32:36.516   Training iter 550, batch loss 1.0846, batch acc 0.6780
11:32:37.049   Training iter 600, batch loss 1.0758, batch acc 0.6712
11:32:37.051 Training @ 180 epoch...
11:32:37.586   Training iter 50, batch loss 1.0797, batch acc 0.6740
11:32:38.116   Training iter 100, batch loss 1.0778, batch acc 0.6760
11:32:38.667   Training iter 150, batch loss 1.1218, batch acc 0.6608
11:32:39.229   Training iter 200, batch loss 1.0935, batch acc 0.6700
11:32:39.806   Training iter 250, batch loss 1.0856, batch acc 0.6736
11:32:40.379   Training iter 300, batch loss 1.0961, batch acc 0.6694
11:32:40.941   Training iter 350, batch loss 1.0556, batch acc 0.6888
11:32:41.496   Training iter 400, batch loss 1.0477, batch acc 0.6860
11:32:42.061   Training iter 450, batch loss 1.0856, batch acc 0.6736
11:32:42.614   Training iter 500, batch loss 1.0741, batch acc 0.6784
11:32:43.147   Training iter 550, batch loss 1.0982, batch acc 0.6622
11:32:43.690   Training iter 600, batch loss 1.0915, batch acc 0.6730
11:32:43.692 Testing @ 180 epoch...
11:32:43.734     Testing, total mean loss 1.05572, total acc 0.68240
11:32:43.734 Training @ 181 epoch...
11:32:44.284   Training iter 50, batch loss 1.0848, batch acc 0.6716
11:32:44.818   Training iter 100, batch loss 1.0862, batch acc 0.6718
11:32:45.350   Training iter 150, batch loss 1.0802, batch acc 0.6728
11:32:45.895   Training iter 200, batch loss 1.0833, batch acc 0.6712
11:32:46.444   Training iter 250, batch loss 1.0843, batch acc 0.6714
11:32:46.972   Training iter 300, batch loss 1.0766, batch acc 0.6756
11:32:47.493   Training iter 350, batch loss 1.0870, batch acc 0.6772
11:32:48.055   Training iter 400, batch loss 1.0870, batch acc 0.6748
11:32:48.614   Training iter 450, batch loss 1.0814, batch acc 0.6736
11:32:49.172   Training iter 500, batch loss 1.0963, batch acc 0.6756
11:32:49.736   Training iter 550, batch loss 1.0780, batch acc 0.6734
11:32:50.294   Training iter 600, batch loss 1.0809, batch acc 0.6770
11:32:50.296 Training @ 182 epoch...
11:32:50.835   Training iter 50, batch loss 1.0988, batch acc 0.6652
11:32:51.375   Training iter 100, batch loss 1.0703, batch acc 0.6786
11:32:51.911   Training iter 150, batch loss 1.0953, batch acc 0.6720
11:32:52.447   Training iter 200, batch loss 1.0959, batch acc 0.6712
11:32:52.998   Training iter 250, batch loss 1.0914, batch acc 0.6720
11:32:53.553   Training iter 300, batch loss 1.0703, batch acc 0.6820
11:32:54.110   Training iter 350, batch loss 1.0945, batch acc 0.6728
11:32:54.692   Training iter 400, batch loss 1.0886, batch acc 0.6660
11:32:55.290   Training iter 450, batch loss 1.0582, batch acc 0.6836
11:32:55.872   Training iter 500, batch loss 1.0987, batch acc 0.6630
11:32:56.463   Training iter 550, batch loss 1.0560, batch acc 0.6864
11:32:57.043   Training iter 600, batch loss 1.0868, batch acc 0.6730
11:32:57.045 Training @ 183 epoch...
11:32:57.633   Training iter 50, batch loss 1.0741, batch acc 0.6704
11:32:58.208   Training iter 100, batch loss 1.0897, batch acc 0.6706
11:32:58.770   Training iter 150, batch loss 1.0965, batch acc 0.6728
11:32:59.312   Training iter 200, batch loss 1.0957, batch acc 0.6752
11:32:59.837   Training iter 250, batch loss 1.0611, batch acc 0.6784
11:33:00.380   Training iter 300, batch loss 1.0888, batch acc 0.6696
11:33:00.920   Training iter 350, batch loss 1.0981, batch acc 0.6652
11:33:01.469   Training iter 400, batch loss 1.0658, batch acc 0.6796
11:33:02.037   Training iter 450, batch loss 1.0880, batch acc 0.6764
11:33:02.605   Training iter 500, batch loss 1.0745, batch acc 0.6820
11:33:03.170   Training iter 550, batch loss 1.0889, batch acc 0.6724
11:33:03.730   Training iter 600, batch loss 1.0823, batch acc 0.6748
11:33:03.732 Training @ 184 epoch...
11:33:04.303   Training iter 50, batch loss 1.0882, batch acc 0.6730
11:33:04.868   Training iter 100, batch loss 1.0952, batch acc 0.6714
11:33:05.412   Training iter 150, batch loss 1.0810, batch acc 0.6738
11:33:05.925   Training iter 200, batch loss 1.0900, batch acc 0.6748
11:33:06.470   Training iter 250, batch loss 1.0848, batch acc 0.6696
11:33:06.990   Training iter 300, batch loss 1.0604, batch acc 0.6822
11:33:07.554   Training iter 350, batch loss 1.0731, batch acc 0.6770
11:33:08.109   Training iter 400, batch loss 1.1018, batch acc 0.6648
11:33:08.650   Training iter 450, batch loss 1.0753, batch acc 0.6728
11:33:09.200   Training iter 500, batch loss 1.0732, batch acc 0.6794
11:33:09.747   Training iter 550, batch loss 1.0898, batch acc 0.6722
11:33:10.312   Training iter 600, batch loss 1.0893, batch acc 0.6738
11:33:10.314 Training @ 185 epoch...
11:33:10.869   Training iter 50, batch loss 1.0795, batch acc 0.6800
11:33:11.402   Training iter 100, batch loss 1.1110, batch acc 0.6652
11:33:11.952   Training iter 150, batch loss 1.0629, batch acc 0.6836
11:33:12.531   Training iter 200, batch loss 1.1055, batch acc 0.6626
11:33:13.136   Training iter 250, batch loss 1.0697, batch acc 0.6812
11:33:13.710   Training iter 300, batch loss 1.0794, batch acc 0.6720
11:33:14.276   Training iter 350, batch loss 1.0864, batch acc 0.6708
11:33:14.823   Training iter 400, batch loss 1.0975, batch acc 0.6658
11:33:15.354   Training iter 450, batch loss 1.0593, batch acc 0.6862
11:33:15.876   Training iter 500, batch loss 1.0824, batch acc 0.6728
11:33:16.415   Training iter 550, batch loss 1.0979, batch acc 0.6670
11:33:16.954   Training iter 600, batch loss 1.0694, batch acc 0.6760
11:33:16.956 Testing @ 185 epoch...
11:33:16.998     Testing, total mean loss 1.05520, total acc 0.68270
11:33:16.998 Training @ 186 epoch...
11:33:17.535   Training iter 50, batch loss 1.0841, batch acc 0.6696
11:33:18.052   Training iter 100, batch loss 1.0797, batch acc 0.6752
11:33:18.566   Training iter 150, batch loss 1.0823, batch acc 0.6730
11:33:19.084   Training iter 200, batch loss 1.0795, batch acc 0.6746
11:33:19.609   Training iter 250, batch loss 1.1018, batch acc 0.6736
11:33:20.153   Training iter 300, batch loss 1.0884, batch acc 0.6804
11:33:20.686   Training iter 350, batch loss 1.0992, batch acc 0.6650
11:33:21.213   Training iter 400, batch loss 1.0624, batch acc 0.6762
11:33:21.750   Training iter 450, batch loss 1.0650, batch acc 0.6800
11:33:22.308   Training iter 500, batch loss 1.0923, batch acc 0.6742
11:33:22.868   Training iter 550, batch loss 1.0636, batch acc 0.6774
11:33:23.442   Training iter 600, batch loss 1.1016, batch acc 0.6696
11:33:23.444 Training @ 187 epoch...
11:33:24.005   Training iter 50, batch loss 1.0814, batch acc 0.6694
11:33:24.538   Training iter 100, batch loss 1.0694, batch acc 0.6828
11:33:25.062   Training iter 150, batch loss 1.0782, batch acc 0.6762
11:33:25.599   Training iter 200, batch loss 1.0815, batch acc 0.6762
11:33:26.149   Training iter 250, batch loss 1.0849, batch acc 0.6766
11:33:26.687   Training iter 300, batch loss 1.1185, batch acc 0.6586
11:33:27.236   Training iter 350, batch loss 1.0928, batch acc 0.6742
11:33:27.783   Training iter 400, batch loss 1.0864, batch acc 0.6722
11:33:28.323   Training iter 450, batch loss 1.0578, batch acc 0.6856
11:33:28.830   Training iter 500, batch loss 1.0908, batch acc 0.6684
11:33:29.342   Training iter 550, batch loss 1.0820, batch acc 0.6720
11:33:29.851   Training iter 600, batch loss 1.0749, batch acc 0.6740
11:33:29.853 Training @ 188 epoch...
11:33:30.380   Training iter 50, batch loss 1.0936, batch acc 0.6702
11:33:30.896   Training iter 100, batch loss 1.0978, batch acc 0.6662
11:33:31.433   Training iter 150, batch loss 1.0803, batch acc 0.6742
11:33:32.009   Training iter 200, batch loss 1.0622, batch acc 0.6816
11:33:32.574   Training iter 250, batch loss 1.0961, batch acc 0.6686
11:33:33.110   Training iter 300, batch loss 1.0898, batch acc 0.6764
11:33:33.594   Training iter 350, batch loss 1.0593, batch acc 0.6870
11:33:34.086   Training iter 400, batch loss 1.0734, batch acc 0.6750
11:33:34.609   Training iter 450, batch loss 1.0805, batch acc 0.6736
11:33:35.141   Training iter 500, batch loss 1.0882, batch acc 0.6732
11:33:35.677   Training iter 550, batch loss 1.0858, batch acc 0.6724
11:33:36.224   Training iter 600, batch loss 1.0904, batch acc 0.6706
11:33:36.226 Training @ 189 epoch...
11:33:36.762   Training iter 50, batch loss 1.0655, batch acc 0.6840
11:33:37.290   Training iter 100, batch loss 1.0813, batch acc 0.6722
11:33:37.816   Training iter 150, batch loss 1.0630, batch acc 0.6794
11:33:38.367   Training iter 200, batch loss 1.1213, batch acc 0.6520
11:33:38.985   Training iter 250, batch loss 1.0748, batch acc 0.6750
11:33:39.513   Training iter 300, batch loss 1.0696, batch acc 0.6794
11:33:40.055   Training iter 350, batch loss 1.0731, batch acc 0.6862
11:33:40.602   Training iter 400, batch loss 1.0943, batch acc 0.6654
11:33:41.147   Training iter 450, batch loss 1.1071, batch acc 0.6646
11:33:41.690   Training iter 500, batch loss 1.1067, batch acc 0.6666
11:33:42.255   Training iter 550, batch loss 1.0605, batch acc 0.6844
11:33:42.799   Training iter 600, batch loss 1.0791, batch acc 0.6740
11:33:42.801 Training @ 190 epoch...
11:33:43.389   Training iter 50, batch loss 1.0796, batch acc 0.6754
11:33:43.986   Training iter 100, batch loss 1.0940, batch acc 0.6712
11:33:44.595   Training iter 150, batch loss 1.0863, batch acc 0.6750
11:33:45.191   Training iter 200, batch loss 1.0999, batch acc 0.6714
11:33:45.734   Training iter 250, batch loss 1.0705, batch acc 0.6784
11:33:46.282   Training iter 300, batch loss 1.0935, batch acc 0.6684
11:33:46.834   Training iter 350, batch loss 1.0651, batch acc 0.6794
11:33:47.375   Training iter 400, batch loss 1.0877, batch acc 0.6770
11:33:47.915   Training iter 450, batch loss 1.0790, batch acc 0.6684
11:33:48.463   Training iter 500, batch loss 1.0767, batch acc 0.6798
11:33:48.988   Training iter 550, batch loss 1.0653, batch acc 0.6784
11:33:49.526   Training iter 600, batch loss 1.0981, batch acc 0.6652
11:33:49.528 Testing @ 190 epoch...
11:33:49.571     Testing, total mean loss 1.05463, total acc 0.68180
11:33:49.571 Training @ 191 epoch...
11:33:50.126   Training iter 50, batch loss 1.1211, batch acc 0.6608
11:33:50.670   Training iter 100, batch loss 1.0794, batch acc 0.6766
11:33:51.219   Training iter 150, batch loss 1.0836, batch acc 0.6724
11:33:51.775   Training iter 200, batch loss 1.0582, batch acc 0.6832
11:33:52.327   Training iter 250, batch loss 1.0817, batch acc 0.6770
11:33:52.870   Training iter 300, batch loss 1.0676, batch acc 0.6788
11:33:53.420   Training iter 350, batch loss 1.1018, batch acc 0.6674
11:33:53.962   Training iter 400, batch loss 1.0866, batch acc 0.6720
11:33:54.505   Training iter 450, batch loss 1.0949, batch acc 0.6656
11:33:55.053   Training iter 500, batch loss 1.0669, batch acc 0.6768
11:33:55.595   Training iter 550, batch loss 1.0936, batch acc 0.6764
11:33:56.134   Training iter 600, batch loss 1.0589, batch acc 0.6798
11:33:56.136 Training @ 192 epoch...
11:33:56.686   Training iter 50, batch loss 1.0886, batch acc 0.6768
11:33:57.265   Training iter 100, batch loss 1.0825, batch acc 0.6748
11:33:57.787   Training iter 150, batch loss 1.0762, batch acc 0.6748
11:33:58.321   Training iter 200, batch loss 1.0694, batch acc 0.6778
11:33:58.857   Training iter 250, batch loss 1.0769, batch acc 0.6738
11:33:59.392   Training iter 300, batch loss 1.0683, batch acc 0.6772
11:33:59.917   Training iter 350, batch loss 1.1009, batch acc 0.6712
11:34:00.471   Training iter 400, batch loss 1.1007, batch acc 0.6640
11:34:01.029   Training iter 450, batch loss 1.0696, batch acc 0.6810
11:34:01.598   Training iter 500, batch loss 1.0747, batch acc 0.6774
11:34:02.179   Training iter 550, batch loss 1.0700, batch acc 0.6790
11:34:02.725   Training iter 600, batch loss 1.1155, batch acc 0.6592
11:34:02.726 Training @ 193 epoch...
11:34:03.267   Training iter 50, batch loss 1.0939, batch acc 0.6738
11:34:03.806   Training iter 100, batch loss 1.0885, batch acc 0.6716
11:34:04.336   Training iter 150, batch loss 1.0650, batch acc 0.6792
11:34:04.857   Training iter 200, batch loss 1.0958, batch acc 0.6690
11:34:05.414   Training iter 250, batch loss 1.0485, batch acc 0.6900
11:34:05.972   Training iter 300, batch loss 1.0740, batch acc 0.6694
11:34:06.530   Training iter 350, batch loss 1.0814, batch acc 0.6782
11:34:07.093   Training iter 400, batch loss 1.0851, batch acc 0.6724
11:34:07.658   Training iter 450, batch loss 1.1145, batch acc 0.6626
11:34:08.209   Training iter 500, batch loss 1.0680, batch acc 0.6782
11:34:08.746   Training iter 550, batch loss 1.0878, batch acc 0.6676
11:34:09.270   Training iter 600, batch loss 1.0897, batch acc 0.6760
11:34:09.272 Training @ 194 epoch...
11:34:09.774   Training iter 50, batch loss 1.0733, batch acc 0.6772
11:34:10.284   Training iter 100, batch loss 1.1106, batch acc 0.6666
11:34:10.767   Training iter 150, batch loss 1.1127, batch acc 0.6682
11:34:11.245   Training iter 200, batch loss 1.0823, batch acc 0.6690
11:34:11.766   Training iter 250, batch loss 1.0839, batch acc 0.6794
11:34:12.277   Training iter 300, batch loss 1.0847, batch acc 0.6676
11:34:12.779   Training iter 350, batch loss 1.0877, batch acc 0.6742
11:34:13.310   Training iter 400, batch loss 1.0818, batch acc 0.6782
11:34:13.854   Training iter 450, batch loss 1.0685, batch acc 0.6738
11:34:14.406   Training iter 500, batch loss 1.0758, batch acc 0.6782
11:34:14.930   Training iter 550, batch loss 1.0840, batch acc 0.6718
11:34:15.471   Training iter 600, batch loss 1.0459, batch acc 0.6880
11:34:15.473 Training @ 195 epoch...
11:34:16.047   Training iter 50, batch loss 1.0986, batch acc 0.6640
11:34:16.610   Training iter 100, batch loss 1.0959, batch acc 0.6694
11:34:17.171   Training iter 150, batch loss 1.0822, batch acc 0.6700
11:34:17.711   Training iter 200, batch loss 1.1141, batch acc 0.6584
11:34:18.250   Training iter 250, batch loss 1.0782, batch acc 0.6766
11:34:18.789   Training iter 300, batch loss 1.0657, batch acc 0.6852
11:34:19.328   Training iter 350, batch loss 1.0737, batch acc 0.6790
11:34:19.863   Training iter 400, batch loss 1.0991, batch acc 0.6680
11:34:20.400   Training iter 450, batch loss 1.0719, batch acc 0.6790
11:34:20.928   Training iter 500, batch loss 1.0744, batch acc 0.6736
11:34:21.448   Training iter 550, batch loss 1.0862, batch acc 0.6784
11:34:21.962   Training iter 600, batch loss 1.0502, batch acc 0.6886
11:34:21.964 Testing @ 195 epoch...
11:34:22.007     Testing, total mean loss 1.05426, total acc 0.68210
11:34:22.007 Training @ 196 epoch...
11:34:22.534   Training iter 50, batch loss 1.0609, batch acc 0.6800
11:34:23.086   Training iter 100, batch loss 1.1034, batch acc 0.6640
11:34:23.603   Training iter 150, batch loss 1.0815, batch acc 0.6724
11:34:24.093   Training iter 200, batch loss 1.0990, batch acc 0.6706
11:34:24.606   Training iter 250, batch loss 1.0906, batch acc 0.6704
11:34:25.132   Training iter 300, batch loss 1.0635, batch acc 0.6832
11:34:25.646   Training iter 350, batch loss 1.0916, batch acc 0.6742
11:34:26.169   Training iter 400, batch loss 1.0895, batch acc 0.6746
11:34:26.663   Training iter 450, batch loss 1.0673, batch acc 0.6780
11:34:27.190   Training iter 500, batch loss 1.0678, batch acc 0.6800
11:34:27.703   Training iter 550, batch loss 1.0754, batch acc 0.6754
11:34:28.231   Training iter 600, batch loss 1.0987, batch acc 0.6676
11:34:28.233 Training @ 197 epoch...
11:34:28.819   Training iter 50, batch loss 1.0825, batch acc 0.6782
11:34:29.403   Training iter 100, batch loss 1.0998, batch acc 0.6682
11:34:29.983   Training iter 150, batch loss 1.0984, batch acc 0.6646
11:34:30.522   Training iter 200, batch loss 1.0975, batch acc 0.6734
11:34:31.077   Training iter 250, batch loss 1.0802, batch acc 0.6728
11:34:31.631   Training iter 300, batch loss 1.0824, batch acc 0.6702
11:34:32.182   Training iter 350, batch loss 1.0852, batch acc 0.6770
11:34:32.728   Training iter 400, batch loss 1.0833, batch acc 0.6772
11:34:33.277   Training iter 450, batch loss 1.0485, batch acc 0.6858
11:34:33.801   Training iter 500, batch loss 1.0855, batch acc 0.6702
11:34:34.310   Training iter 550, batch loss 1.0810, batch acc 0.6752
11:34:34.798   Training iter 600, batch loss 1.0641, batch acc 0.6784
11:34:34.800 Training @ 198 epoch...
11:34:35.275   Training iter 50, batch loss 1.0834, batch acc 0.6746
11:34:35.744   Training iter 100, batch loss 1.0805, batch acc 0.6786
11:34:36.226   Training iter 150, batch loss 1.0850, batch acc 0.6762
11:34:36.700   Training iter 200, batch loss 1.0549, batch acc 0.6842
11:34:37.184   Training iter 250, batch loss 1.1003, batch acc 0.6688
11:34:37.650   Training iter 300, batch loss 1.0768, batch acc 0.6790
11:34:38.168   Training iter 350, batch loss 1.1184, batch acc 0.6608
11:34:38.682   Training iter 400, batch loss 1.0859, batch acc 0.6674
11:34:39.172   Training iter 450, batch loss 1.0641, batch acc 0.6852
11:34:39.673   Training iter 500, batch loss 1.0865, batch acc 0.6646
11:34:40.183   Training iter 550, batch loss 1.0785, batch acc 0.6776
11:34:40.695   Training iter 600, batch loss 1.0728, batch acc 0.6698
11:34:40.697 Training @ 199 epoch...
11:34:41.209   Training iter 50, batch loss 1.0700, batch acc 0.6840
11:34:41.727   Training iter 100, batch loss 1.0782, batch acc 0.6694
11:34:42.236   Training iter 150, batch loss 1.0852, batch acc 0.6738
11:34:42.760   Training iter 200, batch loss 1.0896, batch acc 0.6776
11:34:43.300   Training iter 250, batch loss 1.0926, batch acc 0.6732
11:34:43.830   Training iter 300, batch loss 1.0649, batch acc 0.6820
11:34:44.380   Training iter 350, batch loss 1.0911, batch acc 0.6718
11:34:44.931   Training iter 400, batch loss 1.0749, batch acc 0.6748
11:34:45.493   Training iter 450, batch loss 1.0874, batch acc 0.6682
11:34:46.055   Training iter 500, batch loss 1.0826, batch acc 0.6704
11:34:46.609   Training iter 550, batch loss 1.0789, batch acc 0.6754
11:34:47.154   Training iter 600, batch loss 1.0910, batch acc 0.6730
11:34:47.156 Training @ 200 epoch...
11:34:47.707   Training iter 50, batch loss 1.0896, batch acc 0.6782
11:34:48.253   Training iter 100, batch loss 1.0918, batch acc 0.6706
11:34:48.794   Training iter 150, batch loss 1.0819, batch acc 0.6740
11:34:49.341   Training iter 200, batch loss 1.0694, batch acc 0.6752
11:34:49.879   Training iter 250, batch loss 1.0851, batch acc 0.6740
11:34:50.377   Training iter 300, batch loss 1.0810, batch acc 0.6764
11:34:50.853   Training iter 350, batch loss 1.0921, batch acc 0.6742
11:34:51.341   Training iter 400, batch loss 1.0876, batch acc 0.6662
11:34:51.826   Training iter 450, batch loss 1.0632, batch acc 0.6792
11:34:52.315   Training iter 500, batch loss 1.0916, batch acc 0.6744
11:34:52.815   Training iter 550, batch loss 1.0710, batch acc 0.6784
11:34:53.316   Training iter 600, batch loss 1.0809, batch acc 0.6696
11:34:53.317 Testing @ 200 epoch...
11:34:53.359     Testing, total mean loss 1.05409, total acc 0.68270
11:34:53.359 Plot @ 200 epoch...
11:34:53.359 Training @ 201 epoch...
11:34:53.861   Training iter 50, batch loss 1.0785, batch acc 0.6780
11:34:54.368   Training iter 100, batch loss 1.0631, batch acc 0.6836
11:34:54.873   Training iter 150, batch loss 1.0862, batch acc 0.6724
11:34:55.385   Training iter 200, batch loss 1.0648, batch acc 0.6750
11:34:55.932   Training iter 250, batch loss 1.0747, batch acc 0.6778
11:34:56.485   Training iter 300, batch loss 1.0820, batch acc 0.6750
11:34:57.037   Training iter 350, batch loss 1.0784, batch acc 0.6740
11:34:57.596   Training iter 400, batch loss 1.1102, batch acc 0.6626
11:34:58.160   Training iter 450, batch loss 1.1181, batch acc 0.6610
11:34:58.712   Training iter 500, batch loss 1.0937, batch acc 0.6750
11:34:59.246   Training iter 550, batch loss 1.0673, batch acc 0.6788
11:34:59.782   Training iter 600, batch loss 1.0675, batch acc 0.6778
11:34:59.784 Training @ 202 epoch...
11:35:00.326   Training iter 50, batch loss 1.0610, batch acc 0.6856
11:35:00.855   Training iter 100, batch loss 1.0652, batch acc 0.6830
11:35:01.405   Training iter 150, batch loss 1.0716, batch acc 0.6760
11:35:01.969   Training iter 200, batch loss 1.1038, batch acc 0.6688
11:35:02.496   Training iter 250, batch loss 1.0748, batch acc 0.6744
11:35:03.028   Training iter 300, batch loss 1.0798, batch acc 0.6704
11:35:03.561   Training iter 350, batch loss 1.0647, batch acc 0.6748
11:35:04.101   Training iter 400, batch loss 1.1018, batch acc 0.6684
11:35:04.609   Training iter 450, batch loss 1.1059, batch acc 0.6646
11:35:05.141   Training iter 500, batch loss 1.0961, batch acc 0.6722
11:35:05.704   Training iter 550, batch loss 1.0591, batch acc 0.6840
11:35:06.276   Training iter 600, batch loss 1.0998, batch acc 0.6690
11:35:06.278 Training @ 203 epoch...
11:35:06.855   Training iter 50, batch loss 1.0657, batch acc 0.6832
11:35:07.416   Training iter 100, batch loss 1.1057, batch acc 0.6626
11:35:07.937   Training iter 150, batch loss 1.0533, batch acc 0.6860
11:35:08.478   Training iter 200, batch loss 1.1005, batch acc 0.6658
11:35:09.013   Training iter 250, batch loss 1.0721, batch acc 0.6764
11:35:09.537   Training iter 300, batch loss 1.0874, batch acc 0.6688
11:35:10.079   Training iter 350, batch loss 1.0698, batch acc 0.6796
11:35:10.582   Training iter 400, batch loss 1.0815, batch acc 0.6764
11:35:11.135   Training iter 450, batch loss 1.0620, batch acc 0.6764
11:35:11.619   Training iter 500, batch loss 1.0930, batch acc 0.6734
11:35:12.122   Training iter 550, batch loss 1.1056, batch acc 0.6686
11:35:12.638   Training iter 600, batch loss 1.0859, batch acc 0.6762
11:35:12.639 Training @ 204 epoch...
11:35:13.141   Training iter 50, batch loss 1.0802, batch acc 0.6740
11:35:13.648   Training iter 100, batch loss 1.0680, batch acc 0.6778
11:35:14.159   Training iter 150, batch loss 1.0879, batch acc 0.6798
11:35:14.695   Training iter 200, batch loss 1.0758, batch acc 0.6726
11:35:15.224   Training iter 250, batch loss 1.1074, batch acc 0.6682
11:35:15.738   Training iter 300, batch loss 1.0618, batch acc 0.6828
11:35:16.273   Training iter 350, batch loss 1.0935, batch acc 0.6698
11:35:16.794   Training iter 400, batch loss 1.0828, batch acc 0.6738
11:35:17.316   Training iter 450, batch loss 1.0957, batch acc 0.6690
11:35:17.845   Training iter 500, batch loss 1.0836, batch acc 0.6722
11:35:18.376   Training iter 550, batch loss 1.0813, batch acc 0.6726
11:35:18.891   Training iter 600, batch loss 1.0632, batch acc 0.6824
11:35:18.893 Training @ 205 epoch...
11:35:19.388   Training iter 50, batch loss 1.0652, batch acc 0.6816
11:35:19.870   Training iter 100, batch loss 1.0711, batch acc 0.6744
11:35:20.350   Training iter 150, batch loss 1.0891, batch acc 0.6726
11:35:20.851   Training iter 200, batch loss 1.0820, batch acc 0.6726
11:35:21.342   Training iter 250, batch loss 1.1000, batch acc 0.6678
11:35:21.839   Training iter 300, batch loss 1.0900, batch acc 0.6674
11:35:22.371   Training iter 350, batch loss 1.0757, batch acc 0.6786
11:35:22.914   Training iter 400, batch loss 1.0664, batch acc 0.6812
11:35:23.500   Training iter 450, batch loss 1.0689, batch acc 0.6776
11:35:24.042   Training iter 500, batch loss 1.0952, batch acc 0.6728
11:35:24.546   Training iter 550, batch loss 1.0776, batch acc 0.6762
11:35:25.060   Training iter 600, batch loss 1.0995, batch acc 0.6724
11:35:25.062 Testing @ 205 epoch...
11:35:25.105     Testing, total mean loss 1.05369, total acc 0.68220
11:35:25.105 Training @ 206 epoch...
11:35:25.615   Training iter 50, batch loss 1.0890, batch acc 0.6706
11:35:26.128   Training iter 100, batch loss 1.0916, batch acc 0.6724
11:35:26.635   Training iter 150, batch loss 1.0591, batch acc 0.6864
11:35:27.170   Training iter 200, batch loss 1.0664, batch acc 0.6794
11:35:27.661   Training iter 250, batch loss 1.0894, batch acc 0.6670
11:35:28.170   Training iter 300, batch loss 1.0655, batch acc 0.6842
11:35:28.684   Training iter 350, batch loss 1.0999, batch acc 0.6730
11:35:29.217   Training iter 400, batch loss 1.0796, batch acc 0.6778
11:35:29.765   Training iter 450, batch loss 1.1009, batch acc 0.6608
11:35:30.324   Training iter 500, batch loss 1.0854, batch acc 0.6810
11:35:30.881   Training iter 550, batch loss 1.0711, batch acc 0.6784
11:35:31.423   Training iter 600, batch loss 1.0819, batch acc 0.6642
11:35:31.425 Training @ 207 epoch...
11:35:31.971   Training iter 50, batch loss 1.0980, batch acc 0.6648
11:35:32.509   Training iter 100, batch loss 1.0978, batch acc 0.6674
11:35:33.052   Training iter 150, batch loss 1.0444, batch acc 0.6840
11:35:33.597   Training iter 200, batch loss 1.0825, batch acc 0.6776
11:35:34.139   Training iter 250, batch loss 1.0903, batch acc 0.6782
11:35:34.706   Training iter 300, batch loss 1.0685, batch acc 0.6750
11:35:35.266   Training iter 350, batch loss 1.0822, batch acc 0.6744
11:35:35.835   Training iter 400, batch loss 1.0508, batch acc 0.6886
11:35:36.425   Training iter 450, batch loss 1.0871, batch acc 0.6768
11:35:37.009   Training iter 500, batch loss 1.1155, batch acc 0.6622
11:35:37.620   Training iter 550, batch loss 1.0770, batch acc 0.6734
11:35:38.207   Training iter 600, batch loss 1.0850, batch acc 0.6726
11:35:38.209 Training @ 208 epoch...
11:35:38.829   Training iter 50, batch loss 1.0750, batch acc 0.6718
11:35:39.423   Training iter 100, batch loss 1.0857, batch acc 0.6748
11:35:40.035   Training iter 150, batch loss 1.1057, batch acc 0.6694
11:35:40.598   Training iter 200, batch loss 1.0712, batch acc 0.6796
11:35:41.137   Training iter 250, batch loss 1.0717, batch acc 0.6798
11:35:41.699   Training iter 300, batch loss 1.0994, batch acc 0.6678
11:35:42.263   Training iter 350, batch loss 1.0981, batch acc 0.6628
11:35:42.828   Training iter 400, batch loss 1.0849, batch acc 0.6748
11:35:43.404   Training iter 450, batch loss 1.0552, batch acc 0.6864
11:35:43.968   Training iter 500, batch loss 1.0784, batch acc 0.6840
11:35:44.515   Training iter 550, batch loss 1.0765, batch acc 0.6706
11:35:45.037   Training iter 600, batch loss 1.0761, batch acc 0.6746
11:35:45.039 Training @ 209 epoch...
11:35:45.575   Training iter 50, batch loss 1.1021, batch acc 0.6688
11:35:46.115   Training iter 100, batch loss 1.0712, batch acc 0.6782
11:35:46.652   Training iter 150, batch loss 1.0871, batch acc 0.6720
11:35:47.188   Training iter 200, batch loss 1.0824, batch acc 0.6764
11:35:47.712   Training iter 250, batch loss 1.0641, batch acc 0.6834
11:35:48.239   Training iter 300, batch loss 1.1030, batch acc 0.6630
11:35:48.771   Training iter 350, batch loss 1.0770, batch acc 0.6720
11:35:49.287   Training iter 400, batch loss 1.0783, batch acc 0.6748
11:35:49.816   Training iter 450, batch loss 1.0902, batch acc 0.6706
11:35:50.343   Training iter 500, batch loss 1.0590, batch acc 0.6854
11:35:50.869   Training iter 550, batch loss 1.0939, batch acc 0.6718
11:35:51.395   Training iter 600, batch loss 1.0690, batch acc 0.6754
11:35:51.397 Training @ 210 epoch...
11:35:51.909   Training iter 50, batch loss 1.0814, batch acc 0.6740
11:35:52.409   Training iter 100, batch loss 1.0649, batch acc 0.6824
11:35:52.903   Training iter 150, batch loss 1.1000, batch acc 0.6656
11:35:53.394   Training iter 200, batch loss 1.0638, batch acc 0.6744
11:35:53.874   Training iter 250, batch loss 1.0800, batch acc 0.6726
11:35:54.379   Training iter 300, batch loss 1.0687, batch acc 0.6734
11:35:54.866   Training iter 350, batch loss 1.0719, batch acc 0.6800
11:35:55.376   Training iter 400, batch loss 1.0933, batch acc 0.6730
11:35:55.877   Training iter 450, batch loss 1.1073, batch acc 0.6696
11:35:56.368   Training iter 500, batch loss 1.0703, batch acc 0.6768
11:35:56.861   Training iter 550, batch loss 1.0782, batch acc 0.6764
11:35:57.357   Training iter 600, batch loss 1.0964, batch acc 0.6714
11:35:57.359 Testing @ 210 epoch...
11:35:57.401     Testing, total mean loss 1.05336, total acc 0.68370
11:35:57.401 Training @ 211 epoch...
11:35:57.937   Training iter 50, batch loss 1.0881, batch acc 0.6674
11:35:58.503   Training iter 100, batch loss 1.0893, batch acc 0.6704
11:35:59.076   Training iter 150, batch loss 1.0832, batch acc 0.6746
11:35:59.637   Training iter 200, batch loss 1.1082, batch acc 0.6626
11:36:00.130   Training iter 250, batch loss 1.0709, batch acc 0.6778
11:36:00.639   Training iter 300, batch loss 1.0724, batch acc 0.6808
11:36:01.151   Training iter 350, batch loss 1.0705, batch acc 0.6760
11:36:01.695   Training iter 400, batch loss 1.0903, batch acc 0.6706
11:36:02.256   Training iter 450, batch loss 1.0666, batch acc 0.6790
11:36:02.797   Training iter 500, batch loss 1.1013, batch acc 0.6678
11:36:03.283   Training iter 550, batch loss 1.0723, batch acc 0.6828
11:36:03.787   Training iter 600, batch loss 1.0627, batch acc 0.6894
11:36:03.788 Training @ 212 epoch...
11:36:04.310   Training iter 50, batch loss 1.0705, batch acc 0.6784
11:36:04.841   Training iter 100, batch loss 1.1075, batch acc 0.6610
11:36:05.386   Training iter 150, batch loss 1.1028, batch acc 0.6742
11:36:05.937   Training iter 200, batch loss 1.0709, batch acc 0.6786
11:36:06.491   Training iter 250, batch loss 1.0753, batch acc 0.6788
11:36:07.060   Training iter 300, batch loss 1.0524, batch acc 0.6842
11:36:07.605   Training iter 350, batch loss 1.0801, batch acc 0.6744
11:36:08.149   Training iter 400, batch loss 1.0726, batch acc 0.6786
11:36:08.686   Training iter 450, batch loss 1.0685, batch acc 0.6764
11:36:09.227   Training iter 500, batch loss 1.0913, batch acc 0.6720
11:36:09.773   Training iter 550, batch loss 1.0937, batch acc 0.6656
11:36:10.290   Training iter 600, batch loss 1.0893, batch acc 0.6730
11:36:10.292 Training @ 213 epoch...
11:36:10.816   Training iter 50, batch loss 1.0603, batch acc 0.6840
11:36:11.324   Training iter 100, batch loss 1.0750, batch acc 0.6796
11:36:11.829   Training iter 150, batch loss 1.0898, batch acc 0.6670
11:36:12.356   Training iter 200, batch loss 1.0829, batch acc 0.6712
11:36:12.862   Training iter 250, batch loss 1.1044, batch acc 0.6670
11:36:13.377   Training iter 300, batch loss 1.0692, batch acc 0.6804
11:36:13.889   Training iter 350, batch loss 1.1094, batch acc 0.6660
11:36:14.409   Training iter 400, batch loss 1.0858, batch acc 0.6746
11:36:14.917   Training iter 450, batch loss 1.0746, batch acc 0.6794
11:36:15.436   Training iter 500, batch loss 1.0617, batch acc 0.6818
11:36:15.955   Training iter 550, batch loss 1.0870, batch acc 0.6694
11:36:16.474   Training iter 600, batch loss 1.0740, batch acc 0.6778
11:36:16.476 Training @ 214 epoch...
11:36:17.004   Training iter 50, batch loss 1.0678, batch acc 0.6806
11:36:17.524   Training iter 100, batch loss 1.0571, batch acc 0.6812
11:36:18.047   Training iter 150, batch loss 1.0671, batch acc 0.6770
11:36:18.579   Training iter 200, batch loss 1.0708, batch acc 0.6772
11:36:19.112   Training iter 250, batch loss 1.1035, batch acc 0.6662
11:36:19.652   Training iter 300, batch loss 1.0984, batch acc 0.6658
11:36:20.169   Training iter 350, batch loss 1.1036, batch acc 0.6608
11:36:20.685   Training iter 400, batch loss 1.0926, batch acc 0.6796
11:36:21.192   Training iter 450, batch loss 1.0868, batch acc 0.6764
11:36:21.705   Training iter 500, batch loss 1.0465, batch acc 0.6856
11:36:22.246   Training iter 550, batch loss 1.0855, batch acc 0.6742
11:36:22.796   Training iter 600, batch loss 1.0935, batch acc 0.6688
11:36:22.798 Training @ 215 epoch...
11:36:23.314   Training iter 50, batch loss 1.0815, batch acc 0.6730
11:36:23.828   Training iter 100, batch loss 1.0686, batch acc 0.6764
11:36:24.353   Training iter 150, batch loss 1.0807, batch acc 0.6732
11:36:24.865   Training iter 200, batch loss 1.0659, batch acc 0.6792
11:36:25.390   Training iter 250, batch loss 1.0754, batch acc 0.6816
11:36:25.910   Training iter 300, batch loss 1.1006, batch acc 0.6766
11:36:26.424   Training iter 350, batch loss 1.0843, batch acc 0.6730
11:36:26.924   Training iter 400, batch loss 1.0958, batch acc 0.6700
11:36:27.431   Training iter 450, batch loss 1.0678, batch acc 0.6762
11:36:27.938   Training iter 500, batch loss 1.0825, batch acc 0.6676
11:36:28.449   Training iter 550, batch loss 1.0688, batch acc 0.6880
11:36:28.963   Training iter 600, batch loss 1.1008, batch acc 0.6598
11:36:28.964 Testing @ 215 epoch...
11:36:29.010     Testing, total mean loss 1.05301, total acc 0.68300
11:36:29.010 Training @ 216 epoch...
11:36:29.520   Training iter 50, batch loss 1.0817, batch acc 0.6694
11:36:30.052   Training iter 100, batch loss 1.0976, batch acc 0.6694
11:36:30.560   Training iter 150, batch loss 1.0709, batch acc 0.6798
11:36:31.094   Training iter 200, batch loss 1.0604, batch acc 0.6778
11:36:31.619   Training iter 250, batch loss 1.0632, batch acc 0.6810
11:36:32.148   Training iter 300, batch loss 1.0919, batch acc 0.6698
11:36:32.658   Training iter 350, batch loss 1.1038, batch acc 0.6604
11:36:33.173   Training iter 400, batch loss 1.0655, batch acc 0.6846
11:36:33.713   Training iter 450, batch loss 1.0654, batch acc 0.6818
11:36:34.277   Training iter 500, batch loss 1.1066, batch acc 0.6666
11:36:34.806   Training iter 550, batch loss 1.0860, batch acc 0.6734
11:36:35.336   Training iter 600, batch loss 1.0784, batch acc 0.6786
11:36:35.338 Training @ 217 epoch...
11:36:35.875   Training iter 50, batch loss 1.0441, batch acc 0.6876
11:36:36.416   Training iter 100, batch loss 1.0797, batch acc 0.6758
11:36:36.947   Training iter 150, batch loss 1.0713, batch acc 0.6784
11:36:37.479   Training iter 200, batch loss 1.0842, batch acc 0.6764
11:36:38.016   Training iter 250, batch loss 1.0715, batch acc 0.6746
11:36:38.546   Training iter 300, batch loss 1.0983, batch acc 0.6664
11:36:39.085   Training iter 350, batch loss 1.0815, batch acc 0.6734
11:36:39.625   Training iter 400, batch loss 1.0811, batch acc 0.6772
11:36:40.183   Training iter 450, batch loss 1.0976, batch acc 0.6668
11:36:40.728   Training iter 500, batch loss 1.0825, batch acc 0.6824
11:36:41.263   Training iter 550, batch loss 1.0997, batch acc 0.6648
11:36:41.801   Training iter 600, batch loss 1.0794, batch acc 0.6744
11:36:41.803 Training @ 218 epoch...
11:36:42.352   Training iter 50, batch loss 1.0943, batch acc 0.6694
11:36:42.887   Training iter 100, batch loss 1.0719, batch acc 0.6784
11:36:43.420   Training iter 150, batch loss 1.0808, batch acc 0.6740
11:36:43.961   Training iter 200, batch loss 1.0717, batch acc 0.6758
11:36:44.503   Training iter 250, batch loss 1.0789, batch acc 0.6798
11:36:45.042   Training iter 300, batch loss 1.0917, batch acc 0.6696
11:36:45.571   Training iter 350, batch loss 1.0819, batch acc 0.6688
11:36:46.093   Training iter 400, batch loss 1.0682, batch acc 0.6796
11:36:46.575   Training iter 450, batch loss 1.0719, batch acc 0.6750
11:36:47.058   Training iter 500, batch loss 1.0880, batch acc 0.6710
11:36:47.544   Training iter 550, batch loss 1.0783, batch acc 0.6796
11:36:48.019   Training iter 600, batch loss 1.0928, batch acc 0.6746
11:36:48.021 Training @ 219 epoch...
11:36:48.512   Training iter 50, batch loss 1.0684, batch acc 0.6786
11:36:48.981   Training iter 100, batch loss 1.0497, batch acc 0.6846
11:36:49.465   Training iter 150, batch loss 1.0980, batch acc 0.6682
11:36:49.953   Training iter 200, batch loss 1.0752, batch acc 0.6770
11:36:50.469   Training iter 250, batch loss 1.1046, batch acc 0.6606
11:36:50.979   Training iter 300, batch loss 1.0979, batch acc 0.6770
11:36:51.504   Training iter 350, batch loss 1.0698, batch acc 0.6770
11:36:52.025   Training iter 400, batch loss 1.0751, batch acc 0.6732
11:36:52.506   Training iter 450, batch loss 1.0752, batch acc 0.6824
11:36:52.986   Training iter 500, batch loss 1.0873, batch acc 0.6692
11:36:53.452   Training iter 550, batch loss 1.0791, batch acc 0.6756
11:36:53.914   Training iter 600, batch loss 1.0891, batch acc 0.6726
11:36:53.916 Training @ 220 epoch...
11:36:54.417   Training iter 50, batch loss 1.1037, batch acc 0.6680
11:36:54.932   Training iter 100, batch loss 1.0639, batch acc 0.6766
11:36:55.479   Training iter 150, batch loss 1.0774, batch acc 0.6800
11:36:56.054   Training iter 200, batch loss 1.0497, batch acc 0.6840
11:36:56.611   Training iter 250, batch loss 1.0772, batch acc 0.6762
11:36:57.150   Training iter 300, batch loss 1.0822, batch acc 0.6708
11:36:57.667   Training iter 350, batch loss 1.1254, batch acc 0.6552
11:36:58.159   Training iter 400, batch loss 1.0667, batch acc 0.6864
11:36:58.647   Training iter 450, batch loss 1.0921, batch acc 0.6716
11:36:59.153   Training iter 500, batch loss 1.1008, batch acc 0.6676
11:36:59.633   Training iter 550, batch loss 1.0587, batch acc 0.6832
11:37:00.149   Training iter 600, batch loss 1.0711, batch acc 0.6782
11:37:00.151 Testing @ 220 epoch...
11:37:00.194     Testing, total mean loss 1.05259, total acc 0.68300
11:37:00.195 Training @ 221 epoch...
11:37:00.713   Training iter 50, batch loss 1.0975, batch acc 0.6646
11:37:01.225   Training iter 100, batch loss 1.0559, batch acc 0.6916
11:37:01.759   Training iter 150, batch loss 1.0818, batch acc 0.6744
11:37:02.328   Training iter 200, batch loss 1.0875, batch acc 0.6688
11:37:02.872   Training iter 250, batch loss 1.0796, batch acc 0.6766
11:37:03.414   Training iter 300, batch loss 1.0703, batch acc 0.6776
11:37:03.939   Training iter 350, batch loss 1.0832, batch acc 0.6762
11:37:04.467   Training iter 400, batch loss 1.0872, batch acc 0.6736
11:37:04.995   Training iter 450, batch loss 1.0700, batch acc 0.6776
11:37:05.511   Training iter 500, batch loss 1.0846, batch acc 0.6704
11:37:06.060   Training iter 550, batch loss 1.0786, batch acc 0.6744
11:37:06.615   Training iter 600, batch loss 1.0919, batch acc 0.6728
11:37:06.617 Training @ 222 epoch...
11:37:07.189   Training iter 50, batch loss 1.0800, batch acc 0.6756
11:37:07.720   Training iter 100, batch loss 1.0884, batch acc 0.6708
11:37:08.287   Training iter 150, batch loss 1.0889, batch acc 0.6714
11:37:08.867   Training iter 200, batch loss 1.0493, batch acc 0.6812
11:37:09.454   Training iter 250, batch loss 1.0882, batch acc 0.6718
11:37:10.028   Training iter 300, batch loss 1.1133, batch acc 0.6646
11:37:10.601   Training iter 350, batch loss 1.0705, batch acc 0.6766
11:37:11.198   Training iter 400, batch loss 1.0733, batch acc 0.6758
11:37:11.777   Training iter 450, batch loss 1.0662, batch acc 0.6810
11:37:12.362   Training iter 500, batch loss 1.1089, batch acc 0.6680
11:37:12.942   Training iter 550, batch loss 1.0762, batch acc 0.6796
11:37:13.523   Training iter 600, batch loss 1.0641, batch acc 0.6824
11:37:13.525 Training @ 223 epoch...
11:37:14.097   Training iter 50, batch loss 1.0802, batch acc 0.6824
11:37:14.657   Training iter 100, batch loss 1.1010, batch acc 0.6610
11:37:15.201   Training iter 150, batch loss 1.0819, batch acc 0.6758
11:37:15.713   Training iter 200, batch loss 1.0780, batch acc 0.6722
11:37:16.236   Training iter 250, batch loss 1.0630, batch acc 0.6728
11:37:16.791   Training iter 300, batch loss 1.1128, batch acc 0.6602
11:37:17.296   Training iter 350, batch loss 1.0778, batch acc 0.6788
11:37:17.837   Training iter 400, batch loss 1.1005, batch acc 0.6688
11:37:18.398   Training iter 450, batch loss 1.0511, batch acc 0.6892
11:37:18.932   Training iter 500, batch loss 1.0725, batch acc 0.6794
11:37:19.481   Training iter 550, batch loss 1.0823, batch acc 0.6746
11:37:20.123   Training iter 600, batch loss 1.0655, batch acc 0.6824
11:37:20.125 Training @ 224 epoch...
11:37:20.829   Training iter 50, batch loss 1.0611, batch acc 0.6774
11:37:21.528   Training iter 100, batch loss 1.0918, batch acc 0.6696
11:37:22.031   Training iter 150, batch loss 1.0941, batch acc 0.6700
11:37:22.544   Training iter 200, batch loss 1.0766, batch acc 0.6706
11:37:23.055   Training iter 250, batch loss 1.0634, batch acc 0.6856
11:37:23.554   Training iter 300, batch loss 1.0950, batch acc 0.6726
11:37:24.063   Training iter 350, batch loss 1.0870, batch acc 0.6772
11:37:24.580   Training iter 400, batch loss 1.0646, batch acc 0.6808
11:37:25.128   Training iter 450, batch loss 1.0681, batch acc 0.6768
11:37:25.658   Training iter 500, batch loss 1.0806, batch acc 0.6754
11:37:26.228   Training iter 550, batch loss 1.1060, batch acc 0.6652
11:37:26.778   Training iter 600, batch loss 1.0775, batch acc 0.6784
11:37:26.780 Training @ 225 epoch...
11:37:27.380   Training iter 50, batch loss 1.1060, batch acc 0.6686
11:37:27.954   Training iter 100, batch loss 1.0608, batch acc 0.6862
11:37:28.521   Training iter 150, batch loss 1.0742, batch acc 0.6754
11:37:29.087   Training iter 200, batch loss 1.0778, batch acc 0.6726
11:37:29.664   Training iter 250, batch loss 1.0987, batch acc 0.6682
11:37:30.206   Training iter 300, batch loss 1.0698, batch acc 0.6776
11:37:30.756   Training iter 350, batch loss 1.0801, batch acc 0.6756
11:37:31.259   Training iter 400, batch loss 1.0779, batch acc 0.6794
11:37:31.759   Training iter 450, batch loss 1.0613, batch acc 0.6792
11:37:32.257   Training iter 500, batch loss 1.1015, batch acc 0.6628
11:37:32.739   Training iter 550, batch loss 1.0677, batch acc 0.6782
11:37:33.219   Training iter 600, batch loss 1.0894, batch acc 0.6720
11:37:33.221 Testing @ 225 epoch...
11:37:33.262     Testing, total mean loss 1.05246, total acc 0.68260
11:37:33.262 Training @ 226 epoch...
11:37:33.754   Training iter 50, batch loss 1.0749, batch acc 0.6776
11:37:34.249   Training iter 100, batch loss 1.1184, batch acc 0.6610
11:37:34.760   Training iter 150, batch loss 1.0920, batch acc 0.6670
11:37:35.250   Training iter 200, batch loss 1.0978, batch acc 0.6698
11:37:35.751   Training iter 250, batch loss 1.0762, batch acc 0.6798
11:37:36.247   Training iter 300, batch loss 1.0570, batch acc 0.6880
11:37:36.736   Training iter 350, batch loss 1.0582, batch acc 0.6860
11:37:37.220   Training iter 400, batch loss 1.0771, batch acc 0.6726
11:37:37.705   Training iter 450, batch loss 1.0813, batch acc 0.6708
11:37:38.191   Training iter 500, batch loss 1.0498, batch acc 0.6878
11:37:38.684   Training iter 550, batch loss 1.0691, batch acc 0.6710
11:37:39.193   Training iter 600, batch loss 1.1128, batch acc 0.6682
11:37:39.194 Training @ 227 epoch...
11:37:39.746   Training iter 50, batch loss 1.0839, batch acc 0.6678
11:37:40.299   Training iter 100, batch loss 1.0659, batch acc 0.6802
11:37:40.846   Training iter 150, batch loss 1.0593, batch acc 0.6850
11:37:41.383   Training iter 200, batch loss 1.0736, batch acc 0.6828
11:37:41.919   Training iter 250, batch loss 1.0801, batch acc 0.6714
11:37:42.472   Training iter 300, batch loss 1.0868, batch acc 0.6708
11:37:43.012   Training iter 350, batch loss 1.0703, batch acc 0.6770
11:37:43.561   Training iter 400, batch loss 1.1138, batch acc 0.6652
11:37:44.107   Training iter 450, batch loss 1.0774, batch acc 0.6770
11:37:44.651   Training iter 500, batch loss 1.0909, batch acc 0.6668
11:37:45.187   Training iter 550, batch loss 1.0920, batch acc 0.6722
11:37:45.720   Training iter 600, batch loss 1.0701, batch acc 0.6834
11:37:45.722 Training @ 228 epoch...
11:37:46.223   Training iter 50, batch loss 1.0775, batch acc 0.6786
11:37:46.693   Training iter 100, batch loss 1.0826, batch acc 0.6718
11:37:47.180   Training iter 150, batch loss 1.0934, batch acc 0.6688
11:37:47.666   Training iter 200, batch loss 1.0615, batch acc 0.6806
11:37:48.148   Training iter 250, batch loss 1.0828, batch acc 0.6740
11:37:48.633   Training iter 300, batch loss 1.0704, batch acc 0.6794
11:37:49.122   Training iter 350, batch loss 1.0822, batch acc 0.6770
11:37:49.619   Training iter 400, batch loss 1.0886, batch acc 0.6740
11:37:50.132   Training iter 450, batch loss 1.0816, batch acc 0.6712
11:37:50.652   Training iter 500, batch loss 1.0760, batch acc 0.6816
11:37:51.177   Training iter 550, batch loss 1.0873, batch acc 0.6658
11:37:51.714   Training iter 600, batch loss 1.0791, batch acc 0.6718
11:37:51.716 Training @ 229 epoch...
11:37:52.230   Training iter 50, batch loss 1.0846, batch acc 0.6750
11:37:52.717   Training iter 100, batch loss 1.0429, batch acc 0.6952
11:37:53.196   Training iter 150, batch loss 1.0838, batch acc 0.6710
11:37:53.761   Training iter 200, batch loss 1.1333, batch acc 0.6484
11:37:54.316   Training iter 250, batch loss 1.0746, batch acc 0.6706
11:37:54.876   Training iter 300, batch loss 1.0837, batch acc 0.6768
11:37:55.400   Training iter 350, batch loss 1.0717, batch acc 0.6808
11:37:55.919   Training iter 400, batch loss 1.0977, batch acc 0.6718
11:37:56.452   Training iter 450, batch loss 1.1024, batch acc 0.6740
11:37:56.993   Training iter 500, batch loss 1.0556, batch acc 0.6808
11:37:57.565   Training iter 550, batch loss 1.0792, batch acc 0.6736
11:37:58.109   Training iter 600, batch loss 1.0535, batch acc 0.6800
11:37:58.110 Training @ 230 epoch...
11:37:58.629   Training iter 50, batch loss 1.0695, batch acc 0.6824
11:37:59.138   Training iter 100, batch loss 1.0602, batch acc 0.6840
11:37:59.630   Training iter 150, batch loss 1.0739, batch acc 0.6726
11:38:00.137   Training iter 200, batch loss 1.0612, batch acc 0.6802
11:38:00.628   Training iter 250, batch loss 1.0832, batch acc 0.6774
11:38:01.119   Training iter 300, batch loss 1.0705, batch acc 0.6788
11:38:01.661   Training iter 350, batch loss 1.1048, batch acc 0.6636
11:38:02.200   Training iter 400, batch loss 1.0768, batch acc 0.6780
11:38:02.715   Training iter 450, batch loss 1.1157, batch acc 0.6608
11:38:03.217   Training iter 500, batch loss 1.0769, batch acc 0.6716
11:38:03.690   Training iter 550, batch loss 1.0909, batch acc 0.6684
11:38:04.163   Training iter 600, batch loss 1.0780, batch acc 0.6808
11:38:04.165 Testing @ 230 epoch...
11:38:04.205     Testing, total mean loss 1.05238, total acc 0.68290
11:38:04.205 Training @ 231 epoch...
11:38:04.689   Training iter 50, batch loss 1.0951, batch acc 0.6650
11:38:05.188   Training iter 100, batch loss 1.0647, batch acc 0.6834
11:38:05.688   Training iter 150, batch loss 1.0908, batch acc 0.6694
11:38:06.175   Training iter 200, batch loss 1.0633, batch acc 0.6814
11:38:06.656   Training iter 250, batch loss 1.0685, batch acc 0.6772
11:38:07.130   Training iter 300, batch loss 1.1085, batch acc 0.6636
11:38:07.600   Training iter 350, batch loss 1.0925, batch acc 0.6730
11:38:08.074   Training iter 400, batch loss 1.0800, batch acc 0.6766
11:38:08.539   Training iter 450, batch loss 1.0698, batch acc 0.6776
11:38:09.020   Training iter 500, batch loss 1.0895, batch acc 0.6724
11:38:09.511   Training iter 550, batch loss 1.0703, batch acc 0.6752
11:38:10.006   Training iter 600, batch loss 1.0687, batch acc 0.6846
11:38:10.008 Training @ 232 epoch...
11:38:10.530   Training iter 50, batch loss 1.0955, batch acc 0.6710
11:38:11.025   Training iter 100, batch loss 1.0560, batch acc 0.6806
11:38:11.527   Training iter 150, batch loss 1.1045, batch acc 0.6716
11:38:12.033   Training iter 200, batch loss 1.0686, batch acc 0.6806
11:38:12.546   Training iter 250, batch loss 1.0594, batch acc 0.6886
11:38:13.060   Training iter 300, batch loss 1.0794, batch acc 0.6804
11:38:13.571   Training iter 350, batch loss 1.0893, batch acc 0.6686
11:38:14.087   Training iter 400, batch loss 1.0804, batch acc 0.6722
11:38:14.589   Training iter 450, batch loss 1.0866, batch acc 0.6694
11:38:15.096   Training iter 500, batch loss 1.0756, batch acc 0.6760
11:38:15.604   Training iter 550, batch loss 1.1015, batch acc 0.6618
11:38:16.146   Training iter 600, batch loss 1.0642, batch acc 0.6796
11:38:16.147 Training @ 233 epoch...
11:38:16.688   Training iter 50, batch loss 1.0866, batch acc 0.6778
11:38:17.228   Training iter 100, batch loss 1.0842, batch acc 0.6704
11:38:17.756   Training iter 150, batch loss 1.0646, batch acc 0.6794
11:38:18.249   Training iter 200, batch loss 1.0911, batch acc 0.6682
11:38:18.731   Training iter 250, batch loss 1.0680, batch acc 0.6858
11:38:19.190   Training iter 300, batch loss 1.0836, batch acc 0.6776
11:38:19.659   Training iter 350, batch loss 1.0621, batch acc 0.6820
11:38:20.170   Training iter 400, batch loss 1.0846, batch acc 0.6760
11:38:20.671   Training iter 450, batch loss 1.0720, batch acc 0.6724
11:38:21.183   Training iter 500, batch loss 1.0667, batch acc 0.6822
11:38:21.693   Training iter 550, batch loss 1.0928, batch acc 0.6708
11:38:22.217   Training iter 600, batch loss 1.1039, batch acc 0.6564
11:38:22.219 Training @ 234 epoch...
11:38:22.740   Training iter 50, batch loss 1.0946, batch acc 0.6710
11:38:23.249   Training iter 100, batch loss 1.0771, batch acc 0.6698
11:38:23.769   Training iter 150, batch loss 1.0654, batch acc 0.6774
11:38:24.286   Training iter 200, batch loss 1.0814, batch acc 0.6740
11:38:24.792   Training iter 250, batch loss 1.1070, batch acc 0.6638
11:38:25.303   Training iter 300, batch loss 1.0690, batch acc 0.6760
11:38:25.839   Training iter 350, batch loss 1.0824, batch acc 0.6732
11:38:26.382   Training iter 400, batch loss 1.0844, batch acc 0.6776
11:38:26.935   Training iter 450, batch loss 1.0806, batch acc 0.6798
11:38:27.495   Training iter 500, batch loss 1.0607, batch acc 0.6812
11:38:28.027   Training iter 550, batch loss 1.0840, batch acc 0.6784
11:38:28.543   Training iter 600, batch loss 1.0731, batch acc 0.6784
11:38:28.545 Training @ 235 epoch...
11:38:29.077   Training iter 50, batch loss 1.1077, batch acc 0.6594
11:38:29.597   Training iter 100, batch loss 1.0994, batch acc 0.6644
11:38:30.124   Training iter 150, batch loss 1.0629, batch acc 0.6864
11:38:30.645   Training iter 200, batch loss 1.0649, batch acc 0.6804
11:38:31.145   Training iter 250, batch loss 1.0669, batch acc 0.6760
11:38:31.644   Training iter 300, batch loss 1.0678, batch acc 0.6838
11:38:32.152   Training iter 350, batch loss 1.1058, batch acc 0.6714
11:38:32.670   Training iter 400, batch loss 1.0710, batch acc 0.6770
11:38:33.195   Training iter 450, batch loss 1.0780, batch acc 0.6696
11:38:33.729   Training iter 500, batch loss 1.0761, batch acc 0.6738
11:38:34.261   Training iter 550, batch loss 1.0829, batch acc 0.6758
11:38:34.764   Training iter 600, batch loss 1.0754, batch acc 0.6808
11:38:34.766 Testing @ 235 epoch...
11:38:34.809     Testing, total mean loss 1.05200, total acc 0.68290
11:38:34.809 Training @ 236 epoch...
11:38:35.337   Training iter 50, batch loss 1.0915, batch acc 0.6710
11:38:35.856   Training iter 100, batch loss 1.0629, batch acc 0.6786
11:38:36.374   Training iter 150, batch loss 1.0883, batch acc 0.6662
11:38:36.878   Training iter 200, batch loss 1.0369, batch acc 0.6890
11:38:37.384   Training iter 250, batch loss 1.0845, batch acc 0.6784
11:38:37.889   Training iter 300, batch loss 1.0924, batch acc 0.6746
11:38:38.395   Training iter 350, batch loss 1.0709, batch acc 0.6772
11:38:38.908   Training iter 400, batch loss 1.0713, batch acc 0.6752
11:38:39.430   Training iter 450, batch loss 1.0913, batch acc 0.6738
11:38:39.947   Training iter 500, batch loss 1.1036, batch acc 0.6690
11:38:40.432   Training iter 550, batch loss 1.0874, batch acc 0.6688
11:38:40.938   Training iter 600, batch loss 1.0776, batch acc 0.6792
11:38:40.940 Training @ 237 epoch...
11:38:41.478   Training iter 50, batch loss 1.0662, batch acc 0.6834
11:38:42.009   Training iter 100, batch loss 1.0859, batch acc 0.6698
11:38:42.515   Training iter 150, batch loss 1.0924, batch acc 0.6664
11:38:43.032   Training iter 200, batch loss 1.0549, batch acc 0.6854
11:38:43.548   Training iter 250, batch loss 1.0686, batch acc 0.6862
11:38:44.092   Training iter 300, batch loss 1.0699, batch acc 0.6794
11:38:44.634   Training iter 350, batch loss 1.0854, batch acc 0.6670
11:38:45.177   Training iter 400, batch loss 1.0589, batch acc 0.6820
11:38:45.720   Training iter 450, batch loss 1.0885, batch acc 0.6738
11:38:46.273   Training iter 500, batch loss 1.0823, batch acc 0.6736
11:38:46.845   Training iter 550, batch loss 1.0970, batch acc 0.6656
11:38:47.428   Training iter 600, batch loss 1.1077, batch acc 0.6636
11:38:47.430 Training @ 238 epoch...
11:38:47.997   Training iter 50, batch loss 1.1012, batch acc 0.6754
11:38:48.547   Training iter 100, batch loss 1.0533, batch acc 0.6906
11:38:49.088   Training iter 150, batch loss 1.0703, batch acc 0.6694
11:38:49.603   Training iter 200, batch loss 1.0709, batch acc 0.6782
11:38:50.155   Training iter 250, batch loss 1.0713, batch acc 0.6768
11:38:50.690   Training iter 300, batch loss 1.1072, batch acc 0.6654
11:38:51.209   Training iter 350, batch loss 1.0973, batch acc 0.6654
11:38:51.731   Training iter 400, batch loss 1.0899, batch acc 0.6724
11:38:52.251   Training iter 450, batch loss 1.0516, batch acc 0.6818
11:38:52.765   Training iter 500, batch loss 1.0836, batch acc 0.6754
11:38:53.291   Training iter 550, batch loss 1.0826, batch acc 0.6746
11:38:53.812   Training iter 600, batch loss 1.0781, batch acc 0.6758
11:38:53.813 Training @ 239 epoch...
11:38:54.335   Training iter 50, batch loss 1.0843, batch acc 0.6798
11:38:54.857   Training iter 100, batch loss 1.0804, batch acc 0.6748
11:38:55.384   Training iter 150, batch loss 1.0819, batch acc 0.6762
11:38:55.898   Training iter 200, batch loss 1.0987, batch acc 0.6632
11:38:56.423   Training iter 250, batch loss 1.0671, batch acc 0.6788
11:38:56.946   Training iter 300, batch loss 1.0931, batch acc 0.6784
11:38:57.478   Training iter 350, batch loss 1.0880, batch acc 0.6748
11:38:57.998   Training iter 400, batch loss 1.1038, batch acc 0.6630
11:38:58.534   Training iter 450, batch loss 1.0645, batch acc 0.6778
11:38:59.071   Training iter 500, batch loss 1.0684, batch acc 0.6774
11:38:59.613   Training iter 550, batch loss 1.0538, batch acc 0.6850
11:39:00.163   Training iter 600, batch loss 1.0729, batch acc 0.6736
11:39:00.165 Training @ 240 epoch...
11:39:00.723   Training iter 50, batch loss 1.0845, batch acc 0.6780
11:39:01.277   Training iter 100, batch loss 1.0690, batch acc 0.6742
11:39:01.846   Training iter 150, batch loss 1.0752, batch acc 0.6754
11:39:02.419   Training iter 200, batch loss 1.0851, batch acc 0.6718
11:39:02.966   Training iter 250, batch loss 1.0945, batch acc 0.6764
11:39:03.519   Training iter 300, batch loss 1.0617, batch acc 0.6838
11:39:04.065   Training iter 350, batch loss 1.0843, batch acc 0.6694
11:39:04.611   Training iter 400, batch loss 1.0722, batch acc 0.6790
11:39:05.164   Training iter 450, batch loss 1.0617, batch acc 0.6796
11:39:05.708   Training iter 500, batch loss 1.0988, batch acc 0.6718
11:39:06.238   Training iter 550, batch loss 1.0950, batch acc 0.6646
11:39:06.781   Training iter 600, batch loss 1.0742, batch acc 0.6754
11:39:06.782 Testing @ 240 epoch...
11:39:06.823     Testing, total mean loss 1.05173, total acc 0.68350
11:39:06.823 Training @ 241 epoch...
11:39:07.369   Training iter 50, batch loss 1.0957, batch acc 0.6676
11:39:07.880   Training iter 100, batch loss 1.0916, batch acc 0.6752
11:39:08.380   Training iter 150, batch loss 1.0657, batch acc 0.6800
11:39:08.883   Training iter 200, batch loss 1.0699, batch acc 0.6856
11:39:09.396   Training iter 250, batch loss 1.0569, batch acc 0.6808
11:39:09.899   Training iter 300, batch loss 1.0706, batch acc 0.6820
11:39:10.417   Training iter 350, batch loss 1.0749, batch acc 0.6754
11:39:10.943   Training iter 400, batch loss 1.0828, batch acc 0.6734
11:39:11.463   Training iter 450, batch loss 1.0954, batch acc 0.6634
11:39:11.985   Training iter 500, batch loss 1.0858, batch acc 0.6752
11:39:12.522   Training iter 550, batch loss 1.1062, batch acc 0.6582
11:39:13.045   Training iter 600, batch loss 1.0601, batch acc 0.6858
11:39:13.046 Training @ 242 epoch...
11:39:13.567   Training iter 50, batch loss 1.0795, batch acc 0.6670
11:39:14.091   Training iter 100, batch loss 1.0954, batch acc 0.6652
11:39:14.614   Training iter 150, batch loss 1.0821, batch acc 0.6758
11:39:15.143   Training iter 200, batch loss 1.0893, batch acc 0.6724
11:39:15.660   Training iter 250, batch loss 1.0991, batch acc 0.6714
11:39:16.202   Training iter 300, batch loss 1.0507, batch acc 0.6880
11:39:16.731   Training iter 350, batch loss 1.0967, batch acc 0.6734
11:39:17.266   Training iter 400, batch loss 1.0590, batch acc 0.6810
11:39:17.790   Training iter 450, batch loss 1.0626, batch acc 0.6780
11:39:18.310   Training iter 500, batch loss 1.0804, batch acc 0.6772
11:39:18.843   Training iter 550, batch loss 1.0827, batch acc 0.6706
11:39:19.368   Training iter 600, batch loss 1.0779, batch acc 0.6808
11:39:19.370 Training @ 243 epoch...
11:39:19.898   Training iter 50, batch loss 1.0662, batch acc 0.6830
11:39:20.414   Training iter 100, batch loss 1.0960, batch acc 0.6724
11:39:20.938   Training iter 150, batch loss 1.0975, batch acc 0.6682
11:39:21.455   Training iter 200, batch loss 1.0807, batch acc 0.6746
11:39:21.950   Training iter 250, batch loss 1.0839, batch acc 0.6756
11:39:22.402   Training iter 300, batch loss 1.0666, batch acc 0.6766
11:39:22.873   Training iter 350, batch loss 1.0871, batch acc 0.6724
11:39:23.396   Training iter 400, batch loss 1.1075, batch acc 0.6600
11:39:23.909   Training iter 450, batch loss 1.0658, batch acc 0.6768
11:39:24.420   Training iter 500, batch loss 1.0749, batch acc 0.6738
11:39:24.923   Training iter 550, batch loss 1.0436, batch acc 0.6904
11:39:25.423   Training iter 600, batch loss 1.0847, batch acc 0.6782
11:39:25.425 Training @ 244 epoch...
11:39:25.930   Training iter 50, batch loss 1.0943, batch acc 0.6686
11:39:26.449   Training iter 100, batch loss 1.0782, batch acc 0.6792
11:39:26.977   Training iter 150, batch loss 1.0590, batch acc 0.6842
11:39:27.544   Training iter 200, batch loss 1.0528, batch acc 0.6802
11:39:28.106   Training iter 250, batch loss 1.0958, batch acc 0.6694
11:39:28.666   Training iter 300, batch loss 1.0994, batch acc 0.6596
11:39:29.210   Training iter 350, batch loss 1.0987, batch acc 0.6706
11:39:29.755   Training iter 400, batch loss 1.0690, batch acc 0.6850
11:39:30.316   Training iter 450, batch loss 1.0724, batch acc 0.6764
11:39:30.878   Training iter 500, batch loss 1.0824, batch acc 0.6752
11:39:31.427   Training iter 550, batch loss 1.0999, batch acc 0.6722
11:39:31.967   Training iter 600, batch loss 1.0522, batch acc 0.6816
11:39:31.969 Training @ 245 epoch...
11:39:32.547   Training iter 50, batch loss 1.0795, batch acc 0.6806
11:39:33.106   Training iter 100, batch loss 1.0588, batch acc 0.6838
11:39:33.666   Training iter 150, batch loss 1.0821, batch acc 0.6718
11:39:34.232   Training iter 200, batch loss 1.0776, batch acc 0.6734
11:39:34.795   Training iter 250, batch loss 1.0769, batch acc 0.6702
11:39:35.362   Training iter 300, batch loss 1.0777, batch acc 0.6754
11:39:35.929   Training iter 350, batch loss 1.0405, batch acc 0.6918
11:39:36.525   Training iter 400, batch loss 1.0991, batch acc 0.6716
11:39:37.101   Training iter 450, batch loss 1.1012, batch acc 0.6642
11:39:37.673   Training iter 500, batch loss 1.0654, batch acc 0.6840
11:39:38.200   Training iter 550, batch loss 1.1117, batch acc 0.6548
11:39:38.727   Training iter 600, batch loss 1.0827, batch acc 0.6776
11:39:38.729 Testing @ 245 epoch...
11:39:38.775     Testing, total mean loss 1.05164, total acc 0.68320
11:39:38.775 Training @ 246 epoch...
11:39:39.310   Training iter 50, batch loss 1.0538, batch acc 0.6834
11:39:39.793   Training iter 100, batch loss 1.0501, batch acc 0.6876
11:39:40.320   Training iter 150, batch loss 1.0965, batch acc 0.6722
11:39:40.855   Training iter 200, batch loss 1.0677, batch acc 0.6772
11:39:41.394   Training iter 250, batch loss 1.0734, batch acc 0.6752
11:39:41.935   Training iter 300, batch loss 1.0677, batch acc 0.6814
11:39:42.466   Training iter 350, batch loss 1.0975, batch acc 0.6680
11:39:43.015   Training iter 400, batch loss 1.1102, batch acc 0.6572
11:39:43.557   Training iter 450, batch loss 1.0823, batch acc 0.6756
11:39:44.117   Training iter 500, batch loss 1.0772, batch acc 0.6770
11:39:44.666   Training iter 550, batch loss 1.0793, batch acc 0.6788
11:39:45.242   Training iter 600, batch loss 1.0974, batch acc 0.6714
11:39:45.244 Training @ 247 epoch...
11:39:45.817   Training iter 50, batch loss 1.0816, batch acc 0.6812
11:39:46.410   Training iter 100, batch loss 1.0737, batch acc 0.6764
11:39:46.968   Training iter 150, batch loss 1.0831, batch acc 0.6686
11:39:47.529   Training iter 200, batch loss 1.1097, batch acc 0.6620
11:39:48.084   Training iter 250, batch loss 1.0862, batch acc 0.6722
11:39:48.627   Training iter 300, batch loss 1.0823, batch acc 0.6768
11:39:49.190   Training iter 350, batch loss 1.0677, batch acc 0.6764
11:39:49.721   Training iter 400, batch loss 1.0645, batch acc 0.6810
11:39:50.256   Training iter 450, batch loss 1.0603, batch acc 0.6854
11:39:50.771   Training iter 500, batch loss 1.0762, batch acc 0.6756
11:39:51.293   Training iter 550, batch loss 1.0826, batch acc 0.6750
11:39:51.828   Training iter 600, batch loss 1.0846, batch acc 0.6700
11:39:51.830 Training @ 248 epoch...
11:39:52.313   Training iter 50, batch loss 1.0851, batch acc 0.6696
11:39:52.780   Training iter 100, batch loss 1.0648, batch acc 0.6858
11:39:53.257   Training iter 150, batch loss 1.0729, batch acc 0.6786
11:39:53.739   Training iter 200, batch loss 1.0658, batch acc 0.6860
11:39:54.229   Training iter 250, batch loss 1.0904, batch acc 0.6650
11:39:54.700   Training iter 300, batch loss 1.0986, batch acc 0.6694
11:39:55.152   Training iter 350, batch loss 1.0901, batch acc 0.6732
11:39:55.605   Training iter 400, batch loss 1.0912, batch acc 0.6672
11:39:56.057   Training iter 450, batch loss 1.0681, batch acc 0.6748
11:39:56.511   Training iter 500, batch loss 1.0771, batch acc 0.6762
11:39:56.971   Training iter 550, batch loss 1.0730, batch acc 0.6784
11:39:57.438   Training iter 600, batch loss 1.0748, batch acc 0.6752
11:39:57.440 Training @ 249 epoch...
11:39:57.903   Training iter 50, batch loss 1.1079, batch acc 0.6630
11:39:58.357   Training iter 100, batch loss 1.0836, batch acc 0.6792
11:39:58.816   Training iter 150, batch loss 1.0985, batch acc 0.6712
11:39:59.276   Training iter 200, batch loss 1.0767, batch acc 0.6752
11:39:59.732   Training iter 250, batch loss 1.0767, batch acc 0.6786
11:40:00.191   Training iter 300, batch loss 1.0678, batch acc 0.6834
11:40:00.649   Training iter 350, batch loss 1.0694, batch acc 0.6748
11:40:01.112   Training iter 400, batch loss 1.0571, batch acc 0.6796
11:40:01.624   Training iter 450, batch loss 1.0879, batch acc 0.6728
11:40:02.175   Training iter 500, batch loss 1.0798, batch acc 0.6726
11:40:02.682   Training iter 550, batch loss 1.0866, batch acc 0.6768
11:40:03.199   Training iter 600, batch loss 1.0597, batch acc 0.6790
11:40:03.200 Training @ 250 epoch...
11:40:03.734   Training iter 50, batch loss 1.0843, batch acc 0.6738
11:40:04.250   Training iter 100, batch loss 1.0951, batch acc 0.6686
11:40:04.780   Training iter 150, batch loss 1.0686, batch acc 0.6800
11:40:05.326   Training iter 200, batch loss 1.0751, batch acc 0.6776
11:40:05.863   Training iter 250, batch loss 1.0333, batch acc 0.6946
11:40:06.411   Training iter 300, batch loss 1.0755, batch acc 0.6782
11:40:06.953   Training iter 350, batch loss 1.0889, batch acc 0.6764
11:40:07.499   Training iter 400, batch loss 1.0874, batch acc 0.6730
11:40:08.036   Training iter 450, batch loss 1.0736, batch acc 0.6694
11:40:08.550   Training iter 500, batch loss 1.0600, batch acc 0.6780
11:40:09.064   Training iter 550, batch loss 1.1147, batch acc 0.6608
11:40:09.569   Training iter 600, batch loss 1.0943, batch acc 0.6700
11:40:09.571 Testing @ 250 epoch...
11:40:09.612     Testing, total mean loss 1.05141, total acc 0.68300
11:40:09.612 Training @ 251 epoch...
11:40:10.141   Training iter 50, batch loss 1.1024, batch acc 0.6704
11:40:10.677   Training iter 100, batch loss 1.0796, batch acc 0.6720
11:40:11.189   Training iter 150, batch loss 1.1108, batch acc 0.6632
11:40:11.704   Training iter 200, batch loss 1.0710, batch acc 0.6846
11:40:12.225   Training iter 250, batch loss 1.0642, batch acc 0.6896
11:40:12.722   Training iter 300, batch loss 1.0354, batch acc 0.6942
11:40:13.226   Training iter 350, batch loss 1.0906, batch acc 0.6680
11:40:13.710   Training iter 400, batch loss 1.0924, batch acc 0.6658
11:40:14.213   Training iter 450, batch loss 1.0871, batch acc 0.6654
11:40:14.689   Training iter 500, batch loss 1.0714, batch acc 0.6786
11:40:15.165   Training iter 550, batch loss 1.0916, batch acc 0.6690
11:40:15.649   Training iter 600, batch loss 1.0541, batch acc 0.6826
11:40:15.651 Training @ 252 epoch...
11:40:16.168   Training iter 50, batch loss 1.0840, batch acc 0.6726
11:40:16.667   Training iter 100, batch loss 1.0326, batch acc 0.6932
11:40:17.201   Training iter 150, batch loss 1.0839, batch acc 0.6754
11:40:17.726   Training iter 200, batch loss 1.0907, batch acc 0.6740
11:40:18.249   Training iter 250, batch loss 1.0852, batch acc 0.6774
11:40:18.764   Training iter 300, batch loss 1.0927, batch acc 0.6676
11:40:19.289   Training iter 350, batch loss 1.1070, batch acc 0.6648
11:40:19.806   Training iter 400, batch loss 1.0952, batch acc 0.6690
11:40:20.330   Training iter 450, batch loss 1.0827, batch acc 0.6744
11:40:20.831   Training iter 500, batch loss 1.0641, batch acc 0.6758
11:40:21.325   Training iter 550, batch loss 1.0688, batch acc 0.6740
11:40:21.821   Training iter 600, batch loss 1.0630, batch acc 0.6846
11:40:21.823 Training @ 253 epoch...
11:40:22.337   Training iter 50, batch loss 1.0758, batch acc 0.6742
11:40:22.858   Training iter 100, batch loss 1.0856, batch acc 0.6708
11:40:23.381   Training iter 150, batch loss 1.0592, batch acc 0.6808
11:40:23.892   Training iter 200, batch loss 1.0672, batch acc 0.6810
11:40:24.400   Training iter 250, batch loss 1.0772, batch acc 0.6768
11:40:24.919   Training iter 300, batch loss 1.0834, batch acc 0.6720
11:40:25.436   Training iter 350, batch loss 1.0965, batch acc 0.6730
11:40:25.957   Training iter 400, batch loss 1.0689, batch acc 0.6752
11:40:26.493   Training iter 450, batch loss 1.0763, batch acc 0.6784
11:40:27.022   Training iter 500, batch loss 1.0955, batch acc 0.6690
11:40:27.540   Training iter 550, batch loss 1.0823, batch acc 0.6766
11:40:28.053   Training iter 600, batch loss 1.0817, batch acc 0.6728
11:40:28.055 Training @ 254 epoch...
11:40:28.572   Training iter 50, batch loss 1.0726, batch acc 0.6774
11:40:29.099   Training iter 100, batch loss 1.0508, batch acc 0.6876
11:40:29.619   Training iter 150, batch loss 1.0745, batch acc 0.6694
11:40:30.154   Training iter 200, batch loss 1.1147, batch acc 0.6582
11:40:30.682   Training iter 250, batch loss 1.0784, batch acc 0.6772
11:40:31.210   Training iter 300, batch loss 1.1007, batch acc 0.6646
11:40:31.727   Training iter 350, batch loss 1.0506, batch acc 0.6864
11:40:32.264   Training iter 400, batch loss 1.0733, batch acc 0.6800
11:40:32.795   Training iter 450, batch loss 1.1241, batch acc 0.6540
11:40:33.312   Training iter 500, batch loss 1.0784, batch acc 0.6806
11:40:33.819   Training iter 550, batch loss 1.0669, batch acc 0.6808
11:40:34.340   Training iter 600, batch loss 1.0642, batch acc 0.6900
11:40:34.341 Training @ 255 epoch...
11:40:34.862   Training iter 50, batch loss 1.0765, batch acc 0.6758
11:40:35.334   Training iter 100, batch loss 1.0461, batch acc 0.6918
11:40:35.828   Training iter 150, batch loss 1.1116, batch acc 0.6646
11:40:36.317   Training iter 200, batch loss 1.0864, batch acc 0.6702
11:40:36.790   Training iter 250, batch loss 1.1050, batch acc 0.6652
11:40:37.301   Training iter 300, batch loss 1.0656, batch acc 0.6750
11:40:37.813   Training iter 350, batch loss 1.0622, batch acc 0.6746
11:40:38.336   Training iter 400, batch loss 1.0770, batch acc 0.6768
11:40:38.853   Training iter 450, batch loss 1.0761, batch acc 0.6800
11:40:39.373   Training iter 500, batch loss 1.0909, batch acc 0.6710
11:40:39.896   Training iter 550, batch loss 1.0764, batch acc 0.6774
11:40:40.437   Training iter 600, batch loss 1.0746, batch acc 0.6808
11:40:40.439 Testing @ 255 epoch...
11:40:40.479     Testing, total mean loss 1.05125, total acc 0.68380
11:40:40.479 Training @ 256 epoch...
11:40:40.991   Training iter 50, batch loss 1.0750, batch acc 0.6754
11:40:41.502   Training iter 100, batch loss 1.0616, batch acc 0.6790
11:40:42.007   Training iter 150, batch loss 1.0710, batch acc 0.6818
11:40:42.531   Training iter 200, batch loss 1.0733, batch acc 0.6780
11:40:43.054   Training iter 250, batch loss 1.0750, batch acc 0.6750
11:40:43.571   Training iter 300, batch loss 1.1006, batch acc 0.6676
11:40:44.098   Training iter 350, batch loss 1.0814, batch acc 0.6750
11:40:44.635   Training iter 400, batch loss 1.0574, batch acc 0.6792
11:40:45.152   Training iter 450, batch loss 1.0800, batch acc 0.6772
11:40:45.657   Training iter 500, batch loss 1.0938, batch acc 0.6698
11:40:46.161   Training iter 550, batch loss 1.0959, batch acc 0.6690
11:40:46.678   Training iter 600, batch loss 1.0831, batch acc 0.6776
11:40:46.679 Training @ 257 epoch...
11:40:47.198   Training iter 50, batch loss 1.0814, batch acc 0.6744
11:40:47.708   Training iter 100, batch loss 1.0780, batch acc 0.6754
11:40:48.213   Training iter 150, batch loss 1.0874, batch acc 0.6684
11:40:48.718   Training iter 200, batch loss 1.0635, batch acc 0.6766
11:40:49.227   Training iter 250, batch loss 1.0825, batch acc 0.6738
11:40:49.735   Training iter 300, batch loss 1.0667, batch acc 0.6798
11:40:50.253   Training iter 350, batch loss 1.0882, batch acc 0.6738
11:40:50.760   Training iter 400, batch loss 1.0955, batch acc 0.6732
11:40:51.259   Training iter 450, batch loss 1.0745, batch acc 0.6754
11:40:51.742   Training iter 500, batch loss 1.0608, batch acc 0.6808
11:40:52.249   Training iter 550, batch loss 1.0688, batch acc 0.6804
11:40:52.768   Training iter 600, batch loss 1.1001, batch acc 0.6726
11:40:52.770 Training @ 258 epoch...
11:40:53.284   Training iter 50, batch loss 1.0797, batch acc 0.6694
11:40:53.800   Training iter 100, batch loss 1.0909, batch acc 0.6696
11:40:54.326   Training iter 150, batch loss 1.0890, batch acc 0.6756
11:40:54.849   Training iter 200, batch loss 1.0843, batch acc 0.6758
11:40:55.384   Training iter 250, batch loss 1.0598, batch acc 0.6786
11:40:55.901   Training iter 300, batch loss 1.0848, batch acc 0.6752
11:40:56.416   Training iter 350, batch loss 1.0941, batch acc 0.6674
11:40:56.954   Training iter 400, batch loss 1.0777, batch acc 0.6774
11:40:57.519   Training iter 450, batch loss 1.0811, batch acc 0.6794
11:40:58.038   Training iter 500, batch loss 1.0892, batch acc 0.6728
11:40:58.568   Training iter 550, batch loss 1.0617, batch acc 0.6858
11:40:59.093   Training iter 600, batch loss 1.0548, batch acc 0.6782
11:40:59.095 Training @ 259 epoch...
11:40:59.611   Training iter 50, batch loss 1.0455, batch acc 0.6900
11:41:00.137   Training iter 100, batch loss 1.0898, batch acc 0.6684
11:41:00.663   Training iter 150, batch loss 1.1054, batch acc 0.6596
11:41:01.191   Training iter 200, batch loss 1.0729, batch acc 0.6836
11:41:01.754   Training iter 250, batch loss 1.1007, batch acc 0.6706
11:41:02.272   Training iter 300, batch loss 1.0576, batch acc 0.6818
11:41:02.770   Training iter 350, batch loss 1.0692, batch acc 0.6790
11:41:03.262   Training iter 400, batch loss 1.1013, batch acc 0.6684
11:41:03.744   Training iter 450, batch loss 1.0855, batch acc 0.6712
11:41:04.227   Training iter 500, batch loss 1.0787, batch acc 0.6770
11:41:04.719   Training iter 550, batch loss 1.0483, batch acc 0.6866
11:41:05.208   Training iter 600, batch loss 1.0922, batch acc 0.6680
11:41:05.209 Training @ 260 epoch...
11:41:05.700   Training iter 50, batch loss 1.0631, batch acc 0.6798
11:41:06.187   Training iter 100, batch loss 1.0740, batch acc 0.6786
11:41:06.664   Training iter 150, batch loss 1.0742, batch acc 0.6688
11:41:07.152   Training iter 200, batch loss 1.0887, batch acc 0.6772
11:41:07.646   Training iter 250, batch loss 1.1004, batch acc 0.6652
11:41:08.148   Training iter 300, batch loss 1.0671, batch acc 0.6774
11:41:08.645   Training iter 350, batch loss 1.0631, batch acc 0.6862
11:41:09.140   Training iter 400, batch loss 1.0921, batch acc 0.6684
11:41:09.624   Training iter 450, batch loss 1.0433, batch acc 0.6934
11:41:10.117   Training iter 500, batch loss 1.1022, batch acc 0.6678
11:41:10.616   Training iter 550, batch loss 1.0683, batch acc 0.6808
11:41:11.106   Training iter 600, batch loss 1.1097, batch acc 0.6614
11:41:11.108 Testing @ 260 epoch...
11:41:11.148     Testing, total mean loss 1.05104, total acc 0.68360
11:41:11.148 Training @ 261 epoch...
11:41:11.641   Training iter 50, batch loss 1.0805, batch acc 0.6778
11:41:12.155   Training iter 100, batch loss 1.0850, batch acc 0.6694
11:41:12.687   Training iter 150, batch loss 1.0768, batch acc 0.6734
11:41:13.234   Training iter 200, batch loss 1.0692, batch acc 0.6834
11:41:13.771   Training iter 250, batch loss 1.0910, batch acc 0.6704
11:41:14.322   Training iter 300, batch loss 1.0768, batch acc 0.6780
11:41:14.848   Training iter 350, batch loss 1.0849, batch acc 0.6702
11:41:15.375   Training iter 400, batch loss 1.0904, batch acc 0.6720
11:41:15.919   Training iter 450, batch loss 1.0529, batch acc 0.6872
11:41:16.499   Training iter 500, batch loss 1.0791, batch acc 0.6764
11:41:17.080   Training iter 550, batch loss 1.0933, batch acc 0.6720
11:41:17.657   Training iter 600, batch loss 1.0659, batch acc 0.6744
11:41:17.658 Training @ 262 epoch...
11:41:18.150   Training iter 50, batch loss 1.0899, batch acc 0.6728
11:41:18.636   Training iter 100, batch loss 1.0978, batch acc 0.6638
11:41:19.134   Training iter 150, batch loss 1.1092, batch acc 0.6650
11:41:19.638   Training iter 200, batch loss 1.0645, batch acc 0.6840
11:41:20.139   Training iter 250, batch loss 1.0751, batch acc 0.6718
11:41:20.661   Training iter 300, batch loss 1.0801, batch acc 0.6734
11:41:21.178   Training iter 350, batch loss 1.0777, batch acc 0.6782
11:41:21.686   Training iter 400, batch loss 1.0731, batch acc 0.6794
11:41:22.198   Training iter 450, batch loss 1.0792, batch acc 0.6824
11:41:22.720   Training iter 500, batch loss 1.0788, batch acc 0.6732
11:41:23.253   Training iter 550, batch loss 1.0632, batch acc 0.6774
11:41:23.780   Training iter 600, batch loss 1.0569, batch acc 0.6810
11:41:23.781 Training @ 263 epoch...
11:41:24.304   Training iter 50, batch loss 1.0534, batch acc 0.6800
11:41:24.828   Training iter 100, batch loss 1.0922, batch acc 0.6706
11:41:25.351   Training iter 150, batch loss 1.1002, batch acc 0.6664
11:41:25.866   Training iter 200, batch loss 1.0909, batch acc 0.6740
11:41:26.392   Training iter 250, batch loss 1.0703, batch acc 0.6768
11:41:26.906   Training iter 300, batch loss 1.0689, batch acc 0.6734
11:41:27.426   Training iter 350, batch loss 1.0592, batch acc 0.6854
11:41:27.943   Training iter 400, batch loss 1.0968, batch acc 0.6722
11:41:28.467   Training iter 450, batch loss 1.0858, batch acc 0.6784
11:41:28.968   Training iter 500, batch loss 1.0771, batch acc 0.6764
11:41:29.455   Training iter 550, batch loss 1.0812, batch acc 0.6682
11:41:29.915   Training iter 600, batch loss 1.0688, batch acc 0.6822
11:41:29.917 Training @ 264 epoch...
11:41:30.419   Training iter 50, batch loss 1.0869, batch acc 0.6692
11:41:30.928   Training iter 100, batch loss 1.0728, batch acc 0.6780
11:41:31.441   Training iter 150, batch loss 1.0613, batch acc 0.6774
11:41:31.965   Training iter 200, batch loss 1.1185, batch acc 0.6596
11:41:32.492   Training iter 250, batch loss 1.0857, batch acc 0.6826
11:41:33.016   Training iter 300, batch loss 1.1032, batch acc 0.6702
11:41:33.526   Training iter 350, batch loss 1.0404, batch acc 0.6910
11:41:34.041   Training iter 400, batch loss 1.0725, batch acc 0.6746
11:41:34.554   Training iter 450, batch loss 1.0918, batch acc 0.6728
11:41:35.062   Training iter 500, batch loss 1.0618, batch acc 0.6828
11:41:35.575   Training iter 550, batch loss 1.0655, batch acc 0.6748
11:41:36.087   Training iter 600, batch loss 1.0844, batch acc 0.6738
11:41:36.089 Training @ 265 epoch...
11:41:36.591   Training iter 50, batch loss 1.0821, batch acc 0.6724
11:41:37.108   Training iter 100, batch loss 1.0707, batch acc 0.6860
11:41:37.627   Training iter 150, batch loss 1.0669, batch acc 0.6828
11:41:38.152   Training iter 200, batch loss 1.0727, batch acc 0.6760
11:41:38.692   Training iter 250, batch loss 1.0775, batch acc 0.6752
11:41:39.276   Training iter 300, batch loss 1.0603, batch acc 0.6888
11:41:39.814   Training iter 350, batch loss 1.1003, batch acc 0.6666
11:41:40.363   Training iter 400, batch loss 1.0909, batch acc 0.6658
11:41:40.866   Training iter 450, batch loss 1.0841, batch acc 0.6690
11:41:41.381   Training iter 500, batch loss 1.1027, batch acc 0.6664
11:41:41.890   Training iter 550, batch loss 1.0784, batch acc 0.6714
11:41:42.414   Training iter 600, batch loss 1.0575, batch acc 0.6832
11:41:42.416 Testing @ 265 epoch...
11:41:42.458     Testing, total mean loss 1.05086, total acc 0.68350
11:41:42.459 Training @ 266 epoch...
11:41:42.988   Training iter 50, batch loss 1.0746, batch acc 0.6832
11:41:43.498   Training iter 100, batch loss 1.0691, batch acc 0.6770
11:41:44.024   Training iter 150, batch loss 1.0860, batch acc 0.6744
11:41:44.573   Training iter 200, batch loss 1.1048, batch acc 0.6734
11:41:45.148   Training iter 250, batch loss 1.0609, batch acc 0.6862
11:41:45.711   Training iter 300, batch loss 1.0802, batch acc 0.6700
11:41:46.265   Training iter 350, batch loss 1.0592, batch acc 0.6798
11:41:46.816   Training iter 400, batch loss 1.0807, batch acc 0.6720
11:41:47.384   Training iter 450, batch loss 1.0939, batch acc 0.6698
11:41:47.926   Training iter 500, batch loss 1.0803, batch acc 0.6708
11:41:48.475   Training iter 550, batch loss 1.0782, batch acc 0.6744
11:41:49.016   Training iter 600, batch loss 1.0756, batch acc 0.6730
11:41:49.017 Training @ 267 epoch...
11:41:49.565   Training iter 50, batch loss 1.0953, batch acc 0.6678
11:41:50.122   Training iter 100, batch loss 1.0792, batch acc 0.6754
11:41:50.656   Training iter 150, batch loss 1.0801, batch acc 0.6742
11:41:51.162   Training iter 200, batch loss 1.0716, batch acc 0.6806
11:41:51.669   Training iter 250, batch loss 1.0786, batch acc 0.6834
11:41:52.183   Training iter 300, batch loss 1.0678, batch acc 0.6790
11:41:52.686   Training iter 350, batch loss 1.0715, batch acc 0.6772
11:41:53.196   Training iter 400, batch loss 1.0778, batch acc 0.6724
11:41:53.708   Training iter 450, batch loss 1.0801, batch acc 0.6782
11:41:54.215   Training iter 500, batch loss 1.0780, batch acc 0.6732
11:41:54.739   Training iter 550, batch loss 1.0747, batch acc 0.6742
11:41:55.288   Training iter 600, batch loss 1.0889, batch acc 0.6696
11:41:55.290 Training @ 268 epoch...
11:41:55.823   Training iter 50, batch loss 1.0563, batch acc 0.6860
11:41:56.348   Training iter 100, batch loss 1.1022, batch acc 0.6654
11:41:56.869   Training iter 150, batch loss 1.0837, batch acc 0.6712
11:41:57.402   Training iter 200, batch loss 1.0747, batch acc 0.6820
11:41:57.937   Training iter 250, batch loss 1.0728, batch acc 0.6754
11:41:58.456   Training iter 300, batch loss 1.0690, batch acc 0.6774
11:41:58.978   Training iter 350, batch loss 1.0857, batch acc 0.6734
11:41:59.501   Training iter 400, batch loss 1.1018, batch acc 0.6668
11:42:00.051   Training iter 450, batch loss 1.0584, batch acc 0.6822
11:42:00.572   Training iter 500, batch loss 1.0780, batch acc 0.6752
11:42:01.125   Training iter 550, batch loss 1.0806, batch acc 0.6718
11:42:01.684   Training iter 600, batch loss 1.0797, batch acc 0.6794
11:42:01.686 Training @ 269 epoch...
11:42:02.260   Training iter 50, batch loss 1.0981, batch acc 0.6718
11:42:02.799   Training iter 100, batch loss 1.0702, batch acc 0.6758
11:42:03.343   Training iter 150, batch loss 1.0726, batch acc 0.6746
11:42:03.852   Training iter 200, batch loss 1.0660, batch acc 0.6850
11:42:04.370   Training iter 250, batch loss 1.0874, batch acc 0.6766
11:42:04.886   Training iter 300, batch loss 1.0732, batch acc 0.6832
11:42:05.399   Training iter 350, batch loss 1.0695, batch acc 0.6794
11:42:05.907   Training iter 400, batch loss 1.1126, batch acc 0.6602
11:42:06.446   Training iter 450, batch loss 1.0738, batch acc 0.6726
11:42:06.972   Training iter 500, batch loss 1.0565, batch acc 0.6760
11:42:07.477   Training iter 550, batch loss 1.0943, batch acc 0.6726
11:42:07.979   Training iter 600, batch loss 1.0684, batch acc 0.6750
11:42:07.981 Training @ 270 epoch...
11:42:08.484   Training iter 50, batch loss 1.0756, batch acc 0.6712
11:42:08.986   Training iter 100, batch loss 1.0642, batch acc 0.6814
11:42:09.497   Training iter 150, batch loss 1.0928, batch acc 0.6644
11:42:09.984   Training iter 200, batch loss 1.0674, batch acc 0.6750
11:42:10.474   Training iter 250, batch loss 1.1102, batch acc 0.6604
11:42:10.966   Training iter 300, batch loss 1.0627, batch acc 0.6854
11:42:11.433   Training iter 350, batch loss 1.0311, batch acc 0.6972
11:42:11.935   Training iter 400, batch loss 1.1010, batch acc 0.6698
11:42:12.471   Training iter 450, batch loss 1.0635, batch acc 0.6830
11:42:12.986   Training iter 500, batch loss 1.0778, batch acc 0.6728
11:42:13.508   Training iter 550, batch loss 1.0997, batch acc 0.6664
11:42:14.053   Training iter 600, batch loss 1.0962, batch acc 0.6766
11:42:14.054 Testing @ 270 epoch...
11:42:14.100     Testing, total mean loss 1.05077, total acc 0.68410
11:42:14.100 Training @ 271 epoch...
11:42:14.638   Training iter 50, batch loss 1.0883, batch acc 0.6720
11:42:15.175   Training iter 100, batch loss 1.0535, batch acc 0.6854
11:42:15.700   Training iter 150, batch loss 1.0916, batch acc 0.6722
11:42:16.239   Training iter 200, batch loss 1.0818, batch acc 0.6810
11:42:16.782   Training iter 250, batch loss 1.0693, batch acc 0.6758
11:42:17.321   Training iter 300, batch loss 1.0613, batch acc 0.6804
11:42:17.855   Training iter 350, batch loss 1.0790, batch acc 0.6750
11:42:18.385   Training iter 400, batch loss 1.0700, batch acc 0.6744
11:42:18.910   Training iter 450, batch loss 1.0951, batch acc 0.6704
11:42:19.442   Training iter 500, batch loss 1.0792, batch acc 0.6748
11:42:20.027   Training iter 550, batch loss 1.0729, batch acc 0.6746
11:42:20.595   Training iter 600, batch loss 1.0999, batch acc 0.6694
11:42:20.596 Training @ 272 epoch...
11:42:21.180   Training iter 50, batch loss 1.0762, batch acc 0.6808
11:42:21.748   Training iter 100, batch loss 1.0888, batch acc 0.6758
11:42:22.289   Training iter 150, batch loss 1.0947, batch acc 0.6724
11:42:22.806   Training iter 200, batch loss 1.0818, batch acc 0.6772
11:42:23.355   Training iter 250, batch loss 1.0747, batch acc 0.6730
11:42:23.876   Training iter 300, batch loss 1.0832, batch acc 0.6704
11:42:24.408   Training iter 350, batch loss 1.0813, batch acc 0.6680
11:42:24.934   Training iter 400, batch loss 1.0769, batch acc 0.6732
11:42:25.472   Training iter 450, batch loss 1.1015, batch acc 0.6750
11:42:25.992   Training iter 500, batch loss 1.0570, batch acc 0.6822
11:42:26.517   Training iter 550, batch loss 1.0563, batch acc 0.6858
11:42:27.048   Training iter 600, batch loss 1.0693, batch acc 0.6762
11:42:27.050 Training @ 273 epoch...
11:42:27.584   Training iter 50, batch loss 1.1027, batch acc 0.6678
11:42:28.133   Training iter 100, batch loss 1.0749, batch acc 0.6714
11:42:28.681   Training iter 150, batch loss 1.0751, batch acc 0.6800
11:42:29.236   Training iter 200, batch loss 1.0913, batch acc 0.6754
11:42:29.794   Training iter 250, batch loss 1.0781, batch acc 0.6748
11:42:30.370   Training iter 300, batch loss 1.0629, batch acc 0.6782
11:42:30.919   Training iter 350, batch loss 1.0751, batch acc 0.6764
11:42:31.497   Training iter 400, batch loss 1.0631, batch acc 0.6790
11:42:32.037   Training iter 450, batch loss 1.0883, batch acc 0.6712
11:42:32.577   Training iter 500, batch loss 1.0839, batch acc 0.6764
11:42:33.141   Training iter 550, batch loss 1.0657, batch acc 0.6792
11:42:33.698   Training iter 600, batch loss 1.0799, batch acc 0.6752
11:42:33.700 Training @ 274 epoch...
11:42:34.274   Training iter 50, batch loss 1.0796, batch acc 0.6714
11:42:34.818   Training iter 100, batch loss 1.0724, batch acc 0.6736
11:42:35.349   Training iter 150, batch loss 1.0904, batch acc 0.6720
11:42:35.862   Training iter 200, batch loss 1.1022, batch acc 0.6640
11:42:36.393   Training iter 250, batch loss 1.0913, batch acc 0.6748
11:42:36.914   Training iter 300, batch loss 1.0767, batch acc 0.6796
11:42:37.447   Training iter 350, batch loss 1.0697, batch acc 0.6806
11:42:37.998   Training iter 400, batch loss 1.0541, batch acc 0.6828
11:42:38.548   Training iter 450, batch loss 1.0824, batch acc 0.6754
11:42:39.071   Training iter 500, batch loss 1.0597, batch acc 0.6892
11:42:39.582   Training iter 550, batch loss 1.1008, batch acc 0.6662
11:42:40.104   Training iter 600, batch loss 1.0617, batch acc 0.6776
11:42:40.106 Training @ 275 epoch...
11:42:40.635   Training iter 50, batch loss 1.0836, batch acc 0.6768
11:42:41.157   Training iter 100, batch loss 1.0495, batch acc 0.6856
11:42:41.687   Training iter 150, batch loss 1.0877, batch acc 0.6696
11:42:42.223   Training iter 200, batch loss 1.0873, batch acc 0.6770
11:42:42.753   Training iter 250, batch loss 1.0745, batch acc 0.6716
11:42:43.305   Training iter 300, batch loss 1.0953, batch acc 0.6652
11:42:43.846   Training iter 350, batch loss 1.0701, batch acc 0.6782
11:42:44.409   Training iter 400, batch loss 1.0639, batch acc 0.6816
11:42:44.970   Training iter 450, batch loss 1.0666, batch acc 0.6822
11:42:45.486   Training iter 500, batch loss 1.0761, batch acc 0.6778
11:42:45.982   Training iter 550, batch loss 1.1077, batch acc 0.6648
11:42:46.462   Training iter 600, batch loss 1.0779, batch acc 0.6762
11:42:46.463 Testing @ 275 epoch...
11:42:46.505     Testing, total mean loss 1.05057, total acc 0.68350
11:42:46.505 Training @ 276 epoch...
11:42:46.994   Training iter 50, batch loss 1.1012, batch acc 0.6666
11:42:47.454   Training iter 100, batch loss 1.0704, batch acc 0.6762
11:42:47.911   Training iter 150, batch loss 1.0601, batch acc 0.6878
11:42:48.382   Training iter 200, batch loss 1.0797, batch acc 0.6714
11:42:48.857   Training iter 250, batch loss 1.0913, batch acc 0.6708
11:42:49.340   Training iter 300, batch loss 1.0939, batch acc 0.6684
11:42:49.819   Training iter 350, batch loss 1.0953, batch acc 0.6702
11:42:50.285   Training iter 400, batch loss 1.0705, batch acc 0.6804
11:42:50.749   Training iter 450, batch loss 1.0539, batch acc 0.6838
11:42:51.235   Training iter 500, batch loss 1.0599, batch acc 0.6858
11:42:51.736   Training iter 550, batch loss 1.0886, batch acc 0.6696
11:42:52.255   Training iter 600, batch loss 1.0753, batch acc 0.6760
11:42:52.257 Training @ 277 epoch...
11:42:52.745   Training iter 50, batch loss 1.0341, batch acc 0.6874
11:42:53.221   Training iter 100, batch loss 1.0760, batch acc 0.6750
11:42:53.717   Training iter 150, batch loss 1.0571, batch acc 0.6842
11:42:54.209   Training iter 200, batch loss 1.0944, batch acc 0.6686
11:42:54.692   Training iter 250, batch loss 1.0658, batch acc 0.6838
11:42:55.183   Training iter 300, batch loss 1.0892, batch acc 0.6740
11:42:55.677   Training iter 350, batch loss 1.0565, batch acc 0.6830
11:42:56.182   Training iter 400, batch loss 1.1030, batch acc 0.6686
11:42:56.654   Training iter 450, batch loss 1.0926, batch acc 0.6656
11:42:57.150   Training iter 500, batch loss 1.0850, batch acc 0.6782
11:42:57.648   Training iter 550, batch loss 1.1022, batch acc 0.6650
11:42:58.164   Training iter 600, batch loss 1.0837, batch acc 0.6734
11:42:58.166 Training @ 278 epoch...
11:42:58.686   Training iter 50, batch loss 1.0702, batch acc 0.6768
11:42:59.201   Training iter 100, batch loss 1.0843, batch acc 0.6692
11:42:59.727   Training iter 150, batch loss 1.0810, batch acc 0.6756
11:43:00.248   Training iter 200, batch loss 1.0613, batch acc 0.6802
11:43:00.756   Training iter 250, batch loss 1.0835, batch acc 0.6726
11:43:01.270   Training iter 300, batch loss 1.0890, batch acc 0.6734
11:43:01.802   Training iter 350, batch loss 1.0708, batch acc 0.6804
11:43:02.359   Training iter 400, batch loss 1.0702, batch acc 0.6800
11:43:02.894   Training iter 450, batch loss 1.0653, batch acc 0.6722
11:43:03.434   Training iter 500, batch loss 1.1008, batch acc 0.6722
11:43:03.958   Training iter 550, batch loss 1.0953, batch acc 0.6746
11:43:04.516   Training iter 600, batch loss 1.0675, batch acc 0.6800
11:43:04.518 Training @ 279 epoch...
11:43:05.081   Training iter 50, batch loss 1.0663, batch acc 0.6806
11:43:05.630   Training iter 100, batch loss 1.0857, batch acc 0.6760
11:43:06.208   Training iter 150, batch loss 1.0626, batch acc 0.6774
11:43:06.954   Training iter 200, batch loss 1.0873, batch acc 0.6762
11:43:07.706   Training iter 250, batch loss 1.0760, batch acc 0.6774
11:43:08.257   Training iter 300, batch loss 1.0889, batch acc 0.6760
11:43:08.802   Training iter 350, batch loss 1.0820, batch acc 0.6696
11:43:09.360   Training iter 400, batch loss 1.0895, batch acc 0.6698
11:43:09.917   Training iter 450, batch loss 1.0778, batch acc 0.6784
11:43:10.461   Training iter 500, batch loss 1.0854, batch acc 0.6724
11:43:10.997   Training iter 550, batch loss 1.0629, batch acc 0.6800
11:43:11.548   Training iter 600, batch loss 1.0744, batch acc 0.6758
11:43:11.550 Training @ 280 epoch...
11:43:12.118   Training iter 50, batch loss 1.0890, batch acc 0.6718
11:43:12.681   Training iter 100, batch loss 1.0601, batch acc 0.6844
11:43:13.215   Training iter 150, batch loss 1.0882, batch acc 0.6720
11:43:13.747   Training iter 200, batch loss 1.0776, batch acc 0.6744
11:43:14.272   Training iter 250, batch loss 1.0740, batch acc 0.6734
11:43:14.795   Training iter 300, batch loss 1.0673, batch acc 0.6832
11:43:15.343   Training iter 350, batch loss 1.0922, batch acc 0.6754
11:43:15.875   Training iter 400, batch loss 1.1061, batch acc 0.6650
11:43:16.396   Training iter 450, batch loss 1.0724, batch acc 0.6770
11:43:16.915   Training iter 500, batch loss 1.0729, batch acc 0.6752
11:43:17.434   Training iter 550, batch loss 1.0643, batch acc 0.6778
11:43:17.960   Training iter 600, batch loss 1.0746, batch acc 0.6742
11:43:17.962 Testing @ 280 epoch...
11:43:18.005     Testing, total mean loss 1.05053, total acc 0.68380
11:43:18.005 Training @ 281 epoch...
11:43:18.530   Training iter 50, batch loss 1.0895, batch acc 0.6636
11:43:19.047   Training iter 100, batch loss 1.1032, batch acc 0.6726
11:43:19.562   Training iter 150, batch loss 1.0760, batch acc 0.6744
11:43:20.081   Training iter 200, batch loss 1.0537, batch acc 0.6866
11:43:20.590   Training iter 250, batch loss 1.0843, batch acc 0.6756
11:43:21.111   Training iter 300, batch loss 1.0949, batch acc 0.6696
11:43:21.626   Training iter 350, batch loss 1.0807, batch acc 0.6726
11:43:22.161   Training iter 400, batch loss 1.0549, batch acc 0.6876
11:43:22.701   Training iter 450, batch loss 1.1314, batch acc 0.6548
11:43:23.284   Training iter 500, batch loss 1.0577, batch acc 0.6854
11:43:23.833   Training iter 550, batch loss 1.0373, batch acc 0.6948
11:43:24.365   Training iter 600, batch loss 1.0750, batch acc 0.6718
11:43:24.367 Training @ 282 epoch...
11:43:24.941   Training iter 50, batch loss 1.0900, batch acc 0.6712
11:43:25.490   Training iter 100, batch loss 1.0501, batch acc 0.6788
11:43:26.008   Training iter 150, batch loss 1.0610, batch acc 0.6884
11:43:26.528   Training iter 200, batch loss 1.0568, batch acc 0.6814
11:43:27.044   Training iter 250, batch loss 1.0817, batch acc 0.6664
11:43:27.585   Training iter 300, batch loss 1.1011, batch acc 0.6718
11:43:28.144   Training iter 350, batch loss 1.0859, batch acc 0.6718
11:43:28.674   Training iter 400, batch loss 1.0672, batch acc 0.6826
11:43:29.216   Training iter 450, batch loss 1.0951, batch acc 0.6702
11:43:29.751   Training iter 500, batch loss 1.0931, batch acc 0.6702
11:43:30.305   Training iter 550, batch loss 1.0741, batch acc 0.6766
11:43:30.858   Training iter 600, batch loss 1.0817, batch acc 0.6758
11:43:30.860 Training @ 283 epoch...
11:43:31.406   Training iter 50, batch loss 1.0680, batch acc 0.6722
11:43:31.954   Training iter 100, batch loss 1.0768, batch acc 0.6804
11:43:32.509   Training iter 150, batch loss 1.0681, batch acc 0.6812
11:43:33.047   Training iter 200, batch loss 1.1070, batch acc 0.6648
11:43:33.601   Training iter 250, batch loss 1.0789, batch acc 0.6744
11:43:34.144   Training iter 300, batch loss 1.0778, batch acc 0.6768
11:43:34.699   Training iter 350, batch loss 1.0812, batch acc 0.6736
11:43:35.220   Training iter 400, batch loss 1.0745, batch acc 0.6746
11:43:35.698   Training iter 450, batch loss 1.0830, batch acc 0.6722
11:43:36.207   Training iter 500, batch loss 1.0953, batch acc 0.6720
11:43:36.696   Training iter 550, batch loss 1.0658, batch acc 0.6818
11:43:37.206   Training iter 600, batch loss 1.0612, batch acc 0.6814
11:43:37.207 Training @ 284 epoch...
11:43:37.727   Training iter 50, batch loss 1.0718, batch acc 0.6736
11:43:38.236   Training iter 100, batch loss 1.0846, batch acc 0.6714
11:43:38.733   Training iter 150, batch loss 1.0701, batch acc 0.6734
11:43:39.266   Training iter 200, batch loss 1.0757, batch acc 0.6844
11:43:39.800   Training iter 250, batch loss 1.1119, batch acc 0.6654
11:43:40.363   Training iter 300, batch loss 1.0639, batch acc 0.6808
11:43:40.906   Training iter 350, batch loss 1.0675, batch acc 0.6796
11:43:41.425   Training iter 400, batch loss 1.0905, batch acc 0.6700
11:43:41.925   Training iter 450, batch loss 1.0614, batch acc 0.6804
11:43:42.454   Training iter 500, batch loss 1.0510, batch acc 0.6836
11:43:42.985   Training iter 550, batch loss 1.0960, batch acc 0.6738
11:43:43.510   Training iter 600, batch loss 1.0929, batch acc 0.6704
11:43:43.512 Training @ 285 epoch...
11:43:44.052   Training iter 50, batch loss 1.1068, batch acc 0.6678
11:43:44.584   Training iter 100, batch loss 1.0611, batch acc 0.6848
11:43:45.082   Training iter 150, batch loss 1.0618, batch acc 0.6806
11:43:45.553   Training iter 200, batch loss 1.0684, batch acc 0.6752
11:43:46.056   Training iter 250, batch loss 1.0757, batch acc 0.6786
11:43:46.569   Training iter 300, batch loss 1.0972, batch acc 0.6710
11:43:47.062   Training iter 350, batch loss 1.0456, batch acc 0.6782
11:43:47.553   Training iter 400, batch loss 1.0845, batch acc 0.6720
11:43:48.052   Training iter 450, batch loss 1.0687, batch acc 0.6836
11:43:48.549   Training iter 500, batch loss 1.0913, batch acc 0.6712
11:43:49.046   Training iter 550, batch loss 1.0858, batch acc 0.6728
11:43:49.541   Training iter 600, batch loss 1.0899, batch acc 0.6722
11:43:49.542 Testing @ 285 epoch...
11:43:49.583     Testing, total mean loss 1.05046, total acc 0.68400
11:43:49.583 Training @ 286 epoch...
11:43:50.106   Training iter 50, batch loss 1.0645, batch acc 0.6818
11:43:50.623   Training iter 100, batch loss 1.0784, batch acc 0.6750
11:43:51.124   Training iter 150, batch loss 1.0882, batch acc 0.6724
11:43:51.612   Training iter 200, batch loss 1.0823, batch acc 0.6788
11:43:52.118   Training iter 250, batch loss 1.0885, batch acc 0.6714
11:43:52.642   Training iter 300, batch loss 1.0911, batch acc 0.6648
11:43:53.169   Training iter 350, batch loss 1.0706, batch acc 0.6772
11:43:53.690   Training iter 400, batch loss 1.0777, batch acc 0.6726
11:43:54.201   Training iter 450, batch loss 1.0629, batch acc 0.6848
11:43:54.723   Training iter 500, batch loss 1.0696, batch acc 0.6786
11:43:55.274   Training iter 550, batch loss 1.1022, batch acc 0.6670
11:43:55.816   Training iter 600, batch loss 1.0608, batch acc 0.6812
11:43:55.817 Training @ 287 epoch...
11:43:56.356   Training iter 50, batch loss 1.0965, batch acc 0.6614
11:43:56.858   Training iter 100, batch loss 1.0764, batch acc 0.6764
11:43:57.371   Training iter 150, batch loss 1.0786, batch acc 0.6776
11:43:57.888   Training iter 200, batch loss 1.0729, batch acc 0.6810
11:43:58.404   Training iter 250, batch loss 1.0840, batch acc 0.6782
11:43:58.923   Training iter 300, batch loss 1.0968, batch acc 0.6710
11:43:59.451   Training iter 350, batch loss 1.0665, batch acc 0.6788
11:43:59.944   Training iter 400, batch loss 1.0670, batch acc 0.6772
11:44:00.445   Training iter 450, batch loss 1.0642, batch acc 0.6852
11:44:00.943   Training iter 500, batch loss 1.0856, batch acc 0.6774
11:44:01.462   Training iter 550, batch loss 1.0644, batch acc 0.6766
11:44:01.987   Training iter 600, batch loss 1.0835, batch acc 0.6678
11:44:01.989 Training @ 288 epoch...
11:44:02.517   Training iter 50, batch loss 1.0921, batch acc 0.6748
11:44:03.021   Training iter 100, batch loss 1.0640, batch acc 0.6874
11:44:03.523   Training iter 150, batch loss 1.0928, batch acc 0.6716
11:44:03.993   Training iter 200, batch loss 1.0726, batch acc 0.6714
11:44:04.475   Training iter 250, batch loss 1.1103, batch acc 0.6610
11:44:04.976   Training iter 300, batch loss 1.0523, batch acc 0.6890
11:44:05.481   Training iter 350, batch loss 1.0777, batch acc 0.6764
11:44:05.971   Training iter 400, batch loss 1.0611, batch acc 0.6822
11:44:06.464   Training iter 450, batch loss 1.0775, batch acc 0.6754
11:44:06.954   Training iter 500, batch loss 1.1219, batch acc 0.6600
11:44:07.448   Training iter 550, batch loss 1.0608, batch acc 0.6722
11:44:07.941   Training iter 600, batch loss 1.0528, batch acc 0.6850
11:44:07.943 Training @ 289 epoch...
11:44:08.434   Training iter 50, batch loss 1.0749, batch acc 0.6760
11:44:08.926   Training iter 100, batch loss 1.0545, batch acc 0.6868
11:44:09.408   Training iter 150, batch loss 1.1008, batch acc 0.6614
11:44:09.902   Training iter 200, batch loss 1.0651, batch acc 0.6856
11:44:10.413   Training iter 250, batch loss 1.0735, batch acc 0.6754
11:44:10.937   Training iter 300, batch loss 1.0822, batch acc 0.6740
11:44:11.474   Training iter 350, batch loss 1.0697, batch acc 0.6762
11:44:12.003   Training iter 400, batch loss 1.0848, batch acc 0.6768
11:44:12.555   Training iter 450, batch loss 1.0995, batch acc 0.6658
11:44:13.098   Training iter 500, batch loss 1.0798, batch acc 0.6814
11:44:13.637   Training iter 550, batch loss 1.0700, batch acc 0.6738
11:44:14.163   Training iter 600, batch loss 1.0813, batch acc 0.6718
11:44:14.165 Training @ 290 epoch...
11:44:14.697   Training iter 50, batch loss 1.0626, batch acc 0.6808
11:44:15.222   Training iter 100, batch loss 1.1058, batch acc 0.6628
11:44:15.755   Training iter 150, batch loss 1.0783, batch acc 0.6758
11:44:16.311   Training iter 200, batch loss 1.0824, batch acc 0.6692
11:44:16.866   Training iter 250, batch loss 1.0786, batch acc 0.6754
11:44:17.400   Training iter 300, batch loss 1.0836, batch acc 0.6760
11:44:17.921   Training iter 350, batch loss 1.0947, batch acc 0.6672
11:44:18.443   Training iter 400, batch loss 1.0703, batch acc 0.6754
11:44:18.969   Training iter 450, batch loss 1.0657, batch acc 0.6850
11:44:19.496   Training iter 500, batch loss 1.0662, batch acc 0.6820
11:44:20.025   Training iter 550, batch loss 1.0668, batch acc 0.6802
11:44:20.557   Training iter 600, batch loss 1.0805, batch acc 0.6788
11:44:20.558 Testing @ 290 epoch...
11:44:20.599     Testing, total mean loss 1.05020, total acc 0.68340
11:44:20.599 Training @ 291 epoch...
11:44:21.135   Training iter 50, batch loss 1.0791, batch acc 0.6742
11:44:21.663   Training iter 100, batch loss 1.0814, batch acc 0.6774
11:44:22.217   Training iter 150, batch loss 1.0857, batch acc 0.6798
11:44:22.776   Training iter 200, batch loss 1.0648, batch acc 0.6802
11:44:23.329   Training iter 250, batch loss 1.0940, batch acc 0.6634
11:44:23.880   Training iter 300, batch loss 1.0804, batch acc 0.6694
11:44:24.429   Training iter 350, batch loss 1.0847, batch acc 0.6788
11:44:24.955   Training iter 400, batch loss 1.0788, batch acc 0.6724
11:44:25.484   Training iter 450, batch loss 1.0899, batch acc 0.6704
11:44:25.992   Training iter 500, batch loss 1.0588, batch acc 0.6868
11:44:26.521   Training iter 550, batch loss 1.0584, batch acc 0.6850
11:44:27.062   Training iter 600, batch loss 1.0792, batch acc 0.6712
11:44:27.064 Training @ 292 epoch...
11:44:27.624   Training iter 50, batch loss 1.0756, batch acc 0.6776
11:44:28.177   Training iter 100, batch loss 1.0558, batch acc 0.6814
11:44:28.719   Training iter 150, batch loss 1.0812, batch acc 0.6750
11:44:29.270   Training iter 200, batch loss 1.0822, batch acc 0.6734
11:44:29.823   Training iter 250, batch loss 1.0534, batch acc 0.6852
11:44:30.378   Training iter 300, batch loss 1.0884, batch acc 0.6714
11:44:30.927   Training iter 350, batch loss 1.0683, batch acc 0.6852
11:44:31.480   Training iter 400, batch loss 1.0703, batch acc 0.6808
11:44:32.023   Training iter 450, batch loss 1.0705, batch acc 0.6730
11:44:32.545   Training iter 500, batch loss 1.0970, batch acc 0.6714
11:44:33.064   Training iter 550, batch loss 1.0820, batch acc 0.6742
11:44:33.571   Training iter 600, batch loss 1.1102, batch acc 0.6600
11:44:33.572 Training @ 293 epoch...
11:44:34.076   Training iter 50, batch loss 1.0876, batch acc 0.6672
11:44:34.587   Training iter 100, batch loss 1.0792, batch acc 0.6688
11:44:35.087   Training iter 150, batch loss 1.0644, batch acc 0.6786
11:44:35.590   Training iter 200, batch loss 1.0643, batch acc 0.6820
11:44:36.080   Training iter 250, batch loss 1.0758, batch acc 0.6792
11:44:36.568   Training iter 300, batch loss 1.0693, batch acc 0.6740
11:44:37.073   Training iter 350, batch loss 1.0416, batch acc 0.6876
11:44:37.565   Training iter 400, batch loss 1.0834, batch acc 0.6754
11:44:38.050   Training iter 450, batch loss 1.1068, batch acc 0.6662
11:44:38.528   Training iter 500, batch loss 1.1070, batch acc 0.6648
11:44:39.014   Training iter 550, batch loss 1.0793, batch acc 0.6788
11:44:39.505   Training iter 600, batch loss 1.0757, batch acc 0.6808
11:44:39.507 Training @ 294 epoch...
11:44:39.988   Training iter 50, batch loss 1.0768, batch acc 0.6770
11:44:40.487   Training iter 100, batch loss 1.0722, batch acc 0.6758
11:44:40.968   Training iter 150, batch loss 1.0984, batch acc 0.6688
11:44:41.459   Training iter 200, batch loss 1.0674, batch acc 0.6778
11:44:41.964   Training iter 250, batch loss 1.0591, batch acc 0.6856
11:44:42.484   Training iter 300, batch loss 1.0970, batch acc 0.6720
11:44:43.014   Training iter 350, batch loss 1.1037, batch acc 0.6688
11:44:43.554   Training iter 400, batch loss 1.0791, batch acc 0.6748
11:44:44.100   Training iter 450, batch loss 1.0703, batch acc 0.6786
11:44:44.636   Training iter 500, batch loss 1.0673, batch acc 0.6754
11:44:45.183   Training iter 550, batch loss 1.0920, batch acc 0.6722
11:44:45.738   Training iter 600, batch loss 1.0508, batch acc 0.6820
11:44:45.740 Training @ 295 epoch...
11:44:46.286   Training iter 50, batch loss 1.0512, batch acc 0.6804
11:44:46.840   Training iter 100, batch loss 1.0933, batch acc 0.6742
11:44:47.385   Training iter 150, batch loss 1.0679, batch acc 0.6736
11:44:47.922   Training iter 200, batch loss 1.0675, batch acc 0.6776
11:44:48.475   Training iter 250, batch loss 1.0922, batch acc 0.6736
11:44:49.020   Training iter 300, batch loss 1.0767, batch acc 0.6754
11:44:49.547   Training iter 350, batch loss 1.0667, batch acc 0.6854
11:44:50.071   Training iter 400, batch loss 1.0805, batch acc 0.6738
11:44:50.598   Training iter 450, batch loss 1.0726, batch acc 0.6762
11:44:51.129   Training iter 500, batch loss 1.0749, batch acc 0.6750
11:44:51.644   Training iter 550, batch loss 1.0878, batch acc 0.6742
11:44:52.170   Training iter 600, batch loss 1.1026, batch acc 0.6698
11:44:52.172 Testing @ 295 epoch...
11:44:52.212     Testing, total mean loss 1.05012, total acc 0.68420
11:44:52.212 Training @ 296 epoch...
11:44:52.749   Training iter 50, batch loss 1.0623, batch acc 0.6862
11:44:53.286   Training iter 100, batch loss 1.0664, batch acc 0.6692
11:44:53.805   Training iter 150, batch loss 1.0898, batch acc 0.6734
11:44:54.339   Training iter 200, batch loss 1.0659, batch acc 0.6844
11:44:54.864   Training iter 250, batch loss 1.0601, batch acc 0.6810
11:44:55.386   Training iter 300, batch loss 1.0844, batch acc 0.6730
11:44:55.883   Training iter 350, batch loss 1.0929, batch acc 0.6748
11:44:56.390   Training iter 400, batch loss 1.0796, batch acc 0.6676
11:44:56.898   Training iter 450, batch loss 1.0873, batch acc 0.6744
11:44:57.386   Training iter 500, batch loss 1.0589, batch acc 0.6844
11:44:57.887   Training iter 550, batch loss 1.0913, batch acc 0.6714
11:44:58.369   Training iter 600, batch loss 1.0946, batch acc 0.6668
11:44:58.371 Training @ 297 epoch...
11:44:58.864   Training iter 50, batch loss 1.0920, batch acc 0.6714
11:44:59.368   Training iter 100, batch loss 1.0787, batch acc 0.6748
11:44:59.873   Training iter 150, batch loss 1.0738, batch acc 0.6764
11:45:00.364   Training iter 200, batch loss 1.0715, batch acc 0.6782
11:45:00.842   Training iter 250, batch loss 1.0848, batch acc 0.6708
11:45:01.379   Training iter 300, batch loss 1.0901, batch acc 0.6768
11:45:01.937   Training iter 350, batch loss 1.0904, batch acc 0.6736
11:45:02.489   Training iter 400, batch loss 1.0752, batch acc 0.6806
11:45:03.036   Training iter 450, batch loss 1.0698, batch acc 0.6726
11:45:03.598   Training iter 500, batch loss 1.0529, batch acc 0.6852
11:45:04.158   Training iter 550, batch loss 1.0565, batch acc 0.6812
11:45:04.692   Training iter 600, batch loss 1.0976, batch acc 0.6686
11:45:04.694 Training @ 298 epoch...
11:45:05.259   Training iter 50, batch loss 1.0681, batch acc 0.6882
11:45:05.775   Training iter 100, batch loss 1.0655, batch acc 0.6814
11:45:06.318   Training iter 150, batch loss 1.0736, batch acc 0.6766
11:45:06.853   Training iter 200, batch loss 1.0899, batch acc 0.6694
11:45:07.383   Training iter 250, batch loss 1.0659, batch acc 0.6782
11:45:07.885   Training iter 300, batch loss 1.0761, batch acc 0.6716
11:45:08.385   Training iter 350, batch loss 1.0949, batch acc 0.6742
11:45:08.896   Training iter 400, batch loss 1.0823, batch acc 0.6686
11:45:09.410   Training iter 450, batch loss 1.0708, batch acc 0.6746
11:45:09.936   Training iter 500, batch loss 1.0822, batch acc 0.6746
11:45:10.476   Training iter 550, batch loss 1.0903, batch acc 0.6730
11:45:10.998   Training iter 600, batch loss 1.0737, batch acc 0.6756
11:45:10.999 Training @ 299 epoch...
11:45:11.522   Training iter 50, batch loss 1.0787, batch acc 0.6742
11:45:12.050   Training iter 100, batch loss 1.0874, batch acc 0.6734
11:45:12.578   Training iter 150, batch loss 1.0982, batch acc 0.6708
11:45:13.117   Training iter 200, batch loss 1.0879, batch acc 0.6708
11:45:13.653   Training iter 250, batch loss 1.0861, batch acc 0.6712
11:45:14.177   Training iter 300, batch loss 1.0654, batch acc 0.6858
11:45:14.711   Training iter 350, batch loss 1.0773, batch acc 0.6758
11:45:15.264   Training iter 400, batch loss 1.0712, batch acc 0.6786
11:45:15.811   Training iter 450, batch loss 1.0697, batch acc 0.6798
11:45:16.357   Training iter 500, batch loss 1.0784, batch acc 0.6766
11:45:16.911   Training iter 550, batch loss 1.0755, batch acc 0.6760
11:45:17.470   Training iter 600, batch loss 1.0570, batch acc 0.6780
11:45:17.471 Training @ 300 epoch...
11:45:18.023   Training iter 50, batch loss 1.0531, batch acc 0.6832
11:45:18.566   Training iter 100, batch loss 1.0921, batch acc 0.6762
11:45:19.110   Training iter 150, batch loss 1.0667, batch acc 0.6784
11:45:19.668   Training iter 200, batch loss 1.0834, batch acc 0.6736
11:45:20.227   Training iter 250, batch loss 1.0936, batch acc 0.6720
11:45:20.761   Training iter 300, batch loss 1.0602, batch acc 0.6822
11:45:21.288   Training iter 350, batch loss 1.0703, batch acc 0.6798
11:45:21.801   Training iter 400, batch loss 1.0804, batch acc 0.6776
11:45:22.321   Training iter 450, batch loss 1.0987, batch acc 0.6642
11:45:22.856   Training iter 500, batch loss 1.0742, batch acc 0.6722
11:45:23.381   Training iter 550, batch loss 1.0782, batch acc 0.6758
11:45:23.906   Training iter 600, batch loss 1.0820, batch acc 0.6732
11:45:23.908 Testing @ 300 epoch...
11:45:23.948     Testing, total mean loss 1.04999, total acc 0.68390
11:45:23.948 Plot @ 300 epoch...
11:45:23.948 Training @ 301 epoch...
11:45:24.475   Training iter 50, batch loss 1.0453, batch acc 0.6862
11:45:25.010   Training iter 100, batch loss 1.0623, batch acc 0.6838
11:45:25.540   Training iter 150, batch loss 1.0622, batch acc 0.6824
11:45:26.038   Training iter 200, batch loss 1.1018, batch acc 0.6688
11:45:26.530   Training iter 250, batch loss 1.0711, batch acc 0.6804
11:45:27.053   Training iter 300, batch loss 1.0926, batch acc 0.6706
11:45:27.582   Training iter 350, batch loss 1.0821, batch acc 0.6742
11:45:28.111   Training iter 400, batch loss 1.0923, batch acc 0.6652
11:45:28.654   Training iter 450, batch loss 1.0731, batch acc 0.6798
11:45:29.195   Training iter 500, batch loss 1.0905, batch acc 0.6672
11:45:29.731   Training iter 550, batch loss 1.0766, batch acc 0.6748
11:45:30.254   Training iter 600, batch loss 1.0823, batch acc 0.6746
11:45:30.255 Training @ 302 epoch...
11:45:30.791   Training iter 50, batch loss 1.0703, batch acc 0.6752
11:45:31.339   Training iter 100, batch loss 1.0937, batch acc 0.6748
11:45:31.888   Training iter 150, batch loss 1.0743, batch acc 0.6790
11:45:32.433   Training iter 200, batch loss 1.0802, batch acc 0.6678
11:45:32.983   Training iter 250, batch loss 1.0801, batch acc 0.6740
11:45:33.511   Training iter 300, batch loss 1.0581, batch acc 0.6840
11:45:34.057   Training iter 350, batch loss 1.0832, batch acc 0.6820
11:45:34.585   Training iter 400, batch loss 1.0736, batch acc 0.6738
11:45:35.118   Training iter 450, batch loss 1.0927, batch acc 0.6682
11:45:35.637   Training iter 500, batch loss 1.0913, batch acc 0.6716
11:45:36.164   Training iter 550, batch loss 1.0703, batch acc 0.6772
11:45:36.695   Training iter 600, batch loss 1.0644, batch acc 0.6810
11:45:36.696 Training @ 303 epoch...
11:45:37.235   Training iter 50, batch loss 1.0727, batch acc 0.6750
11:45:37.739   Training iter 100, batch loss 1.0778, batch acc 0.6684
11:45:38.246   Training iter 150, batch loss 1.0790, batch acc 0.6754
11:45:38.747   Training iter 200, batch loss 1.0516, batch acc 0.6874
11:45:39.255   Training iter 250, batch loss 1.1162, batch acc 0.6614
11:45:39.774   Training iter 300, batch loss 1.1081, batch acc 0.6640
11:45:40.294   Training iter 350, batch loss 1.0585, batch acc 0.6844
11:45:40.807   Training iter 400, batch loss 1.0733, batch acc 0.6758
11:45:41.330   Training iter 450, batch loss 1.0786, batch acc 0.6760
11:45:41.839   Training iter 500, batch loss 1.0845, batch acc 0.6744
11:45:42.353   Training iter 550, batch loss 1.0789, batch acc 0.6812
11:45:42.844   Training iter 600, batch loss 1.0527, batch acc 0.6854
11:45:42.845 Training @ 304 epoch...
11:45:43.362   Training iter 50, batch loss 1.0633, batch acc 0.6836
11:45:43.881   Training iter 100, batch loss 1.0728, batch acc 0.6750
11:45:44.397   Training iter 150, batch loss 1.1009, batch acc 0.6666
11:45:44.878   Training iter 200, batch loss 1.0800, batch acc 0.6746
11:45:45.364   Training iter 250, batch loss 1.0557, batch acc 0.6880
11:45:45.861   Training iter 300, batch loss 1.0600, batch acc 0.6796
11:45:46.364   Training iter 350, batch loss 1.0932, batch acc 0.6740
11:45:46.872   Training iter 400, batch loss 1.0751, batch acc 0.6734
11:45:47.380   Training iter 450, batch loss 1.0776, batch acc 0.6738
11:45:47.910   Training iter 500, batch loss 1.0854, batch acc 0.6726
11:45:48.426   Training iter 550, batch loss 1.0919, batch acc 0.6664
11:45:48.942   Training iter 600, batch loss 1.0755, batch acc 0.6806
11:45:48.944 Training @ 305 epoch...
11:45:49.455   Training iter 50, batch loss 1.0855, batch acc 0.6732
11:45:49.962   Training iter 100, batch loss 1.0803, batch acc 0.6776
11:45:50.472   Training iter 150, batch loss 1.0868, batch acc 0.6720
11:45:50.998   Training iter 200, batch loss 1.0706, batch acc 0.6782
11:45:51.549   Training iter 250, batch loss 1.0499, batch acc 0.6844
11:45:52.083   Training iter 300, batch loss 1.0760, batch acc 0.6788
11:45:52.613   Training iter 350, batch loss 1.0900, batch acc 0.6710
11:45:53.156   Training iter 400, batch loss 1.0930, batch acc 0.6686
11:45:53.695   Training iter 450, batch loss 1.0756, batch acc 0.6772
11:45:54.200   Training iter 500, batch loss 1.0714, batch acc 0.6760
11:45:54.704   Training iter 550, batch loss 1.0945, batch acc 0.6722
11:45:55.215   Training iter 600, batch loss 1.0577, batch acc 0.6820
11:45:55.217 Testing @ 305 epoch...
11:45:55.257     Testing, total mean loss 1.05018, total acc 0.68410
11:45:55.257 Training @ 306 epoch...
11:45:55.755   Training iter 50, batch loss 1.0721, batch acc 0.6770
11:45:56.259   Training iter 100, batch loss 1.0459, batch acc 0.6944
11:45:56.752   Training iter 150, batch loss 1.0774, batch acc 0.6718
11:45:57.266   Training iter 200, batch loss 1.0830, batch acc 0.6714
11:45:57.767   Training iter 250, batch loss 1.0996, batch acc 0.6622
11:45:58.275   Training iter 300, batch loss 1.0640, batch acc 0.6806
11:45:58.782   Training iter 350, batch loss 1.0940, batch acc 0.6732
11:45:59.293   Training iter 400, batch loss 1.0636, batch acc 0.6832
11:45:59.803   Training iter 450, batch loss 1.0876, batch acc 0.6706
11:46:00.335   Training iter 500, batch loss 1.0792, batch acc 0.6774
11:46:00.868   Training iter 550, batch loss 1.0894, batch acc 0.6702
11:46:01.400   Training iter 600, batch loss 1.0751, batch acc 0.6752
11:46:01.402 Training @ 307 epoch...
11:46:01.945   Training iter 50, batch loss 1.0806, batch acc 0.6736
11:46:02.476   Training iter 100, batch loss 1.0552, batch acc 0.6832
11:46:03.021   Training iter 150, batch loss 1.1000, batch acc 0.6640
11:46:03.550   Training iter 200, batch loss 1.0644, batch acc 0.6810
11:46:04.119   Training iter 250, batch loss 1.1109, batch acc 0.6674
11:46:04.693   Training iter 300, batch loss 1.0646, batch acc 0.6768
11:46:05.260   Training iter 350, batch loss 1.0811, batch acc 0.6694
11:46:05.819   Training iter 400, batch loss 1.0640, batch acc 0.6810
11:46:06.387   Training iter 450, batch loss 1.0892, batch acc 0.6814
11:46:06.938   Training iter 500, batch loss 1.0897, batch acc 0.6706
11:46:07.503   Training iter 550, batch loss 1.0710, batch acc 0.6758
11:46:08.063   Training iter 600, batch loss 1.0600, batch acc 0.6840
11:46:08.065 Training @ 308 epoch...
11:46:08.621   Training iter 50, batch loss 1.0841, batch acc 0.6730
11:46:09.179   Training iter 100, batch loss 1.0759, batch acc 0.6730
11:46:09.725   Training iter 150, batch loss 1.0680, batch acc 0.6820
11:46:10.242   Training iter 200, batch loss 1.0803, batch acc 0.6804
11:46:10.758   Training iter 250, batch loss 1.0662, batch acc 0.6760
11:46:11.277   Training iter 300, batch loss 1.0591, batch acc 0.6798
11:46:11.802   Training iter 350, batch loss 1.0778, batch acc 0.6734
11:46:12.324   Training iter 400, batch loss 1.0891, batch acc 0.6664
11:46:12.841   Training iter 450, batch loss 1.0826, batch acc 0.6750
11:46:13.371   Training iter 500, batch loss 1.0667, batch acc 0.6856
11:46:13.894   Training iter 550, batch loss 1.0876, batch acc 0.6760
11:46:14.392   Training iter 600, batch loss 1.0931, batch acc 0.6696
11:46:14.393 Training @ 309 epoch...
11:46:14.853   Training iter 50, batch loss 1.0836, batch acc 0.6740
11:46:15.328   Training iter 100, batch loss 1.0611, batch acc 0.6808
11:46:15.783   Training iter 150, batch loss 1.0912, batch acc 0.6768
11:46:16.243   Training iter 200, batch loss 1.0838, batch acc 0.6712
11:46:16.694   Training iter 250, batch loss 1.0791, batch acc 0.6710
11:46:17.157   Training iter 300, batch loss 1.0726, batch acc 0.6776
11:46:17.618   Training iter 350, batch loss 1.0507, batch acc 0.6826
11:46:18.094   Training iter 400, batch loss 1.1048, batch acc 0.6582
11:46:18.565   Training iter 450, batch loss 1.0678, batch acc 0.6866
11:46:19.035   Training iter 500, batch loss 1.0845, batch acc 0.6764
11:46:19.503   Training iter 550, batch loss 1.0757, batch acc 0.6774
11:46:19.987   Training iter 600, batch loss 1.0751, batch acc 0.6800
11:46:19.989 Training @ 310 epoch...
11:46:20.483   Training iter 50, batch loss 1.0683, batch acc 0.6804
11:46:20.957   Training iter 100, batch loss 1.0843, batch acc 0.6782
11:46:21.462   Training iter 150, batch loss 1.0803, batch acc 0.6708
11:46:21.973   Training iter 200, batch loss 1.1011, batch acc 0.6626
11:46:22.475   Training iter 250, batch loss 1.0654, batch acc 0.6834
11:46:22.973   Training iter 300, batch loss 1.0813, batch acc 0.6740
11:46:23.486   Training iter 350, batch loss 1.0535, batch acc 0.6826
11:46:23.997   Training iter 400, batch loss 1.0932, batch acc 0.6744
11:46:24.517   Training iter 450, batch loss 1.0672, batch acc 0.6794
11:46:25.071   Training iter 500, batch loss 1.0569, batch acc 0.6806
11:46:25.587   Training iter 550, batch loss 1.0931, batch acc 0.6758
11:46:26.113   Training iter 600, batch loss 1.0858, batch acc 0.6662
11:46:26.115 Testing @ 310 epoch...
11:46:26.156     Testing, total mean loss 1.04986, total acc 0.68360
11:46:26.156 Training @ 311 epoch...
11:46:26.670   Training iter 50, batch loss 1.0813, batch acc 0.6782
11:46:27.201   Training iter 100, batch loss 1.0808, batch acc 0.6740
11:46:27.723   Training iter 150, batch loss 1.0821, batch acc 0.6646
11:46:28.255   Training iter 200, batch loss 1.0921, batch acc 0.6730
11:46:28.773   Training iter 250, batch loss 1.0603, batch acc 0.6830
11:46:29.300   Training iter 300, batch loss 1.0711, batch acc 0.6804
11:46:29.806   Training iter 350, batch loss 1.0756, batch acc 0.6798
11:46:30.323   Training iter 400, batch loss 1.0599, batch acc 0.6838
11:46:30.847   Training iter 450, batch loss 1.0834, batch acc 0.6714
11:46:31.369   Training iter 500, batch loss 1.0515, batch acc 0.6818
11:46:31.896   Training iter 550, batch loss 1.0953, batch acc 0.6686
11:46:32.424   Training iter 600, batch loss 1.0963, batch acc 0.6708
11:46:32.425 Training @ 312 epoch...
11:46:32.945   Training iter 50, batch loss 1.0535, batch acc 0.6876
11:46:33.457   Training iter 100, batch loss 1.0693, batch acc 0.6830
11:46:33.978   Training iter 150, batch loss 1.0816, batch acc 0.6752
11:46:34.502   Training iter 200, batch loss 1.0704, batch acc 0.6834
11:46:35.027   Training iter 250, batch loss 1.0604, batch acc 0.6802
11:46:35.545   Training iter 300, batch loss 1.0916, batch acc 0.6664
11:46:36.087   Training iter 350, batch loss 1.0844, batch acc 0.6680
11:46:36.615   Training iter 400, batch loss 1.0855, batch acc 0.6694
11:46:37.104   Training iter 450, batch loss 1.0866, batch acc 0.6724
11:46:37.606   Training iter 500, batch loss 1.0801, batch acc 0.6746
11:46:38.110   Training iter 550, batch loss 1.0899, batch acc 0.6716
11:46:38.653   Training iter 600, batch loss 1.0762, batch acc 0.6794
11:46:38.654 Training @ 313 epoch...
11:46:39.209   Training iter 50, batch loss 1.0760, batch acc 0.6774
11:46:39.722   Training iter 100, batch loss 1.0811, batch acc 0.6714
11:46:40.263   Training iter 150, batch loss 1.0782, batch acc 0.6754
11:46:40.787   Training iter 200, batch loss 1.0642, batch acc 0.6818
11:46:41.307   Training iter 250, batch loss 1.0524, batch acc 0.6902
11:46:41.854   Training iter 300, batch loss 1.0787, batch acc 0.6726
11:46:42.422   Training iter 350, batch loss 1.0752, batch acc 0.6762
11:46:42.928   Training iter 400, batch loss 1.0734, batch acc 0.6760
11:46:43.435   Training iter 450, batch loss 1.0734, batch acc 0.6712
11:46:43.955   Training iter 500, batch loss 1.1003, batch acc 0.6742
11:46:44.464   Training iter 550, batch loss 1.0937, batch acc 0.6664
11:46:44.972   Training iter 600, batch loss 1.0826, batch acc 0.6756
11:46:44.973 Training @ 314 epoch...
11:46:45.482   Training iter 50, batch loss 1.0883, batch acc 0.6744
11:46:45.983   Training iter 100, batch loss 1.0673, batch acc 0.6766
11:46:46.497   Training iter 150, batch loss 1.0762, batch acc 0.6780
11:46:47.003   Training iter 200, batch loss 1.0855, batch acc 0.6722
11:46:47.475   Training iter 250, batch loss 1.0748, batch acc 0.6784
11:46:47.926   Training iter 300, batch loss 1.0950, batch acc 0.6662
11:46:48.377   Training iter 350, batch loss 1.0612, batch acc 0.6792
11:46:48.833   Training iter 400, batch loss 1.0547, batch acc 0.6852
11:46:49.285   Training iter 450, batch loss 1.0694, batch acc 0.6754
11:46:49.750   Training iter 500, batch loss 1.0794, batch acc 0.6812
11:46:50.217   Training iter 550, batch loss 1.0979, batch acc 0.6724
11:46:50.680   Training iter 600, batch loss 1.0794, batch acc 0.6714
11:46:50.682 Training @ 315 epoch...
11:46:51.155   Training iter 50, batch loss 1.0743, batch acc 0.6808
11:46:51.627   Training iter 100, batch loss 1.0688, batch acc 0.6824
11:46:52.092   Training iter 150, batch loss 1.0728, batch acc 0.6802
11:46:52.579   Training iter 200, batch loss 1.0699, batch acc 0.6710
11:46:53.068   Training iter 250, batch loss 1.0786, batch acc 0.6792
11:46:53.581   Training iter 300, batch loss 1.1121, batch acc 0.6604
11:46:54.103   Training iter 350, batch loss 1.0632, batch acc 0.6796
11:46:54.627   Training iter 400, batch loss 1.0596, batch acc 0.6784
11:46:55.146   Training iter 450, batch loss 1.0955, batch acc 0.6648
11:46:55.670   Training iter 500, batch loss 1.0767, batch acc 0.6792
11:46:56.187   Training iter 550, batch loss 1.0800, batch acc 0.6730
11:46:56.703   Training iter 600, batch loss 1.0775, batch acc 0.6800
11:46:56.705 Testing @ 315 epoch...
11:46:56.746     Testing, total mean loss 1.04969, total acc 0.68420
11:46:56.746 Training @ 316 epoch...
11:46:57.281   Training iter 50, batch loss 1.0725, batch acc 0.6756
11:46:57.808   Training iter 100, batch loss 1.0966, batch acc 0.6694
11:46:58.329   Training iter 150, batch loss 1.1021, batch acc 0.6756
11:46:58.845   Training iter 200, batch loss 1.0767, batch acc 0.6724
11:46:59.362   Training iter 250, batch loss 1.0777, batch acc 0.6712
11:46:59.876   Training iter 300, batch loss 1.0807, batch acc 0.6854
11:47:00.406   Training iter 350, batch loss 1.0530, batch acc 0.6796
11:47:00.927   Training iter 400, batch loss 1.0711, batch acc 0.6780
11:47:01.459   Training iter 450, batch loss 1.0795, batch acc 0.6750
11:47:02.003   Training iter 500, batch loss 1.0657, batch acc 0.6770
11:47:02.530   Training iter 550, batch loss 1.0856, batch acc 0.6680
11:47:03.077   Training iter 600, batch loss 1.0674, batch acc 0.6848
11:47:03.079 Training @ 317 epoch...
11:47:03.626   Training iter 50, batch loss 1.0746, batch acc 0.6824
11:47:04.170   Training iter 100, batch loss 1.0963, batch acc 0.6702
11:47:04.717   Training iter 150, batch loss 1.0695, batch acc 0.6810
11:47:05.258   Training iter 200, batch loss 1.0758, batch acc 0.6786
11:47:05.809   Training iter 250, batch loss 1.0973, batch acc 0.6666
11:47:06.361   Training iter 300, batch loss 1.0679, batch acc 0.6784
11:47:06.890   Training iter 350, batch loss 1.0518, batch acc 0.6858
11:47:07.394   Training iter 400, batch loss 1.0599, batch acc 0.6830
11:47:07.891   Training iter 450, batch loss 1.0955, batch acc 0.6658
11:47:08.391   Training iter 500, batch loss 1.0594, batch acc 0.6846
11:47:08.885   Training iter 550, batch loss 1.1076, batch acc 0.6642
11:47:09.417   Training iter 600, batch loss 1.0726, batch acc 0.6704
11:47:09.418 Training @ 318 epoch...
11:47:09.950   Training iter 50, batch loss 1.0659, batch acc 0.6896
11:47:10.476   Training iter 100, batch loss 1.0799, batch acc 0.6768
11:47:10.988   Training iter 150, batch loss 1.0584, batch acc 0.6822
11:47:11.516   Training iter 200, batch loss 1.0748, batch acc 0.6764
11:47:12.048   Training iter 250, batch loss 1.0816, batch acc 0.6688
11:47:12.579   Training iter 300, batch loss 1.0812, batch acc 0.6776
11:47:13.110   Training iter 350, batch loss 1.0678, batch acc 0.6768
11:47:13.637   Training iter 400, batch loss 1.1090, batch acc 0.6652
11:47:14.168   Training iter 450, batch loss 1.0710, batch acc 0.6808
11:47:14.689   Training iter 500, batch loss 1.0530, batch acc 0.6830
11:47:15.212   Training iter 550, batch loss 1.1041, batch acc 0.6662
11:47:15.726   Training iter 600, batch loss 1.0816, batch acc 0.6676
11:47:15.728 Training @ 319 epoch...
11:47:16.250   Training iter 50, batch loss 1.0878, batch acc 0.6714
11:47:16.765   Training iter 100, batch loss 1.0859, batch acc 0.6746
11:47:17.292   Training iter 150, batch loss 1.0663, batch acc 0.6818
11:47:17.814   Training iter 200, batch loss 1.0932, batch acc 0.6724
11:47:18.345   Training iter 250, batch loss 1.0586, batch acc 0.6822
11:47:18.868   Training iter 300, batch loss 1.0910, batch acc 0.6684
11:47:19.372   Training iter 350, batch loss 1.0841, batch acc 0.6676
11:47:19.884   Training iter 400, batch loss 1.0995, batch acc 0.6684
11:47:20.401   Training iter 450, batch loss 1.0914, batch acc 0.6716
11:47:20.914   Training iter 500, batch loss 1.0537, batch acc 0.6924
11:47:21.420   Training iter 550, batch loss 1.0586, batch acc 0.6840
11:47:21.927   Training iter 600, batch loss 1.0579, batch acc 0.6772
11:47:21.929 Training @ 320 epoch...
11:47:22.455   Training iter 50, batch loss 1.0974, batch acc 0.6674
11:47:22.963   Training iter 100, batch loss 1.0872, batch acc 0.6772
11:47:23.505   Training iter 150, batch loss 1.0850, batch acc 0.6740
11:47:24.034   Training iter 200, batch loss 1.0615, batch acc 0.6786
11:47:24.564   Training iter 250, batch loss 1.0546, batch acc 0.6758
11:47:25.110   Training iter 300, batch loss 1.0833, batch acc 0.6774
11:47:25.649   Training iter 350, batch loss 1.0790, batch acc 0.6738
11:47:26.190   Training iter 400, batch loss 1.0693, batch acc 0.6864
11:47:26.726   Training iter 450, batch loss 1.0678, batch acc 0.6786
11:47:27.257   Training iter 500, batch loss 1.0909, batch acc 0.6652
11:47:27.767   Training iter 550, batch loss 1.0815, batch acc 0.6772
11:47:28.299   Training iter 600, batch loss 1.0706, batch acc 0.6790
11:47:28.301 Testing @ 320 epoch...
11:47:28.342     Testing, total mean loss 1.04971, total acc 0.68370
11:47:28.342 Training @ 321 epoch...
11:47:28.884   Training iter 50, batch loss 1.0820, batch acc 0.6784
11:47:29.406   Training iter 100, batch loss 1.0968, batch acc 0.6628
11:47:29.918   Training iter 150, batch loss 1.0877, batch acc 0.6696
11:47:30.437   Training iter 200, batch loss 1.0802, batch acc 0.6742
11:47:30.980   Training iter 250, batch loss 1.0957, batch acc 0.6692
11:47:31.545   Training iter 300, batch loss 1.0644, batch acc 0.6838
11:47:32.111   Training iter 350, batch loss 1.0685, batch acc 0.6772
11:47:32.658   Training iter 400, batch loss 1.0893, batch acc 0.6728
11:47:33.162   Training iter 450, batch loss 1.0628, batch acc 0.6820
11:47:33.664   Training iter 500, batch loss 1.0699, batch acc 0.6776
11:47:34.177   Training iter 550, batch loss 1.0524, batch acc 0.6848
11:47:34.684   Training iter 600, batch loss 1.0777, batch acc 0.6782
11:47:34.686 Training @ 322 epoch...
11:47:35.201   Training iter 50, batch loss 1.0775, batch acc 0.6764
11:47:35.699   Training iter 100, batch loss 1.0850, batch acc 0.6734
11:47:36.178   Training iter 150, batch loss 1.0758, batch acc 0.6674
11:47:36.621   Training iter 200, batch loss 1.0741, batch acc 0.6728
11:47:37.111   Training iter 250, batch loss 1.0640, batch acc 0.6868
11:47:37.605   Training iter 300, batch loss 1.0796, batch acc 0.6746
11:47:38.098   Training iter 350, batch loss 1.0603, batch acc 0.6886
11:47:38.594   Training iter 400, batch loss 1.0986, batch acc 0.6630
11:47:39.089   Training iter 450, batch loss 1.0475, batch acc 0.6886
11:47:39.584   Training iter 500, batch loss 1.0753, batch acc 0.6730
11:47:40.103   Training iter 550, batch loss 1.0922, batch acc 0.6732
11:47:40.616   Training iter 600, batch loss 1.0974, batch acc 0.6722
11:47:40.618 Training @ 323 epoch...
11:47:41.094   Training iter 50, batch loss 1.0886, batch acc 0.6722
11:47:41.540   Training iter 100, batch loss 1.0966, batch acc 0.6648
11:47:42.002   Training iter 150, batch loss 1.0860, batch acc 0.6728
11:47:42.486   Training iter 200, batch loss 1.0934, batch acc 0.6682
11:47:42.962   Training iter 250, batch loss 1.0819, batch acc 0.6778
11:47:43.439   Training iter 300, batch loss 1.0995, batch acc 0.6692
11:47:43.911   Training iter 350, batch loss 1.0541, batch acc 0.6860
11:47:44.400   Training iter 400, batch loss 1.0705, batch acc 0.6746
11:47:44.881   Training iter 450, batch loss 1.0684, batch acc 0.6808
11:47:45.369   Training iter 500, batch loss 1.0986, batch acc 0.6750
11:47:45.856   Training iter 550, batch loss 1.0310, batch acc 0.6958
11:47:46.331   Training iter 600, batch loss 1.0586, batch acc 0.6772
11:47:46.333 Training @ 324 epoch...
11:47:46.809   Training iter 50, batch loss 1.0509, batch acc 0.6832
11:47:47.273   Training iter 100, batch loss 1.0775, batch acc 0.6766
11:47:47.743   Training iter 150, batch loss 1.0858, batch acc 0.6720
11:47:48.217   Training iter 200, batch loss 1.0893, batch acc 0.6712
11:47:48.677   Training iter 250, batch loss 1.0897, batch acc 0.6702
11:47:49.129   Training iter 300, batch loss 1.0671, batch acc 0.6760
11:47:49.572   Training iter 350, batch loss 1.0894, batch acc 0.6728
11:47:50.020   Training iter 400, batch loss 1.0550, batch acc 0.6908
11:47:50.486   Training iter 450, batch loss 1.0811, batch acc 0.6722
11:47:50.946   Training iter 500, batch loss 1.1012, batch acc 0.6712
11:47:51.420   Training iter 550, batch loss 1.0719, batch acc 0.6776
11:47:51.890   Training iter 600, batch loss 1.0680, batch acc 0.6778
11:47:51.892 Training @ 325 epoch...
11:47:52.358   Training iter 50, batch loss 1.0637, batch acc 0.6846
11:47:52.832   Training iter 100, batch loss 1.0966, batch acc 0.6736
11:47:53.314   Training iter 150, batch loss 1.0741, batch acc 0.6810
11:47:53.793   Training iter 200, batch loss 1.0640, batch acc 0.6800
11:47:54.263   Training iter 250, batch loss 1.0802, batch acc 0.6728
11:47:54.730   Training iter 300, batch loss 1.0717, batch acc 0.6754
11:47:55.225   Training iter 350, batch loss 1.0670, batch acc 0.6824
11:47:55.737   Training iter 400, batch loss 1.0966, batch acc 0.6608
11:47:56.255   Training iter 450, batch loss 1.0593, batch acc 0.6834
11:47:56.776   Training iter 500, batch loss 1.0910, batch acc 0.6690
11:47:57.304   Training iter 550, batch loss 1.0809, batch acc 0.6722
11:47:57.833   Training iter 600, batch loss 1.0817, batch acc 0.6764
11:47:57.834 Testing @ 325 epoch...
11:47:57.875     Testing, total mean loss 1.04955, total acc 0.68370
11:47:57.875 Training @ 326 epoch...
11:47:58.408   Training iter 50, batch loss 1.0677, batch acc 0.6726
11:47:58.916   Training iter 100, batch loss 1.0826, batch acc 0.6708
11:47:59.411   Training iter 150, batch loss 1.0630, batch acc 0.6782
11:47:59.906   Training iter 200, batch loss 1.0595, batch acc 0.6816
11:48:00.404   Training iter 250, batch loss 1.0753, batch acc 0.6772
11:48:00.911   Training iter 300, batch loss 1.0808, batch acc 0.6838
11:48:01.409   Training iter 350, batch loss 1.0698, batch acc 0.6816
11:48:01.947   Training iter 400, batch loss 1.0941, batch acc 0.6726
11:48:02.460   Training iter 450, batch loss 1.0904, batch acc 0.6748
11:48:02.962   Training iter 500, batch loss 1.1028, batch acc 0.6644
11:48:03.486   Training iter 550, batch loss 1.0792, batch acc 0.6754
11:48:03.969   Training iter 600, batch loss 1.0614, batch acc 0.6776
11:48:03.971 Training @ 327 epoch...
11:48:04.480   Training iter 50, batch loss 1.0663, batch acc 0.6774
11:48:05.002   Training iter 100, batch loss 1.0851, batch acc 0.6740
11:48:05.487   Training iter 150, batch loss 1.0556, batch acc 0.6832
11:48:05.922   Training iter 200, batch loss 1.0789, batch acc 0.6758
11:48:06.372   Training iter 250, batch loss 1.0639, batch acc 0.6848
11:48:06.846   Training iter 300, batch loss 1.0715, batch acc 0.6752
11:48:07.342   Training iter 350, batch loss 1.0524, batch acc 0.6852
11:48:07.865   Training iter 400, batch loss 1.0603, batch acc 0.6842
11:48:08.350   Training iter 450, batch loss 1.1026, batch acc 0.6636
11:48:08.815   Training iter 500, batch loss 1.1071, batch acc 0.6632
11:48:09.284   Training iter 550, batch loss 1.0828, batch acc 0.6782
11:48:09.747   Training iter 600, batch loss 1.0998, batch acc 0.6646
11:48:09.749 Training @ 328 epoch...
11:48:10.231   Training iter 50, batch loss 1.0619, batch acc 0.6838
11:48:10.724   Training iter 100, batch loss 1.0747, batch acc 0.6744
11:48:11.228   Training iter 150, batch loss 1.0764, batch acc 0.6744
11:48:11.720   Training iter 200, batch loss 1.0870, batch acc 0.6782
11:48:12.219   Training iter 250, batch loss 1.0751, batch acc 0.6788
11:48:12.762   Training iter 300, batch loss 1.0993, batch acc 0.6688
11:48:13.302   Training iter 350, batch loss 1.0430, batch acc 0.6896
11:48:13.839   Training iter 400, batch loss 1.0792, batch acc 0.6736
11:48:14.355   Training iter 450, batch loss 1.0939, batch acc 0.6672
11:48:14.866   Training iter 500, batch loss 1.0993, batch acc 0.6642
11:48:15.395   Training iter 550, batch loss 1.0579, batch acc 0.6784
11:48:15.914   Training iter 600, batch loss 1.0784, batch acc 0.6812
11:48:15.916 Training @ 329 epoch...
11:48:16.437   Training iter 50, batch loss 1.0910, batch acc 0.6690
11:48:16.943   Training iter 100, batch loss 1.0665, batch acc 0.6780
11:48:17.416   Training iter 150, batch loss 1.0868, batch acc 0.6734
11:48:17.884   Training iter 200, batch loss 1.0555, batch acc 0.6864
11:48:18.375   Training iter 250, batch loss 1.0939, batch acc 0.6722
11:48:18.856   Training iter 300, batch loss 1.1057, batch acc 0.6644
11:48:19.326   Training iter 350, batch loss 1.0674, batch acc 0.6810
11:48:19.801   Training iter 400, batch loss 1.0750, batch acc 0.6762
11:48:20.260   Training iter 450, batch loss 1.0647, batch acc 0.6822
11:48:20.724   Training iter 500, batch loss 1.0738, batch acc 0.6742
11:48:21.185   Training iter 550, batch loss 1.0725, batch acc 0.6748
11:48:21.654   Training iter 600, batch loss 1.0731, batch acc 0.6792
11:48:21.656 Training @ 330 epoch...
11:48:22.138   Training iter 50, batch loss 1.0863, batch acc 0.6730
11:48:22.619   Training iter 100, batch loss 1.0656, batch acc 0.6778
11:48:23.125   Training iter 150, batch loss 1.0809, batch acc 0.6788
11:48:23.662   Training iter 200, batch loss 1.0888, batch acc 0.6732
11:48:24.195   Training iter 250, batch loss 1.0747, batch acc 0.6776
11:48:24.711   Training iter 300, batch loss 1.0772, batch acc 0.6692
11:48:25.248   Training iter 350, batch loss 1.0743, batch acc 0.6698
11:48:25.781   Training iter 400, batch loss 1.0861, batch acc 0.6682
11:48:26.307   Training iter 450, batch loss 1.0624, batch acc 0.6830
11:48:26.819   Training iter 500, batch loss 1.0641, batch acc 0.6848
11:48:27.361   Training iter 550, batch loss 1.0715, batch acc 0.6812
11:48:27.920   Training iter 600, batch loss 1.0937, batch acc 0.6744
11:48:27.922 Testing @ 330 epoch...
11:48:27.962     Testing, total mean loss 1.04954, total acc 0.68380
11:48:27.963 Training @ 331 epoch...
11:48:28.540   Training iter 50, batch loss 1.0750, batch acc 0.6748
11:48:29.101   Training iter 100, batch loss 1.0828, batch acc 0.6744
11:48:29.664   Training iter 150, batch loss 1.0538, batch acc 0.6868
11:48:30.169   Training iter 200, batch loss 1.0822, batch acc 0.6756
11:48:30.680   Training iter 250, batch loss 1.1060, batch acc 0.6656
11:48:31.197   Training iter 300, batch loss 1.0819, batch acc 0.6770
11:48:31.710   Training iter 350, batch loss 1.0641, batch acc 0.6806
11:48:32.234   Training iter 400, batch loss 1.0570, batch acc 0.6842
11:48:32.765   Training iter 450, batch loss 1.0873, batch acc 0.6748
11:48:33.256   Training iter 500, batch loss 1.0534, batch acc 0.6818
11:48:33.761   Training iter 550, batch loss 1.0835, batch acc 0.6738
11:48:34.264   Training iter 600, batch loss 1.0985, batch acc 0.6640
11:48:34.265 Training @ 332 epoch...
11:48:34.783   Training iter 50, batch loss 1.0809, batch acc 0.6758
11:48:35.289   Training iter 100, batch loss 1.0695, batch acc 0.6786
11:48:35.792   Training iter 150, batch loss 1.0705, batch acc 0.6792
11:48:36.292   Training iter 200, batch loss 1.0722, batch acc 0.6742
11:48:36.783   Training iter 250, batch loss 1.0965, batch acc 0.6706
11:48:37.288   Training iter 300, batch loss 1.0789, batch acc 0.6754
11:48:37.794   Training iter 350, batch loss 1.0878, batch acc 0.6728
11:48:38.316   Training iter 400, batch loss 1.0675, batch acc 0.6814
11:48:38.815   Training iter 450, batch loss 1.0506, batch acc 0.6804
11:48:39.332   Training iter 500, batch loss 1.0953, batch acc 0.6664
11:48:39.840   Training iter 550, batch loss 1.0775, batch acc 0.6772
11:48:40.359   Training iter 600, batch loss 1.0780, batch acc 0.6796
11:48:40.360 Training @ 333 epoch...
11:48:40.874   Training iter 50, batch loss 1.0903, batch acc 0.6752
11:48:41.394   Training iter 100, batch loss 1.0760, batch acc 0.6798
11:48:41.913   Training iter 150, batch loss 1.0510, batch acc 0.6818
11:48:42.443   Training iter 200, batch loss 1.0548, batch acc 0.6844
11:48:42.963   Training iter 250, batch loss 1.0646, batch acc 0.6866
11:48:43.478   Training iter 300, batch loss 1.0615, batch acc 0.6808
11:48:43.991   Training iter 350, batch loss 1.0879, batch acc 0.6732
11:48:44.511   Training iter 400, batch loss 1.0821, batch acc 0.6706
11:48:45.028   Training iter 450, batch loss 1.1085, batch acc 0.6618
11:48:45.546   Training iter 500, batch loss 1.0953, batch acc 0.6664
11:48:46.057   Training iter 550, batch loss 1.0974, batch acc 0.6616
11:48:46.523   Training iter 600, batch loss 1.0555, batch acc 0.6862
11:48:46.524 Training @ 334 epoch...
11:48:46.992   Training iter 50, batch loss 1.0707, batch acc 0.6768
11:48:47.484   Training iter 100, batch loss 1.0742, batch acc 0.6768
11:48:47.972   Training iter 150, batch loss 1.0892, batch acc 0.6708
11:48:48.471   Training iter 200, batch loss 1.0905, batch acc 0.6742
11:48:48.978   Training iter 250, batch loss 1.0470, batch acc 0.6858
11:48:49.487   Training iter 300, batch loss 1.0737, batch acc 0.6770
11:48:49.996   Training iter 350, batch loss 1.0776, batch acc 0.6788
11:48:50.498   Training iter 400, batch loss 1.0866, batch acc 0.6768
11:48:51.008   Training iter 450, batch loss 1.1075, batch acc 0.6602
11:48:51.520   Training iter 500, batch loss 1.0857, batch acc 0.6688
11:48:52.027   Training iter 550, batch loss 1.0554, batch acc 0.6830
11:48:52.533   Training iter 600, batch loss 1.0667, batch acc 0.6840
11:48:52.535 Training @ 335 epoch...
11:48:53.039   Training iter 50, batch loss 1.0647, batch acc 0.6798
11:48:53.540   Training iter 100, batch loss 1.0861, batch acc 0.6710
11:48:54.031   Training iter 150, batch loss 1.0705, batch acc 0.6760
11:48:54.517   Training iter 200, batch loss 1.0723, batch acc 0.6740
11:48:55.013   Training iter 250, batch loss 1.1001, batch acc 0.6692
11:48:55.515   Training iter 300, batch loss 1.0768, batch acc 0.6764
11:48:56.003   Training iter 350, batch loss 1.0754, batch acc 0.6784
11:48:56.494   Training iter 400, batch loss 1.0574, batch acc 0.6820
11:48:56.983   Training iter 450, batch loss 1.0858, batch acc 0.6722
11:48:57.492   Training iter 500, batch loss 1.0995, batch acc 0.6648
11:48:57.994   Training iter 550, batch loss 1.0584, batch acc 0.6850
11:48:58.493   Training iter 600, batch loss 1.0778, batch acc 0.6850
11:48:58.495 Testing @ 335 epoch...
11:48:58.535     Testing, total mean loss 1.04950, total acc 0.68390
11:48:58.535 Training @ 336 epoch...
11:48:59.063   Training iter 50, batch loss 1.0651, batch acc 0.6796
11:48:59.602   Training iter 100, batch loss 1.0846, batch acc 0.6754
11:49:00.170   Training iter 150, batch loss 1.0849, batch acc 0.6722
11:49:00.722   Training iter 200, batch loss 1.0739, batch acc 0.6818
11:49:01.265   Training iter 250, batch loss 1.0878, batch acc 0.6706
11:49:01.814   Training iter 300, batch loss 1.0670, batch acc 0.6814
11:49:02.384   Training iter 350, batch loss 1.0698, batch acc 0.6744
11:49:02.937   Training iter 400, batch loss 1.0695, batch acc 0.6776
11:49:03.476   Training iter 450, batch loss 1.1063, batch acc 0.6638
11:49:04.026   Training iter 500, batch loss 1.0991, batch acc 0.6736
11:49:04.575   Training iter 550, batch loss 1.0443, batch acc 0.6866
11:49:05.111   Training iter 600, batch loss 1.0725, batch acc 0.6728
11:49:05.113 Training @ 337 epoch...
11:49:05.640   Training iter 50, batch loss 1.0707, batch acc 0.6734
11:49:06.184   Training iter 100, batch loss 1.0611, batch acc 0.6802
11:49:06.732   Training iter 150, batch loss 1.0707, batch acc 0.6776
11:49:07.272   Training iter 200, batch loss 1.0801, batch acc 0.6754
11:49:07.804   Training iter 250, batch loss 1.0565, batch acc 0.6842
11:49:08.336   Training iter 300, batch loss 1.0738, batch acc 0.6834
11:49:08.841   Training iter 350, batch loss 1.0916, batch acc 0.6728
11:49:09.354   Training iter 400, batch loss 1.1160, batch acc 0.6594
11:49:09.872   Training iter 450, batch loss 1.0930, batch acc 0.6704
11:49:10.416   Training iter 500, batch loss 1.0681, batch acc 0.6790
11:49:10.953   Training iter 550, batch loss 1.0958, batch acc 0.6652
11:49:11.478   Training iter 600, batch loss 1.0472, batch acc 0.6900
11:49:11.480 Training @ 338 epoch...
11:49:11.998   Training iter 50, batch loss 1.0623, batch acc 0.6774
11:49:12.530   Training iter 100, batch loss 1.0933, batch acc 0.6706
11:49:13.060   Training iter 150, batch loss 1.0830, batch acc 0.6750
11:49:13.588   Training iter 200, batch loss 1.0708, batch acc 0.6816
11:49:14.109   Training iter 250, batch loss 1.0667, batch acc 0.6778
11:49:14.631   Training iter 300, batch loss 1.0582, batch acc 0.6858
11:49:15.151   Training iter 350, batch loss 1.0602, batch acc 0.6792
11:49:15.671   Training iter 400, batch loss 1.0810, batch acc 0.6762
11:49:16.200   Training iter 450, batch loss 1.0945, batch acc 0.6692
11:49:16.719   Training iter 500, batch loss 1.0791, batch acc 0.6744
11:49:17.224   Training iter 550, batch loss 1.0799, batch acc 0.6748
11:49:17.747   Training iter 600, batch loss 1.0953, batch acc 0.6690
11:49:17.749 Training @ 339 epoch...
11:49:18.293   Training iter 50, batch loss 1.0690, batch acc 0.6750
11:49:18.807   Training iter 100, batch loss 1.0751, batch acc 0.6798
11:49:19.313   Training iter 150, batch loss 1.0596, batch acc 0.6856
11:49:19.819   Training iter 200, batch loss 1.0680, batch acc 0.6866
11:49:20.327   Training iter 250, batch loss 1.0524, batch acc 0.6834
11:49:20.825   Training iter 300, batch loss 1.0811, batch acc 0.6724
11:49:21.312   Training iter 350, batch loss 1.0890, batch acc 0.6690
11:49:21.809   Training iter 400, batch loss 1.0792, batch acc 0.6790
11:49:22.307   Training iter 450, batch loss 1.0939, batch acc 0.6662
11:49:22.803   Training iter 500, batch loss 1.1036, batch acc 0.6632
11:49:23.317   Training iter 550, batch loss 1.0771, batch acc 0.6738
11:49:23.841   Training iter 600, batch loss 1.0759, batch acc 0.6794
11:49:23.842 Training @ 340 epoch...
11:49:24.353   Training iter 50, batch loss 1.0744, batch acc 0.6796
11:49:24.842   Training iter 100, batch loss 1.0587, batch acc 0.6856
11:49:25.298   Training iter 150, batch loss 1.0672, batch acc 0.6796
11:49:25.740   Training iter 200, batch loss 1.0705, batch acc 0.6762
11:49:26.207   Training iter 250, batch loss 1.0791, batch acc 0.6710
11:49:26.703   Training iter 300, batch loss 1.0989, batch acc 0.6684
11:49:27.200   Training iter 350, batch loss 1.0839, batch acc 0.6754
11:49:27.708   Training iter 400, batch loss 1.0559, batch acc 0.6816
11:49:28.209   Training iter 450, batch loss 1.0926, batch acc 0.6668
11:49:28.712   Training iter 500, batch loss 1.0629, batch acc 0.6774
11:49:29.212   Training iter 550, batch loss 1.0794, batch acc 0.6808
11:49:29.712   Training iter 600, batch loss 1.1005, batch acc 0.6696
11:49:29.714 Testing @ 340 epoch...
11:49:29.754     Testing, total mean loss 1.04941, total acc 0.68400
11:49:29.754 Training @ 341 epoch...
11:49:30.244   Training iter 50, batch loss 1.0860, batch acc 0.6752
11:49:30.730   Training iter 100, batch loss 1.0877, batch acc 0.6758
11:49:31.242   Training iter 150, batch loss 1.0861, batch acc 0.6662
11:49:31.726   Training iter 200, batch loss 1.0701, batch acc 0.6820
11:49:32.210   Training iter 250, batch loss 1.0783, batch acc 0.6782
11:49:32.691   Training iter 300, batch loss 1.0916, batch acc 0.6738
11:49:33.186   Training iter 350, batch loss 1.0833, batch acc 0.6688
11:49:33.667   Training iter 400, batch loss 1.0547, batch acc 0.6930
11:49:34.145   Training iter 450, batch loss 1.0881, batch acc 0.6734
11:49:34.625   Training iter 500, batch loss 1.0593, batch acc 0.6744
11:49:35.130   Training iter 550, batch loss 1.0775, batch acc 0.6784
11:49:35.612   Training iter 600, batch loss 1.0609, batch acc 0.6758
11:49:35.614 Training @ 342 epoch...
11:49:36.097   Training iter 50, batch loss 1.1220, batch acc 0.6482
11:49:36.570   Training iter 100, batch loss 1.0737, batch acc 0.6810
11:49:37.043   Training iter 150, batch loss 1.0589, batch acc 0.6800
11:49:37.499   Training iter 200, batch loss 1.0757, batch acc 0.6768
11:49:37.961   Training iter 250, batch loss 1.0766, batch acc 0.6720
11:49:38.434   Training iter 300, batch loss 1.0539, batch acc 0.6804
11:49:38.906   Training iter 350, batch loss 1.0604, batch acc 0.6816
11:49:39.374   Training iter 400, batch loss 1.0807, batch acc 0.6784
11:49:39.836   Training iter 450, batch loss 1.0711, batch acc 0.6844
11:49:40.323   Training iter 500, batch loss 1.0904, batch acc 0.6730
11:49:40.809   Training iter 550, batch loss 1.0844, batch acc 0.6770
11:49:41.280   Training iter 600, batch loss 1.0757, batch acc 0.6804
11:49:41.282 Training @ 343 epoch...
11:49:41.740   Training iter 50, batch loss 1.0743, batch acc 0.6698
11:49:42.186   Training iter 100, batch loss 1.0974, batch acc 0.6708
11:49:42.668   Training iter 150, batch loss 1.0923, batch acc 0.6674
11:49:43.182   Training iter 200, batch loss 1.0935, batch acc 0.6694
11:49:43.678   Training iter 250, batch loss 1.0749, batch acc 0.6764
11:49:44.174   Training iter 300, batch loss 1.0662, batch acc 0.6820
11:49:44.669   Training iter 350, batch loss 1.1047, batch acc 0.6618
11:49:45.177   Training iter 400, batch loss 1.0752, batch acc 0.6790
11:49:45.678   Training iter 450, batch loss 1.0661, batch acc 0.6786
11:49:46.147   Training iter 500, batch loss 1.0498, batch acc 0.6890
11:49:46.622   Training iter 550, batch loss 1.0776, batch acc 0.6826
11:49:47.137   Training iter 600, batch loss 1.0515, batch acc 0.6860
11:49:47.138 Training @ 344 epoch...
11:49:47.660   Training iter 50, batch loss 1.0829, batch acc 0.6782
11:49:48.171   Training iter 100, batch loss 1.0806, batch acc 0.6742
11:49:48.700   Training iter 150, batch loss 1.0825, batch acc 0.6730
11:49:49.225   Training iter 200, batch loss 1.0597, batch acc 0.6832
11:49:49.740   Training iter 250, batch loss 1.0814, batch acc 0.6724
11:49:50.269   Training iter 300, batch loss 1.0813, batch acc 0.6710
11:49:50.811   Training iter 350, batch loss 1.0804, batch acc 0.6748
11:49:51.332   Training iter 400, batch loss 1.0796, batch acc 0.6754
11:49:51.852   Training iter 450, batch loss 1.1013, batch acc 0.6692
11:49:52.377   Training iter 500, batch loss 1.0518, batch acc 0.6898
11:49:52.918   Training iter 550, batch loss 1.0683, batch acc 0.6740
11:49:53.453   Training iter 600, batch loss 1.0735, batch acc 0.6754
11:49:53.455 Training @ 345 epoch...
11:49:53.940   Training iter 50, batch loss 1.0814, batch acc 0.6788
11:49:54.436   Training iter 100, batch loss 1.0556, batch acc 0.6812
11:49:54.928   Training iter 150, batch loss 1.0974, batch acc 0.6668
11:49:55.441   Training iter 200, batch loss 1.0685, batch acc 0.6828
11:49:55.929   Training iter 250, batch loss 1.0687, batch acc 0.6804
11:49:56.419   Training iter 300, batch loss 1.1023, batch acc 0.6660
11:49:56.902   Training iter 350, batch loss 1.0747, batch acc 0.6768
11:49:57.398   Training iter 400, batch loss 1.0789, batch acc 0.6742
11:49:57.907   Training iter 450, batch loss 1.0714, batch acc 0.6742
11:49:58.420   Training iter 500, batch loss 1.0688, batch acc 0.6788
11:49:58.916   Training iter 550, batch loss 1.0942, batch acc 0.6724
11:49:59.409   Training iter 600, batch loss 1.0611, batch acc 0.6768
11:49:59.411 Testing @ 345 epoch...
11:49:59.451     Testing, total mean loss 1.04927, total acc 0.68380
11:49:59.451 Training @ 346 epoch...
11:49:59.958   Training iter 50, batch loss 1.0713, batch acc 0.6740
11:50:00.507   Training iter 100, batch loss 1.0667, batch acc 0.6794
11:50:01.043   Training iter 150, batch loss 1.0630, batch acc 0.6842
11:50:01.611   Training iter 200, batch loss 1.0744, batch acc 0.6706
11:50:02.176   Training iter 250, batch loss 1.0691, batch acc 0.6790
11:50:02.684   Training iter 300, batch loss 1.0808, batch acc 0.6792
11:50:03.203   Training iter 350, batch loss 1.1016, batch acc 0.6714
11:50:03.748   Training iter 400, batch loss 1.0674, batch acc 0.6790
11:50:04.269   Training iter 450, batch loss 1.0967, batch acc 0.6656
11:50:04.788   Training iter 500, batch loss 1.0743, batch acc 0.6788
11:50:05.316   Training iter 550, batch loss 1.0954, batch acc 0.6692
11:50:05.826   Training iter 600, batch loss 1.0624, batch acc 0.6820
11:50:05.828 Training @ 347 epoch...
11:50:06.356   Training iter 50, batch loss 1.0698, batch acc 0.6786
11:50:06.874   Training iter 100, batch loss 1.0686, batch acc 0.6768
11:50:07.396   Training iter 150, batch loss 1.0652, batch acc 0.6780
11:50:07.953   Training iter 200, batch loss 1.0834, batch acc 0.6722
11:50:08.509   Training iter 250, batch loss 1.1242, batch acc 0.6610
11:50:09.022   Training iter 300, batch loss 1.1053, batch acc 0.6664
11:50:09.521   Training iter 350, batch loss 1.0552, batch acc 0.6804
11:50:10.019   Training iter 400, batch loss 1.0912, batch acc 0.6730
11:50:10.538   Training iter 450, batch loss 1.0253, batch acc 0.6960
11:50:11.065   Training iter 500, batch loss 1.0780, batch acc 0.6762
11:50:11.576   Training iter 550, batch loss 1.0743, batch acc 0.6740
11:50:12.089   Training iter 600, batch loss 1.0822, batch acc 0.6782
11:50:12.091 Training @ 348 epoch...
11:50:12.609   Training iter 50, batch loss 1.0568, batch acc 0.6864
11:50:13.148   Training iter 100, batch loss 1.0563, batch acc 0.6878
11:50:13.674   Training iter 150, batch loss 1.0798, batch acc 0.6766
11:50:14.207   Training iter 200, batch loss 1.0841, batch acc 0.6716
11:50:14.712   Training iter 250, batch loss 1.0676, batch acc 0.6734
11:50:15.222   Training iter 300, batch loss 1.1021, batch acc 0.6654
11:50:15.755   Training iter 350, batch loss 1.0733, batch acc 0.6754
11:50:16.258   Training iter 400, batch loss 1.0703, batch acc 0.6768
11:50:16.765   Training iter 450, batch loss 1.0752, batch acc 0.6764
11:50:17.282   Training iter 500, batch loss 1.0845, batch acc 0.6722
11:50:17.805   Training iter 550, batch loss 1.1072, batch acc 0.6680
11:50:18.314   Training iter 600, batch loss 1.0654, batch acc 0.6792
11:50:18.316 Training @ 349 epoch...
11:50:18.820   Training iter 50, batch loss 1.0736, batch acc 0.6758
11:50:19.322   Training iter 100, batch loss 1.0889, batch acc 0.6704
11:50:19.813   Training iter 150, batch loss 1.0733, batch acc 0.6724
11:50:20.287   Training iter 200, batch loss 1.0952, batch acc 0.6702
11:50:20.748   Training iter 250, batch loss 1.0562, batch acc 0.6880
11:50:21.192   Training iter 300, batch loss 1.0713, batch acc 0.6788
11:50:21.636   Training iter 350, batch loss 1.0710, batch acc 0.6726
11:50:22.097   Training iter 400, batch loss 1.0750, batch acc 0.6768
11:50:22.568   Training iter 450, batch loss 1.0764, batch acc 0.6748
11:50:23.041   Training iter 500, batch loss 1.0794, batch acc 0.6782
11:50:23.563   Training iter 550, batch loss 1.0845, batch acc 0.6810
11:50:24.088   Training iter 600, batch loss 1.0775, batch acc 0.6736
11:50:24.090 Training @ 350 epoch...
11:50:24.634   Training iter 50, batch loss 1.0852, batch acc 0.6692
11:50:25.169   Training iter 100, batch loss 1.0677, batch acc 0.6816
11:50:25.697   Training iter 150, batch loss 1.0855, batch acc 0.6766
11:50:26.212   Training iter 200, batch loss 1.0591, batch acc 0.6814
11:50:26.685   Training iter 250, batch loss 1.0909, batch acc 0.6722
11:50:27.154   Training iter 300, batch loss 1.0638, batch acc 0.6752
11:50:27.640   Training iter 350, batch loss 1.0755, batch acc 0.6830
11:50:28.122   Training iter 400, batch loss 1.0992, batch acc 0.6630
11:50:28.608   Training iter 450, batch loss 1.0903, batch acc 0.6696
11:50:29.104   Training iter 500, batch loss 1.0592, batch acc 0.6852
11:50:29.588   Training iter 550, batch loss 1.0698, batch acc 0.6752
11:50:30.073   Training iter 600, batch loss 1.0760, batch acc 0.6822
11:50:30.075 Testing @ 350 epoch...
11:50:30.116     Testing, total mean loss 1.04931, total acc 0.68370
11:50:30.116 Training @ 351 epoch...
11:50:30.610   Training iter 50, batch loss 1.0605, batch acc 0.6782
11:50:31.110   Training iter 100, batch loss 1.0615, batch acc 0.6814
11:50:31.609   Training iter 150, batch loss 1.0940, batch acc 0.6740
11:50:32.097   Training iter 200, batch loss 1.0893, batch acc 0.6720
11:50:32.583   Training iter 250, batch loss 1.0886, batch acc 0.6704
11:50:33.064   Training iter 300, batch loss 1.0840, batch acc 0.6694
11:50:33.570   Training iter 350, batch loss 1.0519, batch acc 0.6824
11:50:34.060   Training iter 400, batch loss 1.0828, batch acc 0.6784
11:50:34.542   Training iter 450, batch loss 1.0727, batch acc 0.6788
11:50:35.043   Training iter 500, batch loss 1.0789, batch acc 0.6766
11:50:35.544   Training iter 550, batch loss 1.0758, batch acc 0.6744
11:50:36.058   Training iter 600, batch loss 1.0819, batch acc 0.6748
11:50:36.060 Training @ 352 epoch...
11:50:36.545   Training iter 50, batch loss 1.1030, batch acc 0.6664
11:50:37.013   Training iter 100, batch loss 1.0345, batch acc 0.6930
11:50:37.518   Training iter 150, batch loss 1.1008, batch acc 0.6626
11:50:38.012   Training iter 200, batch loss 1.0464, batch acc 0.6888
11:50:38.484   Training iter 250, batch loss 1.0909, batch acc 0.6708
11:50:38.945   Training iter 300, batch loss 1.0750, batch acc 0.6760
11:50:39.410   Training iter 350, batch loss 1.0797, batch acc 0.6740
11:50:39.892   Training iter 400, batch loss 1.0686, batch acc 0.6824
11:50:40.398   Training iter 450, batch loss 1.0686, batch acc 0.6738
11:50:40.870   Training iter 500, batch loss 1.0770, batch acc 0.6832
11:50:41.363   Training iter 550, batch loss 1.0857, batch acc 0.6710
11:50:41.853   Training iter 600, batch loss 1.0918, batch acc 0.6690
11:50:41.855 Training @ 353 epoch...
11:50:42.369   Training iter 50, batch loss 1.0911, batch acc 0.6750
11:50:42.888   Training iter 100, batch loss 1.0630, batch acc 0.6816
11:50:43.414   Training iter 150, batch loss 1.0627, batch acc 0.6784
11:50:43.929   Training iter 200, batch loss 1.0837, batch acc 0.6770
11:50:44.443   Training iter 250, batch loss 1.0514, batch acc 0.6802
11:50:44.953   Training iter 300, batch loss 1.1004, batch acc 0.6684
11:50:45.478   Training iter 350, batch loss 1.0589, batch acc 0.6830
11:50:46.007   Training iter 400, batch loss 1.0821, batch acc 0.6678
11:50:46.531   Training iter 450, batch loss 1.1089, batch acc 0.6612
11:50:47.033   Training iter 500, batch loss 1.0582, batch acc 0.6818
11:50:47.524   Training iter 550, batch loss 1.0997, batch acc 0.6680
11:50:48.042   Training iter 600, batch loss 1.0619, batch acc 0.6918
11:50:48.044 Training @ 354 epoch...
11:50:48.553   Training iter 50, batch loss 1.0700, batch acc 0.6784
11:50:49.040   Training iter 100, batch loss 1.0699, batch acc 0.6810
11:50:49.528   Training iter 150, batch loss 1.0941, batch acc 0.6660
11:50:50.032   Training iter 200, batch loss 1.0627, batch acc 0.6858
11:50:50.528   Training iter 250, batch loss 1.0776, batch acc 0.6708
11:50:51.033   Training iter 300, batch loss 1.0801, batch acc 0.6754
11:50:51.530   Training iter 350, batch loss 1.0651, batch acc 0.6846
11:50:52.034   Training iter 400, batch loss 1.0862, batch acc 0.6722
11:50:52.533   Training iter 450, batch loss 1.0922, batch acc 0.6754
11:50:53.046   Training iter 500, batch loss 1.0824, batch acc 0.6714
11:50:53.552   Training iter 550, batch loss 1.0825, batch acc 0.6704
11:50:54.052   Training iter 600, batch loss 1.0587, batch acc 0.6808
11:50:54.054 Training @ 355 epoch...
11:50:54.552   Training iter 50, batch loss 1.0612, batch acc 0.6838
11:50:55.054   Training iter 100, batch loss 1.0869, batch acc 0.6752
11:50:55.556   Training iter 150, batch loss 1.0792, batch acc 0.6760
11:50:56.080   Training iter 200, batch loss 1.0687, batch acc 0.6804
11:50:56.592   Training iter 250, batch loss 1.0531, batch acc 0.6828
11:50:57.080   Training iter 300, batch loss 1.0712, batch acc 0.6748
11:50:57.614   Training iter 350, batch loss 1.0684, batch acc 0.6744
11:50:58.154   Training iter 400, batch loss 1.0915, batch acc 0.6694
11:50:58.679   Training iter 450, batch loss 1.0953, batch acc 0.6634
11:50:59.212   Training iter 500, batch loss 1.0848, batch acc 0.6718
11:50:59.751   Training iter 550, batch loss 1.0870, batch acc 0.6752
11:51:00.292   Training iter 600, batch loss 1.0742, batch acc 0.6844
11:51:00.293 Testing @ 355 epoch...
11:51:00.334     Testing, total mean loss 1.04924, total acc 0.68410
11:51:00.334 Training @ 356 epoch...
11:51:00.863   Training iter 50, batch loss 1.0705, batch acc 0.6762
11:51:01.401   Training iter 100, batch loss 1.0775, batch acc 0.6776
11:51:01.963   Training iter 150, batch loss 1.0561, batch acc 0.6860
11:51:02.446   Training iter 200, batch loss 1.0848, batch acc 0.6738
11:51:02.928   Training iter 250, batch loss 1.1025, batch acc 0.6652
11:51:03.420   Training iter 300, batch loss 1.0752, batch acc 0.6742
11:51:03.908   Training iter 350, batch loss 1.0598, batch acc 0.6902
11:51:04.414   Training iter 400, batch loss 1.0788, batch acc 0.6766
11:51:04.942   Training iter 450, batch loss 1.0807, batch acc 0.6782
11:51:05.485   Training iter 500, batch loss 1.0791, batch acc 0.6686
11:51:06.032   Training iter 550, batch loss 1.0748, batch acc 0.6774
11:51:06.581   Training iter 600, batch loss 1.0813, batch acc 0.6716
11:51:06.582 Training @ 357 epoch...
11:51:07.141   Training iter 50, batch loss 1.0744, batch acc 0.6824
11:51:07.676   Training iter 100, batch loss 1.0763, batch acc 0.6744
11:51:08.216   Training iter 150, batch loss 1.0907, batch acc 0.6672
11:51:08.736   Training iter 200, batch loss 1.0732, batch acc 0.6772
11:51:09.230   Training iter 250, batch loss 1.0696, batch acc 0.6772
11:51:09.731   Training iter 300, batch loss 1.0553, batch acc 0.6896
11:51:10.231   Training iter 350, batch loss 1.0755, batch acc 0.6748
11:51:10.736   Training iter 400, batch loss 1.0834, batch acc 0.6714
11:51:11.244   Training iter 450, batch loss 1.0832, batch acc 0.6754
11:51:11.778   Training iter 500, batch loss 1.0804, batch acc 0.6748
11:51:12.312   Training iter 550, batch loss 1.0758, batch acc 0.6762
11:51:12.843   Training iter 600, batch loss 1.0834, batch acc 0.6706
11:51:12.845 Training @ 358 epoch...
11:51:13.372   Training iter 50, batch loss 1.0703, batch acc 0.6786
11:51:13.888   Training iter 100, batch loss 1.0852, batch acc 0.6712
11:51:14.388   Training iter 150, batch loss 1.0768, batch acc 0.6784
11:51:14.873   Training iter 200, batch loss 1.0801, batch acc 0.6728
11:51:15.368   Training iter 250, batch loss 1.0533, batch acc 0.6842
11:51:15.857   Training iter 300, batch loss 1.0551, batch acc 0.6874
11:51:16.366   Training iter 350, batch loss 1.0769, batch acc 0.6718
11:51:16.869   Training iter 400, batch loss 1.0603, batch acc 0.6832
11:51:17.375   Training iter 450, batch loss 1.0756, batch acc 0.6780
11:51:17.880   Training iter 500, batch loss 1.1071, batch acc 0.6636
11:51:18.422   Training iter 550, batch loss 1.0848, batch acc 0.6740
11:51:18.916   Training iter 600, batch loss 1.0952, batch acc 0.6696
11:51:18.918 Training @ 359 epoch...
11:51:19.437   Training iter 50, batch loss 1.0992, batch acc 0.6644
11:51:19.948   Training iter 100, batch loss 1.0775, batch acc 0.6778
11:51:20.451   Training iter 150, batch loss 1.0773, batch acc 0.6724
11:51:20.946   Training iter 200, batch loss 1.0623, batch acc 0.6772
11:51:21.441   Training iter 250, batch loss 1.0751, batch acc 0.6746
11:51:21.956   Training iter 300, batch loss 1.0815, batch acc 0.6764
11:51:22.480   Training iter 350, batch loss 1.0797, batch acc 0.6760
11:51:22.999   Training iter 400, batch loss 1.0763, batch acc 0.6770
11:51:23.545   Training iter 450, batch loss 1.0950, batch acc 0.6714
11:51:24.084   Training iter 500, batch loss 1.0621, batch acc 0.6880
11:51:24.619   Training iter 550, batch loss 1.0805, batch acc 0.6714
11:51:25.143   Training iter 600, batch loss 1.0543, batch acc 0.6878
11:51:25.145 Training @ 360 epoch...
11:51:25.668   Training iter 50, batch loss 1.0317, batch acc 0.6948
11:51:26.209   Training iter 100, batch loss 1.1154, batch acc 0.6638
11:51:26.743   Training iter 150, batch loss 1.0975, batch acc 0.6678
11:51:27.288   Training iter 200, batch loss 1.1012, batch acc 0.6696
11:51:27.833   Training iter 250, batch loss 1.0558, batch acc 0.6874
11:51:28.368   Training iter 300, batch loss 1.0974, batch acc 0.6680
11:51:28.909   Training iter 350, batch loss 1.0951, batch acc 0.6692
11:51:29.408   Training iter 400, batch loss 1.0355, batch acc 0.6912
11:51:29.878   Training iter 450, batch loss 1.0610, batch acc 0.6778
11:51:30.387   Training iter 500, batch loss 1.0807, batch acc 0.6766
11:51:30.903   Training iter 550, batch loss 1.0695, batch acc 0.6758
11:51:31.415   Training iter 600, batch loss 1.0801, batch acc 0.6734
11:51:31.417 Testing @ 360 epoch...
11:51:31.457     Testing, total mean loss 1.04918, total acc 0.68430
11:51:31.457 Training @ 361 epoch...
11:51:31.983   Training iter 50, batch loss 1.0570, batch acc 0.6864
11:51:32.509   Training iter 100, batch loss 1.0838, batch acc 0.6756
11:51:33.035   Training iter 150, batch loss 1.0620, batch acc 0.6838
11:51:33.549   Training iter 200, batch loss 1.0919, batch acc 0.6734
11:51:34.071   Training iter 250, batch loss 1.0785, batch acc 0.6736
11:51:34.584   Training iter 300, batch loss 1.0710, batch acc 0.6770
11:51:35.081   Training iter 350, batch loss 1.0558, batch acc 0.6850
11:51:35.595   Training iter 400, batch loss 1.0909, batch acc 0.6666
11:51:36.113   Training iter 450, batch loss 1.0919, batch acc 0.6680
11:51:36.615   Training iter 500, batch loss 1.0801, batch acc 0.6730
11:51:37.120   Training iter 550, batch loss 1.0698, batch acc 0.6800
11:51:37.622   Training iter 600, batch loss 1.0879, batch acc 0.6674
11:51:37.624 Training @ 362 epoch...
11:51:38.122   Training iter 50, batch loss 1.0746, batch acc 0.6780
11:51:38.630   Training iter 100, batch loss 1.0575, batch acc 0.6850
11:51:39.138   Training iter 150, batch loss 1.0965, batch acc 0.6696
11:51:39.672   Training iter 200, batch loss 1.0861, batch acc 0.6694
11:51:40.205   Training iter 250, batch loss 1.0635, batch acc 0.6808
11:51:40.737   Training iter 300, batch loss 1.0490, batch acc 0.6844
11:51:41.281   Training iter 350, batch loss 1.0861, batch acc 0.6714
11:51:41.803   Training iter 400, batch loss 1.0716, batch acc 0.6774
11:51:42.337   Training iter 450, batch loss 1.0842, batch acc 0.6736
11:51:42.864   Training iter 500, batch loss 1.0904, batch acc 0.6728
11:51:43.398   Training iter 550, batch loss 1.0870, batch acc 0.6700
11:51:43.944   Training iter 600, batch loss 1.0741, batch acc 0.6778
11:51:43.946 Training @ 363 epoch...
11:51:44.518   Training iter 50, batch loss 1.0679, batch acc 0.6812
11:51:45.091   Training iter 100, batch loss 1.0954, batch acc 0.6724
11:51:45.655   Training iter 150, batch loss 1.0929, batch acc 0.6696
11:51:46.195   Training iter 200, batch loss 1.0698, batch acc 0.6754
11:51:46.728   Training iter 250, batch loss 1.0650, batch acc 0.6834
11:51:47.266   Training iter 300, batch loss 1.0730, batch acc 0.6788
11:51:47.817   Training iter 350, batch loss 1.0767, batch acc 0.6776
11:51:48.365   Training iter 400, batch loss 1.0758, batch acc 0.6754
11:51:48.860   Training iter 450, batch loss 1.0706, batch acc 0.6770
11:51:49.344   Training iter 500, batch loss 1.0783, batch acc 0.6740
11:51:49.812   Training iter 550, batch loss 1.0710, batch acc 0.6742
11:51:50.326   Training iter 600, batch loss 1.0837, batch acc 0.6754
11:51:50.328 Training @ 364 epoch...
11:51:50.870   Training iter 50, batch loss 1.0862, batch acc 0.6750
11:51:51.399   Training iter 100, batch loss 1.0809, batch acc 0.6728
11:51:51.939   Training iter 150, batch loss 1.0674, batch acc 0.6746
11:51:52.482   Training iter 200, batch loss 1.0693, batch acc 0.6838
11:51:53.022   Training iter 250, batch loss 1.0799, batch acc 0.6712
11:51:53.553   Training iter 300, batch loss 1.0577, batch acc 0.6842
11:51:54.080   Training iter 350, batch loss 1.0590, batch acc 0.6842
11:51:54.619   Training iter 400, batch loss 1.0815, batch acc 0.6740
11:51:55.141   Training iter 450, batch loss 1.0473, batch acc 0.6858
11:51:55.674   Training iter 500, batch loss 1.1066, batch acc 0.6652
11:51:56.229   Training iter 550, batch loss 1.0908, batch acc 0.6664
11:51:56.767   Training iter 600, batch loss 1.0936, batch acc 0.6758
11:51:56.769 Training @ 365 epoch...
11:51:57.312   Training iter 50, batch loss 1.0674, batch acc 0.6762
11:51:57.863   Training iter 100, batch loss 1.1005, batch acc 0.6660
11:51:58.405   Training iter 150, batch loss 1.0635, batch acc 0.6868
11:51:58.947   Training iter 200, batch loss 1.0535, batch acc 0.6784
11:51:59.489   Training iter 250, batch loss 1.0641, batch acc 0.6838
11:52:00.015   Training iter 300, batch loss 1.0824, batch acc 0.6734
11:52:00.553   Training iter 350, batch loss 1.0925, batch acc 0.6718
11:52:01.102   Training iter 400, batch loss 1.0562, batch acc 0.6848
11:52:01.698   Training iter 450, batch loss 1.0846, batch acc 0.6688
11:52:02.262   Training iter 500, batch loss 1.0782, batch acc 0.6778
11:52:02.819   Training iter 550, batch loss 1.1001, batch acc 0.6724
11:52:03.364   Training iter 600, batch loss 1.0770, batch acc 0.6734
11:52:03.366 Testing @ 365 epoch...
11:52:03.410     Testing, total mean loss 1.04906, total acc 0.68410
11:52:03.410 Training @ 366 epoch...
11:52:03.945   Training iter 50, batch loss 1.0810, batch acc 0.6738
11:52:04.464   Training iter 100, batch loss 1.1145, batch acc 0.6594
11:52:04.969   Training iter 150, batch loss 1.0473, batch acc 0.6892
11:52:05.482   Training iter 200, batch loss 1.0719, batch acc 0.6838
11:52:06.014   Training iter 250, batch loss 1.0817, batch acc 0.6740
11:52:06.557   Training iter 300, batch loss 1.0809, batch acc 0.6782
11:52:07.064   Training iter 350, batch loss 1.0755, batch acc 0.6770
11:52:07.566   Training iter 400, batch loss 1.0678, batch acc 0.6860
11:52:08.077   Training iter 450, batch loss 1.0714, batch acc 0.6718
11:52:08.582   Training iter 500, batch loss 1.0632, batch acc 0.6760
11:52:09.089   Training iter 550, batch loss 1.0794, batch acc 0.6726
11:52:09.608   Training iter 600, batch loss 1.0852, batch acc 0.6694
11:52:09.609 Training @ 367 epoch...
11:52:10.135   Training iter 50, batch loss 1.0601, batch acc 0.6826
11:52:10.663   Training iter 100, batch loss 1.1011, batch acc 0.6696
11:52:11.193   Training iter 150, batch loss 1.0893, batch acc 0.6742
11:52:11.701   Training iter 200, batch loss 1.0842, batch acc 0.6786
11:52:12.224   Training iter 250, batch loss 1.0845, batch acc 0.6716
11:52:12.747   Training iter 300, batch loss 1.0641, batch acc 0.6736
11:52:13.286   Training iter 350, batch loss 1.0652, batch acc 0.6802
11:52:13.821   Training iter 400, batch loss 1.0655, batch acc 0.6864
11:52:14.397   Training iter 450, batch loss 1.0863, batch acc 0.6738
11:52:14.956   Training iter 500, batch loss 1.0726, batch acc 0.6722
11:52:15.501   Training iter 550, batch loss 1.0735, batch acc 0.6756
11:52:16.033   Training iter 600, batch loss 1.0734, batch acc 0.6750
11:52:16.035 Training @ 368 epoch...
11:52:16.564   Training iter 50, batch loss 1.0709, batch acc 0.6764
11:52:17.084   Training iter 100, batch loss 1.0849, batch acc 0.6728
11:52:17.611   Training iter 150, batch loss 1.0525, batch acc 0.6912
11:52:18.146   Training iter 200, batch loss 1.0786, batch acc 0.6724
11:52:18.674   Training iter 250, batch loss 1.0793, batch acc 0.6748
11:52:19.178   Training iter 300, batch loss 1.0820, batch acc 0.6740
11:52:19.683   Training iter 350, batch loss 1.0909, batch acc 0.6812
11:52:20.186   Training iter 400, batch loss 1.0860, batch acc 0.6706
11:52:20.712   Training iter 450, batch loss 1.0732, batch acc 0.6716
11:52:21.224   Training iter 500, batch loss 1.0764, batch acc 0.6746
11:52:21.715   Training iter 550, batch loss 1.0565, batch acc 0.6812
11:52:22.225   Training iter 600, batch loss 1.0884, batch acc 0.6712
11:52:22.227 Training @ 369 epoch...
11:52:22.751   Training iter 50, batch loss 1.0597, batch acc 0.6838
11:52:23.293   Training iter 100, batch loss 1.0841, batch acc 0.6696
11:52:23.823   Training iter 150, batch loss 1.0675, batch acc 0.6750
11:52:24.331   Training iter 200, batch loss 1.0896, batch acc 0.6718
11:52:24.835   Training iter 250, batch loss 1.0678, batch acc 0.6844
11:52:25.375   Training iter 300, batch loss 1.0606, batch acc 0.6800
11:52:25.903   Training iter 350, batch loss 1.0834, batch acc 0.6762
11:52:26.410   Training iter 400, batch loss 1.0871, batch acc 0.6720
11:52:26.916   Training iter 450, batch loss 1.0688, batch acc 0.6724
11:52:27.431   Training iter 500, batch loss 1.0742, batch acc 0.6804
11:52:27.977   Training iter 550, batch loss 1.0861, batch acc 0.6668
11:52:28.540   Training iter 600, batch loss 1.0904, batch acc 0.6798
11:52:28.542 Training @ 370 epoch...
11:52:29.111   Training iter 50, batch loss 1.0788, batch acc 0.6718
11:52:29.669   Training iter 100, batch loss 1.0583, batch acc 0.6834
11:52:30.233   Training iter 150, batch loss 1.0799, batch acc 0.6726
11:52:30.796   Training iter 200, batch loss 1.0614, batch acc 0.6856
11:52:31.352   Training iter 250, batch loss 1.0885, batch acc 0.6738
11:52:31.882   Training iter 300, batch loss 1.0927, batch acc 0.6796
11:52:32.421   Training iter 350, batch loss 1.0707, batch acc 0.6712
11:52:32.975   Training iter 400, batch loss 1.0898, batch acc 0.6686
11:52:33.528   Training iter 450, batch loss 1.0643, batch acc 0.6850
11:52:34.044   Training iter 500, batch loss 1.0780, batch acc 0.6690
11:52:34.553   Training iter 550, batch loss 1.0745, batch acc 0.6764
11:52:35.058   Training iter 600, batch loss 1.0821, batch acc 0.6784
11:52:35.059 Testing @ 370 epoch...
11:52:35.101     Testing, total mean loss 1.04915, total acc 0.68340
11:52:35.101 Training @ 371 epoch...
11:52:35.603   Training iter 50, batch loss 1.0637, batch acc 0.6768
11:52:36.119   Training iter 100, batch loss 1.0572, batch acc 0.6880
11:52:36.635   Training iter 150, batch loss 1.0955, batch acc 0.6720
11:52:37.145   Training iter 200, batch loss 1.1067, batch acc 0.6602
11:52:37.649   Training iter 250, batch loss 1.0784, batch acc 0.6796
11:52:38.144   Training iter 300, batch loss 1.0650, batch acc 0.6802
11:52:38.610   Training iter 350, batch loss 1.0911, batch acc 0.6722
11:52:39.067   Training iter 400, batch loss 1.0731, batch acc 0.6784
11:52:39.531   Training iter 450, batch loss 1.0808, batch acc 0.6768
11:52:40.007   Training iter 500, batch loss 1.0716, batch acc 0.6796
11:52:40.535   Training iter 550, batch loss 1.0624, batch acc 0.6776
11:52:41.025   Training iter 600, batch loss 1.0739, batch acc 0.6738
11:52:41.026 Training @ 372 epoch...
11:52:41.491   Training iter 50, batch loss 1.0617, batch acc 0.6734
11:52:41.937   Training iter 100, batch loss 1.0985, batch acc 0.6602
11:52:42.399   Training iter 150, batch loss 1.0981, batch acc 0.6720
11:52:42.869   Training iter 200, batch loss 1.0431, batch acc 0.6902
11:52:43.366   Training iter 250, batch loss 1.0762, batch acc 0.6774
11:52:43.870   Training iter 300, batch loss 1.0940, batch acc 0.6678
11:52:44.384   Training iter 350, batch loss 1.0845, batch acc 0.6706
11:52:44.896   Training iter 400, batch loss 1.0748, batch acc 0.6776
11:52:45.403   Training iter 450, batch loss 1.0700, batch acc 0.6732
11:52:45.951   Training iter 500, batch loss 1.0667, batch acc 0.6842
11:52:46.521   Training iter 550, batch loss 1.0683, batch acc 0.6872
11:52:47.099   Training iter 600, batch loss 1.0832, batch acc 0.6770
11:52:47.100 Training @ 373 epoch...
11:52:47.661   Training iter 50, batch loss 1.0843, batch acc 0.6746
11:52:48.212   Training iter 100, batch loss 1.0541, batch acc 0.6868
11:52:48.759   Training iter 150, batch loss 1.0981, batch acc 0.6724
11:52:49.319   Training iter 200, batch loss 1.0788, batch acc 0.6720
11:52:49.863   Training iter 250, batch loss 1.0639, batch acc 0.6804
11:52:50.406   Training iter 300, batch loss 1.0696, batch acc 0.6742
11:52:50.930   Training iter 350, batch loss 1.0959, batch acc 0.6708
11:52:51.441   Training iter 400, batch loss 1.0640, batch acc 0.6832
11:52:51.944   Training iter 450, batch loss 1.0893, batch acc 0.6688
11:52:52.464   Training iter 500, batch loss 1.0793, batch acc 0.6782
11:52:52.971   Training iter 550, batch loss 1.0618, batch acc 0.6752
11:52:53.473   Training iter 600, batch loss 1.0798, batch acc 0.6778
11:52:53.475 Training @ 374 epoch...
11:52:53.962   Training iter 50, batch loss 1.0723, batch acc 0.6780
11:52:54.438   Training iter 100, batch loss 1.0911, batch acc 0.6748
11:52:54.923   Training iter 150, batch loss 1.1108, batch acc 0.6676
11:52:55.454   Training iter 200, batch loss 1.0688, batch acc 0.6772
11:52:55.980   Training iter 250, batch loss 1.0673, batch acc 0.6860
11:52:56.477   Training iter 300, batch loss 1.0679, batch acc 0.6714
11:52:56.959   Training iter 350, batch loss 1.0499, batch acc 0.6884
11:52:57.452   Training iter 400, batch loss 1.0962, batch acc 0.6596
11:52:57.973   Training iter 450, batch loss 1.0669, batch acc 0.6774
11:52:58.520   Training iter 500, batch loss 1.0797, batch acc 0.6756
11:52:59.045   Training iter 550, batch loss 1.0709, batch acc 0.6816
11:52:59.537   Training iter 600, batch loss 1.0770, batch acc 0.6736
11:52:59.538 Training @ 375 epoch...
11:53:00.065   Training iter 50, batch loss 1.0876, batch acc 0.6732
11:53:00.604   Training iter 100, batch loss 1.0789, batch acc 0.6784
11:53:01.143   Training iter 150, batch loss 1.0807, batch acc 0.6740
11:53:01.704   Training iter 200, batch loss 1.0787, batch acc 0.6834
11:53:02.282   Training iter 250, batch loss 1.0798, batch acc 0.6698
11:53:02.850   Training iter 300, batch loss 1.0693, batch acc 0.6778
11:53:03.423   Training iter 350, batch loss 1.0638, batch acc 0.6786
11:53:03.985   Training iter 400, batch loss 1.0886, batch acc 0.6698
11:53:04.557   Training iter 450, batch loss 1.0702, batch acc 0.6772
11:53:05.133   Training iter 500, batch loss 1.0926, batch acc 0.6668
11:53:05.690   Training iter 550, batch loss 1.0581, batch acc 0.6822
11:53:06.243   Training iter 600, batch loss 1.0705, batch acc 0.6800
11:53:06.245 Testing @ 375 epoch...
11:53:06.287     Testing, total mean loss 1.04906, total acc 0.68410
11:53:06.287 Training @ 376 epoch...
11:53:06.822   Training iter 50, batch loss 1.0666, batch acc 0.6774
11:53:07.362   Training iter 100, batch loss 1.0822, batch acc 0.6724
11:53:07.888   Training iter 150, batch loss 1.1047, batch acc 0.6606
11:53:08.401   Training iter 200, batch loss 1.0726, batch acc 0.6782
11:53:08.890   Training iter 250, batch loss 1.0758, batch acc 0.6754
11:53:09.404   Training iter 300, batch loss 1.0738, batch acc 0.6816
11:53:09.906   Training iter 350, batch loss 1.0762, batch acc 0.6774
11:53:10.412   Training iter 400, batch loss 1.0820, batch acc 0.6746
11:53:10.906   Training iter 450, batch loss 1.0788, batch acc 0.6734
11:53:11.404   Training iter 500, batch loss 1.0390, batch acc 0.6892
11:53:11.898   Training iter 550, batch loss 1.1088, batch acc 0.6652
11:53:12.398   Training iter 600, batch loss 1.0580, batch acc 0.6876
11:53:12.400 Training @ 377 epoch...
11:53:12.908   Training iter 50, batch loss 1.0512, batch acc 0.6876
11:53:13.423   Training iter 100, batch loss 1.1092, batch acc 0.6608
11:53:13.896   Training iter 150, batch loss 1.0570, batch acc 0.6770
11:53:14.394   Training iter 200, batch loss 1.0694, batch acc 0.6758
11:53:14.884   Training iter 250, batch loss 1.0768, batch acc 0.6802
11:53:15.382   Training iter 300, batch loss 1.0729, batch acc 0.6764
11:53:15.880   Training iter 350, batch loss 1.0728, batch acc 0.6824
11:53:16.389   Training iter 400, batch loss 1.0730, batch acc 0.6790
11:53:16.914   Training iter 450, batch loss 1.0958, batch acc 0.6704
11:53:17.449   Training iter 500, batch loss 1.0859, batch acc 0.6752
11:53:17.975   Training iter 550, batch loss 1.0764, batch acc 0.6720
11:53:18.520   Training iter 600, batch loss 1.0780, batch acc 0.6802
11:53:18.521 Training @ 378 epoch...
11:53:19.067   Training iter 50, batch loss 1.0972, batch acc 0.6662
11:53:19.608   Training iter 100, batch loss 1.0767, batch acc 0.6740
11:53:20.167   Training iter 150, batch loss 1.0802, batch acc 0.6778
11:53:20.728   Training iter 200, batch loss 1.0727, batch acc 0.6748
11:53:21.277   Training iter 250, batch loss 1.0675, batch acc 0.6818
11:53:21.800   Training iter 300, batch loss 1.0635, batch acc 0.6810
11:53:22.300   Training iter 350, batch loss 1.1125, batch acc 0.6658
11:53:22.768   Training iter 400, batch loss 1.0794, batch acc 0.6724
11:53:23.242   Training iter 450, batch loss 1.0676, batch acc 0.6764
11:53:23.711   Training iter 500, batch loss 1.0490, batch acc 0.6878
11:53:24.191   Training iter 550, batch loss 1.0448, batch acc 0.6910
11:53:24.667   Training iter 600, batch loss 1.1073, batch acc 0.6598
11:53:24.669 Training @ 379 epoch...
11:53:25.157   Training iter 50, batch loss 1.0881, batch acc 0.6670
11:53:25.637   Training iter 100, batch loss 1.0520, batch acc 0.6876
11:53:26.121   Training iter 150, batch loss 1.0996, batch acc 0.6668
11:53:26.597   Training iter 200, batch loss 1.0710, batch acc 0.6798
11:53:27.077   Training iter 250, batch loss 1.0565, batch acc 0.6824
11:53:27.591   Training iter 300, batch loss 1.0939, batch acc 0.6702
11:53:28.102   Training iter 350, batch loss 1.0910, batch acc 0.6740
11:53:28.626   Training iter 400, batch loss 1.0594, batch acc 0.6762
11:53:29.157   Training iter 450, batch loss 1.0649, batch acc 0.6864
11:53:29.680   Training iter 500, batch loss 1.0833, batch acc 0.6768
11:53:30.215   Training iter 550, batch loss 1.0797, batch acc 0.6746
11:53:30.755   Training iter 600, batch loss 1.0787, batch acc 0.6752
11:53:30.757 Training @ 380 epoch...
11:53:31.312   Training iter 50, batch loss 1.0686, batch acc 0.6734
11:53:31.896   Training iter 100, batch loss 1.0750, batch acc 0.6736
11:53:32.485   Training iter 150, batch loss 1.0627, batch acc 0.6866
11:53:33.073   Training iter 200, batch loss 1.0902, batch acc 0.6672
11:53:33.643   Training iter 250, batch loss 1.0847, batch acc 0.6666
11:53:34.211   Training iter 300, batch loss 1.0670, batch acc 0.6820
11:53:34.778   Training iter 350, batch loss 1.0795, batch acc 0.6758
11:53:35.345   Training iter 400, batch loss 1.0920, batch acc 0.6796
11:53:35.902   Training iter 450, batch loss 1.0657, batch acc 0.6836
11:53:36.462   Training iter 500, batch loss 1.0949, batch acc 0.6660
11:53:37.016   Training iter 550, batch loss 1.0721, batch acc 0.6760
11:53:37.559   Training iter 600, batch loss 1.0660, batch acc 0.6826
11:53:37.561 Testing @ 380 epoch...
11:53:37.603     Testing, total mean loss 1.04900, total acc 0.68390
11:53:37.603 Training @ 381 epoch...
11:53:38.173   Training iter 50, batch loss 1.0902, batch acc 0.6752
11:53:38.738   Training iter 100, batch loss 1.0380, batch acc 0.6958
11:53:39.297   Training iter 150, batch loss 1.0742, batch acc 0.6764
11:53:39.848   Training iter 200, batch loss 1.0704, batch acc 0.6786
11:53:40.414   Training iter 250, batch loss 1.0816, batch acc 0.6772
11:53:40.974   Training iter 300, batch loss 1.0769, batch acc 0.6730
11:53:41.543   Training iter 350, batch loss 1.0727, batch acc 0.6772
11:53:42.101   Training iter 400, batch loss 1.0993, batch acc 0.6720
11:53:42.645   Training iter 450, batch loss 1.0713, batch acc 0.6730
11:53:43.199   Training iter 500, batch loss 1.0647, batch acc 0.6834
11:53:43.748   Training iter 550, batch loss 1.0934, batch acc 0.6654
11:53:44.293   Training iter 600, batch loss 1.0852, batch acc 0.6688
11:53:44.295 Training @ 382 epoch...
11:53:44.820   Training iter 50, batch loss 1.0943, batch acc 0.6678
11:53:45.370   Training iter 100, batch loss 1.0733, batch acc 0.6812
11:53:45.893   Training iter 150, batch loss 1.0665, batch acc 0.6806
11:53:46.415   Training iter 200, batch loss 1.0827, batch acc 0.6708
11:53:46.915   Training iter 250, batch loss 1.0814, batch acc 0.6748
11:53:47.416   Training iter 300, batch loss 1.0852, batch acc 0.6700
11:53:47.921   Training iter 350, batch loss 1.0641, batch acc 0.6810
11:53:48.439   Training iter 400, batch loss 1.0757, batch acc 0.6726
11:53:48.972   Training iter 450, batch loss 1.0636, batch acc 0.6796
11:53:49.517   Training iter 500, batch loss 1.0721, batch acc 0.6814
11:53:50.072   Training iter 550, batch loss 1.0841, batch acc 0.6748
11:53:50.625   Training iter 600, batch loss 1.0751, batch acc 0.6788
11:53:50.627 Training @ 383 epoch...
11:53:51.189   Training iter 50, batch loss 1.0684, batch acc 0.6748
11:53:51.777   Training iter 100, batch loss 1.1029, batch acc 0.6686
11:53:52.360   Training iter 150, batch loss 1.0617, batch acc 0.6836
11:53:52.949   Training iter 200, batch loss 1.0723, batch acc 0.6814
11:53:53.524   Training iter 250, batch loss 1.0661, batch acc 0.6816
11:53:54.068   Training iter 300, batch loss 1.0989, batch acc 0.6646
11:53:54.613   Training iter 350, batch loss 1.0792, batch acc 0.6716
11:53:55.126   Training iter 400, batch loss 1.0651, batch acc 0.6838
11:53:55.631   Training iter 450, batch loss 1.0806, batch acc 0.6826
11:53:56.160   Training iter 500, batch loss 1.0558, batch acc 0.6766
11:53:56.678   Training iter 550, batch loss 1.0876, batch acc 0.6678
11:53:57.200   Training iter 600, batch loss 1.0789, batch acc 0.6772
11:53:57.202 Training @ 384 epoch...
11:53:57.723   Training iter 50, batch loss 1.0839, batch acc 0.6768
11:53:58.247   Training iter 100, batch loss 1.0658, batch acc 0.6850
11:53:58.767   Training iter 150, batch loss 1.0665, batch acc 0.6782
11:53:59.292   Training iter 200, batch loss 1.0401, batch acc 0.6934
11:53:59.816   Training iter 250, batch loss 1.0825, batch acc 0.6746
11:54:00.366   Training iter 300, batch loss 1.0871, batch acc 0.6692
11:54:00.902   Training iter 350, batch loss 1.0725, batch acc 0.6776
11:54:01.465   Training iter 400, batch loss 1.0857, batch acc 0.6726
11:54:02.007   Training iter 450, batch loss 1.0827, batch acc 0.6702
11:54:02.511   Training iter 500, batch loss 1.0870, batch acc 0.6716
11:54:03.017   Training iter 550, batch loss 1.0909, batch acc 0.6680
11:54:03.527   Training iter 600, batch loss 1.0730, batch acc 0.6782
11:54:03.529 Training @ 385 epoch...
11:54:04.071   Training iter 50, batch loss 1.0735, batch acc 0.6760
11:54:04.588   Training iter 100, batch loss 1.0996, batch acc 0.6656
11:54:05.144   Training iter 150, batch loss 1.0456, batch acc 0.6852
11:54:05.694   Training iter 200, batch loss 1.0917, batch acc 0.6710
11:54:06.252   Training iter 250, batch loss 1.0715, batch acc 0.6758
11:54:06.800   Training iter 300, batch loss 1.0927, batch acc 0.6706
11:54:07.389   Training iter 350, batch loss 1.1078, batch acc 0.6684
11:54:07.974   Training iter 400, batch loss 1.0686, batch acc 0.6840
11:54:08.561   Training iter 450, batch loss 1.0624, batch acc 0.6806
11:54:09.152   Training iter 500, batch loss 1.0648, batch acc 0.6740
11:54:09.722   Training iter 550, batch loss 1.0641, batch acc 0.6784
11:54:10.268   Training iter 600, batch loss 1.0753, batch acc 0.6846
11:54:10.270 Testing @ 385 epoch...
11:54:10.312     Testing, total mean loss 1.04898, total acc 0.68430
11:54:10.312 Training @ 386 epoch...
11:54:10.848   Training iter 50, batch loss 1.1022, batch acc 0.6604
11:54:11.358   Training iter 100, batch loss 1.0635, batch acc 0.6838
11:54:11.874   Training iter 150, batch loss 1.0573, batch acc 0.6792
11:54:12.407   Training iter 200, batch loss 1.0691, batch acc 0.6806
11:54:12.925   Training iter 250, batch loss 1.1065, batch acc 0.6682
11:54:13.449   Training iter 300, batch loss 1.0608, batch acc 0.6848
11:54:13.962   Training iter 350, batch loss 1.0758, batch acc 0.6794
11:54:14.477   Training iter 400, batch loss 1.0857, batch acc 0.6748
11:54:15.002   Training iter 450, batch loss 1.0885, batch acc 0.6728
11:54:15.527   Training iter 500, batch loss 1.0803, batch acc 0.6728
11:54:16.061   Training iter 550, batch loss 1.0596, batch acc 0.6798
11:54:16.590   Training iter 600, batch loss 1.0683, batch acc 0.6788
11:54:16.592 Training @ 387 epoch...
11:54:17.134   Training iter 50, batch loss 1.0348, batch acc 0.6894
11:54:17.681   Training iter 100, batch loss 1.0929, batch acc 0.6744
11:54:18.258   Training iter 150, batch loss 1.0826, batch acc 0.6720
11:54:18.822   Training iter 200, batch loss 1.0674, batch acc 0.6822
11:54:19.386   Training iter 250, batch loss 1.0828, batch acc 0.6728
11:54:19.954   Training iter 300, batch loss 1.0953, batch acc 0.6644
11:54:20.537   Training iter 350, batch loss 1.0660, batch acc 0.6786
11:54:21.133   Training iter 400, batch loss 1.0735, batch acc 0.6754
11:54:21.731   Training iter 450, batch loss 1.0599, batch acc 0.6796
11:54:22.333   Training iter 500, batch loss 1.1154, batch acc 0.6656
11:54:22.906   Training iter 550, batch loss 1.0637, batch acc 0.6848
11:54:23.458   Training iter 600, batch loss 1.0830, batch acc 0.6748
11:54:23.460 Training @ 388 epoch...
11:54:23.990   Training iter 50, batch loss 1.0974, batch acc 0.6672
11:54:24.507   Training iter 100, batch loss 1.0755, batch acc 0.6776
11:54:25.038   Training iter 150, batch loss 1.0584, batch acc 0.6882
11:54:25.587   Training iter 200, batch loss 1.0795, batch acc 0.6746
11:54:26.129   Training iter 250, batch loss 1.1026, batch acc 0.6666
11:54:26.640   Training iter 300, batch loss 1.0918, batch acc 0.6702
11:54:27.146   Training iter 350, batch loss 1.0709, batch acc 0.6772
11:54:27.669   Training iter 400, batch loss 1.0737, batch acc 0.6736
11:54:28.194   Training iter 450, batch loss 1.0766, batch acc 0.6756
11:54:28.713   Training iter 500, batch loss 1.0341, batch acc 0.6860
11:54:29.245   Training iter 550, batch loss 1.0717, batch acc 0.6876
11:54:29.827   Training iter 600, batch loss 1.0851, batch acc 0.6718
11:54:29.829 Training @ 389 epoch...
11:54:30.416   Training iter 50, batch loss 1.0711, batch acc 0.6802
11:54:30.999   Training iter 100, batch loss 1.0946, batch acc 0.6716
11:54:31.512   Training iter 150, batch loss 1.0631, batch acc 0.6846
11:54:31.992   Training iter 200, batch loss 1.0800, batch acc 0.6738
11:54:32.472   Training iter 250, batch loss 1.0932, batch acc 0.6708
11:54:32.960   Training iter 300, batch loss 1.0774, batch acc 0.6734
11:54:33.472   Training iter 350, batch loss 1.0724, batch acc 0.6776
11:54:33.978   Training iter 400, batch loss 1.0907, batch acc 0.6718
11:54:34.472   Training iter 450, batch loss 1.0606, batch acc 0.6730
11:54:34.950   Training iter 500, batch loss 1.0654, batch acc 0.6810
11:54:35.422   Training iter 550, batch loss 1.0487, batch acc 0.6888
11:54:35.894   Training iter 600, batch loss 1.0998, batch acc 0.6660
11:54:35.896 Training @ 390 epoch...
11:54:36.376   Training iter 50, batch loss 1.0666, batch acc 0.6814
11:54:36.868   Training iter 100, batch loss 1.0687, batch acc 0.6740
11:54:37.390   Training iter 150, batch loss 1.0868, batch acc 0.6724
11:54:37.901   Training iter 200, batch loss 1.0699, batch acc 0.6782
11:54:38.420   Training iter 250, batch loss 1.0591, batch acc 0.6816
11:54:38.944   Training iter 300, batch loss 1.1038, batch acc 0.6732
11:54:39.470   Training iter 350, batch loss 1.0666, batch acc 0.6762
11:54:39.996   Training iter 400, batch loss 1.0605, batch acc 0.6788
11:54:40.529   Training iter 450, batch loss 1.0639, batch acc 0.6820
11:54:41.052   Training iter 500, batch loss 1.0802, batch acc 0.6780
11:54:41.568   Training iter 550, batch loss 1.0960, batch acc 0.6736
11:54:42.096   Training iter 600, batch loss 1.0951, batch acc 0.6640
11:54:42.097 Testing @ 390 epoch...
11:54:42.140     Testing, total mean loss 1.04898, total acc 0.68420
11:54:42.140 Training @ 391 epoch...
11:54:42.672   Training iter 50, batch loss 1.0598, batch acc 0.6794
11:54:43.193   Training iter 100, batch loss 1.0788, batch acc 0.6788
11:54:43.667   Training iter 150, batch loss 1.0427, batch acc 0.6870
11:54:44.146   Training iter 200, batch loss 1.1030, batch acc 0.6592
11:54:44.619   Training iter 250, batch loss 1.0761, batch acc 0.6828
11:54:45.099   Training iter 300, batch loss 1.0911, batch acc 0.6674
11:54:45.571   Training iter 350, batch loss 1.0647, batch acc 0.6742
11:54:46.048   Training iter 400, batch loss 1.0918, batch acc 0.6742
11:54:46.521   Training iter 450, batch loss 1.0622, batch acc 0.6866
11:54:46.993   Training iter 500, batch loss 1.1042, batch acc 0.6666
11:54:47.471   Training iter 550, batch loss 1.0447, batch acc 0.6834
11:54:47.983   Training iter 600, batch loss 1.0979, batch acc 0.6728
11:54:47.985 Training @ 392 epoch...
11:54:48.525   Training iter 50, batch loss 1.1038, batch acc 0.6690
11:54:48.973   Training iter 100, batch loss 1.0643, batch acc 0.6768
11:54:49.434   Training iter 150, batch loss 1.0745, batch acc 0.6730
11:54:49.928   Training iter 200, batch loss 1.0778, batch acc 0.6810
11:54:50.431   Training iter 250, batch loss 1.0657, batch acc 0.6762
11:54:50.913   Training iter 300, batch loss 1.0756, batch acc 0.6848
11:54:51.380   Training iter 350, batch loss 1.1073, batch acc 0.6680
11:54:51.883   Training iter 400, batch loss 1.0962, batch acc 0.6610
11:54:52.397   Training iter 450, batch loss 1.0450, batch acc 0.6912
11:54:52.927   Training iter 500, batch loss 1.0533, batch acc 0.6838
11:54:53.448   Training iter 550, batch loss 1.0943, batch acc 0.6662
11:54:53.954   Training iter 600, batch loss 1.0591, batch acc 0.6840
11:54:53.955 Training @ 393 epoch...
11:54:54.477   Training iter 50, batch loss 1.0791, batch acc 0.6752
11:54:54.971   Training iter 100, batch loss 1.0697, batch acc 0.6754
11:54:55.466   Training iter 150, batch loss 1.0645, batch acc 0.6816
11:54:55.971   Training iter 200, batch loss 1.1128, batch acc 0.6650
11:54:56.486   Training iter 250, batch loss 1.0677, batch acc 0.6800
11:54:56.994   Training iter 300, batch loss 1.0498, batch acc 0.6836
11:54:57.518   Training iter 350, batch loss 1.0868, batch acc 0.6738
11:54:58.030   Training iter 400, batch loss 1.0355, batch acc 0.6960
11:54:58.551   Training iter 450, batch loss 1.1077, batch acc 0.6676
11:54:59.092   Training iter 500, batch loss 1.0706, batch acc 0.6748
11:54:59.619   Training iter 550, batch loss 1.0738, batch acc 0.6758
11:55:00.138   Training iter 600, batch loss 1.0989, batch acc 0.6650
11:55:00.139 Training @ 394 epoch...
11:55:00.664   Training iter 50, batch loss 1.0777, batch acc 0.6704
11:55:01.182   Training iter 100, batch loss 1.0739, batch acc 0.6780
11:55:01.726   Training iter 150, batch loss 1.0601, batch acc 0.6884
11:55:02.319   Training iter 200, batch loss 1.0611, batch acc 0.6834
11:55:02.831   Training iter 250, batch loss 1.0702, batch acc 0.6822
11:55:03.349   Training iter 300, batch loss 1.0896, batch acc 0.6670
11:55:03.861   Training iter 350, batch loss 1.0752, batch acc 0.6734
11:55:04.376   Training iter 400, batch loss 1.1076, batch acc 0.6610
11:55:04.893   Training iter 450, batch loss 1.0623, batch acc 0.6828
11:55:05.391   Training iter 500, batch loss 1.0820, batch acc 0.6752
11:55:05.890   Training iter 550, batch loss 1.0582, batch acc 0.6800
11:55:06.394   Training iter 600, batch loss 1.0985, batch acc 0.6700
11:55:06.395 Training @ 395 epoch...
11:55:06.907   Training iter 50, batch loss 1.0801, batch acc 0.6758
11:55:07.390   Training iter 100, batch loss 1.0841, batch acc 0.6742
11:55:07.869   Training iter 150, batch loss 1.0875, batch acc 0.6674
11:55:08.345   Training iter 200, batch loss 1.0748, batch acc 0.6772
11:55:08.823   Training iter 250, batch loss 1.0370, batch acc 0.6918
11:55:09.344   Training iter 300, batch loss 1.0531, batch acc 0.6816
11:55:09.859   Training iter 350, batch loss 1.1107, batch acc 0.6584
11:55:10.379   Training iter 400, batch loss 1.0968, batch acc 0.6740
11:55:10.881   Training iter 450, batch loss 1.0705, batch acc 0.6802
11:55:11.379   Training iter 500, batch loss 1.0666, batch acc 0.6804
11:55:11.879   Training iter 550, batch loss 1.0940, batch acc 0.6676
11:55:12.387   Training iter 600, batch loss 1.0614, batch acc 0.6820
11:55:12.389 Testing @ 395 epoch...
11:55:12.432     Testing, total mean loss 1.04899, total acc 0.68410
11:55:12.432 Training @ 396 epoch...
11:55:12.954   Training iter 50, batch loss 1.0834, batch acc 0.6698
11:55:13.466   Training iter 100, batch loss 1.0722, batch acc 0.6784
11:55:13.991   Training iter 150, batch loss 1.1209, batch acc 0.6608
11:55:14.530   Training iter 200, batch loss 1.0575, batch acc 0.6868
11:55:15.055   Training iter 250, batch loss 1.0678, batch acc 0.6788
11:55:15.537   Training iter 300, batch loss 1.0683, batch acc 0.6772
11:55:16.005   Training iter 350, batch loss 1.0727, batch acc 0.6830
11:55:16.477   Training iter 400, batch loss 1.0809, batch acc 0.6694
11:55:16.935   Training iter 450, batch loss 1.0736, batch acc 0.6860
11:55:17.386   Training iter 500, batch loss 1.0914, batch acc 0.6678
11:55:17.840   Training iter 550, batch loss 1.0581, batch acc 0.6798
11:55:18.295   Training iter 600, batch loss 1.0695, batch acc 0.6798
11:55:18.297 Training @ 397 epoch...
11:55:18.765   Training iter 50, batch loss 1.0582, batch acc 0.6812
11:55:19.222   Training iter 100, batch loss 1.0936, batch acc 0.6708
11:55:19.682   Training iter 150, batch loss 1.0767, batch acc 0.6724
11:55:20.152   Training iter 200, batch loss 1.0509, batch acc 0.6786
11:55:20.612   Training iter 250, batch loss 1.0820, batch acc 0.6774
11:55:21.074   Training iter 300, batch loss 1.0643, batch acc 0.6804
11:55:21.534   Training iter 350, batch loss 1.0836, batch acc 0.6716
11:55:21.985   Training iter 400, batch loss 1.0976, batch acc 0.6742
11:55:22.453   Training iter 450, batch loss 1.0761, batch acc 0.6756
11:55:22.909   Training iter 500, batch loss 1.0724, batch acc 0.6752
11:55:23.380   Training iter 550, batch loss 1.0890, batch acc 0.6760
11:55:23.888   Training iter 600, batch loss 1.0718, batch acc 0.6808
11:55:23.890 Training @ 398 epoch...
11:55:24.397   Training iter 50, batch loss 1.0393, batch acc 0.6866
11:55:24.917   Training iter 100, batch loss 1.0741, batch acc 0.6734
11:55:25.457   Training iter 150, batch loss 1.0700, batch acc 0.6742
11:55:25.995   Training iter 200, batch loss 1.0893, batch acc 0.6714
11:55:26.537   Training iter 250, batch loss 1.0780, batch acc 0.6770
11:55:27.080   Training iter 300, batch loss 1.0711, batch acc 0.6798
11:55:27.638   Training iter 350, batch loss 1.1011, batch acc 0.6724
11:55:28.224   Training iter 400, batch loss 1.0613, batch acc 0.6856
11:55:28.810   Training iter 450, batch loss 1.0723, batch acc 0.6772
11:55:29.388   Training iter 500, batch loss 1.0889, batch acc 0.6702
11:55:29.924   Training iter 550, batch loss 1.0867, batch acc 0.6716
11:55:30.473   Training iter 600, batch loss 1.0842, batch acc 0.6728
11:55:30.475 Training @ 399 epoch...
11:55:31.006   Training iter 50, batch loss 1.0471, batch acc 0.6898
11:55:31.514   Training iter 100, batch loss 1.0974, batch acc 0.6632
11:55:32.019   Training iter 150, batch loss 1.0731, batch acc 0.6830
11:55:32.513   Training iter 200, batch loss 1.1036, batch acc 0.6646
11:55:33.018   Training iter 250, batch loss 1.0772, batch acc 0.6740
11:55:33.520   Training iter 300, batch loss 1.0893, batch acc 0.6662
11:55:34.026   Training iter 350, batch loss 1.0803, batch acc 0.6750
11:55:34.528   Training iter 400, batch loss 1.0769, batch acc 0.6724
11:55:35.029   Training iter 450, batch loss 1.0533, batch acc 0.6852
11:55:35.543   Training iter 500, batch loss 1.0828, batch acc 0.6826
11:55:36.060   Training iter 550, batch loss 1.0622, batch acc 0.6820
11:55:36.564   Training iter 600, batch loss 1.0728, batch acc 0.6778
11:55:36.565 Training @ 400 epoch...
11:55:37.092   Training iter 50, batch loss 1.0470, batch acc 0.6888
11:55:37.574   Training iter 100, batch loss 1.0740, batch acc 0.6754
11:55:38.075   Training iter 150, batch loss 1.0952, batch acc 0.6698
11:55:38.590   Training iter 200, batch loss 1.0933, batch acc 0.6704
11:55:39.090   Training iter 250, batch loss 1.0857, batch acc 0.6732
11:55:39.594   Training iter 300, batch loss 1.0542, batch acc 0.6844
11:55:40.106   Training iter 350, batch loss 1.0824, batch acc 0.6778
11:55:40.621   Training iter 400, batch loss 1.0818, batch acc 0.6698
11:55:41.126   Training iter 450, batch loss 1.0876, batch acc 0.6712
11:55:41.625   Training iter 500, batch loss 1.0587, batch acc 0.6806
11:55:42.130   Training iter 550, batch loss 1.0755, batch acc 0.6810
11:55:42.600   Training iter 600, batch loss 1.0806, batch acc 0.6744
11:55:42.602 Testing @ 400 epoch...
11:55:42.644     Testing, total mean loss 1.04886, total acc 0.68400
11:55:42.644 Plot @ 400 epoch...
11:55:42.644 Training @ 401 epoch...
11:55:43.128   Training iter 50, batch loss 1.0786, batch acc 0.6768
11:55:43.625   Training iter 100, batch loss 1.0728, batch acc 0.6754
11:55:44.146   Training iter 150, batch loss 1.0898, batch acc 0.6704
11:55:44.648   Training iter 200, batch loss 1.0730, batch acc 0.6782
11:55:45.148   Training iter 250, batch loss 1.0866, batch acc 0.6698
11:55:45.651   Training iter 300, batch loss 1.0766, batch acc 0.6740
11:55:46.144   Training iter 350, batch loss 1.0818, batch acc 0.6700
11:55:46.631   Training iter 400, batch loss 1.0726, batch acc 0.6770
11:55:47.109   Training iter 450, batch loss 1.0492, batch acc 0.6874
11:55:47.576   Training iter 500, batch loss 1.0901, batch acc 0.6696
11:55:48.052   Training iter 550, batch loss 1.0734, batch acc 0.6832
11:55:48.515   Training iter 600, batch loss 1.0715, batch acc 0.6826
11:55:48.516 Training @ 402 epoch...
11:55:48.975   Training iter 50, batch loss 1.0525, batch acc 0.6830
11:55:49.427   Training iter 100, batch loss 1.0783, batch acc 0.6706
11:55:49.881   Training iter 150, batch loss 1.0810, batch acc 0.6716
11:55:50.363   Training iter 200, batch loss 1.0608, batch acc 0.6808
11:55:50.851   Training iter 250, batch loss 1.0980, batch acc 0.6692
11:55:51.327   Training iter 300, batch loss 1.0694, batch acc 0.6826
11:55:51.790   Training iter 350, batch loss 1.1107, batch acc 0.6662
11:55:52.264   Training iter 400, batch loss 1.0738, batch acc 0.6794
11:55:52.746   Training iter 450, batch loss 1.0795, batch acc 0.6784
11:55:53.233   Training iter 500, batch loss 1.0823, batch acc 0.6672
11:55:53.719   Training iter 550, batch loss 1.0576, batch acc 0.6880
11:55:54.239   Training iter 600, batch loss 1.0718, batch acc 0.6748
11:55:54.240 Training @ 403 epoch...
11:55:54.751   Training iter 50, batch loss 1.1122, batch acc 0.6636
11:55:55.272   Training iter 100, batch loss 1.0711, batch acc 0.6806
11:55:55.787   Training iter 150, batch loss 1.0592, batch acc 0.6788
11:55:56.287   Training iter 200, batch loss 1.0903, batch acc 0.6724
11:55:56.779   Training iter 250, batch loss 1.0920, batch acc 0.6720
11:55:57.303   Training iter 300, batch loss 1.0981, batch acc 0.6674
11:55:57.844   Training iter 350, batch loss 1.0575, batch acc 0.6798
11:55:58.385   Training iter 400, batch loss 1.0368, batch acc 0.6906
11:55:58.919   Training iter 450, batch loss 1.1017, batch acc 0.6682
11:55:59.451   Training iter 500, batch loss 1.0802, batch acc 0.6796
11:55:59.980   Training iter 550, batch loss 1.0531, batch acc 0.6852
11:56:00.526   Training iter 600, batch loss 1.0635, batch acc 0.6790
11:56:00.528 Training @ 404 epoch...
11:56:01.056   Training iter 50, batch loss 1.0795, batch acc 0.6744
11:56:01.600   Training iter 100, batch loss 1.0727, batch acc 0.6772
11:56:02.177   Training iter 150, batch loss 1.0793, batch acc 0.6762
11:56:02.692   Training iter 200, batch loss 1.0809, batch acc 0.6728
11:56:03.210   Training iter 250, batch loss 1.0602, batch acc 0.6812
11:56:03.713   Training iter 300, batch loss 1.0904, batch acc 0.6686
11:56:04.204   Training iter 350, batch loss 1.1113, batch acc 0.6600
11:56:04.706   Training iter 400, batch loss 1.0770, batch acc 0.6766
11:56:05.214   Training iter 450, batch loss 1.0585, batch acc 0.6818
11:56:05.723   Training iter 500, batch loss 1.0572, batch acc 0.6824
11:56:06.207   Training iter 550, batch loss 1.0678, batch acc 0.6800
11:56:06.693   Training iter 600, batch loss 1.0808, batch acc 0.6836
11:56:06.695 Training @ 405 epoch...
11:56:07.198   Training iter 50, batch loss 1.0628, batch acc 0.6782
11:56:07.687   Training iter 100, batch loss 1.1048, batch acc 0.6640
11:56:08.186   Training iter 150, batch loss 1.0867, batch acc 0.6742
11:56:08.682   Training iter 200, batch loss 1.0579, batch acc 0.6874
11:56:09.187   Training iter 250, batch loss 1.0830, batch acc 0.6754
11:56:09.690   Training iter 300, batch loss 1.0748, batch acc 0.6722
11:56:10.205   Training iter 350, batch loss 1.0845, batch acc 0.6704
11:56:10.708   Training iter 400, batch loss 1.0805, batch acc 0.6782
11:56:11.204   Training iter 450, batch loss 1.0666, batch acc 0.6742
11:56:11.702   Training iter 500, batch loss 1.0535, batch acc 0.6872
11:56:12.211   Training iter 550, batch loss 1.0756, batch acc 0.6776
11:56:12.723   Training iter 600, batch loss 1.0851, batch acc 0.6742
11:56:12.725 Testing @ 405 epoch...
11:56:12.767     Testing, total mean loss 1.04880, total acc 0.68430
11:56:12.767 Training @ 406 epoch...
11:56:13.298   Training iter 50, batch loss 1.0687, batch acc 0.6802
11:56:13.829   Training iter 100, batch loss 1.0814, batch acc 0.6780
11:56:14.374   Training iter 150, batch loss 1.0754, batch acc 0.6702
11:56:14.911   Training iter 200, batch loss 1.0839, batch acc 0.6758
11:56:15.461   Training iter 250, batch loss 1.0843, batch acc 0.6744
11:56:15.994   Training iter 300, batch loss 1.0709, batch acc 0.6822
11:56:16.528   Training iter 350, batch loss 1.0793, batch acc 0.6706
11:56:17.060   Training iter 400, batch loss 1.0889, batch acc 0.6724
11:56:17.588   Training iter 450, batch loss 1.0534, batch acc 0.6892
11:56:18.122   Training iter 500, batch loss 1.0971, batch acc 0.6692
11:56:18.668   Training iter 550, batch loss 1.0772, batch acc 0.6708
11:56:19.203   Training iter 600, batch loss 1.0551, batch acc 0.6814
11:56:19.205 Training @ 407 epoch...
11:56:19.711   Training iter 50, batch loss 1.0584, batch acc 0.6858
11:56:20.221   Training iter 100, batch loss 1.0769, batch acc 0.6800
11:56:20.751   Training iter 150, batch loss 1.0894, batch acc 0.6712
11:56:21.275   Training iter 200, batch loss 1.0620, batch acc 0.6796
11:56:21.779   Training iter 250, batch loss 1.0613, batch acc 0.6818
11:56:22.295   Training iter 300, batch loss 1.0583, batch acc 0.6782
11:56:22.814   Training iter 350, batch loss 1.0959, batch acc 0.6740
11:56:23.351   Training iter 400, batch loss 1.0845, batch acc 0.6736
11:56:23.860   Training iter 450, batch loss 1.0932, batch acc 0.6650
11:56:24.361   Training iter 500, batch loss 1.0661, batch acc 0.6820
11:56:24.877   Training iter 550, batch loss 1.1008, batch acc 0.6660
11:56:25.424   Training iter 600, batch loss 1.0684, batch acc 0.6736
11:56:25.426 Training @ 408 epoch...
11:56:25.950   Training iter 50, batch loss 1.0753, batch acc 0.6746
11:56:26.472   Training iter 100, batch loss 1.0424, batch acc 0.6866
11:56:26.995   Training iter 150, batch loss 1.0931, batch acc 0.6708
11:56:27.522   Training iter 200, batch loss 1.0751, batch acc 0.6760
11:56:28.059   Training iter 250, batch loss 1.0767, batch acc 0.6794
11:56:28.565   Training iter 300, batch loss 1.0585, batch acc 0.6814
11:56:29.058   Training iter 350, batch loss 1.0460, batch acc 0.6876
11:56:29.571   Training iter 400, batch loss 1.0820, batch acc 0.6792
11:56:30.110   Training iter 450, batch loss 1.1058, batch acc 0.6670
11:56:30.646   Training iter 500, batch loss 1.0800, batch acc 0.6728
11:56:31.183   Training iter 550, batch loss 1.0956, batch acc 0.6694
11:56:31.718   Training iter 600, batch loss 1.0847, batch acc 0.6720
11:56:31.720 Training @ 409 epoch...
11:56:32.258   Training iter 50, batch loss 1.0548, batch acc 0.6806
11:56:32.802   Training iter 100, batch loss 1.0780, batch acc 0.6766
11:56:33.331   Training iter 150, batch loss 1.0531, batch acc 0.6826
11:56:33.860   Training iter 200, batch loss 1.0993, batch acc 0.6690
11:56:34.393   Training iter 250, batch loss 1.0516, batch acc 0.6846
11:56:34.938   Training iter 300, batch loss 1.1060, batch acc 0.6686
11:56:35.456   Training iter 350, batch loss 1.0776, batch acc 0.6736
11:56:35.952   Training iter 400, batch loss 1.0783, batch acc 0.6760
11:56:36.452   Training iter 450, batch loss 1.0756, batch acc 0.6766
11:56:36.937   Training iter 500, batch loss 1.0852, batch acc 0.6720
11:56:37.419   Training iter 550, batch loss 1.0893, batch acc 0.6754
11:56:37.906   Training iter 600, batch loss 1.0663, batch acc 0.6830
11:56:37.908 Training @ 410 epoch...
11:56:38.409   Training iter 50, batch loss 1.0567, batch acc 0.6824
11:56:38.889   Training iter 100, batch loss 1.0696, batch acc 0.6832
11:56:39.366   Training iter 150, batch loss 1.0708, batch acc 0.6806
11:56:39.839   Training iter 200, batch loss 1.0841, batch acc 0.6742
11:56:40.351   Training iter 250, batch loss 1.0793, batch acc 0.6740
11:56:40.873   Training iter 300, batch loss 1.0752, batch acc 0.6726
11:56:41.388   Training iter 350, batch loss 1.0689, batch acc 0.6746
11:56:41.898   Training iter 400, batch loss 1.0807, batch acc 0.6762
11:56:42.420   Training iter 450, batch loss 1.0892, batch acc 0.6758
11:56:42.949   Training iter 500, batch loss 1.0715, batch acc 0.6802
11:56:43.470   Training iter 550, batch loss 1.0743, batch acc 0.6744
11:56:43.991   Training iter 600, batch loss 1.0947, batch acc 0.6656
11:56:43.993 Testing @ 410 epoch...
11:56:44.036     Testing, total mean loss 1.04876, total acc 0.68470
11:56:44.036 Training @ 411 epoch...
11:56:44.550   Training iter 50, batch loss 1.0775, batch acc 0.6796
11:56:45.052   Training iter 100, batch loss 1.0987, batch acc 0.6710
11:56:45.570   Training iter 150, batch loss 1.0837, batch acc 0.6760
11:56:46.078   Training iter 200, batch loss 1.0633, batch acc 0.6826
11:56:46.582   Training iter 250, batch loss 1.0750, batch acc 0.6756
11:56:47.078   Training iter 300, batch loss 1.0795, batch acc 0.6730
11:56:47.574   Training iter 350, batch loss 1.0534, batch acc 0.6866
11:56:48.096   Training iter 400, batch loss 1.0649, batch acc 0.6812
11:56:48.616   Training iter 450, batch loss 1.0712, batch acc 0.6770
11:56:49.121   Training iter 500, batch loss 1.0817, batch acc 0.6712
11:56:49.621   Training iter 550, batch loss 1.0862, batch acc 0.6722
11:56:50.132   Training iter 600, batch loss 1.0801, batch acc 0.6706
11:56:50.134 Training @ 412 epoch...
11:56:50.663   Training iter 50, batch loss 1.0585, batch acc 0.6832
11:56:51.174   Training iter 100, batch loss 1.0700, batch acc 0.6806
11:56:51.684   Training iter 150, batch loss 1.0775, batch acc 0.6700
11:56:52.204   Training iter 200, batch loss 1.0511, batch acc 0.6878
11:56:52.713   Training iter 250, batch loss 1.0898, batch acc 0.6730
11:56:53.230   Training iter 300, batch loss 1.0615, batch acc 0.6800
11:56:53.747   Training iter 350, batch loss 1.0901, batch acc 0.6728
11:56:54.259   Training iter 400, batch loss 1.0971, batch acc 0.6666
11:56:54.764   Training iter 450, batch loss 1.0789, batch acc 0.6772
11:56:55.264   Training iter 500, batch loss 1.0939, batch acc 0.6698
11:56:55.779   Training iter 550, batch loss 1.0831, batch acc 0.6726
11:56:56.282   Training iter 600, batch loss 1.0637, batch acc 0.6834
11:56:56.284 Training @ 413 epoch...
11:56:56.784   Training iter 50, batch loss 1.0665, batch acc 0.6780
11:56:57.298   Training iter 100, batch loss 1.0670, batch acc 0.6812
11:56:57.846   Training iter 150, batch loss 1.0833, batch acc 0.6702
11:56:58.413   Training iter 200, batch loss 1.0675, batch acc 0.6810
11:56:58.986   Training iter 250, batch loss 1.0635, batch acc 0.6814
11:56:59.527   Training iter 300, batch loss 1.0664, batch acc 0.6766
11:57:00.009   Training iter 350, batch loss 1.1035, batch acc 0.6644
11:57:00.502   Training iter 400, batch loss 1.0942, batch acc 0.6696
11:57:00.999   Training iter 450, batch loss 1.0882, batch acc 0.6686
11:57:01.529   Training iter 500, batch loss 1.0656, batch acc 0.6816
11:57:02.101   Training iter 550, batch loss 1.0778, batch acc 0.6842
11:57:02.635   Training iter 600, batch loss 1.0714, batch acc 0.6794
11:57:02.637 Training @ 414 epoch...
11:57:03.178   Training iter 50, batch loss 1.0516, batch acc 0.6816
11:57:03.723   Training iter 100, batch loss 1.1232, batch acc 0.6592
11:57:04.268   Training iter 150, batch loss 1.0862, batch acc 0.6746
11:57:04.806   Training iter 200, batch loss 1.0609, batch acc 0.6856
11:57:05.348   Training iter 250, batch loss 1.0756, batch acc 0.6758
11:57:05.918   Training iter 300, batch loss 1.0713, batch acc 0.6754
11:57:06.491   Training iter 350, batch loss 1.0792, batch acc 0.6710
11:57:07.086   Training iter 400, batch loss 1.0746, batch acc 0.6746
11:57:07.676   Training iter 450, batch loss 1.0832, batch acc 0.6724
11:57:08.244   Training iter 500, batch loss 1.0567, batch acc 0.6894
11:57:08.792   Training iter 550, batch loss 1.0821, batch acc 0.6766
11:57:09.334   Training iter 600, batch loss 1.0704, batch acc 0.6766
11:57:09.336 Training @ 415 epoch...
11:57:09.908   Training iter 50, batch loss 1.1061, batch acc 0.6586
11:57:10.475   Training iter 100, batch loss 1.0746, batch acc 0.6768
11:57:11.019   Training iter 150, batch loss 1.0683, batch acc 0.6792
11:57:11.544   Training iter 200, batch loss 1.0926, batch acc 0.6700
11:57:12.082   Training iter 250, batch loss 1.1029, batch acc 0.6702
11:57:12.641   Training iter 300, batch loss 1.0496, batch acc 0.6858
11:57:13.223   Training iter 350, batch loss 1.0736, batch acc 0.6700
11:57:13.755   Training iter 400, batch loss 1.0398, batch acc 0.6926
11:57:14.298   Training iter 450, batch loss 1.0713, batch acc 0.6840
11:57:14.819   Training iter 500, batch loss 1.0842, batch acc 0.6740
11:57:15.347   Training iter 550, batch loss 1.0799, batch acc 0.6732
11:57:15.883   Training iter 600, batch loss 1.0718, batch acc 0.6804
11:57:15.885 Testing @ 415 epoch...
11:57:15.928     Testing, total mean loss 1.04873, total acc 0.68400
11:57:15.928 Training @ 416 epoch...
11:57:16.485   Training iter 50, batch loss 1.0481, batch acc 0.6896
11:57:17.024   Training iter 100, batch loss 1.0934, batch acc 0.6654
11:57:17.555   Training iter 150, batch loss 1.0800, batch acc 0.6712
11:57:18.117   Training iter 200, batch loss 1.0735, batch acc 0.6748
11:57:18.661   Training iter 250, batch loss 1.0813, batch acc 0.6724
11:57:19.210   Training iter 300, batch loss 1.0460, batch acc 0.6922
11:57:19.764   Training iter 350, batch loss 1.0730, batch acc 0.6810
11:57:20.329   Training iter 400, batch loss 1.1097, batch acc 0.6672
11:57:20.905   Training iter 450, batch loss 1.0831, batch acc 0.6754
11:57:21.456   Training iter 500, batch loss 1.0760, batch acc 0.6710
11:57:22.002   Training iter 550, batch loss 1.0795, batch acc 0.6764
11:57:22.548   Training iter 600, batch loss 1.0710, batch acc 0.6804
11:57:22.550 Training @ 417 epoch...
11:57:23.114   Training iter 50, batch loss 1.0867, batch acc 0.6724
11:57:23.658   Training iter 100, batch loss 1.0667, batch acc 0.6818
11:57:24.186   Training iter 150, batch loss 1.0789, batch acc 0.6746
11:57:24.678   Training iter 200, batch loss 1.0702, batch acc 0.6866
11:57:25.172   Training iter 250, batch loss 1.0581, batch acc 0.6864
11:57:25.683   Training iter 300, batch loss 1.0854, batch acc 0.6750
11:57:26.208   Training iter 350, batch loss 1.0819, batch acc 0.6628
11:57:26.728   Training iter 400, batch loss 1.0624, batch acc 0.6772
11:57:27.252   Training iter 450, batch loss 1.0689, batch acc 0.6772
11:57:27.757   Training iter 500, batch loss 1.0827, batch acc 0.6766
11:57:28.264   Training iter 550, batch loss 1.0870, batch acc 0.6758
11:57:28.784   Training iter 600, batch loss 1.0857, batch acc 0.6692
11:57:28.786 Training @ 418 epoch...
11:57:29.315   Training iter 50, batch loss 1.0692, batch acc 0.6756
11:57:29.829   Training iter 100, batch loss 1.1011, batch acc 0.6680
11:57:30.351   Training iter 150, batch loss 1.0801, batch acc 0.6758
11:57:30.868   Training iter 200, batch loss 1.1140, batch acc 0.6640
11:57:31.379   Training iter 250, batch loss 1.0591, batch acc 0.6818
11:57:31.937   Training iter 300, batch loss 1.0765, batch acc 0.6750
11:57:32.484   Training iter 350, batch loss 1.0806, batch acc 0.6812
11:57:33.020   Training iter 400, batch loss 1.0428, batch acc 0.6902
11:57:33.583   Training iter 450, batch loss 1.0796, batch acc 0.6716
11:57:34.165   Training iter 500, batch loss 1.0517, batch acc 0.6762
11:57:34.744   Training iter 550, batch loss 1.0691, batch acc 0.6794
11:57:35.319   Training iter 600, batch loss 1.0908, batch acc 0.6760
11:57:35.321 Training @ 419 epoch...
11:57:35.903   Training iter 50, batch loss 1.0880, batch acc 0.6686
11:57:36.473   Training iter 100, batch loss 1.0737, batch acc 0.6822
11:57:37.047   Training iter 150, batch loss 1.0890, batch acc 0.6716
11:57:37.616   Training iter 200, batch loss 1.0678, batch acc 0.6802
11:57:38.193   Training iter 250, batch loss 1.0475, batch acc 0.6898
11:57:38.766   Training iter 300, batch loss 1.0837, batch acc 0.6774
11:57:39.338   Training iter 350, batch loss 1.0950, batch acc 0.6714
11:57:39.870   Training iter 400, batch loss 1.0825, batch acc 0.6666
11:57:40.407   Training iter 450, batch loss 1.0905, batch acc 0.6680
11:57:40.941   Training iter 500, batch loss 1.0484, batch acc 0.6836
11:57:41.468   Training iter 550, batch loss 1.0836, batch acc 0.6686
11:57:41.994   Training iter 600, batch loss 1.0647, batch acc 0.6862
11:57:41.996 Training @ 420 epoch...
11:57:42.571   Training iter 50, batch loss 1.0903, batch acc 0.6722
11:57:43.127   Training iter 100, batch loss 1.0617, batch acc 0.6784
11:57:43.670   Training iter 150, batch loss 1.0529, batch acc 0.6886
11:57:44.213   Training iter 200, batch loss 1.1003, batch acc 0.6652
11:57:44.755   Training iter 250, batch loss 1.0666, batch acc 0.6858
11:57:45.310   Training iter 300, batch loss 1.0892, batch acc 0.6670
11:57:45.858   Training iter 350, batch loss 1.0747, batch acc 0.6770
11:57:46.393   Training iter 400, batch loss 1.0730, batch acc 0.6770
11:57:46.908   Training iter 450, batch loss 1.0870, batch acc 0.6710
11:57:47.430   Training iter 500, batch loss 1.0767, batch acc 0.6764
11:57:47.948   Training iter 550, batch loss 1.0389, batch acc 0.6916
11:57:48.452   Training iter 600, batch loss 1.1029, batch acc 0.6636
11:57:48.454 Testing @ 420 epoch...
11:57:48.496     Testing, total mean loss 1.04884, total acc 0.68400
11:57:48.497 Training @ 421 epoch...
11:57:49.007   Training iter 50, batch loss 1.0558, batch acc 0.6808
11:57:49.506   Training iter 100, batch loss 1.0745, batch acc 0.6758
11:57:50.041   Training iter 150, batch loss 1.0653, batch acc 0.6796
11:57:50.575   Training iter 200, batch loss 1.0796, batch acc 0.6718
11:57:51.104   Training iter 250, batch loss 1.0835, batch acc 0.6742
11:57:51.640   Training iter 300, batch loss 1.0799, batch acc 0.6702
11:57:52.176   Training iter 350, batch loss 1.0883, batch acc 0.6728
11:57:52.702   Training iter 400, batch loss 1.0856, batch acc 0.6732
11:57:53.246   Training iter 450, batch loss 1.0790, batch acc 0.6732
11:57:53.802   Training iter 500, batch loss 1.0823, batch acc 0.6746
11:57:54.353   Training iter 550, batch loss 1.0709, batch acc 0.6850
11:57:54.897   Training iter 600, batch loss 1.0695, batch acc 0.6812
11:57:54.899 Training @ 422 epoch...
11:57:55.468   Training iter 50, batch loss 1.0721, batch acc 0.6762
11:57:56.040   Training iter 100, batch loss 1.0763, batch acc 0.6794
11:57:56.613   Training iter 150, batch loss 1.0536, batch acc 0.6818
11:57:57.192   Training iter 200, batch loss 1.0804, batch acc 0.6774
11:57:57.711   Training iter 250, batch loss 1.0985, batch acc 0.6676
11:57:58.251   Training iter 300, batch loss 1.0745, batch acc 0.6808
11:57:58.780   Training iter 350, batch loss 1.0692, batch acc 0.6756
11:57:59.321   Training iter 400, batch loss 1.0908, batch acc 0.6676
11:57:59.845   Training iter 450, batch loss 1.0690, batch acc 0.6766
11:58:00.384   Training iter 500, batch loss 1.0731, batch acc 0.6782
11:58:00.916   Training iter 550, batch loss 1.0644, batch acc 0.6838
11:58:01.478   Training iter 600, batch loss 1.0921, batch acc 0.6702
11:58:01.480 Training @ 423 epoch...
11:58:02.035   Training iter 50, batch loss 1.0693, batch acc 0.6786
11:58:02.545   Training iter 100, batch loss 1.0276, batch acc 0.6958
11:58:03.078   Training iter 150, batch loss 1.1068, batch acc 0.6652
11:58:03.604   Training iter 200, batch loss 1.0783, batch acc 0.6784
11:58:04.156   Training iter 250, batch loss 1.0831, batch acc 0.6800
11:58:04.690   Training iter 300, batch loss 1.0973, batch acc 0.6694
11:58:05.211   Training iter 350, batch loss 1.0856, batch acc 0.6776
11:58:05.741   Training iter 400, batch loss 1.0839, batch acc 0.6642
11:58:06.259   Training iter 450, batch loss 1.0773, batch acc 0.6796
11:58:06.780   Training iter 500, batch loss 1.0552, batch acc 0.6800
11:58:07.297   Training iter 550, batch loss 1.0789, batch acc 0.6752
11:58:07.794   Training iter 600, batch loss 1.0707, batch acc 0.6750
11:58:07.796 Training @ 424 epoch...
11:58:08.295   Training iter 50, batch loss 1.0732, batch acc 0.6726
11:58:08.783   Training iter 100, batch loss 1.0714, batch acc 0.6766
11:58:09.277   Training iter 150, batch loss 1.0556, batch acc 0.6826
11:58:09.765   Training iter 200, batch loss 1.0931, batch acc 0.6758
11:58:10.280   Training iter 250, batch loss 1.0620, batch acc 0.6780
11:58:10.828   Training iter 300, batch loss 1.0746, batch acc 0.6752
11:58:11.366   Training iter 350, batch loss 1.0769, batch acc 0.6754
11:58:11.897   Training iter 400, batch loss 1.0780, batch acc 0.6748
11:58:12.442   Training iter 450, batch loss 1.0880, batch acc 0.6706
11:58:12.973   Training iter 500, batch loss 1.0800, batch acc 0.6786
11:58:13.502   Training iter 550, batch loss 1.0814, batch acc 0.6758
11:58:14.009   Training iter 600, batch loss 1.0799, batch acc 0.6786
11:58:14.011 Training @ 425 epoch...
11:58:14.514   Training iter 50, batch loss 1.0748, batch acc 0.6718
11:58:15.025   Training iter 100, batch loss 1.0592, batch acc 0.6840
11:58:15.549   Training iter 150, batch loss 1.0907, batch acc 0.6684
11:58:16.067   Training iter 200, batch loss 1.0602, batch acc 0.6802
11:58:16.562   Training iter 250, batch loss 1.0708, batch acc 0.6788
11:58:17.139   Training iter 300, batch loss 1.0669, batch acc 0.6822
11:58:17.641   Training iter 350, batch loss 1.0819, batch acc 0.6780
11:58:18.154   Training iter 400, batch loss 1.0937, batch acc 0.6684
11:58:18.657   Training iter 450, batch loss 1.0550, batch acc 0.6902
11:58:19.153   Training iter 500, batch loss 1.0880, batch acc 0.6708
11:58:19.656   Training iter 550, batch loss 1.0742, batch acc 0.6720
11:58:20.162   Training iter 600, batch loss 1.0984, batch acc 0.6712
11:58:20.164 Testing @ 425 epoch...
11:58:20.206     Testing, total mean loss 1.04875, total acc 0.68420
11:58:20.206 Training @ 426 epoch...
11:58:20.713   Training iter 50, batch loss 1.0816, batch acc 0.6766
11:58:21.193   Training iter 100, batch loss 1.0721, batch acc 0.6774
11:58:21.678   Training iter 150, batch loss 1.0747, batch acc 0.6738
11:58:22.176   Training iter 200, batch loss 1.0559, batch acc 0.6798
11:58:22.682   Training iter 250, batch loss 1.0798, batch acc 0.6642
11:58:23.207   Training iter 300, batch loss 1.0779, batch acc 0.6750
11:58:23.741   Training iter 350, batch loss 1.0896, batch acc 0.6770
11:58:24.295   Training iter 400, batch loss 1.0791, batch acc 0.6794
11:58:24.843   Training iter 450, batch loss 1.0640, batch acc 0.6858
11:58:25.388   Training iter 500, batch loss 1.0796, batch acc 0.6748
11:58:25.916   Training iter 550, batch loss 1.0903, batch acc 0.6750
11:58:26.441   Training iter 600, batch loss 1.0692, batch acc 0.6778
11:58:26.443 Training @ 427 epoch...
11:58:26.958   Training iter 50, batch loss 1.0799, batch acc 0.6780
11:58:27.481   Training iter 100, batch loss 1.1052, batch acc 0.6690
11:58:27.994   Training iter 150, batch loss 1.0661, batch acc 0.6832
11:58:28.487   Training iter 200, batch loss 1.0710, batch acc 0.6830
11:58:28.964   Training iter 250, batch loss 1.0706, batch acc 0.6748
11:58:29.445   Training iter 300, batch loss 1.0778, batch acc 0.6782
11:58:29.917   Training iter 350, batch loss 1.0730, batch acc 0.6762
11:58:30.409   Training iter 400, batch loss 1.0563, batch acc 0.6828
11:58:30.915   Training iter 450, batch loss 1.0897, batch acc 0.6636
11:58:31.441   Training iter 500, batch loss 1.0691, batch acc 0.6732
11:58:31.970   Training iter 550, batch loss 1.0657, batch acc 0.6774
11:58:32.489   Training iter 600, batch loss 1.0893, batch acc 0.6740
11:58:32.491 Training @ 428 epoch...
11:58:33.013   Training iter 50, batch loss 1.0879, batch acc 0.6758
11:58:33.530   Training iter 100, batch loss 1.0619, batch acc 0.6818
11:58:34.044   Training iter 150, batch loss 1.0777, batch acc 0.6800
11:58:34.570   Training iter 200, batch loss 1.0703, batch acc 0.6798
11:58:35.082   Training iter 250, batch loss 1.0806, batch acc 0.6664
11:58:35.601   Training iter 300, batch loss 1.0789, batch acc 0.6788
11:58:36.124   Training iter 350, batch loss 1.0930, batch acc 0.6696
11:58:36.637   Training iter 400, batch loss 1.0978, batch acc 0.6664
11:58:37.161   Training iter 450, batch loss 1.0791, batch acc 0.6706
11:58:37.700   Training iter 500, batch loss 1.0434, batch acc 0.6888
11:58:38.253   Training iter 550, batch loss 1.0695, batch acc 0.6760
11:58:38.804   Training iter 600, batch loss 1.0738, batch acc 0.6794
11:58:38.806 Training @ 429 epoch...
11:58:39.361   Training iter 50, batch loss 1.0577, batch acc 0.6850
11:58:39.906   Training iter 100, batch loss 1.0720, batch acc 0.6746
11:58:40.457   Training iter 150, batch loss 1.0805, batch acc 0.6766
11:58:40.998   Training iter 200, batch loss 1.0798, batch acc 0.6744
11:58:41.544   Training iter 250, batch loss 1.0998, batch acc 0.6658
11:58:42.086   Training iter 300, batch loss 1.0795, batch acc 0.6784
11:58:42.630   Training iter 350, batch loss 1.0983, batch acc 0.6710
11:58:43.170   Training iter 400, batch loss 1.0976, batch acc 0.6706
11:58:43.707   Training iter 450, batch loss 1.0618, batch acc 0.6792
11:58:44.255   Training iter 500, batch loss 1.0767, batch acc 0.6734
11:58:44.765   Training iter 550, batch loss 1.0484, batch acc 0.6842
11:58:45.285   Training iter 600, batch loss 1.0617, batch acc 0.6832
11:58:45.287 Training @ 430 epoch...
11:58:45.815   Training iter 50, batch loss 1.0859, batch acc 0.6710
11:58:46.343   Training iter 100, batch loss 1.0556, batch acc 0.6850
11:58:46.868   Training iter 150, batch loss 1.0771, batch acc 0.6798
11:58:47.390   Training iter 200, batch loss 1.0870, batch acc 0.6730
11:58:47.908   Training iter 250, batch loss 1.0866, batch acc 0.6762
11:58:48.437   Training iter 300, batch loss 1.0653, batch acc 0.6758
11:58:48.958   Training iter 350, batch loss 1.0550, batch acc 0.6814
11:58:49.463   Training iter 400, batch loss 1.0704, batch acc 0.6752
11:58:49.975   Training iter 450, batch loss 1.0848, batch acc 0.6702
11:58:50.483   Training iter 500, batch loss 1.0718, batch acc 0.6760
11:58:51.018   Training iter 550, batch loss 1.0833, batch acc 0.6828
11:58:51.536   Training iter 600, batch loss 1.0909, batch acc 0.6686
11:58:51.538 Testing @ 430 epoch...
11:58:51.580     Testing, total mean loss 1.04859, total acc 0.68430
11:58:51.580 Training @ 431 epoch...
11:58:52.113   Training iter 50, batch loss 1.0995, batch acc 0.6664
11:58:52.640   Training iter 100, batch loss 1.0735, batch acc 0.6768
11:58:53.175   Training iter 150, batch loss 1.0492, batch acc 0.6848
11:58:53.750   Training iter 200, batch loss 1.0624, batch acc 0.6830
11:58:54.330   Training iter 250, batch loss 1.0956, batch acc 0.6680
11:58:54.910   Training iter 300, batch loss 1.0847, batch acc 0.6722
11:58:55.433   Training iter 350, batch loss 1.0433, batch acc 0.6890
11:58:55.937   Training iter 400, batch loss 1.0878, batch acc 0.6718
11:58:56.452   Training iter 450, batch loss 1.0828, batch acc 0.6758
11:58:56.954   Training iter 500, batch loss 1.0677, batch acc 0.6798
11:58:57.464   Training iter 550, batch loss 1.0946, batch acc 0.6666
11:58:57.987   Training iter 600, batch loss 1.0725, batch acc 0.6798
11:58:57.989 Training @ 432 epoch...
11:58:58.504   Training iter 50, batch loss 1.0788, batch acc 0.6848
11:58:59.016   Training iter 100, batch loss 1.0596, batch acc 0.6798
11:58:59.506   Training iter 150, batch loss 1.0812, batch acc 0.6710
11:59:00.018   Training iter 200, batch loss 1.0853, batch acc 0.6742
11:59:00.539   Training iter 250, batch loss 1.0762, batch acc 0.6742
11:59:01.052   Training iter 300, batch loss 1.0613, batch acc 0.6808
11:59:01.586   Training iter 350, batch loss 1.0857, batch acc 0.6718
11:59:02.153   Training iter 400, batch loss 1.0714, batch acc 0.6798
11:59:02.703   Training iter 450, batch loss 1.0661, batch acc 0.6776
11:59:03.253   Training iter 500, batch loss 1.0817, batch acc 0.6742
11:59:03.808   Training iter 550, batch loss 1.0857, batch acc 0.6752
11:59:04.371   Training iter 600, batch loss 1.0805, batch acc 0.6746
11:59:04.373 Training @ 433 epoch...
11:59:04.948   Training iter 50, batch loss 1.0860, batch acc 0.6744
11:59:05.520   Training iter 100, batch loss 1.0814, batch acc 0.6774
11:59:06.087   Training iter 150, batch loss 1.0675, batch acc 0.6772
11:59:06.652   Training iter 200, batch loss 1.0658, batch acc 0.6756
11:59:07.232   Training iter 250, batch loss 1.0986, batch acc 0.6686
11:59:07.784   Training iter 300, batch loss 1.0683, batch acc 0.6774
11:59:08.338   Training iter 350, batch loss 1.0750, batch acc 0.6760
11:59:08.864   Training iter 400, batch loss 1.0732, batch acc 0.6770
11:59:09.383   Training iter 450, batch loss 1.0857, batch acc 0.6724
11:59:09.900   Training iter 500, batch loss 1.0690, batch acc 0.6838
11:59:10.429   Training iter 550, batch loss 1.0860, batch acc 0.6780
11:59:10.965   Training iter 600, batch loss 1.0570, batch acc 0.6760
11:59:10.967 Training @ 434 epoch...
11:59:11.514   Training iter 50, batch loss 1.0645, batch acc 0.6782
11:59:12.053   Training iter 100, batch loss 1.0975, batch acc 0.6664
11:59:12.597   Training iter 150, batch loss 1.0977, batch acc 0.6644
11:59:13.137   Training iter 200, batch loss 1.0825, batch acc 0.6790
11:59:13.673   Training iter 250, batch loss 1.0819, batch acc 0.6732
11:59:14.220   Training iter 300, batch loss 1.0893, batch acc 0.6690
11:59:14.759   Training iter 350, batch loss 1.0752, batch acc 0.6782
11:59:15.304   Training iter 400, batch loss 1.0545, batch acc 0.6862
11:59:15.844   Training iter 450, batch loss 1.0837, batch acc 0.6738
11:59:16.370   Training iter 500, batch loss 1.0559, batch acc 0.6868
11:59:16.880   Training iter 550, batch loss 1.0626, batch acc 0.6770
11:59:17.359   Training iter 600, batch loss 1.0680, batch acc 0.6862
11:59:17.361 Training @ 435 epoch...
11:59:17.857   Training iter 50, batch loss 1.0973, batch acc 0.6666
11:59:18.363   Training iter 100, batch loss 1.1019, batch acc 0.6660
11:59:18.872   Training iter 150, batch loss 1.0829, batch acc 0.6732
11:59:19.377   Training iter 200, batch loss 1.0848, batch acc 0.6766
11:59:19.876   Training iter 250, batch loss 1.0793, batch acc 0.6780
11:59:20.403   Training iter 300, batch loss 1.0805, batch acc 0.6756
11:59:20.941   Training iter 350, batch loss 1.0652, batch acc 0.6906
11:59:21.446   Training iter 400, batch loss 1.0554, batch acc 0.6812
11:59:21.925   Training iter 450, batch loss 1.0774, batch acc 0.6732
11:59:22.414   Training iter 500, batch loss 1.0666, batch acc 0.6788
11:59:22.899   Training iter 550, batch loss 1.0469, batch acc 0.6850
11:59:23.412   Training iter 600, batch loss 1.0752, batch acc 0.6706
11:59:23.414 Testing @ 435 epoch...
11:59:23.456     Testing, total mean loss 1.04859, total acc 0.68400
11:59:23.456 Training @ 436 epoch...
11:59:23.996   Training iter 50, batch loss 1.0828, batch acc 0.6792
11:59:24.533   Training iter 100, batch loss 1.0823, batch acc 0.6720
11:59:25.077   Training iter 150, batch loss 1.0862, batch acc 0.6708
11:59:25.604   Training iter 200, batch loss 1.0725, batch acc 0.6782
11:59:26.125   Training iter 250, batch loss 1.0794, batch acc 0.6694
11:59:26.642   Training iter 300, batch loss 1.0687, batch acc 0.6748
11:59:27.158   Training iter 350, batch loss 1.0757, batch acc 0.6784
11:59:27.666   Training iter 400, batch loss 1.0586, batch acc 0.6862
11:59:28.207   Training iter 450, batch loss 1.0640, batch acc 0.6768
11:59:28.754   Training iter 500, batch loss 1.0707, batch acc 0.6758
11:59:29.286   Training iter 550, batch loss 1.0949, batch acc 0.6688
11:59:29.784   Training iter 600, batch loss 1.0774, batch acc 0.6796
11:59:29.786 Training @ 437 epoch...
11:59:30.298   Training iter 50, batch loss 1.0642, batch acc 0.6802
11:59:30.827   Training iter 100, batch loss 1.0746, batch acc 0.6714
11:59:31.354   Training iter 150, batch loss 1.0714, batch acc 0.6776
11:59:31.891   Training iter 200, batch loss 1.0623, batch acc 0.6804
11:59:32.425   Training iter 250, batch loss 1.0760, batch acc 0.6762
11:59:32.931   Training iter 300, batch loss 1.0615, batch acc 0.6834
11:59:33.451   Training iter 350, batch loss 1.0985, batch acc 0.6750
11:59:33.970   Training iter 400, batch loss 1.0869, batch acc 0.6742
11:59:34.492   Training iter 450, batch loss 1.0610, batch acc 0.6812
11:59:35.026   Training iter 500, batch loss 1.1033, batch acc 0.6646
11:59:35.577   Training iter 550, batch loss 1.0608, batch acc 0.6792
11:59:36.093   Training iter 600, batch loss 1.0926, batch acc 0.6712
11:59:36.095 Training @ 438 epoch...
11:59:36.583   Training iter 50, batch loss 1.0645, batch acc 0.6842
11:59:37.079   Training iter 100, batch loss 1.0930, batch acc 0.6728
11:59:37.583   Training iter 150, batch loss 1.0664, batch acc 0.6782
11:59:38.098   Training iter 200, batch loss 1.0724, batch acc 0.6772
11:59:38.601   Training iter 250, batch loss 1.0987, batch acc 0.6680
11:59:39.115   Training iter 300, batch loss 1.0683, batch acc 0.6788
11:59:39.607   Training iter 350, batch loss 1.0541, batch acc 0.6844
11:59:40.102   Training iter 400, batch loss 1.0668, batch acc 0.6770
11:59:40.596   Training iter 450, batch loss 1.0993, batch acc 0.6686
11:59:41.077   Training iter 500, batch loss 1.0771, batch acc 0.6748
11:59:41.560   Training iter 550, batch loss 1.0571, batch acc 0.6842
11:59:42.032   Training iter 600, batch loss 1.0954, batch acc 0.6714
11:59:42.034 Training @ 439 epoch...
11:59:42.532   Training iter 50, batch loss 1.0880, batch acc 0.6754
11:59:43.029   Training iter 100, batch loss 1.0834, batch acc 0.6718
11:59:43.526   Training iter 150, batch loss 1.0647, batch acc 0.6830
11:59:44.031   Training iter 200, batch loss 1.0580, batch acc 0.6848
11:59:44.533   Training iter 250, batch loss 1.0901, batch acc 0.6682
11:59:45.044   Training iter 300, batch loss 1.0974, batch acc 0.6650
11:59:45.535   Training iter 350, batch loss 1.0428, batch acc 0.6910
11:59:46.082   Training iter 400, batch loss 1.0732, batch acc 0.6736
11:59:46.655   Training iter 450, batch loss 1.0696, batch acc 0.6788
11:59:47.236   Training iter 500, batch loss 1.0910, batch acc 0.6716
11:59:47.767   Training iter 550, batch loss 1.0764, batch acc 0.6786
11:59:48.282   Training iter 600, batch loss 1.0784, batch acc 0.6758
11:59:48.284 Training @ 440 epoch...
11:59:48.791   Training iter 50, batch loss 1.0708, batch acc 0.6790
11:59:49.296   Training iter 100, batch loss 1.0718, batch acc 0.6806
11:59:49.798   Training iter 150, batch loss 1.0815, batch acc 0.6750
11:59:50.322   Training iter 200, batch loss 1.0776, batch acc 0.6766
11:59:50.832   Training iter 250, batch loss 1.0611, batch acc 0.6768
11:59:51.318   Training iter 300, batch loss 1.0761, batch acc 0.6724
11:59:51.822   Training iter 350, batch loss 1.0893, batch acc 0.6710
11:59:52.356   Training iter 400, batch loss 1.0868, batch acc 0.6698
11:59:52.887   Training iter 450, batch loss 1.0836, batch acc 0.6746
11:59:53.428   Training iter 500, batch loss 1.0702, batch acc 0.6746
11:59:53.963   Training iter 550, batch loss 1.0575, batch acc 0.6862
11:59:54.505   Training iter 600, batch loss 1.0869, batch acc 0.6754
11:59:54.507 Testing @ 440 epoch...
11:59:54.548     Testing, total mean loss 1.04867, total acc 0.68400
11:59:54.548 Training @ 441 epoch...
11:59:55.086   Training iter 50, batch loss 1.0705, batch acc 0.6788
11:59:55.601   Training iter 100, batch loss 1.0956, batch acc 0.6670
11:59:56.089   Training iter 150, batch loss 1.0467, batch acc 0.6860
11:59:56.584   Training iter 200, batch loss 1.0947, batch acc 0.6708
11:59:57.071   Training iter 250, batch loss 1.0991, batch acc 0.6696
11:59:57.577   Training iter 300, batch loss 1.0720, batch acc 0.6780
11:59:58.061   Training iter 350, batch loss 1.0823, batch acc 0.6784
11:59:58.539   Training iter 400, batch loss 1.0935, batch acc 0.6650
11:59:59.009   Training iter 450, batch loss 1.0721, batch acc 0.6772
11:59:59.471   Training iter 500, batch loss 1.0571, batch acc 0.6806
11:59:59.938   Training iter 550, batch loss 1.0577, batch acc 0.6880
12:00:00.467   Training iter 600, batch loss 1.0715, batch acc 0.6774
12:00:00.469 Training @ 442 epoch...
12:00:00.991   Training iter 50, batch loss 1.0506, batch acc 0.6854
12:00:01.527   Training iter 100, batch loss 1.0699, batch acc 0.6772
12:00:02.077   Training iter 150, batch loss 1.0716, batch acc 0.6756
12:00:02.648   Training iter 200, batch loss 1.0824, batch acc 0.6712
12:00:03.190   Training iter 250, batch loss 1.0597, batch acc 0.6806
12:00:03.736   Training iter 300, batch loss 1.0993, batch acc 0.6654
12:00:04.253   Training iter 350, batch loss 1.0938, batch acc 0.6710
12:00:04.773   Training iter 400, batch loss 1.0871, batch acc 0.6770
12:00:05.326   Training iter 450, batch loss 1.0624, batch acc 0.6804
12:00:05.918   Training iter 500, batch loss 1.0909, batch acc 0.6754
12:00:06.507   Training iter 550, batch loss 1.0727, batch acc 0.6718
12:00:07.089   Training iter 600, batch loss 1.0720, batch acc 0.6862
12:00:07.091 Training @ 443 epoch...
12:00:07.703   Training iter 50, batch loss 1.1017, batch acc 0.6652
12:00:08.302   Training iter 100, batch loss 1.0846, batch acc 0.6742
12:00:08.891   Training iter 150, batch loss 1.0789, batch acc 0.6742
12:00:09.465   Training iter 200, batch loss 1.1039, batch acc 0.6664
12:00:10.034   Training iter 250, batch loss 1.0770, batch acc 0.6754
12:00:10.616   Training iter 300, batch loss 1.0653, batch acc 0.6778
12:00:11.257   Training iter 350, batch loss 1.0624, batch acc 0.6792
12:00:11.791   Training iter 400, batch loss 1.0460, batch acc 0.6870
12:00:12.336   Training iter 450, batch loss 1.0386, batch acc 0.6916
12:00:12.955   Training iter 500, batch loss 1.0750, batch acc 0.6780
12:00:13.544   Training iter 550, batch loss 1.1024, batch acc 0.6646
12:00:14.109   Training iter 600, batch loss 1.0773, batch acc 0.6794
12:00:14.111 Training @ 444 epoch...
12:00:14.712   Training iter 50, batch loss 1.0646, batch acc 0.6756
12:00:15.262   Training iter 100, batch loss 1.0568, batch acc 0.6856
12:00:15.881   Training iter 150, batch loss 1.0910, batch acc 0.6752
12:00:16.511   Training iter 200, batch loss 1.0770, batch acc 0.6720
12:00:17.033   Training iter 250, batch loss 1.0942, batch acc 0.6746
12:00:17.634   Training iter 300, batch loss 1.0885, batch acc 0.6658
12:00:18.234   Training iter 350, batch loss 1.0645, batch acc 0.6776
12:00:18.802   Training iter 400, batch loss 1.0788, batch acc 0.6774
12:00:19.421   Training iter 450, batch loss 1.1124, batch acc 0.6616
12:00:19.993   Training iter 500, batch loss 1.0360, batch acc 0.6940
12:00:20.589   Training iter 550, batch loss 1.1123, batch acc 0.6658
12:00:21.223   Training iter 600, batch loss 1.0365, batch acc 0.6894
12:00:21.224 Training @ 445 epoch...
12:00:21.768   Training iter 50, batch loss 1.0803, batch acc 0.6754
12:00:22.356   Training iter 100, batch loss 1.0876, batch acc 0.6732
12:00:22.884   Training iter 150, batch loss 1.0796, batch acc 0.6806
12:00:23.422   Training iter 200, batch loss 1.0834, batch acc 0.6672
12:00:23.948   Training iter 250, batch loss 1.0704, batch acc 0.6796
12:00:24.468   Training iter 300, batch loss 1.0556, batch acc 0.6838
12:00:24.989   Training iter 350, batch loss 1.0791, batch acc 0.6742
12:00:25.513   Training iter 400, batch loss 1.0712, batch acc 0.6792
12:00:26.027   Training iter 450, batch loss 1.0883, batch acc 0.6672
12:00:26.542   Training iter 500, batch loss 1.0893, batch acc 0.6752
12:00:27.075   Training iter 550, batch loss 1.0600, batch acc 0.6814
12:00:27.601   Training iter 600, batch loss 1.0678, batch acc 0.6782
12:00:27.603 Testing @ 445 epoch...
12:00:27.645     Testing, total mean loss 1.04857, total acc 0.68410
12:00:27.645 Training @ 446 epoch...
12:00:28.172   Training iter 50, batch loss 1.0881, batch acc 0.6750
12:00:28.700   Training iter 100, batch loss 1.0983, batch acc 0.6648
12:00:29.239   Training iter 150, batch loss 1.0583, batch acc 0.6820
12:00:29.737   Training iter 200, batch loss 1.0544, batch acc 0.6848
12:00:30.271   Training iter 250, batch loss 1.0785, batch acc 0.6696
12:00:30.807   Training iter 300, batch loss 1.0818, batch acc 0.6742
12:00:31.372   Training iter 350, batch loss 1.0632, batch acc 0.6804
12:00:31.928   Training iter 400, batch loss 1.0783, batch acc 0.6744
12:00:32.477   Training iter 450, batch loss 1.0606, batch acc 0.6792
12:00:33.025   Training iter 500, batch loss 1.0979, batch acc 0.6718
12:00:33.566   Training iter 550, batch loss 1.0771, batch acc 0.6746
12:00:34.119   Training iter 600, batch loss 1.0760, batch acc 0.6828
12:00:34.121 Training @ 447 epoch...
12:00:34.676   Training iter 50, batch loss 1.1000, batch acc 0.6716
12:00:35.221   Training iter 100, batch loss 1.0963, batch acc 0.6678
12:00:35.773   Training iter 150, batch loss 1.0780, batch acc 0.6734
12:00:36.327   Training iter 200, batch loss 1.0714, batch acc 0.6772
12:00:36.868   Training iter 250, batch loss 1.0638, batch acc 0.6830
12:00:37.403   Training iter 300, batch loss 1.0761, batch acc 0.6784
12:00:37.968   Training iter 350, batch loss 1.0793, batch acc 0.6720
12:00:38.512   Training iter 400, batch loss 1.0668, batch acc 0.6840
12:00:39.036   Training iter 450, batch loss 1.0642, batch acc 0.6774
12:00:39.559   Training iter 500, batch loss 1.0729, batch acc 0.6740
12:00:40.106   Training iter 550, batch loss 1.0677, batch acc 0.6842
12:00:40.644   Training iter 600, batch loss 1.0760, batch acc 0.6744
12:00:40.646 Training @ 448 epoch...
12:00:41.186   Training iter 50, batch loss 1.0620, batch acc 0.6826
12:00:41.670   Training iter 100, batch loss 1.0606, batch acc 0.6790
12:00:42.162   Training iter 150, batch loss 1.0573, batch acc 0.6728
12:00:42.660   Training iter 200, batch loss 1.0709, batch acc 0.6784
12:00:43.176   Training iter 250, batch loss 1.0936, batch acc 0.6750
12:00:43.666   Training iter 300, batch loss 1.0927, batch acc 0.6738
12:00:44.195   Training iter 350, batch loss 1.0926, batch acc 0.6710
12:00:44.759   Training iter 400, batch loss 1.0674, batch acc 0.6796
12:00:45.327   Training iter 450, batch loss 1.0878, batch acc 0.6754
12:00:45.872   Training iter 500, batch loss 1.0689, batch acc 0.6806
12:00:46.362   Training iter 550, batch loss 1.0822, batch acc 0.6734
12:00:46.870   Training iter 600, batch loss 1.0765, batch acc 0.6756
12:00:46.872 Training @ 449 epoch...
12:00:47.400   Training iter 50, batch loss 1.0769, batch acc 0.6794
12:00:47.925   Training iter 100, batch loss 1.1107, batch acc 0.6640
12:00:48.459   Training iter 150, batch loss 1.0389, batch acc 0.6860
12:00:48.975   Training iter 200, batch loss 1.1114, batch acc 0.6630
12:00:49.481   Training iter 250, batch loss 1.0844, batch acc 0.6720
12:00:49.988   Training iter 300, batch loss 1.0524, batch acc 0.6826
12:00:50.496   Training iter 350, batch loss 1.0969, batch acc 0.6692
12:00:51.017   Training iter 400, batch loss 1.0617, batch acc 0.6836
12:00:51.534   Training iter 450, batch loss 1.0691, batch acc 0.6806
12:00:52.062   Training iter 500, batch loss 1.0503, batch acc 0.6796
12:00:52.593   Training iter 550, batch loss 1.0815, batch acc 0.6770
12:00:53.121   Training iter 600, batch loss 1.0780, batch acc 0.6760
12:00:53.122 Training @ 450 epoch...
12:00:53.635   Training iter 50, batch loss 1.0635, batch acc 0.6830
12:00:54.150   Training iter 100, batch loss 1.0622, batch acc 0.6830
12:00:54.670   Training iter 150, batch loss 1.0971, batch acc 0.6702
12:00:55.194   Training iter 200, batch loss 1.0876, batch acc 0.6744
12:00:55.730   Training iter 250, batch loss 1.0818, batch acc 0.6754
12:00:56.269   Training iter 300, batch loss 1.0730, batch acc 0.6744
12:00:56.804   Training iter 350, batch loss 1.1068, batch acc 0.6632
12:00:57.308   Training iter 400, batch loss 1.0552, batch acc 0.6802
12:00:57.815   Training iter 450, batch loss 1.0627, batch acc 0.6844
12:00:58.322   Training iter 500, batch loss 1.0759, batch acc 0.6734
12:00:58.804   Training iter 550, batch loss 1.0657, batch acc 0.6778
12:00:59.283   Training iter 600, batch loss 1.0812, batch acc 0.6752
12:00:59.284 Testing @ 450 epoch...
12:00:59.326     Testing, total mean loss 1.04862, total acc 0.68400
12:00:59.326 Training @ 451 epoch...
12:00:59.812   Training iter 50, batch loss 1.0933, batch acc 0.6684
12:01:00.304   Training iter 100, batch loss 1.0779, batch acc 0.6778
12:01:00.796   Training iter 150, batch loss 1.0799, batch acc 0.6714
12:01:01.300   Training iter 200, batch loss 1.0633, batch acc 0.6780
12:01:01.856   Training iter 250, batch loss 1.0792, batch acc 0.6772
12:01:02.414   Training iter 300, batch loss 1.0669, batch acc 0.6810
12:01:02.934   Training iter 350, batch loss 1.0759, batch acc 0.6736
12:01:03.464   Training iter 400, batch loss 1.0780, batch acc 0.6782
12:01:04.000   Training iter 450, batch loss 1.0638, batch acc 0.6794
12:01:04.552   Training iter 500, batch loss 1.0707, batch acc 0.6810
12:01:05.087   Training iter 550, batch loss 1.0780, batch acc 0.6736
12:01:05.581   Training iter 600, batch loss 1.0854, batch acc 0.6744
12:01:05.583 Training @ 452 epoch...
12:01:06.111   Training iter 50, batch loss 1.1034, batch acc 0.6668
12:01:06.625   Training iter 100, batch loss 1.0918, batch acc 0.6706
12:01:07.147   Training iter 150, batch loss 1.0824, batch acc 0.6772
12:01:07.634   Training iter 200, batch loss 1.0716, batch acc 0.6720
12:01:08.101   Training iter 250, batch loss 1.0585, batch acc 0.6834
12:01:08.620   Training iter 300, batch loss 1.0562, batch acc 0.6918
12:01:09.145   Training iter 350, batch loss 1.0891, batch acc 0.6658
12:01:09.686   Training iter 400, batch loss 1.0756, batch acc 0.6782
12:01:10.211   Training iter 450, batch loss 1.0636, batch acc 0.6796
12:01:10.750   Training iter 500, batch loss 1.0973, batch acc 0.6666
12:01:11.287   Training iter 550, batch loss 1.0584, batch acc 0.6878
12:01:11.807   Training iter 600, batch loss 1.0644, batch acc 0.6782
12:01:11.809 Training @ 453 epoch...
12:01:12.351   Training iter 50, batch loss 1.0713, batch acc 0.6830
12:01:12.878   Training iter 100, batch loss 1.0810, batch acc 0.6762
12:01:13.400   Training iter 150, batch loss 1.0836, batch acc 0.6670
12:01:13.924   Training iter 200, batch loss 1.0692, batch acc 0.6826
12:01:14.455   Training iter 250, batch loss 1.0904, batch acc 0.6696
12:01:14.977   Training iter 300, batch loss 1.0682, batch acc 0.6764
12:01:15.504   Training iter 350, batch loss 1.0463, batch acc 0.6866
12:01:16.022   Training iter 400, batch loss 1.0892, batch acc 0.6686
12:01:16.546   Training iter 450, batch loss 1.0852, batch acc 0.6724
12:01:17.060   Training iter 500, batch loss 1.0409, batch acc 0.6944
12:01:17.576   Training iter 550, batch loss 1.0958, batch acc 0.6680
12:01:18.091   Training iter 600, batch loss 1.0910, batch acc 0.6708
12:01:18.093 Training @ 454 epoch...
12:01:18.624   Training iter 50, batch loss 1.0958, batch acc 0.6658
12:01:19.162   Training iter 100, batch loss 1.1153, batch acc 0.6624
12:01:19.720   Training iter 150, batch loss 1.0659, batch acc 0.6786
12:01:20.255   Training iter 200, batch loss 1.0627, batch acc 0.6786
12:01:20.778   Training iter 250, batch loss 1.0789, batch acc 0.6782
12:01:21.306   Training iter 300, batch loss 1.0725, batch acc 0.6784
12:01:21.866   Training iter 350, batch loss 1.0948, batch acc 0.6720
12:01:22.423   Training iter 400, batch loss 1.0453, batch acc 0.6902
12:01:22.972   Training iter 450, batch loss 1.0768, batch acc 0.6780
12:01:23.513   Training iter 500, batch loss 1.0793, batch acc 0.6740
12:01:24.043   Training iter 550, batch loss 1.0701, batch acc 0.6746
12:01:24.585   Training iter 600, batch loss 1.0545, batch acc 0.6842
12:01:24.586 Training @ 455 epoch...
12:01:25.124   Training iter 50, batch loss 1.0806, batch acc 0.6752
12:01:25.620   Training iter 100, batch loss 1.0647, batch acc 0.6836
12:01:26.109   Training iter 150, batch loss 1.0775, batch acc 0.6740
12:01:26.586   Training iter 200, batch loss 1.0345, batch acc 0.6920
12:01:27.079   Training iter 250, batch loss 1.1031, batch acc 0.6672
12:01:27.568   Training iter 300, batch loss 1.0636, batch acc 0.6838
12:01:28.058   Training iter 350, batch loss 1.0878, batch acc 0.6712
12:01:28.573   Training iter 400, batch loss 1.0770, batch acc 0.6726
12:01:29.088   Training iter 450, batch loss 1.0640, batch acc 0.6792
12:01:29.606   Training iter 500, batch loss 1.0729, batch acc 0.6760
12:01:30.137   Training iter 550, batch loss 1.1006, batch acc 0.6676
12:01:30.662   Training iter 600, batch loss 1.0854, batch acc 0.6696
12:01:30.664 Testing @ 455 epoch...
12:01:30.706     Testing, total mean loss 1.04855, total acc 0.68420
12:01:30.706 Training @ 456 epoch...
12:01:31.216   Training iter 50, batch loss 1.0716, batch acc 0.6808
12:01:31.721   Training iter 100, batch loss 1.0670, batch acc 0.6822
12:01:32.243   Training iter 150, batch loss 1.0617, batch acc 0.6792
12:01:32.760   Training iter 200, batch loss 1.0935, batch acc 0.6644
12:01:33.280   Training iter 250, batch loss 1.0707, batch acc 0.6814
12:01:33.766   Training iter 300, batch loss 1.0904, batch acc 0.6680
12:01:34.258   Training iter 350, batch loss 1.0924, batch acc 0.6714
12:01:34.747   Training iter 400, batch loss 1.0834, batch acc 0.6736
12:01:35.226   Training iter 450, batch loss 1.0852, batch acc 0.6684
12:01:35.705   Training iter 500, batch loss 1.0746, batch acc 0.6836
12:01:36.183   Training iter 550, batch loss 1.0659, batch acc 0.6796
12:01:36.657   Training iter 600, batch loss 1.0556, batch acc 0.6868
12:01:36.658 Training @ 457 epoch...
12:01:37.138   Training iter 50, batch loss 1.0765, batch acc 0.6746
12:01:37.625   Training iter 100, batch loss 1.0663, batch acc 0.6848
12:01:38.103   Training iter 150, batch loss 1.0815, batch acc 0.6738
12:01:38.591   Training iter 200, batch loss 1.0972, batch acc 0.6650
12:01:39.068   Training iter 250, batch loss 1.0517, batch acc 0.6870
12:01:39.543   Training iter 300, batch loss 1.0617, batch acc 0.6818
12:01:40.001   Training iter 350, batch loss 1.1008, batch acc 0.6656
12:01:40.487   Training iter 400, batch loss 1.0599, batch acc 0.6814
12:01:40.967   Training iter 450, batch loss 1.0720, batch acc 0.6734
12:01:41.454   Training iter 500, batch loss 1.0982, batch acc 0.6680
12:01:41.960   Training iter 550, batch loss 1.0870, batch acc 0.6760
12:01:42.537   Training iter 600, batch loss 1.0589, batch acc 0.6806
12:01:42.538 Training @ 458 epoch...
12:01:43.119   Training iter 50, batch loss 1.0924, batch acc 0.6696
12:01:43.700   Training iter 100, batch loss 1.0708, batch acc 0.6764
12:01:44.229   Training iter 150, batch loss 1.0570, batch acc 0.6842
12:01:44.758   Training iter 200, batch loss 1.0747, batch acc 0.6814
12:01:45.290   Training iter 250, batch loss 1.0741, batch acc 0.6794
12:01:45.828   Training iter 300, batch loss 1.0838, batch acc 0.6740
12:01:46.344   Training iter 350, batch loss 1.0679, batch acc 0.6846
12:01:46.853   Training iter 400, batch loss 1.0957, batch acc 0.6688
12:01:47.361   Training iter 450, batch loss 1.0691, batch acc 0.6850
12:01:47.878   Training iter 500, batch loss 1.0521, batch acc 0.6760
12:01:48.406   Training iter 550, batch loss 1.0619, batch acc 0.6768
12:01:48.924   Training iter 600, batch loss 1.1123, batch acc 0.6614
12:01:48.925 Training @ 459 epoch...
12:01:49.447   Training iter 50, batch loss 1.0626, batch acc 0.6738
12:01:49.976   Training iter 100, batch loss 1.0770, batch acc 0.6760
12:01:50.505   Training iter 150, batch loss 1.0863, batch acc 0.6696
12:01:51.031   Training iter 200, batch loss 1.0903, batch acc 0.6752
12:01:51.567   Training iter 250, batch loss 1.0596, batch acc 0.6832
12:01:52.123   Training iter 300, batch loss 1.0877, batch acc 0.6724
12:01:52.661   Training iter 350, batch loss 1.0743, batch acc 0.6746
12:01:53.178   Training iter 400, batch loss 1.0486, batch acc 0.6872
12:01:53.692   Training iter 450, batch loss 1.0826, batch acc 0.6742
12:01:54.218   Training iter 500, batch loss 1.0868, batch acc 0.6712
12:01:54.728   Training iter 550, batch loss 1.0742, batch acc 0.6786
12:01:55.249   Training iter 600, batch loss 1.0820, batch acc 0.6774
12:01:55.251 Training @ 460 epoch...
12:01:55.772   Training iter 50, batch loss 1.0563, batch acc 0.6828
12:01:56.290   Training iter 100, batch loss 1.0903, batch acc 0.6722
12:01:56.799   Training iter 150, batch loss 1.0470, batch acc 0.6876
12:01:57.306   Training iter 200, batch loss 1.0847, batch acc 0.6736
12:01:57.810   Training iter 250, batch loss 1.0674, batch acc 0.6812
12:01:58.321   Training iter 300, batch loss 1.0761, batch acc 0.6732
12:01:58.814   Training iter 350, batch loss 1.0489, batch acc 0.6862
12:01:59.303   Training iter 400, batch loss 1.0753, batch acc 0.6764
12:01:59.797   Training iter 450, batch loss 1.1053, batch acc 0.6630
12:02:00.320   Training iter 500, batch loss 1.0883, batch acc 0.6726
12:02:00.835   Training iter 550, batch loss 1.0848, batch acc 0.6810
12:02:01.368   Training iter 600, batch loss 1.0872, batch acc 0.6682
12:02:01.370 Testing @ 460 epoch...
12:02:01.412     Testing, total mean loss 1.04853, total acc 0.68430
12:02:01.412 Training @ 461 epoch...
12:02:01.977   Training iter 50, batch loss 1.0891, batch acc 0.6694
12:02:02.483   Training iter 100, batch loss 1.0575, batch acc 0.6842
12:02:03.015   Training iter 150, batch loss 1.0791, batch acc 0.6826
12:02:03.561   Training iter 200, batch loss 1.0828, batch acc 0.6776
12:02:04.106   Training iter 250, batch loss 1.0782, batch acc 0.6666
12:02:04.648   Training iter 300, batch loss 1.0899, batch acc 0.6688
12:02:05.160   Training iter 350, batch loss 1.0305, batch acc 0.6960
12:02:05.705   Training iter 400, batch loss 1.0723, batch acc 0.6760
12:02:06.237   Training iter 450, batch loss 1.0597, batch acc 0.6828
12:02:06.774   Training iter 500, batch loss 1.0996, batch acc 0.6672
12:02:07.259   Training iter 550, batch loss 1.0996, batch acc 0.6692
12:02:07.803   Training iter 600, batch loss 1.0735, batch acc 0.6752
12:02:07.805 Training @ 462 epoch...
12:02:08.359   Training iter 50, batch loss 1.0642, batch acc 0.6848
12:02:08.898   Training iter 100, batch loss 1.0775, batch acc 0.6726
12:02:09.440   Training iter 150, batch loss 1.0921, batch acc 0.6746
12:02:09.977   Training iter 200, batch loss 1.0884, batch acc 0.6722
12:02:10.533   Training iter 250, batch loss 1.0708, batch acc 0.6750
12:02:11.080   Training iter 300, batch loss 1.0765, batch acc 0.6818
12:02:11.625   Training iter 350, batch loss 1.1037, batch acc 0.6614
12:02:12.169   Training iter 400, batch loss 1.0982, batch acc 0.6670
12:02:12.715   Training iter 450, batch loss 1.0636, batch acc 0.6786
12:02:13.256   Training iter 500, batch loss 1.0373, batch acc 0.6896
12:02:13.767   Training iter 550, batch loss 1.0598, batch acc 0.6836
12:02:14.280   Training iter 600, batch loss 1.0796, batch acc 0.6738
12:02:14.282 Training @ 463 epoch...
12:02:14.788   Training iter 50, batch loss 1.0854, batch acc 0.6674
12:02:15.308   Training iter 100, batch loss 1.0784, batch acc 0.6788
12:02:15.853   Training iter 150, batch loss 1.0968, batch acc 0.6680
12:02:16.426   Training iter 200, batch loss 1.0556, batch acc 0.6876
12:02:17.002   Training iter 250, batch loss 1.0851, batch acc 0.6726
12:02:17.591   Training iter 300, batch loss 1.0281, batch acc 0.6988
12:02:18.107   Training iter 350, batch loss 1.0840, batch acc 0.6760
12:02:18.662   Training iter 400, batch loss 1.1051, batch acc 0.6642
12:02:19.201   Training iter 450, batch loss 1.0686, batch acc 0.6766
12:02:19.739   Training iter 500, batch loss 1.0754, batch acc 0.6726
12:02:20.277   Training iter 550, batch loss 1.0703, batch acc 0.6774
12:02:20.812   Training iter 600, batch loss 1.0787, batch acc 0.6758
12:02:20.814 Training @ 464 epoch...
12:02:21.342   Training iter 50, batch loss 1.0703, batch acc 0.6762
12:02:21.856   Training iter 100, batch loss 1.0706, batch acc 0.6832
12:02:22.385   Training iter 150, batch loss 1.0845, batch acc 0.6724
12:02:22.916   Training iter 200, batch loss 1.0673, batch acc 0.6818
12:02:23.469   Training iter 250, batch loss 1.0539, batch acc 0.6834
12:02:24.023   Training iter 300, batch loss 1.0659, batch acc 0.6800
12:02:24.552   Training iter 350, batch loss 1.0954, batch acc 0.6690
12:02:25.100   Training iter 400, batch loss 1.0619, batch acc 0.6824
12:02:25.648   Training iter 450, batch loss 1.1001, batch acc 0.6678
12:02:26.194   Training iter 500, batch loss 1.0799, batch acc 0.6740
12:02:26.744   Training iter 550, batch loss 1.0883, batch acc 0.6712
12:02:27.300   Training iter 600, batch loss 1.0734, batch acc 0.6732
12:02:27.301 Training @ 465 epoch...
12:02:27.848   Training iter 50, batch loss 1.1107, batch acc 0.6670
12:02:28.406   Training iter 100, batch loss 1.0767, batch acc 0.6764
12:02:28.974   Training iter 150, batch loss 1.0589, batch acc 0.6862
12:02:29.530   Training iter 200, batch loss 1.0990, batch acc 0.6662
12:02:30.089   Training iter 250, batch loss 1.0691, batch acc 0.6744
12:02:30.650   Training iter 300, batch loss 1.0530, batch acc 0.6852
12:02:31.206   Training iter 350, batch loss 1.0520, batch acc 0.6884
12:02:31.753   Training iter 400, batch loss 1.0854, batch acc 0.6702
12:02:32.288   Training iter 450, batch loss 1.1052, batch acc 0.6596
12:02:32.827   Training iter 500, batch loss 1.0661, batch acc 0.6802
12:02:33.368   Training iter 550, batch loss 1.0735, batch acc 0.6788
12:02:33.890   Training iter 600, batch loss 1.0619, batch acc 0.6848
12:02:33.891 Testing @ 465 epoch...
12:02:33.933     Testing, total mean loss 1.04853, total acc 0.68390
12:02:33.933 Training @ 466 epoch...
12:02:34.465   Training iter 50, batch loss 1.0716, batch acc 0.6856
12:02:34.992   Training iter 100, batch loss 1.1091, batch acc 0.6628
12:02:35.525   Training iter 150, batch loss 1.0340, batch acc 0.6928
12:02:36.061   Training iter 200, batch loss 1.0914, batch acc 0.6702
12:02:36.588   Training iter 250, batch loss 1.0668, batch acc 0.6844
12:02:37.121   Training iter 300, batch loss 1.0572, batch acc 0.6816
12:02:37.660   Training iter 350, batch loss 1.0501, batch acc 0.6858
12:02:38.199   Training iter 400, batch loss 1.0769, batch acc 0.6762
12:02:38.717   Training iter 450, batch loss 1.0650, batch acc 0.6718
12:02:39.215   Training iter 500, batch loss 1.1056, batch acc 0.6608
12:02:39.750   Training iter 550, batch loss 1.0914, batch acc 0.6686
12:02:40.317   Training iter 600, batch loss 1.0924, batch acc 0.6748
12:02:40.319 Training @ 467 epoch...
12:02:40.868   Training iter 50, batch loss 1.0762, batch acc 0.6756
12:02:41.411   Training iter 100, batch loss 1.0708, batch acc 0.6820
12:02:41.955   Training iter 150, batch loss 1.0585, batch acc 0.6844
12:02:42.506   Training iter 200, batch loss 1.0678, batch acc 0.6788
12:02:43.061   Training iter 250, batch loss 1.0916, batch acc 0.6740
12:02:43.609   Training iter 300, batch loss 1.0823, batch acc 0.6750
12:02:44.157   Training iter 350, batch loss 1.0668, batch acc 0.6722
12:02:44.707   Training iter 400, batch loss 1.0827, batch acc 0.6730
12:02:45.256   Training iter 450, batch loss 1.0900, batch acc 0.6730
12:02:45.768   Training iter 500, batch loss 1.0800, batch acc 0.6770
12:02:46.279   Training iter 550, batch loss 1.0769, batch acc 0.6722
12:02:46.789   Training iter 600, batch loss 1.0676, batch acc 0.6810
12:02:46.790 Training @ 468 epoch...
12:02:47.305   Training iter 50, batch loss 1.0711, batch acc 0.6740
12:02:47.822   Training iter 100, batch loss 1.0872, batch acc 0.6732
12:02:48.357   Training iter 150, batch loss 1.0738, batch acc 0.6804
12:02:48.888   Training iter 200, batch loss 1.0737, batch acc 0.6790
12:02:49.416   Training iter 250, batch loss 1.1035, batch acc 0.6624
12:02:49.950   Training iter 300, batch loss 1.0711, batch acc 0.6776
12:02:50.515   Training iter 350, batch loss 1.0688, batch acc 0.6756
12:02:51.072   Training iter 400, batch loss 1.0510, batch acc 0.6836
12:02:51.607   Training iter 450, batch loss 1.0864, batch acc 0.6754
12:02:52.134   Training iter 500, batch loss 1.0724, batch acc 0.6744
12:02:52.661   Training iter 550, batch loss 1.0891, batch acc 0.6770
12:02:53.207   Training iter 600, batch loss 1.0634, batch acc 0.6816
12:02:53.209 Training @ 469 epoch...
12:02:53.746   Training iter 50, batch loss 1.0936, batch acc 0.6708
12:02:54.280   Training iter 100, batch loss 1.0790, batch acc 0.6736
12:02:54.796   Training iter 150, batch loss 1.0906, batch acc 0.6642
12:02:55.334   Training iter 200, batch loss 1.1002, batch acc 0.6738
12:02:55.874   Training iter 250, batch loss 1.0664, batch acc 0.6762
12:02:56.418   Training iter 300, batch loss 1.0797, batch acc 0.6766
12:02:56.968   Training iter 350, batch loss 1.0518, batch acc 0.6864
12:02:57.535   Training iter 400, batch loss 1.0523, batch acc 0.6856
12:02:58.094   Training iter 450, batch loss 1.0703, batch acc 0.6778
12:02:58.657   Training iter 500, batch loss 1.0670, batch acc 0.6862
12:02:59.210   Training iter 550, batch loss 1.0900, batch acc 0.6700
12:02:59.768   Training iter 600, batch loss 1.0704, batch acc 0.6760
12:02:59.770 Training @ 470 epoch...
12:03:00.339   Training iter 50, batch loss 1.0610, batch acc 0.6834
12:03:00.856   Training iter 100, batch loss 1.0996, batch acc 0.6664
12:03:01.382   Training iter 150, batch loss 1.0814, batch acc 0.6718
12:03:01.910   Training iter 200, batch loss 1.0853, batch acc 0.6742
12:03:02.458   Training iter 250, batch loss 1.0734, batch acc 0.6774
12:03:02.989   Training iter 300, batch loss 1.0516, batch acc 0.6840
12:03:03.512   Training iter 350, batch loss 1.0578, batch acc 0.6776
12:03:04.039   Training iter 400, batch loss 1.0741, batch acc 0.6734
12:03:04.577   Training iter 450, batch loss 1.0822, batch acc 0.6770
12:03:05.095   Training iter 500, batch loss 1.1115, batch acc 0.6644
12:03:05.629   Training iter 550, batch loss 1.0647, batch acc 0.6858
12:03:06.154   Training iter 600, batch loss 1.0686, batch acc 0.6806
12:03:06.156 Testing @ 470 epoch...
12:03:06.198     Testing, total mean loss 1.04861, total acc 0.68320
12:03:06.198 Training @ 471 epoch...
12:03:06.702   Training iter 50, batch loss 1.0743, batch acc 0.6788
12:03:07.220   Training iter 100, batch loss 1.0992, batch acc 0.6646
12:03:07.771   Training iter 150, batch loss 1.0658, batch acc 0.6850
12:03:08.307   Training iter 200, batch loss 1.0901, batch acc 0.6778
12:03:08.816   Training iter 250, batch loss 1.0705, batch acc 0.6792
12:03:09.328   Training iter 300, batch loss 1.0715, batch acc 0.6804
12:03:09.890   Training iter 350, batch loss 1.0888, batch acc 0.6682
12:03:10.494   Training iter 400, batch loss 1.0624, batch acc 0.6756
12:03:11.011   Training iter 450, batch loss 1.0709, batch acc 0.6786
12:03:11.532   Training iter 500, batch loss 1.0886, batch acc 0.6704
12:03:12.049   Training iter 550, batch loss 1.0525, batch acc 0.6862
12:03:12.635   Training iter 600, batch loss 1.0766, batch acc 0.6730
12:03:12.637 Training @ 472 epoch...
12:03:13.221   Training iter 50, batch loss 1.0884, batch acc 0.6732
12:03:13.808   Training iter 100, batch loss 1.0654, batch acc 0.6784
12:03:14.369   Training iter 150, batch loss 1.0569, batch acc 0.6764
12:03:14.927   Training iter 200, batch loss 1.0823, batch acc 0.6784
12:03:15.480   Training iter 250, batch loss 1.0980, batch acc 0.6668
12:03:16.051   Training iter 300, batch loss 1.0806, batch acc 0.6776
12:03:16.621   Training iter 350, batch loss 1.0519, batch acc 0.6870
12:03:17.190   Training iter 400, batch loss 1.0739, batch acc 0.6748
12:03:17.743   Training iter 450, batch loss 1.0807, batch acc 0.6816
12:03:18.291   Training iter 500, batch loss 1.0478, batch acc 0.6888
12:03:18.812   Training iter 550, batch loss 1.1053, batch acc 0.6582
12:03:19.333   Training iter 600, batch loss 1.0799, batch acc 0.6724
12:03:19.334 Training @ 473 epoch...
12:03:19.876   Training iter 50, batch loss 1.0838, batch acc 0.6768
12:03:20.445   Training iter 100, batch loss 1.0843, batch acc 0.6752
12:03:20.978   Training iter 150, batch loss 1.0814, batch acc 0.6770
12:03:21.526   Training iter 200, batch loss 1.0728, batch acc 0.6724
12:03:22.079   Training iter 250, batch loss 1.0735, batch acc 0.6716
12:03:22.622   Training iter 300, batch loss 1.0790, batch acc 0.6782
12:03:23.174   Training iter 350, batch loss 1.0567, batch acc 0.6794
12:03:23.739   Training iter 400, batch loss 1.0835, batch acc 0.6714
12:03:24.311   Training iter 450, batch loss 1.0858, batch acc 0.6788
12:03:24.873   Training iter 500, batch loss 1.0701, batch acc 0.6790
12:03:25.437   Training iter 550, batch loss 1.0735, batch acc 0.6772
12:03:25.967   Training iter 600, batch loss 1.0666, batch acc 0.6814
12:03:25.969 Training @ 474 epoch...
12:03:26.530   Training iter 50, batch loss 1.0687, batch acc 0.6760
12:03:27.088   Training iter 100, batch loss 1.0789, batch acc 0.6764
12:03:27.645   Training iter 150, batch loss 1.0685, batch acc 0.6788
12:03:28.215   Training iter 200, batch loss 1.0880, batch acc 0.6744
12:03:28.774   Training iter 250, batch loss 1.0794, batch acc 0.6766
12:03:29.344   Training iter 300, batch loss 1.0908, batch acc 0.6728
12:03:29.916   Training iter 350, batch loss 1.0731, batch acc 0.6800
12:03:30.566   Training iter 400, batch loss 1.0651, batch acc 0.6778
12:03:31.220   Training iter 450, batch loss 1.1014, batch acc 0.6700
12:03:31.830   Training iter 500, batch loss 1.0669, batch acc 0.6756
12:03:32.400   Training iter 550, batch loss 1.0299, batch acc 0.6900
12:03:32.962   Training iter 600, batch loss 1.1003, batch acc 0.6672
12:03:32.964 Training @ 475 epoch...
12:03:33.518   Training iter 50, batch loss 1.0596, batch acc 0.6746
12:03:34.066   Training iter 100, batch loss 1.0746, batch acc 0.6774
12:03:34.617   Training iter 150, batch loss 1.0891, batch acc 0.6756
12:03:35.150   Training iter 200, batch loss 1.0963, batch acc 0.6688
12:03:35.665   Training iter 250, batch loss 1.0848, batch acc 0.6710
12:03:36.185   Training iter 300, batch loss 1.0754, batch acc 0.6732
12:03:36.703   Training iter 350, batch loss 1.0607, batch acc 0.6842
12:03:37.228   Training iter 400, batch loss 1.0650, batch acc 0.6858
12:03:37.755   Training iter 450, batch loss 1.0467, batch acc 0.6824
12:03:38.290   Training iter 500, batch loss 1.0746, batch acc 0.6824
12:03:38.831   Training iter 550, batch loss 1.0895, batch acc 0.6700
12:03:39.370   Training iter 600, batch loss 1.0947, batch acc 0.6738
12:03:39.371 Testing @ 475 epoch...
12:03:39.413     Testing, total mean loss 1.04845, total acc 0.68380
12:03:39.414 Training @ 476 epoch...
12:03:39.947   Training iter 50, batch loss 1.0908, batch acc 0.6732
12:03:40.478   Training iter 100, batch loss 1.0889, batch acc 0.6702
12:03:40.998   Training iter 150, batch loss 1.0791, batch acc 0.6780
12:03:41.524   Training iter 200, batch loss 1.0869, batch acc 0.6718
12:03:42.032   Training iter 250, batch loss 1.0910, batch acc 0.6710
12:03:42.556   Training iter 300, batch loss 1.0560, batch acc 0.6796
12:03:43.087   Training iter 350, batch loss 1.1160, batch acc 0.6628
12:03:43.631   Training iter 400, batch loss 1.0803, batch acc 0.6750
12:03:44.166   Training iter 450, batch loss 1.0601, batch acc 0.6790
12:03:44.707   Training iter 500, batch loss 1.0530, batch acc 0.6886
12:03:45.256   Training iter 550, batch loss 1.0559, batch acc 0.6822
12:03:45.800   Training iter 600, batch loss 1.0530, batch acc 0.6858
12:03:45.802 Training @ 477 epoch...
12:03:46.355   Training iter 50, batch loss 1.0647, batch acc 0.6756
12:03:46.909   Training iter 100, batch loss 1.0848, batch acc 0.6740
12:03:47.459   Training iter 150, batch loss 1.0703, batch acc 0.6814
12:03:48.003   Training iter 200, batch loss 1.0870, batch acc 0.6756
12:03:48.551   Training iter 250, batch loss 1.0705, batch acc 0.6796
12:03:49.096   Training iter 300, batch loss 1.0669, batch acc 0.6824
12:03:49.653   Training iter 350, batch loss 1.1050, batch acc 0.6684
12:03:50.202   Training iter 400, batch loss 1.0678, batch acc 0.6812
12:03:50.749   Training iter 450, batch loss 1.0899, batch acc 0.6602
12:03:51.303   Training iter 500, batch loss 1.0789, batch acc 0.6758
12:03:51.804   Training iter 550, batch loss 1.0725, batch acc 0.6768
12:03:52.315   Training iter 600, batch loss 1.0525, batch acc 0.6816
12:03:52.317 Training @ 478 epoch...
12:03:52.840   Training iter 50, batch loss 1.0610, batch acc 0.6858
12:03:53.364   Training iter 100, batch loss 1.0933, batch acc 0.6716
12:03:53.881   Training iter 150, batch loss 1.0873, batch acc 0.6738
12:03:54.426   Training iter 200, batch loss 1.0791, batch acc 0.6730
12:03:54.956   Training iter 250, batch loss 1.0882, batch acc 0.6720
12:03:55.479   Training iter 300, batch loss 1.0729, batch acc 0.6782
12:03:55.995   Training iter 350, batch loss 1.0839, batch acc 0.6724
12:03:56.513   Training iter 400, batch loss 1.0876, batch acc 0.6688
12:03:57.041   Training iter 450, batch loss 1.0596, batch acc 0.6854
12:03:57.568   Training iter 500, batch loss 1.0661, batch acc 0.6770
12:03:58.075   Training iter 550, batch loss 1.0817, batch acc 0.6748
12:03:58.593   Training iter 600, batch loss 1.0503, batch acc 0.6842
12:03:58.595 Training @ 479 epoch...
12:03:59.122   Training iter 50, batch loss 1.0581, batch acc 0.6784
12:03:59.642   Training iter 100, batch loss 1.0843, batch acc 0.6736
12:04:00.164   Training iter 150, batch loss 1.0630, batch acc 0.6792
12:04:00.722   Training iter 200, batch loss 1.0676, batch acc 0.6768
12:04:01.254   Training iter 250, batch loss 1.0556, batch acc 0.6834
12:04:01.839   Training iter 300, batch loss 1.0997, batch acc 0.6714
12:04:02.451   Training iter 350, batch loss 1.0681, batch acc 0.6848
12:04:03.030   Training iter 400, batch loss 1.0923, batch acc 0.6728
12:04:03.599   Training iter 450, batch loss 1.0800, batch acc 0.6748
12:04:04.163   Training iter 500, batch loss 1.0569, batch acc 0.6828
12:04:04.725   Training iter 550, batch loss 1.0787, batch acc 0.6722
12:04:05.335   Training iter 600, batch loss 1.1063, batch acc 0.6644
12:04:05.338 Training @ 480 epoch...
12:04:05.899   Training iter 50, batch loss 1.0635, batch acc 0.6782
12:04:06.460   Training iter 100, batch loss 1.0759, batch acc 0.6806
12:04:07.068   Training iter 150, batch loss 1.0616, batch acc 0.6820
12:04:07.763   Training iter 200, batch loss 1.0815, batch acc 0.6818
12:04:08.326   Training iter 250, batch loss 1.0957, batch acc 0.6706
12:04:08.883   Training iter 300, batch loss 1.0820, batch acc 0.6764
12:04:09.458   Training iter 350, batch loss 1.0833, batch acc 0.6708
12:04:10.075   Training iter 400, batch loss 1.0971, batch acc 0.6698
12:04:10.684   Training iter 450, batch loss 1.0823, batch acc 0.6722
12:04:11.264   Training iter 500, batch loss 1.0422, batch acc 0.6850
12:04:11.778   Training iter 550, batch loss 1.0807, batch acc 0.6712
12:04:12.311   Training iter 600, batch loss 1.0652, batch acc 0.6800
12:04:12.313 Testing @ 480 epoch...
12:04:12.358     Testing, total mean loss 1.04844, total acc 0.68430
12:04:12.358 Training @ 481 epoch...
12:04:12.902   Training iter 50, batch loss 1.1007, batch acc 0.6670
12:04:13.451   Training iter 100, batch loss 1.0880, batch acc 0.6730
12:04:14.015   Training iter 150, batch loss 1.0651, batch acc 0.6800
12:04:14.560   Training iter 200, batch loss 1.0802, batch acc 0.6760
12:04:15.107   Training iter 250, batch loss 1.0779, batch acc 0.6760
12:04:15.633   Training iter 300, batch loss 1.0795, batch acc 0.6722
12:04:16.168   Training iter 350, batch loss 1.0612, batch acc 0.6844
12:04:16.682   Training iter 400, batch loss 1.0760, batch acc 0.6802
12:04:17.196   Training iter 450, batch loss 1.0529, batch acc 0.6860
12:04:17.723   Training iter 500, batch loss 1.0674, batch acc 0.6818
12:04:18.262   Training iter 550, batch loss 1.0699, batch acc 0.6778
12:04:18.772   Training iter 600, batch loss 1.0920, batch acc 0.6638
12:04:18.774 Training @ 482 epoch...
12:04:19.295   Training iter 50, batch loss 1.0656, batch acc 0.6868
12:04:19.817   Training iter 100, batch loss 1.0893, batch acc 0.6696
12:04:20.339   Training iter 150, batch loss 1.0829, batch acc 0.6760
12:04:20.863   Training iter 200, batch loss 1.0790, batch acc 0.6758
12:04:21.387   Training iter 250, batch loss 1.0969, batch acc 0.6648
12:04:21.908   Training iter 300, batch loss 1.0875, batch acc 0.6730
12:04:22.442   Training iter 350, batch loss 1.0633, batch acc 0.6776
12:04:22.951   Training iter 400, batch loss 1.0708, batch acc 0.6740
12:04:23.488   Training iter 450, batch loss 1.0592, batch acc 0.6840
12:04:24.008   Training iter 500, batch loss 1.0706, batch acc 0.6738
12:04:24.527   Training iter 550, batch loss 1.0770, batch acc 0.6794
12:04:25.053   Training iter 600, batch loss 1.0685, batch acc 0.6802
12:04:25.055 Training @ 483 epoch...
12:04:25.580   Training iter 50, batch loss 1.0810, batch acc 0.6704
12:04:26.101   Training iter 100, batch loss 1.0905, batch acc 0.6730
12:04:26.666   Training iter 150, batch loss 1.0971, batch acc 0.6660
12:04:27.233   Training iter 200, batch loss 1.1134, batch acc 0.6642
12:04:27.807   Training iter 250, batch loss 1.0737, batch acc 0.6748
12:04:28.374   Training iter 300, batch loss 1.0516, batch acc 0.6822
12:04:28.939   Training iter 350, batch loss 1.0662, batch acc 0.6768
12:04:29.506   Training iter 400, batch loss 1.0583, batch acc 0.6888
12:04:30.076   Training iter 450, batch loss 1.0599, batch acc 0.6854
12:04:30.649   Training iter 500, batch loss 1.0871, batch acc 0.6726
12:04:31.192   Training iter 550, batch loss 1.0628, batch acc 0.6820
12:04:31.726   Training iter 600, batch loss 1.0692, batch acc 0.6806
12:04:31.728 Training @ 484 epoch...
12:04:32.272   Training iter 50, batch loss 1.0853, batch acc 0.6714
12:04:32.804   Training iter 100, batch loss 1.0602, batch acc 0.6866
12:04:33.362   Training iter 150, batch loss 1.0685, batch acc 0.6810
12:04:33.917   Training iter 200, batch loss 1.0626, batch acc 0.6844
12:04:34.468   Training iter 250, batch loss 1.0831, batch acc 0.6768
12:04:35.042   Training iter 300, batch loss 1.0825, batch acc 0.6758
12:04:35.620   Training iter 350, batch loss 1.0783, batch acc 0.6702
12:04:36.195   Training iter 400, batch loss 1.0723, batch acc 0.6782
12:04:36.762   Training iter 450, batch loss 1.0709, batch acc 0.6730
12:04:37.330   Training iter 500, batch loss 1.0858, batch acc 0.6688
12:04:37.922   Training iter 550, batch loss 1.0686, batch acc 0.6784
12:04:38.503   Training iter 600, batch loss 1.0925, batch acc 0.6706
12:04:38.505 Training @ 485 epoch...
12:04:39.055   Training iter 50, batch loss 1.0785, batch acc 0.6720
12:04:39.599   Training iter 100, batch loss 1.0799, batch acc 0.6806
12:04:40.129   Training iter 150, batch loss 1.0997, batch acc 0.6694
12:04:40.672   Training iter 200, batch loss 1.0814, batch acc 0.6736
12:04:41.220   Training iter 250, batch loss 1.0791, batch acc 0.6748
12:04:41.763   Training iter 300, batch loss 1.0518, batch acc 0.6832
12:04:42.323   Training iter 350, batch loss 1.0832, batch acc 0.6728
12:04:42.888   Training iter 400, batch loss 1.0753, batch acc 0.6824
12:04:43.421   Training iter 450, batch loss 1.1055, batch acc 0.6646
12:04:43.894   Training iter 500, batch loss 1.0589, batch acc 0.6800
12:04:44.385   Training iter 550, batch loss 1.0886, batch acc 0.6724
12:04:44.877   Training iter 600, batch loss 1.0288, batch acc 0.6896
12:04:44.879 Testing @ 485 epoch...
12:04:44.921     Testing, total mean loss 1.04838, total acc 0.68420
12:04:44.921 Training @ 486 epoch...
12:04:45.454   Training iter 50, batch loss 1.0749, batch acc 0.6764
12:04:45.970   Training iter 100, batch loss 1.1035, batch acc 0.6682
12:04:46.481   Training iter 150, batch loss 1.0664, batch acc 0.6764
12:04:46.983   Training iter 200, batch loss 1.0759, batch acc 0.6814
12:04:47.507   Training iter 250, batch loss 1.0609, batch acc 0.6832
12:04:48.024   Training iter 300, batch loss 1.0680, batch acc 0.6798
12:04:48.538   Training iter 350, batch loss 1.0853, batch acc 0.6728
12:04:49.041   Training iter 400, batch loss 1.0692, batch acc 0.6804
12:04:49.551   Training iter 450, batch loss 1.0514, batch acc 0.6790
12:04:50.074   Training iter 500, batch loss 1.0744, batch acc 0.6792
12:04:50.610   Training iter 550, batch loss 1.0767, batch acc 0.6758
12:04:51.124   Training iter 600, batch loss 1.1039, batch acc 0.6650
12:04:51.125 Training @ 487 epoch...
12:04:51.639   Training iter 50, batch loss 1.0602, batch acc 0.6832
12:04:52.151   Training iter 100, batch loss 1.0868, batch acc 0.6656
12:04:52.707   Training iter 150, batch loss 1.0709, batch acc 0.6842
12:04:53.263   Training iter 200, batch loss 1.0923, batch acc 0.6762
12:04:53.822   Training iter 250, batch loss 1.0896, batch acc 0.6734
12:04:54.405   Training iter 300, batch loss 1.0618, batch acc 0.6832
12:04:54.959   Training iter 350, batch loss 1.0972, batch acc 0.6742
12:04:55.516   Training iter 400, batch loss 1.0652, batch acc 0.6766
12:04:56.033   Training iter 450, batch loss 1.0936, batch acc 0.6648
12:04:56.542   Training iter 500, batch loss 1.0507, batch acc 0.6816
12:04:57.061   Training iter 550, batch loss 1.0716, batch acc 0.6698
12:04:57.578   Training iter 600, batch loss 1.0707, batch acc 0.6824
12:04:57.580 Training @ 488 epoch...
12:04:58.127   Training iter 50, batch loss 1.0799, batch acc 0.6732
12:04:58.637   Training iter 100, batch loss 1.0646, batch acc 0.6788
12:04:59.129   Training iter 150, batch loss 1.0940, batch acc 0.6686
12:04:59.640   Training iter 200, batch loss 1.0757, batch acc 0.6814
12:05:00.157   Training iter 250, batch loss 1.0645, batch acc 0.6844
12:05:00.687   Training iter 300, batch loss 1.0829, batch acc 0.6776
12:05:01.207   Training iter 350, batch loss 1.0758, batch acc 0.6690
12:05:01.836   Training iter 400, batch loss 1.0645, batch acc 0.6822
12:05:02.486   Training iter 450, batch loss 1.0613, batch acc 0.6840
12:05:03.126   Training iter 500, batch loss 1.0922, batch acc 0.6648
12:05:03.769   Training iter 550, batch loss 1.0648, batch acc 0.6794
12:05:04.346   Training iter 600, batch loss 1.0903, batch acc 0.6702
12:05:04.349 Training @ 489 epoch...
12:05:04.916   Training iter 50, batch loss 1.0635, batch acc 0.6816
12:05:05.465   Training iter 100, batch loss 1.0729, batch acc 0.6818
12:05:06.010   Training iter 150, batch loss 1.0567, batch acc 0.6788
12:05:06.559   Training iter 200, batch loss 1.0860, batch acc 0.6738
12:05:07.125   Training iter 250, batch loss 1.0875, batch acc 0.6798
12:05:07.679   Training iter 300, batch loss 1.0612, batch acc 0.6836
12:05:08.210   Training iter 350, batch loss 1.0935, batch acc 0.6638
12:05:08.718   Training iter 400, batch loss 1.0766, batch acc 0.6810
12:05:09.211   Training iter 450, batch loss 1.0706, batch acc 0.6778
12:05:09.687   Training iter 500, batch loss 1.0745, batch acc 0.6720
12:05:10.217   Training iter 550, batch loss 1.0971, batch acc 0.6676
12:05:10.729   Training iter 600, batch loss 1.0702, batch acc 0.6738
12:05:10.731 Training @ 490 epoch...
12:05:11.227   Training iter 50, batch loss 1.0917, batch acc 0.6698
12:05:11.737   Training iter 100, batch loss 1.0894, batch acc 0.6774
12:05:12.241   Training iter 150, batch loss 1.0734, batch acc 0.6756
12:05:12.762   Training iter 200, batch loss 1.0781, batch acc 0.6816
12:05:13.277   Training iter 250, batch loss 1.0789, batch acc 0.6780
12:05:13.764   Training iter 300, batch loss 1.0718, batch acc 0.6762
12:05:14.261   Training iter 350, batch loss 1.0816, batch acc 0.6726
12:05:14.782   Training iter 400, batch loss 1.0802, batch acc 0.6702
12:05:15.313   Training iter 450, batch loss 1.0604, batch acc 0.6772
12:05:15.838   Training iter 500, batch loss 1.0732, batch acc 0.6780
12:05:16.357   Training iter 550, batch loss 1.0797, batch acc 0.6760
12:05:16.878   Training iter 600, batch loss 1.0519, batch acc 0.6852
12:05:16.880 Testing @ 490 epoch...
12:05:16.922     Testing, total mean loss 1.04848, total acc 0.68430
12:05:16.922 Training @ 491 epoch...
12:05:17.451   Training iter 50, batch loss 1.0859, batch acc 0.6744
12:05:17.985   Training iter 100, batch loss 1.0718, batch acc 0.6806
12:05:18.528   Training iter 150, batch loss 1.0648, batch acc 0.6830
12:05:19.037   Training iter 200, batch loss 1.0758, batch acc 0.6732
12:05:19.534   Training iter 250, batch loss 1.0895, batch acc 0.6680
12:05:20.032   Training iter 300, batch loss 1.0654, batch acc 0.6808
12:05:20.546   Training iter 350, batch loss 1.0801, batch acc 0.6722
12:05:21.034   Training iter 400, batch loss 1.0749, batch acc 0.6746
12:05:21.565   Training iter 450, batch loss 1.0886, batch acc 0.6730
12:05:22.122   Training iter 500, batch loss 1.0724, batch acc 0.6742
12:05:22.662   Training iter 550, batch loss 1.0741, batch acc 0.6782
12:05:23.180   Training iter 600, batch loss 1.0671, batch acc 0.6818
12:05:23.181 Training @ 492 epoch...
12:05:23.709   Training iter 50, batch loss 1.1084, batch acc 0.6638
12:05:24.255   Training iter 100, batch loss 1.0978, batch acc 0.6646
12:05:24.813   Training iter 150, batch loss 1.0878, batch acc 0.6692
12:05:25.387   Training iter 200, batch loss 1.0725, batch acc 0.6844
12:05:25.941   Training iter 250, batch loss 1.0728, batch acc 0.6770
12:05:26.499   Training iter 300, batch loss 1.0524, batch acc 0.6824
12:05:27.038   Training iter 350, batch loss 1.0784, batch acc 0.6726
12:05:27.588   Training iter 400, batch loss 1.0639, batch acc 0.6850
12:05:28.149   Training iter 450, batch loss 1.0580, batch acc 0.6890
12:05:28.705   Training iter 500, batch loss 1.0929, batch acc 0.6650
12:05:29.247   Training iter 550, batch loss 1.0740, batch acc 0.6814
12:05:29.782   Training iter 600, batch loss 1.0515, batch acc 0.6832
12:05:29.784 Training @ 493 epoch...
12:05:30.316   Training iter 50, batch loss 1.1187, batch acc 0.6594
12:05:30.842   Training iter 100, batch loss 1.0768, batch acc 0.6798
12:05:31.390   Training iter 150, batch loss 1.0661, batch acc 0.6812
12:05:31.950   Training iter 200, batch loss 1.0915, batch acc 0.6700
12:05:32.523   Training iter 250, batch loss 1.0442, batch acc 0.6888
12:05:33.063   Training iter 300, batch loss 1.0614, batch acc 0.6836
12:05:33.586   Training iter 350, batch loss 1.0866, batch acc 0.6686
12:05:34.106   Training iter 400, batch loss 1.0965, batch acc 0.6696
12:05:34.639   Training iter 450, batch loss 1.0675, batch acc 0.6794
12:05:35.171   Training iter 500, batch loss 1.0475, batch acc 0.6878
12:05:35.708   Training iter 550, batch loss 1.0764, batch acc 0.6716
12:05:36.234   Training iter 600, batch loss 1.0771, batch acc 0.6770
12:05:36.236 Training @ 494 epoch...
12:05:36.758   Training iter 50, batch loss 1.0553, batch acc 0.6892
12:05:37.285   Training iter 100, batch loss 1.0719, batch acc 0.6772
12:05:37.818   Training iter 150, batch loss 1.0957, batch acc 0.6654
12:05:38.346   Training iter 200, batch loss 1.0829, batch acc 0.6686
12:05:38.872   Training iter 250, batch loss 1.0824, batch acc 0.6766
12:05:39.431   Training iter 300, batch loss 1.0590, batch acc 0.6842
12:05:39.984   Training iter 350, batch loss 1.0834, batch acc 0.6768
12:05:40.538   Training iter 400, batch loss 1.0734, batch acc 0.6812
12:05:41.089   Training iter 450, batch loss 1.0945, batch acc 0.6666
12:05:41.639   Training iter 500, batch loss 1.0884, batch acc 0.6702
12:05:42.188   Training iter 550, batch loss 1.0594, batch acc 0.6822
12:05:42.737   Training iter 600, batch loss 1.0639, batch acc 0.6782
12:05:42.738 Training @ 495 epoch...
12:05:43.292   Training iter 50, batch loss 1.0551, batch acc 0.6862
12:05:43.864   Training iter 100, batch loss 1.0429, batch acc 0.6956
12:05:44.433   Training iter 150, batch loss 1.0947, batch acc 0.6664
12:05:44.995   Training iter 200, batch loss 1.0828, batch acc 0.6748
12:05:45.539   Training iter 250, batch loss 1.0972, batch acc 0.6662
12:05:46.069   Training iter 300, batch loss 1.0697, batch acc 0.6792
12:05:46.600   Training iter 350, batch loss 1.0772, batch acc 0.6752
12:05:47.143   Training iter 400, batch loss 1.0836, batch acc 0.6726
12:05:47.697   Training iter 450, batch loss 1.0815, batch acc 0.6720
12:05:48.258   Training iter 500, batch loss 1.0592, batch acc 0.6820
12:05:48.775   Training iter 550, batch loss 1.0801, batch acc 0.6726
12:05:49.299   Training iter 600, batch loss 1.0861, batch acc 0.6732
12:05:49.301 Testing @ 495 epoch...
12:05:49.342     Testing, total mean loss 1.04844, total acc 0.68470
12:05:49.342 Training @ 496 epoch...
12:05:49.877   Training iter 50, batch loss 1.0609, batch acc 0.6808
12:05:50.410   Training iter 100, batch loss 1.0733, batch acc 0.6744
12:05:50.937   Training iter 150, batch loss 1.0888, batch acc 0.6740
12:05:51.450   Training iter 200, batch loss 1.0758, batch acc 0.6728
12:05:51.973   Training iter 250, batch loss 1.0865, batch acc 0.6708
12:05:52.501   Training iter 300, batch loss 1.0493, batch acc 0.6814
12:05:53.031   Training iter 350, batch loss 1.0701, batch acc 0.6766
12:05:53.553   Training iter 400, batch loss 1.0833, batch acc 0.6780
12:05:54.068   Training iter 450, batch loss 1.0712, batch acc 0.6818
12:05:54.588   Training iter 500, batch loss 1.0778, batch acc 0.6790
12:05:55.131   Training iter 550, batch loss 1.0712, batch acc 0.6758
12:05:55.671   Training iter 600, batch loss 1.1021, batch acc 0.6704
12:05:55.673 Training @ 497 epoch...
12:05:56.226   Training iter 50, batch loss 1.0862, batch acc 0.6700
12:05:56.763   Training iter 100, batch loss 1.0793, batch acc 0.6720
12:05:57.314   Training iter 150, batch loss 1.0860, batch acc 0.6732
12:05:57.870   Training iter 200, batch loss 1.0944, batch acc 0.6718
12:05:58.380   Training iter 250, batch loss 1.0800, batch acc 0.6742
12:05:58.900   Training iter 300, batch loss 1.0710, batch acc 0.6798
12:05:59.442   Training iter 350, batch loss 1.0605, batch acc 0.6838
12:06:00.029   Training iter 400, batch loss 1.1025, batch acc 0.6644
12:06:00.614   Training iter 450, batch loss 1.0559, batch acc 0.6828
12:06:01.166   Training iter 500, batch loss 1.0886, batch acc 0.6710
12:06:01.754   Training iter 550, batch loss 1.0495, batch acc 0.6874
12:06:02.343   Training iter 600, batch loss 1.0564, batch acc 0.6856
12:06:02.345 Training @ 498 epoch...
12:06:02.920   Training iter 50, batch loss 1.0952, batch acc 0.6712
12:06:03.484   Training iter 100, batch loss 1.0796, batch acc 0.6742
12:06:04.060   Training iter 150, batch loss 1.0738, batch acc 0.6822
12:06:04.622   Training iter 200, batch loss 1.0812, batch acc 0.6728
12:06:05.202   Training iter 250, batch loss 1.0851, batch acc 0.6716
12:06:05.753   Training iter 300, batch loss 1.0715, batch acc 0.6796
12:06:06.330   Training iter 350, batch loss 1.0876, batch acc 0.6650
12:06:06.893   Training iter 400, batch loss 1.0699, batch acc 0.6834
12:06:07.474   Training iter 450, batch loss 1.0638, batch acc 0.6798
12:06:08.049   Training iter 500, batch loss 1.0736, batch acc 0.6764
12:06:08.628   Training iter 550, batch loss 1.0620, batch acc 0.6798
12:06:09.177   Training iter 600, batch loss 1.0668, batch acc 0.6806
12:06:09.179 Training @ 499 epoch...
12:06:09.745   Training iter 50, batch loss 1.0686, batch acc 0.6772
12:06:10.319   Training iter 100, batch loss 1.0872, batch acc 0.6704
12:06:10.860   Training iter 150, batch loss 1.0908, batch acc 0.6664
12:06:11.436   Training iter 200, batch loss 1.0689, batch acc 0.6834
12:06:12.017   Training iter 250, batch loss 1.1070, batch acc 0.6608
12:06:12.631   Training iter 300, batch loss 1.0731, batch acc 0.6788
12:06:13.208   Training iter 350, batch loss 1.0655, batch acc 0.6816
12:06:13.843   Training iter 400, batch loss 1.0595, batch acc 0.6854
12:06:14.410   Training iter 450, batch loss 1.0657, batch acc 0.6836
12:06:15.069   Training iter 500, batch loss 1.0771, batch acc 0.6750
12:06:15.738   Training iter 550, batch loss 1.0682, batch acc 0.6754
12:06:16.309   Training iter 600, batch loss 1.0784, batch acc 0.6772
======================================================
12:06:16.311 Testing @ final epoch...
12:06:16.371     Testing, total mean loss 1.04840, total acc 0.68460
training time: 3151 seconds
