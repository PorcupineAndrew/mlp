======================================================
learning_rate: 0.1
weight_decay: 0.0001
momentum: 0.1
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Sigm Lin-10-10 Sigm
loss: Softmax
result dir: ./result/exp_8
======================================================
15:34:37.320 Training @ 0 epoch...
15:34:37.895   Training iter 50, batch loss 2.3025, batch acc 0.0954
15:34:38.447   Training iter 100, batch loss 2.3021, batch acc 0.1120
15:34:38.981   Training iter 150, batch loss 2.3016, batch acc 0.1212
15:34:39.492   Training iter 200, batch loss 2.3012, batch acc 0.1146
15:34:39.996   Training iter 250, batch loss 2.3002, batch acc 0.1254
15:34:40.509   Training iter 300, batch loss 2.3014, batch acc 0.1040
15:34:40.975   Training iter 350, batch loss 2.2997, batch acc 0.1146
15:34:41.436   Training iter 400, batch loss 2.3012, batch acc 0.1024
15:34:41.898   Training iter 450, batch loss 2.2987, batch acc 0.1146
15:34:42.375   Training iter 500, batch loss 2.2989, batch acc 0.1124
15:34:42.861   Training iter 550, batch loss 2.2981, batch acc 0.1074
15:34:43.337   Training iter 600, batch loss 2.2966, batch acc 0.1080
15:34:43.339 Testing @ 0 epoch...
15:34:43.385     Testing, total mean loss 2.29486, total acc 0.11350
15:34:43.385 Plot @ 0 epoch...
15:34:43.385 Training @ 1 epoch...
15:34:43.900   Training iter 50, batch loss 2.2960, batch acc 0.1056
15:34:44.397   Training iter 100, batch loss 2.2906, batch acc 0.1194
15:34:44.898   Training iter 150, batch loss 2.2899, batch acc 0.1138
15:34:45.399   Training iter 200, batch loss 2.2858, batch acc 0.1132
15:34:45.891   Training iter 250, batch loss 2.2820, batch acc 0.1174
15:34:46.394   Training iter 300, batch loss 2.2796, batch acc 0.1064
15:34:46.887   Training iter 350, batch loss 2.2707, batch acc 0.1172
15:34:47.376   Training iter 400, batch loss 2.2652, batch acc 0.1128
15:34:47.865   Training iter 450, batch loss 2.2601, batch acc 0.1158
15:34:48.367   Training iter 500, batch loss 2.2507, batch acc 0.1304
15:34:48.861   Training iter 550, batch loss 2.2433, batch acc 0.1384
15:34:49.360   Training iter 600, batch loss 2.2351, batch acc 0.1564
15:34:49.362 Training @ 2 epoch...
15:34:49.873   Training iter 50, batch loss 2.2230, batch acc 0.1898
15:34:50.379   Training iter 100, batch loss 2.2109, batch acc 0.2170
15:34:50.884   Training iter 150, batch loss 2.1982, batch acc 0.2310
15:34:51.380   Training iter 200, batch loss 2.1865, batch acc 0.2612
15:34:51.886   Training iter 250, batch loss 2.1740, batch acc 0.3084
15:34:52.404   Training iter 300, batch loss 2.1614, batch acc 0.3546
15:34:52.917   Training iter 350, batch loss 2.1512, batch acc 0.3808
15:34:53.428   Training iter 400, batch loss 2.1404, batch acc 0.4078
15:34:53.938   Training iter 450, batch loss 2.1291, batch acc 0.4454
15:34:54.448   Training iter 500, batch loss 2.1146, batch acc 0.4734
15:34:54.950   Training iter 550, batch loss 2.1045, batch acc 0.4706
15:34:55.469   Training iter 600, batch loss 2.0919, batch acc 0.5036
15:34:55.471 Training @ 3 epoch...
15:34:55.970   Training iter 50, batch loss 2.0838, batch acc 0.5170
15:34:56.480   Training iter 100, batch loss 2.0722, batch acc 0.5188
15:34:56.985   Training iter 150, batch loss 2.0621, batch acc 0.5288
15:34:57.460   Training iter 200, batch loss 2.0522, batch acc 0.5342
15:34:57.935   Training iter 250, batch loss 2.0437, batch acc 0.5396
15:34:58.433   Training iter 300, batch loss 2.0362, batch acc 0.5562
15:34:58.937   Training iter 350, batch loss 2.0286, batch acc 0.5678
15:34:59.440   Training iter 400, batch loss 2.0192, batch acc 0.5618
15:34:59.943   Training iter 450, batch loss 2.0141, batch acc 0.5748
15:35:00.455   Training iter 500, batch loss 2.0005, batch acc 0.5902
15:35:00.962   Training iter 550, batch loss 1.9982, batch acc 0.5858
15:35:01.489   Training iter 600, batch loss 1.9915, batch acc 0.5972
15:35:01.491 Training @ 4 epoch...
15:35:02.048   Training iter 50, batch loss 1.9856, batch acc 0.6084
15:35:02.592   Training iter 100, batch loss 1.9768, batch acc 0.6018
15:35:03.096   Training iter 150, batch loss 1.9698, batch acc 0.6156
15:35:03.606   Training iter 200, batch loss 1.9653, batch acc 0.6244
15:35:04.115   Training iter 250, batch loss 1.9644, batch acc 0.6138
15:35:04.622   Training iter 300, batch loss 1.9551, batch acc 0.6220
15:35:05.145   Training iter 350, batch loss 1.9539, batch acc 0.6370
15:35:05.671   Training iter 400, batch loss 1.9467, batch acc 0.6288
15:35:06.197   Training iter 450, batch loss 1.9438, batch acc 0.6388
15:35:06.707   Training iter 500, batch loss 1.9377, batch acc 0.6432
15:35:07.216   Training iter 550, batch loss 1.9314, batch acc 0.6490
15:35:07.698   Training iter 600, batch loss 1.9235, batch acc 0.6624
15:35:07.700 Training @ 5 epoch...
15:35:08.184   Training iter 50, batch loss 1.9218, batch acc 0.6512
15:35:08.669   Training iter 100, batch loss 1.9177, batch acc 0.6568
15:35:09.152   Training iter 150, batch loss 1.9114, batch acc 0.6588
15:35:09.629   Training iter 200, batch loss 1.9095, batch acc 0.6730
15:35:10.097   Training iter 250, batch loss 1.9028, batch acc 0.6752
15:35:10.569   Training iter 300, batch loss 1.9004, batch acc 0.6790
15:35:11.043   Training iter 350, batch loss 1.8950, batch acc 0.6844
15:35:11.553   Training iter 400, batch loss 1.8896, batch acc 0.6774
15:35:12.088   Training iter 450, batch loss 1.8925, batch acc 0.6750
15:35:12.613   Training iter 500, batch loss 1.8846, batch acc 0.6942
15:35:13.142   Training iter 550, batch loss 1.8851, batch acc 0.6748
15:35:13.661   Training iter 600, batch loss 1.8754, batch acc 0.6974
15:35:13.663 Testing @ 5 epoch...
15:35:13.710     Testing, total mean loss 1.87324, total acc 0.69200
15:35:13.710 Training @ 6 epoch...
15:35:14.237   Training iter 50, batch loss 1.8740, batch acc 0.6876
15:35:14.749   Training iter 100, batch loss 1.8689, batch acc 0.6898
15:35:15.265   Training iter 150, batch loss 1.8690, batch acc 0.6988
15:35:15.785   Training iter 200, batch loss 1.8629, batch acc 0.6898
15:35:16.297   Training iter 250, batch loss 1.8586, batch acc 0.6930
15:35:16.839   Training iter 300, batch loss 1.8538, batch acc 0.7086
15:35:17.415   Training iter 350, batch loss 1.8539, batch acc 0.7028
15:35:17.986   Training iter 400, batch loss 1.8538, batch acc 0.6930
15:35:18.514   Training iter 450, batch loss 1.8441, batch acc 0.7146
15:35:18.998   Training iter 500, batch loss 1.8463, batch acc 0.7078
15:35:19.486   Training iter 550, batch loss 1.8413, batch acc 0.7000
15:35:19.979   Training iter 600, batch loss 1.8400, batch acc 0.7076
15:35:19.981 Training @ 7 epoch...
15:35:20.494   Training iter 50, batch loss 1.8385, batch acc 0.7130
15:35:21.001   Training iter 100, batch loss 1.8338, batch acc 0.7122
15:35:21.510   Training iter 150, batch loss 1.8344, batch acc 0.7176
15:35:22.004   Training iter 200, batch loss 1.8274, batch acc 0.7322
15:35:22.504   Training iter 250, batch loss 1.8243, batch acc 0.7256
15:35:22.988   Training iter 300, batch loss 1.8179, batch acc 0.7274
15:35:23.474   Training iter 350, batch loss 1.8194, batch acc 0.7270
15:35:23.956   Training iter 400, batch loss 1.8164, batch acc 0.7216
15:35:24.470   Training iter 450, batch loss 1.8173, batch acc 0.7228
15:35:24.984   Training iter 500, batch loss 1.8140, batch acc 0.7268
15:35:25.511   Training iter 550, batch loss 1.8104, batch acc 0.7254
15:35:26.032   Training iter 600, batch loss 1.8125, batch acc 0.7250
15:35:26.033 Training @ 8 epoch...
15:35:26.548   Training iter 50, batch loss 1.8073, batch acc 0.7400
15:35:27.058   Training iter 100, batch loss 1.8050, batch acc 0.7364
15:35:27.574   Training iter 150, batch loss 1.7998, batch acc 0.7386
15:35:28.090   Training iter 200, batch loss 1.7996, batch acc 0.7338
15:35:28.605   Training iter 250, batch loss 1.7980, batch acc 0.7388
15:35:29.122   Training iter 300, batch loss 1.7988, batch acc 0.7482
15:35:29.641   Training iter 350, batch loss 1.7975, batch acc 0.7406
15:35:30.153   Training iter 400, batch loss 1.7904, batch acc 0.7426
15:35:30.672   Training iter 450, batch loss 1.7912, batch acc 0.7418
15:35:31.191   Training iter 500, batch loss 1.7890, batch acc 0.7436
15:35:31.710   Training iter 550, batch loss 1.7866, batch acc 0.7448
15:35:32.238   Training iter 600, batch loss 1.7848, batch acc 0.7550
15:35:32.239 Training @ 9 epoch...
15:35:32.785   Training iter 50, batch loss 1.7815, batch acc 0.7578
15:35:33.311   Training iter 100, batch loss 1.7836, batch acc 0.7534
15:35:33.819   Training iter 150, batch loss 1.7773, batch acc 0.7552
15:35:34.326   Training iter 200, batch loss 1.7779, batch acc 0.7522
15:35:34.828   Training iter 250, batch loss 1.7750, batch acc 0.7562
15:35:35.335   Training iter 300, batch loss 1.7751, batch acc 0.7538
15:35:35.843   Training iter 350, batch loss 1.7735, batch acc 0.7590
15:35:36.345   Training iter 400, batch loss 1.7749, batch acc 0.7482
15:35:36.845   Training iter 450, batch loss 1.7728, batch acc 0.7522
15:35:37.352   Training iter 500, batch loss 1.7648, batch acc 0.7658
15:35:37.866   Training iter 550, batch loss 1.7683, batch acc 0.7616
15:35:38.376   Training iter 600, batch loss 1.7633, batch acc 0.7650
15:35:38.378 Training @ 10 epoch...
15:35:38.893   Training iter 50, batch loss 1.7602, batch acc 0.7668
15:35:39.403   Training iter 100, batch loss 1.7625, batch acc 0.7582
15:35:39.912   Training iter 150, batch loss 1.7617, batch acc 0.7710
15:35:40.436   Training iter 200, batch loss 1.7568, batch acc 0.7738
15:35:40.940   Training iter 250, batch loss 1.7567, batch acc 0.7760
15:35:41.453   Training iter 300, batch loss 1.7540, batch acc 0.7682
15:35:41.962   Training iter 350, batch loss 1.7564, batch acc 0.7642
15:35:42.494   Training iter 400, batch loss 1.7542, batch acc 0.7788
15:35:43.012   Training iter 450, batch loss 1.7575, batch acc 0.7566
15:35:43.522   Training iter 500, batch loss 1.7517, batch acc 0.7790
15:35:44.029   Training iter 550, batch loss 1.7481, batch acc 0.7808
15:35:44.537   Training iter 600, batch loss 1.7514, batch acc 0.7708
15:35:44.539 Testing @ 10 epoch...
15:35:44.586     Testing, total mean loss 1.74422, total acc 0.78110
15:35:44.586 Training @ 11 epoch...
15:35:45.099   Training iter 50, batch loss 1.7488, batch acc 0.7816
15:35:45.605   Training iter 100, batch loss 1.7485, batch acc 0.7738
15:35:46.109   Training iter 150, batch loss 1.7407, batch acc 0.7926
15:35:46.613   Training iter 200, batch loss 1.7452, batch acc 0.7700
15:35:47.112   Training iter 250, batch loss 1.7434, batch acc 0.7862
15:35:47.606   Training iter 300, batch loss 1.7461, batch acc 0.7752
15:35:48.118   Training iter 350, batch loss 1.7370, batch acc 0.7890
15:35:48.623   Training iter 400, batch loss 1.7363, batch acc 0.8000
15:35:49.122   Training iter 450, batch loss 1.7347, batch acc 0.7912
15:35:49.620   Training iter 500, batch loss 1.7393, batch acc 0.7766
15:35:50.121   Training iter 550, batch loss 1.7353, batch acc 0.7936
15:35:50.623   Training iter 600, batch loss 1.7329, batch acc 0.7892
15:35:50.624 Training @ 12 epoch...
15:35:51.128   Training iter 50, batch loss 1.7298, batch acc 0.7970
15:35:51.612   Training iter 100, batch loss 1.7321, batch acc 0.7868
15:35:52.117   Training iter 150, batch loss 1.7312, batch acc 0.7930
15:35:52.600   Training iter 200, batch loss 1.7305, batch acc 0.7918
15:35:53.085   Training iter 250, batch loss 1.7302, batch acc 0.7950
15:35:53.587   Training iter 300, batch loss 1.7295, batch acc 0.7890
15:35:54.098   Training iter 350, batch loss 1.7293, batch acc 0.7868
15:35:54.604   Training iter 400, batch loss 1.7275, batch acc 0.7908
15:35:55.109   Training iter 450, batch loss 1.7240, batch acc 0.8034
15:35:55.639   Training iter 500, batch loss 1.7201, batch acc 0.8100
15:35:56.147   Training iter 550, batch loss 1.7221, batch acc 0.8058
15:35:56.661   Training iter 600, batch loss 1.7239, batch acc 0.8002
15:35:56.663 Training @ 13 epoch...
15:35:57.191   Training iter 50, batch loss 1.7230, batch acc 0.7992
15:35:57.685   Training iter 100, batch loss 1.7209, batch acc 0.8012
15:35:58.194   Training iter 150, batch loss 1.7230, batch acc 0.7980
15:35:58.705   Training iter 200, batch loss 1.7187, batch acc 0.8050
15:35:59.220   Training iter 250, batch loss 1.7143, batch acc 0.8066
15:35:59.729   Training iter 300, batch loss 1.7141, batch acc 0.8090
15:36:00.269   Training iter 350, batch loss 1.7143, batch acc 0.8124
15:36:00.795   Training iter 400, batch loss 1.7143, batch acc 0.8130
15:36:01.334   Training iter 450, batch loss 1.7128, batch acc 0.8082
15:36:01.923   Training iter 500, batch loss 1.7150, batch acc 0.8086
15:36:02.490   Training iter 550, batch loss 1.7104, batch acc 0.8090
15:36:03.047   Training iter 600, batch loss 1.7104, batch acc 0.8110
15:36:03.049 Training @ 14 epoch...
15:36:03.624   Training iter 50, batch loss 1.7089, batch acc 0.8122
15:36:04.180   Training iter 100, batch loss 1.7090, batch acc 0.8126
15:36:04.729   Training iter 150, batch loss 1.7055, batch acc 0.8190
15:36:05.282   Training iter 200, batch loss 1.7075, batch acc 0.8136
15:36:05.834   Training iter 250, batch loss 1.7095, batch acc 0.8092
15:36:06.365   Training iter 300, batch loss 1.7090, batch acc 0.8062
15:36:06.922   Training iter 350, batch loss 1.7090, batch acc 0.8160
15:36:07.488   Training iter 400, batch loss 1.7027, batch acc 0.8216
15:36:08.019   Training iter 450, batch loss 1.7027, batch acc 0.8250
15:36:08.537   Training iter 500, batch loss 1.7021, batch acc 0.8226
15:36:09.038   Training iter 550, batch loss 1.6998, batch acc 0.8274
15:36:09.559   Training iter 600, batch loss 1.7008, batch acc 0.8168
15:36:09.561 Training @ 15 epoch...
15:36:10.105   Training iter 50, batch loss 1.7022, batch acc 0.8260
15:36:10.635   Training iter 100, batch loss 1.6962, batch acc 0.8320
15:36:11.151   Training iter 150, batch loss 1.7030, batch acc 0.8112
15:36:11.675   Training iter 200, batch loss 1.7033, batch acc 0.8082
15:36:12.201   Training iter 250, batch loss 1.6970, batch acc 0.8214
15:36:12.728   Training iter 300, batch loss 1.6964, batch acc 0.8302
15:36:13.233   Training iter 350, batch loss 1.6917, batch acc 0.8326
15:36:13.755   Training iter 400, batch loss 1.6941, batch acc 0.8270
15:36:14.286   Training iter 450, batch loss 1.6956, batch acc 0.8228
15:36:14.827   Training iter 500, batch loss 1.6885, batch acc 0.8364
15:36:15.357   Training iter 550, batch loss 1.6951, batch acc 0.8276
15:36:15.885   Training iter 600, batch loss 1.6908, batch acc 0.8314
15:36:15.886 Testing @ 15 epoch...
15:36:15.933     Testing, total mean loss 1.68756, total acc 0.83090
15:36:15.933 Training @ 16 epoch...
15:36:16.449   Training iter 50, batch loss 1.6900, batch acc 0.8348
15:36:16.983   Training iter 100, batch loss 1.6924, batch acc 0.8274
15:36:17.511   Training iter 150, batch loss 1.6860, batch acc 0.8362
15:36:18.033   Training iter 200, batch loss 1.6917, batch acc 0.8290
15:36:18.548   Training iter 250, batch loss 1.6820, batch acc 0.8340
15:36:19.069   Training iter 300, batch loss 1.6877, batch acc 0.8320
15:36:19.594   Training iter 350, batch loss 1.6891, batch acc 0.8252
15:36:20.128   Training iter 400, batch loss 1.6836, batch acc 0.8324
15:36:20.640   Training iter 450, batch loss 1.6896, batch acc 0.8312
15:36:21.138   Training iter 500, batch loss 1.6899, batch acc 0.8270
15:36:21.631   Training iter 550, batch loss 1.6833, batch acc 0.8368
15:36:22.112   Training iter 600, batch loss 1.6861, batch acc 0.8348
15:36:22.114 Training @ 17 epoch...
15:36:22.588   Training iter 50, batch loss 1.6844, batch acc 0.8314
15:36:23.073   Training iter 100, batch loss 1.6858, batch acc 0.8324
15:36:23.556   Training iter 150, batch loss 1.6778, batch acc 0.8408
15:36:24.042   Training iter 200, batch loss 1.6810, batch acc 0.8334
15:36:24.524   Training iter 250, batch loss 1.6828, batch acc 0.8422
15:36:25.004   Training iter 300, batch loss 1.6831, batch acc 0.8308
15:36:25.507   Training iter 350, batch loss 1.6763, batch acc 0.8518
15:36:26.035   Training iter 400, batch loss 1.6761, batch acc 0.8430
15:36:26.554   Training iter 450, batch loss 1.6791, batch acc 0.8358
15:36:27.076   Training iter 500, batch loss 1.6793, batch acc 0.8350
15:36:27.584   Training iter 550, batch loss 1.6763, batch acc 0.8456
15:36:28.099   Training iter 600, batch loss 1.6752, batch acc 0.8340
15:36:28.101 Training @ 18 epoch...
15:36:28.606   Training iter 50, batch loss 1.6739, batch acc 0.8418
15:36:29.112   Training iter 100, batch loss 1.6753, batch acc 0.8342
15:36:29.630   Training iter 150, batch loss 1.6781, batch acc 0.8344
15:36:30.150   Training iter 200, batch loss 1.6686, batch acc 0.8452
15:36:30.691   Training iter 250, batch loss 1.6730, batch acc 0.8426
15:36:31.204   Training iter 300, batch loss 1.6751, batch acc 0.8388
15:36:31.730   Training iter 350, batch loss 1.6755, batch acc 0.8414
15:36:32.249   Training iter 400, batch loss 1.6690, batch acc 0.8504
15:36:32.754   Training iter 450, batch loss 1.6726, batch acc 0.8414
15:36:33.270   Training iter 500, batch loss 1.6693, batch acc 0.8456
15:36:33.776   Training iter 550, batch loss 1.6686, batch acc 0.8478
15:36:34.273   Training iter 600, batch loss 1.6721, batch acc 0.8352
15:36:34.274 Training @ 19 epoch...
15:36:34.783   Training iter 50, batch loss 1.6666, batch acc 0.8486
15:36:35.299   Training iter 100, batch loss 1.6703, batch acc 0.8422
15:36:35.805   Training iter 150, batch loss 1.6664, batch acc 0.8482
15:36:36.312   Training iter 200, batch loss 1.6701, batch acc 0.8346
15:36:36.793   Training iter 250, batch loss 1.6666, batch acc 0.8420
15:36:37.254   Training iter 300, batch loss 1.6656, batch acc 0.8434
15:36:37.721   Training iter 350, batch loss 1.6649, batch acc 0.8484
15:36:38.208   Training iter 400, batch loss 1.6692, batch acc 0.8418
15:36:38.693   Training iter 450, batch loss 1.6591, batch acc 0.8512
15:36:39.168   Training iter 500, batch loss 1.6631, batch acc 0.8486
15:36:39.653   Training iter 550, batch loss 1.6659, batch acc 0.8500
15:36:40.158   Training iter 600, batch loss 1.6645, batch acc 0.8416
15:36:40.160 Training @ 20 epoch...
15:36:40.688   Training iter 50, batch loss 1.6627, batch acc 0.8520
15:36:41.194   Training iter 100, batch loss 1.6627, batch acc 0.8464
15:36:41.704   Training iter 150, batch loss 1.6592, batch acc 0.8470
15:36:42.218   Training iter 200, batch loss 1.6583, batch acc 0.8500
15:36:42.740   Training iter 250, batch loss 1.6586, batch acc 0.8492
15:36:43.264   Training iter 300, batch loss 1.6624, batch acc 0.8408
15:36:43.765   Training iter 350, batch loss 1.6608, batch acc 0.8420
15:36:44.265   Training iter 400, batch loss 1.6588, batch acc 0.8516
15:36:44.756   Training iter 450, batch loss 1.6550, batch acc 0.8520
15:36:45.260   Training iter 500, batch loss 1.6619, batch acc 0.8436
15:36:45.765   Training iter 550, batch loss 1.6624, batch acc 0.8482
15:36:46.330   Training iter 600, batch loss 1.6593, batch acc 0.8482
15:36:46.332 Testing @ 20 epoch...
15:36:46.379     Testing, total mean loss 1.65300, total acc 0.85110
15:36:46.379 Training @ 21 epoch...
15:36:46.962   Training iter 50, batch loss 1.6583, batch acc 0.8460
15:36:47.530   Training iter 100, batch loss 1.6532, batch acc 0.8498
15:36:48.062   Training iter 150, batch loss 1.6533, batch acc 0.8560
15:36:48.600   Training iter 200, batch loss 1.6581, batch acc 0.8510
15:36:49.135   Training iter 250, batch loss 1.6564, batch acc 0.8436
15:36:49.678   Training iter 300, batch loss 1.6556, batch acc 0.8446
15:36:50.222   Training iter 350, batch loss 1.6554, batch acc 0.8426
15:36:50.752   Training iter 400, batch loss 1.6582, batch acc 0.8488
15:36:51.287   Training iter 450, batch loss 1.6560, batch acc 0.8418
15:36:51.826   Training iter 500, batch loss 1.6494, batch acc 0.8578
15:36:52.384   Training iter 550, batch loss 1.6552, batch acc 0.8540
15:36:52.907   Training iter 600, batch loss 1.6498, batch acc 0.8486
15:36:52.908 Training @ 22 epoch...
15:36:53.434   Training iter 50, batch loss 1.6515, batch acc 0.8536
15:36:53.954   Training iter 100, batch loss 1.6545, batch acc 0.8426
15:36:54.474   Training iter 150, batch loss 1.6505, batch acc 0.8466
15:36:54.994   Training iter 200, batch loss 1.6485, batch acc 0.8510
15:36:55.514   Training iter 250, batch loss 1.6529, batch acc 0.8428
15:36:56.034   Training iter 300, batch loss 1.6506, batch acc 0.8446
15:36:56.552   Training iter 350, batch loss 1.6486, batch acc 0.8410
15:36:57.060   Training iter 400, batch loss 1.6478, batch acc 0.8482
15:36:57.576   Training iter 450, batch loss 1.6529, batch acc 0.8430
15:36:58.097   Training iter 500, batch loss 1.6460, batch acc 0.8576
15:36:58.603   Training iter 550, batch loss 1.6501, batch acc 0.8464
15:36:59.103   Training iter 600, batch loss 1.6486, batch acc 0.8448
15:36:59.104 Training @ 23 epoch...
15:36:59.615   Training iter 50, batch loss 1.6457, batch acc 0.8466
15:37:00.124   Training iter 100, batch loss 1.6493, batch acc 0.8462
15:37:00.633   Training iter 150, batch loss 1.6506, batch acc 0.8480
15:37:01.139   Training iter 200, batch loss 1.6482, batch acc 0.8438
15:37:01.655   Training iter 250, batch loss 1.6490, batch acc 0.8460
15:37:02.210   Training iter 300, batch loss 1.6426, batch acc 0.8518
15:37:02.748   Training iter 350, batch loss 1.6434, batch acc 0.8554
15:37:03.280   Training iter 400, batch loss 1.6443, batch acc 0.8462
15:37:03.799   Training iter 450, batch loss 1.6468, batch acc 0.8432
15:37:04.332   Training iter 500, batch loss 1.6458, batch acc 0.8454
15:37:04.853   Training iter 550, batch loss 1.6411, batch acc 0.8536
15:37:05.378   Training iter 600, batch loss 1.6455, batch acc 0.8430
15:37:05.380 Training @ 24 epoch...
15:37:05.911   Training iter 50, batch loss 1.6380, batch acc 0.8568
15:37:06.447   Training iter 100, batch loss 1.6498, batch acc 0.8480
15:37:06.989   Training iter 150, batch loss 1.6433, batch acc 0.8408
15:37:07.498   Training iter 200, batch loss 1.6403, batch acc 0.8502
15:37:08.005   Training iter 250, batch loss 1.6443, batch acc 0.8406
15:37:08.497   Training iter 300, batch loss 1.6402, batch acc 0.8470
15:37:08.973   Training iter 350, batch loss 1.6419, batch acc 0.8456
15:37:09.457   Training iter 400, batch loss 1.6422, batch acc 0.8484
15:37:09.938   Training iter 450, batch loss 1.6428, batch acc 0.8442
15:37:10.432   Training iter 500, batch loss 1.6435, batch acc 0.8460
15:37:10.914   Training iter 550, batch loss 1.6404, batch acc 0.8436
15:37:11.416   Training iter 600, batch loss 1.6404, batch acc 0.8464
15:37:11.418 Training @ 25 epoch...
15:37:11.922   Training iter 50, batch loss 1.6392, batch acc 0.8450
15:37:12.425   Training iter 100, batch loss 1.6357, batch acc 0.8530
15:37:12.922   Training iter 150, batch loss 1.6402, batch acc 0.8408
15:37:13.424   Training iter 200, batch loss 1.6407, batch acc 0.8396
15:37:13.935   Training iter 250, batch loss 1.6393, batch acc 0.8486
15:37:14.437   Training iter 300, batch loss 1.6420, batch acc 0.8396
15:37:14.946   Training iter 350, batch loss 1.6372, batch acc 0.8444
15:37:15.445   Training iter 400, batch loss 1.6384, batch acc 0.8436
15:37:15.940   Training iter 450, batch loss 1.6371, batch acc 0.8432
15:37:16.439   Training iter 500, batch loss 1.6383, batch acc 0.8416
15:37:16.949   Training iter 550, batch loss 1.6403, batch acc 0.8368
15:37:17.448   Training iter 600, batch loss 1.6377, batch acc 0.8460
15:37:17.450 Testing @ 25 epoch...
15:37:17.496     Testing, total mean loss 1.63316, total acc 0.84500
15:37:17.496 Training @ 26 epoch...
15:37:17.990   Training iter 50, batch loss 1.6390, batch acc 0.8420
15:37:18.494   Training iter 100, batch loss 1.6400, batch acc 0.8332
15:37:18.991   Training iter 150, batch loss 1.6294, batch acc 0.8534
15:37:19.489   Training iter 200, batch loss 1.6319, batch acc 0.8548
15:37:19.996   Training iter 250, batch loss 1.6347, batch acc 0.8376
15:37:20.495   Training iter 300, batch loss 1.6381, batch acc 0.8404
15:37:21.004   Training iter 350, batch loss 1.6337, batch acc 0.8442
15:37:21.504   Training iter 400, batch loss 1.6344, batch acc 0.8446
15:37:21.987   Training iter 450, batch loss 1.6344, batch acc 0.8442
15:37:22.461   Training iter 500, batch loss 1.6390, batch acc 0.8316
15:37:22.925   Training iter 550, batch loss 1.6396, batch acc 0.8338
15:37:23.403   Training iter 600, batch loss 1.6349, batch acc 0.8404
15:37:23.404 Training @ 27 epoch...
15:37:23.873   Training iter 50, batch loss 1.6310, batch acc 0.8482
15:37:24.357   Training iter 100, batch loss 1.6363, batch acc 0.8374
15:37:24.814   Training iter 150, batch loss 1.6407, batch acc 0.8310
15:37:25.314   Training iter 200, batch loss 1.6297, batch acc 0.8404
15:37:25.815   Training iter 250, batch loss 1.6345, batch acc 0.8410
15:37:26.315   Training iter 300, batch loss 1.6360, batch acc 0.8336
15:37:26.815   Training iter 350, batch loss 1.6350, batch acc 0.8394
15:37:27.327   Training iter 400, batch loss 1.6325, batch acc 0.8396
15:37:27.850   Training iter 450, batch loss 1.6265, batch acc 0.8438
15:37:28.365   Training iter 500, batch loss 1.6284, batch acc 0.8460
15:37:28.873   Training iter 550, batch loss 1.6280, batch acc 0.8422
15:37:29.390   Training iter 600, batch loss 1.6360, batch acc 0.8348
15:37:29.392 Training @ 28 epoch...
15:37:29.898   Training iter 50, batch loss 1.6326, batch acc 0.8418
15:37:30.374   Training iter 100, batch loss 1.6322, batch acc 0.8348
15:37:30.841   Training iter 150, batch loss 1.6308, batch acc 0.8418
15:37:31.313   Training iter 200, batch loss 1.6278, batch acc 0.8374
15:37:31.806   Training iter 250, batch loss 1.6306, batch acc 0.8326
15:37:32.291   Training iter 300, batch loss 1.6297, batch acc 0.8350
15:37:32.760   Training iter 350, batch loss 1.6317, batch acc 0.8362
15:37:33.242   Training iter 400, batch loss 1.6288, batch acc 0.8370
15:37:33.721   Training iter 450, batch loss 1.6300, batch acc 0.8374
15:37:34.203   Training iter 500, batch loss 1.6290, batch acc 0.8384
15:37:34.695   Training iter 550, batch loss 1.6286, batch acc 0.8416
15:37:35.203   Training iter 600, batch loss 1.6310, batch acc 0.8352
15:37:35.204 Training @ 29 epoch...
15:37:35.721   Training iter 50, batch loss 1.6289, batch acc 0.8340
15:37:36.229   Training iter 100, batch loss 1.6291, batch acc 0.8408
15:37:36.734   Training iter 150, batch loss 1.6283, batch acc 0.8298
15:37:37.244   Training iter 200, batch loss 1.6307, batch acc 0.8314
15:37:37.750   Training iter 250, batch loss 1.6288, batch acc 0.8314
15:37:38.271   Training iter 300, batch loss 1.6273, batch acc 0.8320
15:37:38.780   Training iter 350, batch loss 1.6241, batch acc 0.8458
15:37:39.293   Training iter 400, batch loss 1.6275, batch acc 0.8406
15:37:39.784   Training iter 450, batch loss 1.6282, batch acc 0.8390
15:37:40.273   Training iter 500, batch loss 1.6276, batch acc 0.8394
15:37:40.767   Training iter 550, batch loss 1.6275, batch acc 0.8310
15:37:41.249   Training iter 600, batch loss 1.6250, batch acc 0.8270
15:37:41.251 Training @ 30 epoch...
15:37:41.738   Training iter 50, batch loss 1.6245, batch acc 0.8408
15:37:42.250   Training iter 100, batch loss 1.6260, batch acc 0.8356
15:37:42.810   Training iter 150, batch loss 1.6257, batch acc 0.8394
15:37:43.370   Training iter 200, batch loss 1.6245, batch acc 0.8334
15:37:43.906   Training iter 250, batch loss 1.6282, batch acc 0.8348
15:37:44.373   Training iter 300, batch loss 1.6253, batch acc 0.8358
15:37:44.832   Training iter 350, batch loss 1.6216, batch acc 0.8410
15:37:45.301   Training iter 400, batch loss 1.6253, batch acc 0.8256
15:37:45.779   Training iter 450, batch loss 1.6256, batch acc 0.8408
15:37:46.294   Training iter 500, batch loss 1.6283, batch acc 0.8292
15:37:46.798   Training iter 550, batch loss 1.6251, batch acc 0.8348
15:37:47.284   Training iter 600, batch loss 1.6253, batch acc 0.8368
15:37:47.286 Testing @ 30 epoch...
15:37:47.333     Testing, total mean loss 1.62049, total acc 0.83250
15:37:47.333 Training @ 31 epoch...
15:37:47.838   Training iter 50, batch loss 1.6264, batch acc 0.8300
15:37:48.344   Training iter 100, batch loss 1.6254, batch acc 0.8326
15:37:48.838   Training iter 150, batch loss 1.6228, batch acc 0.8400
15:37:49.347   Training iter 200, batch loss 1.6260, batch acc 0.8262
15:37:49.859   Training iter 250, batch loss 1.6266, batch acc 0.8306
15:37:50.368   Training iter 300, batch loss 1.6260, batch acc 0.8308
15:37:50.883   Training iter 350, batch loss 1.6220, batch acc 0.8316
15:37:51.406   Training iter 400, batch loss 1.6212, batch acc 0.8334
15:37:51.909   Training iter 450, batch loss 1.6219, batch acc 0.8372
15:37:52.420   Training iter 500, batch loss 1.6170, batch acc 0.8424
15:37:52.918   Training iter 550, batch loss 1.6222, batch acc 0.8332
15:37:53.414   Training iter 600, batch loss 1.6214, batch acc 0.8244
15:37:53.416 Training @ 32 epoch...
15:37:53.921   Training iter 50, batch loss 1.6253, batch acc 0.8276
15:37:54.430   Training iter 100, batch loss 1.6205, batch acc 0.8354
15:37:54.940   Training iter 150, batch loss 1.6231, batch acc 0.8356
15:37:55.456   Training iter 200, batch loss 1.6216, batch acc 0.8294
15:37:55.968   Training iter 250, batch loss 1.6212, batch acc 0.8332
15:37:56.482   Training iter 300, batch loss 1.6257, batch acc 0.8296
15:37:56.989   Training iter 350, batch loss 1.6205, batch acc 0.8388
15:37:57.528   Training iter 400, batch loss 1.6185, batch acc 0.8330
15:37:58.040   Training iter 450, batch loss 1.6178, batch acc 0.8338
15:37:58.574   Training iter 500, batch loss 1.6172, batch acc 0.8418
15:37:59.087   Training iter 550, batch loss 1.6224, batch acc 0.8254
15:37:59.595   Training iter 600, batch loss 1.6201, batch acc 0.8288
15:37:59.597 Training @ 33 epoch...
15:38:00.106   Training iter 50, batch loss 1.6167, batch acc 0.8354
15:38:00.628   Training iter 100, batch loss 1.6160, batch acc 0.8372
15:38:01.143   Training iter 150, batch loss 1.6206, batch acc 0.8308
15:38:01.670   Training iter 200, batch loss 1.6261, batch acc 0.8262
15:38:02.217   Training iter 250, batch loss 1.6232, batch acc 0.8344
15:38:02.757   Training iter 300, batch loss 1.6207, batch acc 0.8278
15:38:03.304   Training iter 350, batch loss 1.6170, batch acc 0.8334
15:38:03.823   Training iter 400, batch loss 1.6160, batch acc 0.8340
15:38:04.328   Training iter 450, batch loss 1.6161, batch acc 0.8326
15:38:04.823   Training iter 500, batch loss 1.6170, batch acc 0.8294
15:38:05.329   Training iter 550, batch loss 1.6208, batch acc 0.8218
15:38:05.842   Training iter 600, batch loss 1.6205, batch acc 0.8310
15:38:05.844 Training @ 34 epoch...
15:38:06.379   Training iter 50, batch loss 1.6204, batch acc 0.8260
15:38:06.912   Training iter 100, batch loss 1.6174, batch acc 0.8376
15:38:07.426   Training iter 150, batch loss 1.6192, batch acc 0.8260
15:38:07.952   Training iter 200, batch loss 1.6174, batch acc 0.8252
15:38:08.503   Training iter 250, batch loss 1.6151, batch acc 0.8344
15:38:09.043   Training iter 300, batch loss 1.6166, batch acc 0.8318
15:38:09.578   Training iter 350, batch loss 1.6198, batch acc 0.8182
15:38:10.109   Training iter 400, batch loss 1.6132, batch acc 0.8350
15:38:10.646   Training iter 450, batch loss 1.6185, batch acc 0.8218
15:38:11.177   Training iter 500, batch loss 1.6187, batch acc 0.8238
15:38:11.703   Training iter 550, batch loss 1.6164, batch acc 0.8302
15:38:12.214   Training iter 600, batch loss 1.6165, batch acc 0.8400
15:38:12.216 Training @ 35 epoch...
15:38:12.726   Training iter 50, batch loss 1.6152, batch acc 0.8376
15:38:13.227   Training iter 100, batch loss 1.6123, batch acc 0.8370
15:38:13.737   Training iter 150, batch loss 1.6160, batch acc 0.8278
15:38:14.236   Training iter 200, batch loss 1.6100, batch acc 0.8356
15:38:14.744   Training iter 250, batch loss 1.6191, batch acc 0.8278
15:38:15.245   Training iter 300, batch loss 1.6205, batch acc 0.8178
15:38:15.756   Training iter 350, batch loss 1.6216, batch acc 0.8212
15:38:16.273   Training iter 400, batch loss 1.6150, batch acc 0.8278
15:38:16.768   Training iter 450, batch loss 1.6161, batch acc 0.8288
15:38:17.274   Training iter 500, batch loss 1.6130, batch acc 0.8358
15:38:17.769   Training iter 550, batch loss 1.6172, batch acc 0.8288
15:38:18.273   Training iter 600, batch loss 1.6122, batch acc 0.8320
15:38:18.274 Testing @ 35 epoch...
15:38:18.321     Testing, total mean loss 1.61166, total acc 0.82510
15:38:18.321 Training @ 36 epoch...
15:38:18.815   Training iter 50, batch loss 1.6135, batch acc 0.8324
15:38:19.329   Training iter 100, batch loss 1.6181, batch acc 0.8270
15:38:19.843   Training iter 150, batch loss 1.6163, batch acc 0.8278
15:38:20.353   Training iter 200, batch loss 1.6139, batch acc 0.8230
15:38:20.858   Training iter 250, batch loss 1.6158, batch acc 0.8222
15:38:21.367   Training iter 300, batch loss 1.6121, batch acc 0.8302
15:38:21.865   Training iter 350, batch loss 1.6148, batch acc 0.8272
15:38:22.344   Training iter 400, batch loss 1.6148, batch acc 0.8294
15:38:22.815   Training iter 450, batch loss 1.6115, batch acc 0.8280
15:38:23.288   Training iter 500, batch loss 1.6151, batch acc 0.8294
15:38:23.760   Training iter 550, batch loss 1.6116, batch acc 0.8288
15:38:24.251   Training iter 600, batch loss 1.6117, batch acc 0.8292
15:38:24.253 Training @ 37 epoch...
15:38:24.725   Training iter 50, batch loss 1.6154, batch acc 0.8208
15:38:25.209   Training iter 100, batch loss 1.6085, batch acc 0.8296
15:38:25.695   Training iter 150, batch loss 1.6132, batch acc 0.8286
15:38:26.192   Training iter 200, batch loss 1.6154, batch acc 0.8282
15:38:26.680   Training iter 250, batch loss 1.6146, batch acc 0.8320
15:38:27.166   Training iter 300, batch loss 1.6161, batch acc 0.8252
15:38:27.664   Training iter 350, batch loss 1.6090, batch acc 0.8358
15:38:28.195   Training iter 400, batch loss 1.6119, batch acc 0.8284
15:38:28.718   Training iter 450, batch loss 1.6165, batch acc 0.8236
15:38:29.213   Training iter 500, batch loss 1.6104, batch acc 0.8294
15:38:29.702   Training iter 550, batch loss 1.6106, batch acc 0.8254
15:38:30.211   Training iter 600, batch loss 1.6095, batch acc 0.8310
15:38:30.212 Training @ 38 epoch...
15:38:30.714   Training iter 50, batch loss 1.6109, batch acc 0.8318
15:38:31.218   Training iter 100, batch loss 1.6146, batch acc 0.8218
15:38:31.776   Training iter 150, batch loss 1.6110, batch acc 0.8342
15:38:32.349   Training iter 200, batch loss 1.6106, batch acc 0.8278
15:38:32.917   Training iter 250, batch loss 1.6089, batch acc 0.8406
15:38:33.471   Training iter 300, batch loss 1.6126, batch acc 0.8184
15:38:33.956   Training iter 350, batch loss 1.6064, batch acc 0.8340
15:38:34.481   Training iter 400, batch loss 1.6118, batch acc 0.8266
15:38:34.976   Training iter 450, batch loss 1.6107, batch acc 0.8226
15:38:35.472   Training iter 500, batch loss 1.6097, batch acc 0.8348
15:38:35.966   Training iter 550, batch loss 1.6149, batch acc 0.8232
15:38:36.472   Training iter 600, batch loss 1.6115, batch acc 0.8300
15:38:36.474 Training @ 39 epoch...
15:38:36.977   Training iter 50, batch loss 1.6130, batch acc 0.8184
15:38:37.477   Training iter 100, batch loss 1.6099, batch acc 0.8334
15:38:37.985   Training iter 150, batch loss 1.6121, batch acc 0.8302
15:38:38.500   Training iter 200, batch loss 1.6072, batch acc 0.8270
15:38:39.004   Training iter 250, batch loss 1.6110, batch acc 0.8292
15:38:39.522   Training iter 300, batch loss 1.6044, batch acc 0.8364
15:38:40.032   Training iter 350, batch loss 1.6085, batch acc 0.8256
15:38:40.552   Training iter 400, batch loss 1.6091, batch acc 0.8286
15:38:41.070   Training iter 450, batch loss 1.6104, batch acc 0.8224
15:38:41.577   Training iter 500, batch loss 1.6072, batch acc 0.8338
15:38:42.092   Training iter 550, batch loss 1.6115, batch acc 0.8264
15:38:42.606   Training iter 600, batch loss 1.6138, batch acc 0.8156
15:38:42.608 Training @ 40 epoch...
15:38:43.129   Training iter 50, batch loss 1.6111, batch acc 0.8220
15:38:43.657   Training iter 100, batch loss 1.6107, batch acc 0.8282
15:38:44.168   Training iter 150, batch loss 1.6108, batch acc 0.8332
15:38:44.663   Training iter 200, batch loss 1.6077, batch acc 0.8310
15:38:45.149   Training iter 250, batch loss 1.6109, batch acc 0.8172
15:38:45.653   Training iter 300, batch loss 1.6066, batch acc 0.8268
15:38:46.170   Training iter 350, batch loss 1.6100, batch acc 0.8212
15:38:46.686   Training iter 400, batch loss 1.6038, batch acc 0.8326
15:38:47.204   Training iter 450, batch loss 1.6025, batch acc 0.8332
15:38:47.720   Training iter 500, batch loss 1.6104, batch acc 0.8320
15:38:48.235   Training iter 550, batch loss 1.6092, batch acc 0.8214
15:38:48.751   Training iter 600, batch loss 1.6093, batch acc 0.8304
15:38:48.753 Testing @ 40 epoch...
15:38:48.799     Testing, total mean loss 1.60521, total acc 0.82550
15:38:48.799 Training @ 41 epoch...
15:38:49.311   Training iter 50, batch loss 1.6018, batch acc 0.8322
15:38:49.823   Training iter 100, batch loss 1.6108, batch acc 0.8216
15:38:50.308   Training iter 150, batch loss 1.6073, batch acc 0.8224
15:38:50.777   Training iter 200, batch loss 1.6067, batch acc 0.8294
15:38:51.276   Training iter 250, batch loss 1.6054, batch acc 0.8332
15:38:51.753   Training iter 300, batch loss 1.6101, batch acc 0.8280
15:38:52.262   Training iter 350, batch loss 1.6086, batch acc 0.8328
15:38:52.741   Training iter 400, batch loss 1.6058, batch acc 0.8290
15:38:53.215   Training iter 450, batch loss 1.6118, batch acc 0.8252
15:38:53.701   Training iter 500, batch loss 1.6093, batch acc 0.8218
15:38:54.206   Training iter 550, batch loss 1.6048, batch acc 0.8338
15:38:54.714   Training iter 600, batch loss 1.6064, batch acc 0.8226
15:38:54.715 Training @ 42 epoch...
15:38:55.222   Training iter 50, batch loss 1.6056, batch acc 0.8238
15:38:55.730   Training iter 100, batch loss 1.6048, batch acc 0.8276
15:38:56.243   Training iter 150, batch loss 1.6075, batch acc 0.8260
15:38:56.752   Training iter 200, batch loss 1.6087, batch acc 0.8250
15:38:57.273   Training iter 250, batch loss 1.6048, batch acc 0.8266
15:38:57.779   Training iter 300, batch loss 1.6043, batch acc 0.8356
15:38:58.295   Training iter 350, batch loss 1.6104, batch acc 0.8262
15:38:58.804   Training iter 400, batch loss 1.6065, batch acc 0.8294
15:38:59.319   Training iter 450, batch loss 1.6068, batch acc 0.8256
15:38:59.831   Training iter 500, batch loss 1.6080, batch acc 0.8146
15:39:00.344   Training iter 550, batch loss 1.6062, batch acc 0.8262
15:39:00.856   Training iter 600, batch loss 1.6021, batch acc 0.8288
15:39:00.857 Training @ 43 epoch...
15:39:01.398   Training iter 50, batch loss 1.6054, batch acc 0.8260
15:39:01.958   Training iter 100, batch loss 1.6085, batch acc 0.8234
15:39:02.520   Training iter 150, batch loss 1.6062, batch acc 0.8274
15:39:03.074   Training iter 200, batch loss 1.6057, batch acc 0.8218
15:39:03.634   Training iter 250, batch loss 1.6036, batch acc 0.8282
15:39:04.179   Training iter 300, batch loss 1.6037, batch acc 0.8268
15:39:04.723   Training iter 350, batch loss 1.6078, batch acc 0.8272
15:39:05.265   Training iter 400, batch loss 1.6054, batch acc 0.8336
15:39:05.815   Training iter 450, batch loss 1.6053, batch acc 0.8248
15:39:06.381   Training iter 500, batch loss 1.6017, batch acc 0.8276
15:39:06.929   Training iter 550, batch loss 1.6062, batch acc 0.8200
15:39:07.452   Training iter 600, batch loss 1.6031, batch acc 0.8306
15:39:07.454 Training @ 44 epoch...
15:39:07.977   Training iter 50, batch loss 1.6071, batch acc 0.8254
15:39:08.503   Training iter 100, batch loss 1.6085, batch acc 0.8238
15:39:09.036   Training iter 150, batch loss 1.6043, batch acc 0.8312
15:39:09.565   Training iter 200, batch loss 1.6028, batch acc 0.8274
15:39:10.098   Training iter 250, batch loss 1.6031, batch acc 0.8286
15:39:10.633   Training iter 300, batch loss 1.6000, batch acc 0.8222
15:39:11.157   Training iter 350, batch loss 1.6066, batch acc 0.8218
15:39:11.683   Training iter 400, batch loss 1.6053, batch acc 0.8282
15:39:12.222   Training iter 450, batch loss 1.6025, batch acc 0.8324
15:39:12.746   Training iter 500, batch loss 1.6020, batch acc 0.8344
15:39:13.276   Training iter 550, batch loss 1.6028, batch acc 0.8274
15:39:13.784   Training iter 600, batch loss 1.6054, batch acc 0.8156
15:39:13.785 Training @ 45 epoch...
15:39:14.328   Training iter 50, batch loss 1.6007, batch acc 0.8338
15:39:14.859   Training iter 100, batch loss 1.6040, batch acc 0.8276
15:39:15.366   Training iter 150, batch loss 1.6045, batch acc 0.8208
15:39:15.887   Training iter 200, batch loss 1.6001, batch acc 0.8310
15:39:16.401   Training iter 250, batch loss 1.6034, batch acc 0.8278
15:39:16.919   Training iter 300, batch loss 1.6069, batch acc 0.8260
15:39:17.428   Training iter 350, batch loss 1.5999, batch acc 0.8250
15:39:17.946   Training iter 400, batch loss 1.6053, batch acc 0.8250
15:39:18.466   Training iter 450, batch loss 1.6016, batch acc 0.8234
15:39:18.988   Training iter 500, batch loss 1.6040, batch acc 0.8202
15:39:19.518   Training iter 550, batch loss 1.6024, batch acc 0.8284
15:39:20.057   Training iter 600, batch loss 1.6064, batch acc 0.8258
15:39:20.059 Testing @ 45 epoch...
15:39:20.105     Testing, total mean loss 1.60030, total acc 0.82550
15:39:20.105 Training @ 46 epoch...
15:39:20.647   Training iter 50, batch loss 1.6059, batch acc 0.8214
15:39:21.168   Training iter 100, batch loss 1.6070, batch acc 0.8150
15:39:21.674   Training iter 150, batch loss 1.6004, batch acc 0.8336
15:39:22.205   Training iter 200, batch loss 1.6024, batch acc 0.8306
15:39:22.739   Training iter 250, batch loss 1.5941, batch acc 0.8330
15:39:23.247   Training iter 300, batch loss 1.6052, batch acc 0.8236
15:39:23.773   Training iter 350, batch loss 1.6025, batch acc 0.8268
15:39:24.300   Training iter 400, batch loss 1.5969, batch acc 0.8328
15:39:24.832   Training iter 450, batch loss 1.5993, batch acc 0.8340
15:39:25.371   Training iter 500, batch loss 1.6019, batch acc 0.8288
15:39:25.891   Training iter 550, batch loss 1.6071, batch acc 0.8242
15:39:26.452   Training iter 600, batch loss 1.6056, batch acc 0.8234
15:39:26.454 Training @ 47 epoch...
15:39:27.021   Training iter 50, batch loss 1.6012, batch acc 0.8306
15:39:27.585   Training iter 100, batch loss 1.6032, batch acc 0.8262
15:39:28.141   Training iter 150, batch loss 1.6046, batch acc 0.8254
15:39:28.697   Training iter 200, batch loss 1.6016, batch acc 0.8268
15:39:29.275   Training iter 250, batch loss 1.6001, batch acc 0.8320
15:39:29.865   Training iter 300, batch loss 1.5967, batch acc 0.8388
15:39:30.453   Training iter 350, batch loss 1.6095, batch acc 0.8152
15:39:30.998   Training iter 400, batch loss 1.5981, batch acc 0.8282
15:39:31.500   Training iter 450, batch loss 1.6019, batch acc 0.8264
15:39:32.013   Training iter 500, batch loss 1.5990, batch acc 0.8258
15:39:32.496   Training iter 550, batch loss 1.6006, batch acc 0.8266
15:39:32.986   Training iter 600, batch loss 1.6013, batch acc 0.8212
15:39:32.987 Training @ 48 epoch...
15:39:33.506   Training iter 50, batch loss 1.5967, batch acc 0.8300
15:39:34.005   Training iter 100, batch loss 1.6027, batch acc 0.8256
15:39:34.505   Training iter 150, batch loss 1.6014, batch acc 0.8288
15:39:35.011   Training iter 200, batch loss 1.6040, batch acc 0.8226
15:39:35.504   Training iter 250, batch loss 1.5997, batch acc 0.8250
15:39:35.998   Training iter 300, batch loss 1.5988, batch acc 0.8324
15:39:36.490   Training iter 350, batch loss 1.6001, batch acc 0.8264
15:39:36.984   Training iter 400, batch loss 1.6003, batch acc 0.8326
15:39:37.479   Training iter 450, batch loss 1.5994, batch acc 0.8240
15:39:37.977   Training iter 500, batch loss 1.6013, batch acc 0.8258
15:39:38.482   Training iter 550, batch loss 1.6011, batch acc 0.8244
15:39:38.981   Training iter 600, batch loss 1.6025, batch acc 0.8226
15:39:38.982 Training @ 49 epoch...
15:39:39.481   Training iter 50, batch loss 1.6009, batch acc 0.8272
15:39:39.977   Training iter 100, batch loss 1.6002, batch acc 0.8294
15:39:40.487   Training iter 150, batch loss 1.6000, batch acc 0.8242
15:39:40.986   Training iter 200, batch loss 1.6021, batch acc 0.8192
15:39:41.492   Training iter 250, batch loss 1.6022, batch acc 0.8272
15:39:41.998   Training iter 300, batch loss 1.6003, batch acc 0.8354
15:39:42.504   Training iter 350, batch loss 1.6012, batch acc 0.8328
15:39:43.011   Training iter 400, batch loss 1.5982, batch acc 0.8268
15:39:43.507   Training iter 450, batch loss 1.6013, batch acc 0.8164
15:39:43.986   Training iter 500, batch loss 1.5959, batch acc 0.8248
15:39:44.456   Training iter 550, batch loss 1.5978, batch acc 0.8312
15:39:44.928   Training iter 600, batch loss 1.5984, batch acc 0.8286
15:39:44.929 Training @ 50 epoch...
15:39:45.414   Training iter 50, batch loss 1.5979, batch acc 0.8274
15:39:45.884   Training iter 100, batch loss 1.5962, batch acc 0.8308
15:39:46.364   Training iter 150, batch loss 1.6002, batch acc 0.8200
15:39:46.831   Training iter 200, batch loss 1.6028, batch acc 0.8200
15:39:47.318   Training iter 250, batch loss 1.6020, batch acc 0.8182
15:39:47.803   Training iter 300, batch loss 1.5977, batch acc 0.8242
15:39:48.297   Training iter 350, batch loss 1.5955, batch acc 0.8420
15:39:48.787   Training iter 400, batch loss 1.5968, batch acc 0.8332
15:39:49.283   Training iter 450, batch loss 1.6003, batch acc 0.8302
15:39:49.783   Training iter 500, batch loss 1.6034, batch acc 0.8198
15:39:50.282   Training iter 550, batch loss 1.5994, batch acc 0.8284
15:39:50.772   Training iter 600, batch loss 1.5969, batch acc 0.8310
15:39:50.774 Testing @ 50 epoch...
15:39:50.820     Testing, total mean loss 1.59629, total acc 0.82370
15:39:50.820 Training @ 51 epoch...
15:39:51.315   Training iter 50, batch loss 1.6013, batch acc 0.8230
15:39:51.834   Training iter 100, batch loss 1.5988, batch acc 0.8268
15:39:52.345   Training iter 150, batch loss 1.5957, batch acc 0.8304
15:39:52.850   Training iter 200, batch loss 1.6032, batch acc 0.8158
15:39:53.356   Training iter 250, batch loss 1.5978, batch acc 0.8298
15:39:53.879   Training iter 300, batch loss 1.5937, batch acc 0.8364
15:39:54.414   Training iter 350, batch loss 1.5977, batch acc 0.8288
15:39:54.933   Training iter 400, batch loss 1.6001, batch acc 0.8280
15:39:55.464   Training iter 450, batch loss 1.6005, batch acc 0.8222
15:39:55.981   Training iter 500, batch loss 1.5961, batch acc 0.8268
15:39:56.496   Training iter 550, batch loss 1.5964, batch acc 0.8236
15:39:57.017   Training iter 600, batch loss 1.5990, batch acc 0.8350
15:39:57.019 Training @ 52 epoch...
15:39:57.560   Training iter 50, batch loss 1.5997, batch acc 0.8184
15:39:58.109   Training iter 100, batch loss 1.6012, batch acc 0.8236
15:39:58.621   Training iter 150, batch loss 1.5963, batch acc 0.8308
15:39:59.123   Training iter 200, batch loss 1.6006, batch acc 0.8274
15:39:59.634   Training iter 250, batch loss 1.6007, batch acc 0.8284
15:40:00.150   Training iter 300, batch loss 1.5962, batch acc 0.8264
15:40:00.688   Training iter 350, batch loss 1.5970, batch acc 0.8340
15:40:01.249   Training iter 400, batch loss 1.5954, batch acc 0.8322
15:40:01.803   Training iter 450, batch loss 1.5926, batch acc 0.8326
15:40:02.353   Training iter 500, batch loss 1.5987, batch acc 0.8298
15:40:02.886   Training iter 550, batch loss 1.5966, batch acc 0.8226
15:40:03.411   Training iter 600, batch loss 1.5971, batch acc 0.8270
15:40:03.413 Training @ 53 epoch...
15:40:03.925   Training iter 50, batch loss 1.5934, batch acc 0.8332
15:40:04.436   Training iter 100, batch loss 1.5980, batch acc 0.8208
15:40:04.980   Training iter 150, batch loss 1.5997, batch acc 0.8248
15:40:05.524   Training iter 200, batch loss 1.5949, batch acc 0.8376
15:40:06.083   Training iter 250, batch loss 1.5946, batch acc 0.8340
15:40:06.658   Training iter 300, batch loss 1.5965, batch acc 0.8250
15:40:07.231   Training iter 350, batch loss 1.5950, batch acc 0.8294
15:40:07.761   Training iter 400, batch loss 1.5993, batch acc 0.8222
15:40:08.283   Training iter 450, batch loss 1.6000, batch acc 0.8226
15:40:08.830   Training iter 500, batch loss 1.5947, batch acc 0.8344
15:40:09.395   Training iter 550, batch loss 1.6021, batch acc 0.8168
15:40:09.944   Training iter 600, batch loss 1.5954, batch acc 0.8318
15:40:09.945 Training @ 54 epoch...
15:40:10.505   Training iter 50, batch loss 1.6012, batch acc 0.8290
15:40:11.055   Training iter 100, batch loss 1.5921, batch acc 0.8308
15:40:11.584   Training iter 150, batch loss 1.5976, batch acc 0.8284
15:40:12.117   Training iter 200, batch loss 1.6000, batch acc 0.8242
15:40:12.648   Training iter 250, batch loss 1.5975, batch acc 0.8206
15:40:13.185   Training iter 300, batch loss 1.5926, batch acc 0.8312
15:40:13.717   Training iter 350, batch loss 1.5959, batch acc 0.8298
15:40:14.296   Training iter 400, batch loss 1.5992, batch acc 0.8308
15:40:14.885   Training iter 450, batch loss 1.5973, batch acc 0.8254
15:40:15.470   Training iter 500, batch loss 1.5959, batch acc 0.8184
15:40:16.039   Training iter 550, batch loss 1.5921, batch acc 0.8338
15:40:16.596   Training iter 600, batch loss 1.5950, batch acc 0.8332
15:40:16.598 Training @ 55 epoch...
15:40:17.173   Training iter 50, batch loss 1.5951, batch acc 0.8214
15:40:17.733   Training iter 100, batch loss 1.6001, batch acc 0.8234
15:40:18.300   Training iter 150, batch loss 1.5949, batch acc 0.8312
15:40:18.842   Training iter 200, batch loss 1.5992, batch acc 0.8226
15:40:19.393   Training iter 250, batch loss 1.5941, batch acc 0.8330
15:40:19.950   Training iter 300, batch loss 1.5965, batch acc 0.8286
15:40:20.483   Training iter 350, batch loss 1.5958, batch acc 0.8322
15:40:20.993   Training iter 400, batch loss 1.5972, batch acc 0.8154
15:40:21.519   Training iter 450, batch loss 1.5928, batch acc 0.8356
15:40:22.054   Training iter 500, batch loss 1.5966, batch acc 0.8250
15:40:22.595   Training iter 550, batch loss 1.5926, batch acc 0.8316
15:40:23.138   Training iter 600, batch loss 1.5941, batch acc 0.8342
15:40:23.140 Testing @ 55 epoch...
15:40:23.189     Testing, total mean loss 1.59338, total acc 0.82540
15:40:23.190 Training @ 56 epoch...
15:40:23.745   Training iter 50, batch loss 1.5992, batch acc 0.8246
15:40:24.315   Training iter 100, batch loss 1.5925, batch acc 0.8330
15:40:24.900   Training iter 150, batch loss 1.5948, batch acc 0.8362
15:40:25.471   Training iter 200, batch loss 1.5933, batch acc 0.8322
15:40:26.032   Training iter 250, batch loss 1.5967, batch acc 0.8252
15:40:26.550   Training iter 300, batch loss 1.5985, batch acc 0.8218
15:40:27.085   Training iter 350, batch loss 1.5986, batch acc 0.8218
15:40:27.624   Training iter 400, batch loss 1.5943, batch acc 0.8332
15:40:28.181   Training iter 450, batch loss 1.5926, batch acc 0.8314
15:40:28.739   Training iter 500, batch loss 1.5933, batch acc 0.8310
15:40:29.308   Training iter 550, batch loss 1.5963, batch acc 0.8210
15:40:29.863   Training iter 600, batch loss 1.5914, batch acc 0.8298
15:40:29.864 Training @ 57 epoch...
15:40:30.461   Training iter 50, batch loss 1.5903, batch acc 0.8396
15:40:31.047   Training iter 100, batch loss 1.5935, batch acc 0.8378
15:40:31.620   Training iter 150, batch loss 1.5923, batch acc 0.8352
15:40:32.197   Training iter 200, batch loss 1.5939, batch acc 0.8348
15:40:32.757   Training iter 250, batch loss 1.5951, batch acc 0.8234
15:40:33.343   Training iter 300, batch loss 1.5926, batch acc 0.8272
15:40:33.897   Training iter 350, batch loss 1.5980, batch acc 0.8248
15:40:34.467   Training iter 400, batch loss 1.5952, batch acc 0.8228
15:40:35.025   Training iter 450, batch loss 1.5974, batch acc 0.8254
15:40:35.582   Training iter 500, batch loss 1.5978, batch acc 0.8238
15:40:36.127   Training iter 550, batch loss 1.5950, batch acc 0.8266
15:40:36.640   Training iter 600, batch loss 1.5938, batch acc 0.8274
15:40:36.642 Training @ 58 epoch...
15:40:37.158   Training iter 50, batch loss 1.5936, batch acc 0.8346
15:40:37.662   Training iter 100, batch loss 1.5963, batch acc 0.8262
15:40:38.170   Training iter 150, batch loss 1.5965, batch acc 0.8220
15:40:38.699   Training iter 200, batch loss 1.5961, batch acc 0.8258
15:40:39.216   Training iter 250, batch loss 1.5900, batch acc 0.8400
15:40:39.730   Training iter 300, batch loss 1.5940, batch acc 0.8266
15:40:40.275   Training iter 350, batch loss 1.5914, batch acc 0.8372
15:40:40.807   Training iter 400, batch loss 1.5904, batch acc 0.8346
15:40:41.337   Training iter 450, batch loss 1.5958, batch acc 0.8264
15:40:41.880   Training iter 500, batch loss 1.5971, batch acc 0.8250
15:40:42.425   Training iter 550, batch loss 1.5934, batch acc 0.8272
15:40:42.942   Training iter 600, batch loss 1.5936, batch acc 0.8254
15:40:42.944 Training @ 59 epoch...
15:40:43.478   Training iter 50, batch loss 1.5901, batch acc 0.8362
15:40:44.022   Training iter 100, batch loss 1.5882, batch acc 0.8370
15:40:44.571   Training iter 150, batch loss 1.5939, batch acc 0.8336
15:40:45.125   Training iter 200, batch loss 1.5940, batch acc 0.8304
15:40:45.673   Training iter 250, batch loss 1.5921, batch acc 0.8280
15:40:46.248   Training iter 300, batch loss 1.5919, batch acc 0.8376
15:40:46.799   Training iter 350, batch loss 1.5975, batch acc 0.8226
15:40:47.346   Training iter 400, batch loss 1.5975, batch acc 0.8206
15:40:47.891   Training iter 450, batch loss 1.5956, batch acc 0.8282
15:40:48.452   Training iter 500, batch loss 1.5945, batch acc 0.8330
15:40:49.004   Training iter 550, batch loss 1.5940, batch acc 0.8300
15:40:49.564   Training iter 600, batch loss 1.5922, batch acc 0.8284
15:40:49.566 Training @ 60 epoch...
15:40:50.120   Training iter 50, batch loss 1.5950, batch acc 0.8244
15:40:50.675   Training iter 100, batch loss 1.5924, batch acc 0.8312
15:40:51.226   Training iter 150, batch loss 1.5895, batch acc 0.8354
15:40:51.774   Training iter 200, batch loss 1.5975, batch acc 0.8224
15:40:52.331   Training iter 250, batch loss 1.5949, batch acc 0.8378
15:40:52.871   Training iter 300, batch loss 1.5948, batch acc 0.8256
15:40:53.389   Training iter 350, batch loss 1.5933, batch acc 0.8246
15:40:53.864   Training iter 400, batch loss 1.5887, batch acc 0.8374
15:40:54.359   Training iter 450, batch loss 1.5917, batch acc 0.8270
15:40:54.850   Training iter 500, batch loss 1.5969, batch acc 0.8298
15:40:55.358   Training iter 550, batch loss 1.5896, batch acc 0.8356
15:40:55.832   Training iter 600, batch loss 1.5915, batch acc 0.8312
15:40:55.833 Testing @ 60 epoch...
15:40:55.880     Testing, total mean loss 1.59094, total acc 0.82780
15:40:55.880 Training @ 61 epoch...
15:40:56.375   Training iter 50, batch loss 1.5886, batch acc 0.8336
15:40:56.860   Training iter 100, batch loss 1.5913, batch acc 0.8306
15:40:57.378   Training iter 150, batch loss 1.5949, batch acc 0.8318
15:40:57.896   Training iter 200, batch loss 1.5916, batch acc 0.8352
15:40:58.391   Training iter 250, batch loss 1.5922, batch acc 0.8298
15:40:58.899   Training iter 300, batch loss 1.5899, batch acc 0.8338
15:40:59.433   Training iter 350, batch loss 1.5932, batch acc 0.8278
15:40:59.944   Training iter 400, batch loss 1.5911, batch acc 0.8252
15:41:00.457   Training iter 450, batch loss 1.5939, batch acc 0.8280
15:41:00.970   Training iter 500, batch loss 1.5907, batch acc 0.8376
15:41:01.506   Training iter 550, batch loss 1.5984, batch acc 0.8238
15:41:02.048   Training iter 600, batch loss 1.5933, batch acc 0.8318
15:41:02.049 Training @ 62 epoch...
15:41:02.628   Training iter 50, batch loss 1.5944, batch acc 0.8290
15:41:03.192   Training iter 100, batch loss 1.5884, batch acc 0.8382
15:41:03.762   Training iter 150, batch loss 1.5950, batch acc 0.8312
15:41:04.325   Training iter 200, batch loss 1.5942, batch acc 0.8320
15:41:04.893   Training iter 250, batch loss 1.5881, batch acc 0.8356
15:41:05.452   Training iter 300, batch loss 1.5925, batch acc 0.8294
15:41:06.009   Training iter 350, batch loss 1.5936, batch acc 0.8274
15:41:06.581   Training iter 400, batch loss 1.5911, batch acc 0.8312
15:41:07.144   Training iter 450, batch loss 1.5915, batch acc 0.8354
15:41:07.681   Training iter 500, batch loss 1.5920, batch acc 0.8276
15:41:08.228   Training iter 550, batch loss 1.5923, batch acc 0.8320
15:41:08.733   Training iter 600, batch loss 1.5910, batch acc 0.8298
15:41:08.735 Training @ 63 epoch...
15:41:09.246   Training iter 50, batch loss 1.5897, batch acc 0.8362
15:41:09.753   Training iter 100, batch loss 1.5899, batch acc 0.8442
15:41:10.261   Training iter 150, batch loss 1.5902, batch acc 0.8296
15:41:10.771   Training iter 200, batch loss 1.5900, batch acc 0.8326
15:41:11.312   Training iter 250, batch loss 1.5951, batch acc 0.8282
15:41:11.858   Training iter 300, batch loss 1.5896, batch acc 0.8322
15:41:12.397   Training iter 350, batch loss 1.5933, batch acc 0.8330
15:41:12.926   Training iter 400, batch loss 1.5916, batch acc 0.8380
15:41:13.453   Training iter 450, batch loss 1.5921, batch acc 0.8248
15:41:13.967   Training iter 500, batch loss 1.5937, batch acc 0.8260
15:41:14.495   Training iter 550, batch loss 1.5919, batch acc 0.8336
15:41:15.034   Training iter 600, batch loss 1.5910, batch acc 0.8334
15:41:15.036 Training @ 64 epoch...
15:41:15.579   Training iter 50, batch loss 1.5940, batch acc 0.8282
15:41:16.102   Training iter 100, batch loss 1.5883, batch acc 0.8406
15:41:16.632   Training iter 150, batch loss 1.5939, batch acc 0.8194
15:41:17.151   Training iter 200, batch loss 1.5893, batch acc 0.8402
15:41:17.677   Training iter 250, batch loss 1.5922, batch acc 0.8348
15:41:18.210   Training iter 300, batch loss 1.5944, batch acc 0.8318
15:41:18.745   Training iter 350, batch loss 1.5909, batch acc 0.8338
15:41:19.275   Training iter 400, batch loss 1.5919, batch acc 0.8346
15:41:19.838   Training iter 450, batch loss 1.5860, batch acc 0.8380
15:41:20.387   Training iter 500, batch loss 1.5917, batch acc 0.8308
15:41:20.917   Training iter 550, batch loss 1.5849, batch acc 0.8372
15:41:21.446   Training iter 600, batch loss 1.5947, batch acc 0.8276
15:41:21.448 Training @ 65 epoch...
15:41:21.988   Training iter 50, batch loss 1.5924, batch acc 0.8304
15:41:22.538   Training iter 100, batch loss 1.5888, batch acc 0.8362
15:41:23.084   Training iter 150, batch loss 1.5896, batch acc 0.8358
15:41:23.626   Training iter 200, batch loss 1.5879, batch acc 0.8362
15:41:24.159   Training iter 250, batch loss 1.5931, batch acc 0.8316
15:41:24.681   Training iter 300, batch loss 1.5892, batch acc 0.8402
15:41:25.187   Training iter 350, batch loss 1.5908, batch acc 0.8386
15:41:25.700   Training iter 400, batch loss 1.5910, batch acc 0.8298
15:41:26.207   Training iter 450, batch loss 1.5941, batch acc 0.8272
15:41:26.704   Training iter 500, batch loss 1.5921, batch acc 0.8310
15:41:27.212   Training iter 550, batch loss 1.5908, batch acc 0.8342
15:41:27.712   Training iter 600, batch loss 1.5880, batch acc 0.8390
15:41:27.714 Testing @ 65 epoch...
15:41:27.760     Testing, total mean loss 1.58897, total acc 0.82740
15:41:27.760 Training @ 66 epoch...
15:41:28.290   Training iter 50, batch loss 1.5891, batch acc 0.8360
15:41:28.852   Training iter 100, batch loss 1.5911, batch acc 0.8336
15:41:29.363   Training iter 150, batch loss 1.5963, batch acc 0.8262
15:41:29.868   Training iter 200, batch loss 1.5902, batch acc 0.8378
15:41:30.381   Training iter 250, batch loss 1.5852, batch acc 0.8366
15:41:30.899   Training iter 300, batch loss 1.5911, batch acc 0.8348
15:41:31.423   Training iter 350, batch loss 1.5916, batch acc 0.8240
15:41:31.963   Training iter 400, batch loss 1.5885, batch acc 0.8352
15:41:32.491   Training iter 450, batch loss 1.5928, batch acc 0.8360
15:41:33.021   Training iter 500, batch loss 1.5890, batch acc 0.8346
15:41:33.539   Training iter 550, batch loss 1.5882, batch acc 0.8336
15:41:34.060   Training iter 600, batch loss 1.5895, batch acc 0.8420
15:41:34.062 Training @ 67 epoch...
15:41:34.574   Training iter 50, batch loss 1.5901, batch acc 0.8366
15:41:35.100   Training iter 100, batch loss 1.5883, batch acc 0.8330
15:41:35.620   Training iter 150, batch loss 1.5910, batch acc 0.8370
15:41:36.139   Training iter 200, batch loss 1.5862, batch acc 0.8402
15:41:36.678   Training iter 250, batch loss 1.5853, batch acc 0.8444
15:41:37.209   Training iter 300, batch loss 1.5901, batch acc 0.8328
15:41:37.739   Training iter 350, batch loss 1.5897, batch acc 0.8368
15:41:38.268   Training iter 400, batch loss 1.5877, batch acc 0.8370
15:41:38.794   Training iter 450, batch loss 1.5927, batch acc 0.8276
15:41:39.332   Training iter 500, batch loss 1.5921, batch acc 0.8346
15:41:39.869   Training iter 550, batch loss 1.5930, batch acc 0.8300
15:41:40.406   Training iter 600, batch loss 1.5915, batch acc 0.8352
15:41:40.408 Training @ 68 epoch...
15:41:40.932   Training iter 50, batch loss 1.5868, batch acc 0.8418
15:41:41.451   Training iter 100, batch loss 1.5894, batch acc 0.8368
15:41:41.979   Training iter 150, batch loss 1.5891, batch acc 0.8358
15:41:42.522   Training iter 200, batch loss 1.5907, batch acc 0.8376
15:41:43.194   Training iter 250, batch loss 1.5857, batch acc 0.8414
15:41:43.851   Training iter 300, batch loss 1.5919, batch acc 0.8276
15:41:44.435   Training iter 350, batch loss 1.5894, batch acc 0.8414
15:41:44.997   Training iter 400, batch loss 1.5912, batch acc 0.8316
15:41:45.563   Training iter 450, batch loss 1.5862, batch acc 0.8428
15:41:46.162   Training iter 500, batch loss 1.5874, batch acc 0.8374
15:41:46.767   Training iter 550, batch loss 1.5881, batch acc 0.8428
15:41:47.402   Training iter 600, batch loss 1.5972, batch acc 0.8302
15:41:47.404 Training @ 69 epoch...
15:41:48.005   Training iter 50, batch loss 1.5858, batch acc 0.8412
15:41:48.569   Training iter 100, batch loss 1.5852, batch acc 0.8438
15:41:49.099   Training iter 150, batch loss 1.5914, batch acc 0.8318
15:41:49.630   Training iter 200, batch loss 1.5935, batch acc 0.8270
15:41:50.145   Training iter 250, batch loss 1.5911, batch acc 0.8378
15:41:50.722   Training iter 300, batch loss 1.5882, batch acc 0.8340
15:41:51.294   Training iter 350, batch loss 1.5906, batch acc 0.8382
15:41:51.864   Training iter 400, batch loss 1.5875, batch acc 0.8352
15:41:52.433   Training iter 450, batch loss 1.5863, batch acc 0.8392
15:41:53.007   Training iter 500, batch loss 1.5880, batch acc 0.8368
15:41:53.561   Training iter 550, batch loss 1.5893, batch acc 0.8426
15:41:54.128   Training iter 600, batch loss 1.5911, batch acc 0.8360
15:41:54.129 Training @ 70 epoch...
15:41:54.695   Training iter 50, batch loss 1.5899, batch acc 0.8398
15:41:55.273   Training iter 100, batch loss 1.5869, batch acc 0.8448
15:41:55.866   Training iter 150, batch loss 1.5914, batch acc 0.8318
15:41:56.437   Training iter 200, batch loss 1.5879, batch acc 0.8392
15:41:56.945   Training iter 250, batch loss 1.5918, batch acc 0.8326
15:41:57.457   Training iter 300, batch loss 1.5855, batch acc 0.8394
15:41:57.959   Training iter 350, batch loss 1.5866, batch acc 0.8366
15:41:58.469   Training iter 400, batch loss 1.5893, batch acc 0.8400
15:41:58.991   Training iter 450, batch loss 1.5825, batch acc 0.8504
15:41:59.549   Training iter 500, batch loss 1.5898, batch acc 0.8352
15:42:00.129   Training iter 550, batch loss 1.5916, batch acc 0.8330
15:42:00.728   Training iter 600, batch loss 1.5904, batch acc 0.8410
15:42:00.730 Testing @ 70 epoch...
15:42:00.791     Testing, total mean loss 1.58727, total acc 0.83630
15:42:00.791 Training @ 71 epoch...
15:42:01.334   Training iter 50, batch loss 1.5894, batch acc 0.8386
15:42:01.907   Training iter 100, batch loss 1.5856, batch acc 0.8398
15:42:02.473   Training iter 150, batch loss 1.5898, batch acc 0.8400
15:42:03.024   Training iter 200, batch loss 1.5925, batch acc 0.8392
15:42:03.571   Training iter 250, batch loss 1.5891, batch acc 0.8318
15:42:04.134   Training iter 300, batch loss 1.5858, batch acc 0.8420
15:42:04.659   Training iter 350, batch loss 1.5894, batch acc 0.8350
15:42:05.173   Training iter 400, batch loss 1.5890, batch acc 0.8410
15:42:05.698   Training iter 450, batch loss 1.5868, batch acc 0.8446
15:42:06.237   Training iter 500, batch loss 1.5872, batch acc 0.8428
15:42:06.776   Training iter 550, batch loss 1.5880, batch acc 0.8420
15:42:07.266   Training iter 600, batch loss 1.5870, batch acc 0.8466
15:42:07.268 Training @ 72 epoch...
15:42:07.775   Training iter 50, batch loss 1.5859, batch acc 0.8402
15:42:08.306   Training iter 100, batch loss 1.5873, batch acc 0.8396
15:42:08.834   Training iter 150, batch loss 1.5907, batch acc 0.8376
15:42:09.348   Training iter 200, batch loss 1.5858, batch acc 0.8412
15:42:09.865   Training iter 250, batch loss 1.5874, batch acc 0.8370
15:42:10.394   Training iter 300, batch loss 1.5863, batch acc 0.8456
15:42:10.933   Training iter 350, batch loss 1.5929, batch acc 0.8326
15:42:11.458   Training iter 400, batch loss 1.5839, batch acc 0.8416
15:42:11.969   Training iter 450, batch loss 1.5888, batch acc 0.8410
15:42:12.489   Training iter 500, batch loss 1.5885, batch acc 0.8420
15:42:12.992   Training iter 550, batch loss 1.5874, batch acc 0.8436
15:42:13.469   Training iter 600, batch loss 1.5907, batch acc 0.8474
15:42:13.470 Training @ 73 epoch...
15:42:13.946   Training iter 50, batch loss 1.5875, batch acc 0.8460
15:42:14.435   Training iter 100, batch loss 1.5871, batch acc 0.8404
15:42:14.902   Training iter 150, batch loss 1.5896, batch acc 0.8386
15:42:15.377   Training iter 200, batch loss 1.5849, batch acc 0.8422
15:42:15.853   Training iter 250, batch loss 1.5854, batch acc 0.8452
15:42:16.337   Training iter 300, batch loss 1.5868, batch acc 0.8424
15:42:16.812   Training iter 350, batch loss 1.5882, batch acc 0.8456
15:42:17.315   Training iter 400, batch loss 1.5865, batch acc 0.8458
15:42:17.817   Training iter 450, batch loss 1.5894, batch acc 0.8364
15:42:18.331   Training iter 500, batch loss 1.5884, batch acc 0.8424
15:42:18.858   Training iter 550, batch loss 1.5914, batch acc 0.8404
15:42:19.385   Training iter 600, batch loss 1.5861, batch acc 0.8458
15:42:19.387 Training @ 74 epoch...
15:42:19.943   Training iter 50, batch loss 1.5894, batch acc 0.8402
15:42:20.486   Training iter 100, batch loss 1.5882, batch acc 0.8410
15:42:21.008   Training iter 150, batch loss 1.5882, batch acc 0.8430
15:42:21.525   Training iter 200, batch loss 1.5874, batch acc 0.8406
15:42:22.058   Training iter 250, batch loss 1.5911, batch acc 0.8352
15:42:22.609   Training iter 300, batch loss 1.5905, batch acc 0.8428
15:42:23.159   Training iter 350, batch loss 1.5883, batch acc 0.8410
15:42:23.724   Training iter 400, batch loss 1.5850, batch acc 0.8488
15:42:24.286   Training iter 450, batch loss 1.5863, batch acc 0.8442
15:42:24.841   Training iter 500, batch loss 1.5873, batch acc 0.8458
15:42:25.401   Training iter 550, batch loss 1.5806, batch acc 0.8578
15:42:25.944   Training iter 600, batch loss 1.5856, batch acc 0.8456
15:42:25.946 Training @ 75 epoch...
15:42:26.492   Training iter 50, batch loss 1.5859, batch acc 0.8496
15:42:27.029   Training iter 100, batch loss 1.5902, batch acc 0.8366
15:42:27.579   Training iter 150, batch loss 1.5861, batch acc 0.8482
15:42:28.110   Training iter 200, batch loss 1.5934, batch acc 0.8376
15:42:28.611   Training iter 250, batch loss 1.5859, batch acc 0.8488
15:42:29.120   Training iter 300, batch loss 1.5856, batch acc 0.8420
15:42:29.616   Training iter 350, batch loss 1.5817, batch acc 0.8504
15:42:30.126   Training iter 400, batch loss 1.5895, batch acc 0.8430
15:42:30.679   Training iter 450, batch loss 1.5929, batch acc 0.8362
15:42:31.224   Training iter 500, batch loss 1.5849, batch acc 0.8528
15:42:31.804   Training iter 550, batch loss 1.5867, batch acc 0.8462
15:42:32.380   Training iter 600, batch loss 1.5807, batch acc 0.8516
15:42:32.381 Testing @ 75 epoch...
15:42:32.429     Testing, total mean loss 1.58571, total acc 0.83950
15:42:32.429 Training @ 76 epoch...
15:42:33.015   Training iter 50, batch loss 1.5866, batch acc 0.8524
15:42:33.554   Training iter 100, batch loss 1.5886, batch acc 0.8444
15:42:34.105   Training iter 150, batch loss 1.5838, batch acc 0.8520
15:42:34.674   Training iter 200, batch loss 1.5842, batch acc 0.8500
15:42:35.232   Training iter 250, batch loss 1.5837, batch acc 0.8606
15:42:35.771   Training iter 300, batch loss 1.5875, batch acc 0.8466
15:42:36.301   Training iter 350, batch loss 1.5911, batch acc 0.8364
15:42:36.807   Training iter 400, batch loss 1.5820, batch acc 0.8490
15:42:37.326   Training iter 450, batch loss 1.5869, batch acc 0.8460
15:42:37.841   Training iter 500, batch loss 1.5867, batch acc 0.8422
15:42:38.385   Training iter 550, batch loss 1.5902, batch acc 0.8414
15:42:38.937   Training iter 600, batch loss 1.5885, batch acc 0.8410
15:42:38.939 Training @ 77 epoch...
15:42:39.492   Training iter 50, batch loss 1.5887, batch acc 0.8482
15:42:40.041   Training iter 100, batch loss 1.5836, batch acc 0.8436
15:42:40.595   Training iter 150, batch loss 1.5882, batch acc 0.8426
15:42:41.148   Training iter 200, batch loss 1.5834, batch acc 0.8566
15:42:41.700   Training iter 250, batch loss 1.5857, batch acc 0.8462
15:42:42.259   Training iter 300, batch loss 1.5859, batch acc 0.8496
15:42:42.822   Training iter 350, batch loss 1.5887, batch acc 0.8448
15:42:43.383   Training iter 400, batch loss 1.5847, batch acc 0.8492
15:42:43.934   Training iter 450, batch loss 1.5887, batch acc 0.8446
15:42:44.479   Training iter 500, batch loss 1.5831, batch acc 0.8604
15:42:45.002   Training iter 550, batch loss 1.5905, batch acc 0.8428
15:42:45.529   Training iter 600, batch loss 1.5854, batch acc 0.8540
15:42:45.531 Training @ 78 epoch...
15:42:46.058   Training iter 50, batch loss 1.5865, batch acc 0.8478
15:42:46.581   Training iter 100, batch loss 1.5852, batch acc 0.8482
15:42:47.099   Training iter 150, batch loss 1.5840, batch acc 0.8490
15:42:47.620   Training iter 200, batch loss 1.5881, batch acc 0.8434
15:42:48.160   Training iter 250, batch loss 1.5842, batch acc 0.8546
15:42:48.697   Training iter 300, batch loss 1.5875, batch acc 0.8530
15:42:49.230   Training iter 350, batch loss 1.5864, batch acc 0.8470
15:42:49.759   Training iter 400, batch loss 1.5871, batch acc 0.8540
15:42:50.292   Training iter 450, batch loss 1.5832, batch acc 0.8566
15:42:50.820   Training iter 500, batch loss 1.5874, batch acc 0.8492
15:42:51.342   Training iter 550, batch loss 1.5896, batch acc 0.8490
15:42:51.869   Training iter 600, batch loss 1.5841, batch acc 0.8520
15:42:51.871 Training @ 79 epoch...
15:42:52.403   Training iter 50, batch loss 1.5898, batch acc 0.8456
15:42:52.929   Training iter 100, batch loss 1.5801, batch acc 0.8570
15:42:53.456   Training iter 150, batch loss 1.5832, batch acc 0.8596
15:42:54.002   Training iter 200, batch loss 1.5805, batch acc 0.8580
15:42:54.536   Training iter 250, batch loss 1.5855, batch acc 0.8496
15:42:55.101   Training iter 300, batch loss 1.5899, batch acc 0.8450
15:42:55.668   Training iter 350, batch loss 1.5847, batch acc 0.8522
15:42:56.232   Training iter 400, batch loss 1.5876, batch acc 0.8520
15:42:56.790   Training iter 450, batch loss 1.5861, batch acc 0.8538
15:42:57.362   Training iter 500, batch loss 1.5859, batch acc 0.8554
15:42:57.900   Training iter 550, batch loss 1.5881, batch acc 0.8492
15:42:58.440   Training iter 600, batch loss 1.5881, batch acc 0.8504
15:42:58.442 Training @ 80 epoch...
15:42:58.980   Training iter 50, batch loss 1.5817, batch acc 0.8598
15:42:59.518   Training iter 100, batch loss 1.5819, batch acc 0.8564
15:43:00.057   Training iter 150, batch loss 1.5879, batch acc 0.8484
15:43:00.603   Training iter 200, batch loss 1.5901, batch acc 0.8438
15:43:01.147   Training iter 250, batch loss 1.5834, batch acc 0.8530
15:43:01.683   Training iter 300, batch loss 1.5860, batch acc 0.8468
15:43:02.171   Training iter 350, batch loss 1.5894, batch acc 0.8474
15:43:02.659   Training iter 400, batch loss 1.5817, batch acc 0.8618
15:43:03.152   Training iter 450, batch loss 1.5824, batch acc 0.8586
15:43:03.660   Training iter 500, batch loss 1.5904, batch acc 0.8406
15:43:04.154   Training iter 550, batch loss 1.5866, batch acc 0.8536
15:43:04.664   Training iter 600, batch loss 1.5847, batch acc 0.8644
15:43:04.666 Testing @ 80 epoch...
15:43:04.714     Testing, total mean loss 1.58448, total acc 0.84950
15:43:04.714 Training @ 81 epoch...
15:43:05.240   Training iter 50, batch loss 1.5835, batch acc 0.8588
15:43:05.748   Training iter 100, batch loss 1.5840, batch acc 0.8596
15:43:06.253   Training iter 150, batch loss 1.5822, batch acc 0.8552
15:43:06.766   Training iter 200, batch loss 1.5842, batch acc 0.8628
15:43:07.263   Training iter 250, batch loss 1.5852, batch acc 0.8580
15:43:07.778   Training iter 300, batch loss 1.5826, batch acc 0.8568
15:43:08.294   Training iter 350, batch loss 1.5859, batch acc 0.8532
15:43:08.799   Training iter 400, batch loss 1.5880, batch acc 0.8572
15:43:09.294   Training iter 450, batch loss 1.5859, batch acc 0.8514
15:43:09.790   Training iter 500, batch loss 1.5852, batch acc 0.8584
15:43:10.292   Training iter 550, batch loss 1.5858, batch acc 0.8550
15:43:10.806   Training iter 600, batch loss 1.5906, batch acc 0.8516
15:43:10.808 Training @ 82 epoch...
15:43:11.367   Training iter 50, batch loss 1.5817, batch acc 0.8604
15:43:11.928   Training iter 100, batch loss 1.5810, batch acc 0.8592
15:43:12.485   Training iter 150, batch loss 1.5869, batch acc 0.8488
15:43:13.053   Training iter 200, batch loss 1.5885, batch acc 0.8590
15:43:13.608   Training iter 250, batch loss 1.5899, batch acc 0.8544
15:43:14.131   Training iter 300, batch loss 1.5849, batch acc 0.8556
15:43:14.670   Training iter 350, batch loss 1.5835, batch acc 0.8580
15:43:15.182   Training iter 400, batch loss 1.5852, batch acc 0.8640
15:43:15.698   Training iter 450, batch loss 1.5827, batch acc 0.8598
15:43:16.245   Training iter 500, batch loss 1.5871, batch acc 0.8566
15:43:16.795   Training iter 550, batch loss 1.5804, batch acc 0.8660
15:43:17.319   Training iter 600, batch loss 1.5882, batch acc 0.8524
15:43:17.321 Training @ 83 epoch...
15:43:17.871   Training iter 50, batch loss 1.5838, batch acc 0.8610
15:43:18.400   Training iter 100, batch loss 1.5864, batch acc 0.8574
15:43:18.916   Training iter 150, batch loss 1.5852, batch acc 0.8586
15:43:19.441   Training iter 200, batch loss 1.5861, batch acc 0.8568
15:43:19.947   Training iter 250, batch loss 1.5894, batch acc 0.8578
15:43:20.470   Training iter 300, batch loss 1.5852, batch acc 0.8596
15:43:20.989   Training iter 350, batch loss 1.5891, batch acc 0.8544
15:43:21.507   Training iter 400, batch loss 1.5792, batch acc 0.8634
15:43:22.031   Training iter 450, batch loss 1.5809, batch acc 0.8604
15:43:22.575   Training iter 500, batch loss 1.5868, batch acc 0.8554
15:43:23.123   Training iter 550, batch loss 1.5811, batch acc 0.8606
15:43:23.669   Training iter 600, batch loss 1.5842, batch acc 0.8580
15:43:23.671 Training @ 84 epoch...
15:43:24.224   Training iter 50, batch loss 1.5878, batch acc 0.8544
15:43:24.759   Training iter 100, batch loss 1.5807, batch acc 0.8626
15:43:25.291   Training iter 150, batch loss 1.5822, batch acc 0.8632
15:43:25.791   Training iter 200, batch loss 1.5825, batch acc 0.8582
15:43:26.301   Training iter 250, batch loss 1.5837, batch acc 0.8598
15:43:26.817   Training iter 300, batch loss 1.5814, batch acc 0.8732
15:43:27.354   Training iter 350, batch loss 1.5882, batch acc 0.8566
15:43:27.919   Training iter 400, batch loss 1.5816, batch acc 0.8652
15:43:28.455   Training iter 450, batch loss 1.5880, batch acc 0.8600
15:43:28.973   Training iter 500, batch loss 1.5848, batch acc 0.8614
15:43:29.515   Training iter 550, batch loss 1.5859, batch acc 0.8580
15:43:30.066   Training iter 600, batch loss 1.5872, batch acc 0.8606
15:43:30.067 Training @ 85 epoch...
15:43:30.626   Training iter 50, batch loss 1.5864, batch acc 0.8606
15:43:31.180   Training iter 100, batch loss 1.5836, batch acc 0.8638
15:43:31.702   Training iter 150, batch loss 1.5823, batch acc 0.8666
15:43:32.214   Training iter 200, batch loss 1.5838, batch acc 0.8616
15:43:32.729   Training iter 250, batch loss 1.5820, batch acc 0.8612
15:43:33.242   Training iter 300, batch loss 1.5822, batch acc 0.8640
15:43:33.706   Training iter 350, batch loss 1.5864, batch acc 0.8602
15:43:34.172   Training iter 400, batch loss 1.5843, batch acc 0.8606
15:43:34.667   Training iter 450, batch loss 1.5912, batch acc 0.8600
15:43:35.137   Training iter 500, batch loss 1.5808, batch acc 0.8670
15:43:35.607   Training iter 550, batch loss 1.5869, batch acc 0.8542
15:43:36.089   Training iter 600, batch loss 1.5814, batch acc 0.8680
15:43:36.091 Testing @ 85 epoch...
15:43:36.140     Testing, total mean loss 1.58353, total acc 0.85750
15:43:36.140 Training @ 86 epoch...
15:43:36.648   Training iter 50, batch loss 1.5797, batch acc 0.8660
15:43:37.147   Training iter 100, batch loss 1.5817, batch acc 0.8648
15:43:37.660   Training iter 150, batch loss 1.5809, batch acc 0.8596
15:43:38.189   Training iter 200, batch loss 1.5851, batch acc 0.8582
15:43:38.728   Training iter 250, batch loss 1.5859, batch acc 0.8674
15:43:39.231   Training iter 300, batch loss 1.5816, batch acc 0.8692
15:43:39.720   Training iter 350, batch loss 1.5852, batch acc 0.8590
15:43:40.225   Training iter 400, batch loss 1.5859, batch acc 0.8670
15:43:40.707   Training iter 450, batch loss 1.5874, batch acc 0.8652
15:43:41.159   Training iter 500, batch loss 1.5864, batch acc 0.8568
15:43:41.619   Training iter 550, batch loss 1.5858, batch acc 0.8616
15:43:42.089   Training iter 600, batch loss 1.5830, batch acc 0.8648
15:43:42.090 Training @ 87 epoch...
15:43:42.554   Training iter 50, batch loss 1.5837, batch acc 0.8670
15:43:43.057   Training iter 100, batch loss 1.5820, batch acc 0.8584
15:43:43.578   Training iter 150, batch loss 1.5827, batch acc 0.8642
15:43:44.087   Training iter 200, batch loss 1.5846, batch acc 0.8612
15:43:44.598   Training iter 250, batch loss 1.5841, batch acc 0.8666
15:43:45.120   Training iter 300, batch loss 1.5879, batch acc 0.8618
15:43:45.636   Training iter 350, batch loss 1.5852, batch acc 0.8600
15:43:46.161   Training iter 400, batch loss 1.5845, batch acc 0.8708
15:43:46.667   Training iter 450, batch loss 1.5802, batch acc 0.8674
15:43:47.169   Training iter 500, batch loss 1.5825, batch acc 0.8696
15:43:47.648   Training iter 550, batch loss 1.5844, batch acc 0.8690
15:43:48.139   Training iter 600, batch loss 1.5842, batch acc 0.8644
15:43:48.140 Training @ 88 epoch...
15:43:48.657   Training iter 50, batch loss 1.5852, batch acc 0.8632
15:43:49.172   Training iter 100, batch loss 1.5832, batch acc 0.8718
15:43:49.665   Training iter 150, batch loss 1.5826, batch acc 0.8628
15:43:50.159   Training iter 200, batch loss 1.5787, batch acc 0.8734
15:43:50.645   Training iter 250, batch loss 1.5827, batch acc 0.8674
15:43:51.110   Training iter 300, batch loss 1.5874, batch acc 0.8596
15:43:51.595   Training iter 350, batch loss 1.5817, batch acc 0.8696
15:43:52.101   Training iter 400, batch loss 1.5846, batch acc 0.8678
15:43:52.614   Training iter 450, batch loss 1.5869, batch acc 0.8642
15:43:53.110   Training iter 500, batch loss 1.5876, batch acc 0.8636
15:43:53.604   Training iter 550, batch loss 1.5792, batch acc 0.8672
15:43:54.103   Training iter 600, batch loss 1.5833, batch acc 0.8680
15:43:54.105 Training @ 89 epoch...
15:43:54.616   Training iter 50, batch loss 1.5855, batch acc 0.8616
15:43:55.121   Training iter 100, batch loss 1.5862, batch acc 0.8662
15:43:55.663   Training iter 150, batch loss 1.5783, batch acc 0.8766
15:43:56.168   Training iter 200, batch loss 1.5859, batch acc 0.8602
15:43:56.687   Training iter 250, batch loss 1.5811, batch acc 0.8676
15:43:57.194   Training iter 300, batch loss 1.5825, batch acc 0.8646
15:43:57.660   Training iter 350, batch loss 1.5870, batch acc 0.8630
15:43:58.110   Training iter 400, batch loss 1.5838, batch acc 0.8650
15:43:58.564   Training iter 450, batch loss 1.5802, batch acc 0.8734
15:43:59.047   Training iter 500, batch loss 1.5844, batch acc 0.8702
15:43:59.545   Training iter 550, batch loss 1.5814, batch acc 0.8772
15:44:00.077   Training iter 600, batch loss 1.5844, batch acc 0.8658
15:44:00.079 Training @ 90 epoch...
15:44:00.611   Training iter 50, batch loss 1.5815, batch acc 0.8714
15:44:01.127   Training iter 100, batch loss 1.5856, batch acc 0.8666
15:44:01.667   Training iter 150, batch loss 1.5882, batch acc 0.8628
15:44:02.226   Training iter 200, batch loss 1.5871, batch acc 0.8630
15:44:02.774   Training iter 250, batch loss 1.5810, batch acc 0.8668
15:44:03.328   Training iter 300, batch loss 1.5816, batch acc 0.8734
15:44:03.894   Training iter 350, batch loss 1.5811, batch acc 0.8700
15:44:04.455   Training iter 400, batch loss 1.5876, batch acc 0.8652
15:44:05.004   Training iter 450, batch loss 1.5795, batch acc 0.8702
15:44:05.553   Training iter 500, batch loss 1.5828, batch acc 0.8714
15:44:06.068   Training iter 550, batch loss 1.5794, batch acc 0.8752
15:44:06.577   Training iter 600, batch loss 1.5828, batch acc 0.8694
15:44:06.579 Testing @ 90 epoch...
15:44:06.627     Testing, total mean loss 1.58270, total acc 0.86270
15:44:06.627 Training @ 91 epoch...
15:44:07.135   Training iter 50, batch loss 1.5837, batch acc 0.8662
15:44:07.641   Training iter 100, batch loss 1.5820, batch acc 0.8752
15:44:08.145   Training iter 150, batch loss 1.5823, batch acc 0.8674
15:44:08.661   Training iter 200, batch loss 1.5788, batch acc 0.8742
15:44:09.228   Training iter 250, batch loss 1.5886, batch acc 0.8652
15:44:09.792   Training iter 300, batch loss 1.5849, batch acc 0.8664
15:44:10.370   Training iter 350, batch loss 1.5782, batch acc 0.8736
15:44:10.932   Training iter 400, batch loss 1.5839, batch acc 0.8736
15:44:11.464   Training iter 450, batch loss 1.5836, batch acc 0.8666
15:44:11.990   Training iter 500, batch loss 1.5767, batch acc 0.8772
15:44:12.523   Training iter 550, batch loss 1.5846, batch acc 0.8738
15:44:13.059   Training iter 600, batch loss 1.5881, batch acc 0.8596
15:44:13.060 Training @ 92 epoch...
15:44:13.641   Training iter 50, batch loss 1.5843, batch acc 0.8718
15:44:14.202   Training iter 100, batch loss 1.5822, batch acc 0.8670
15:44:14.785   Training iter 150, batch loss 1.5788, batch acc 0.8746
15:44:15.348   Training iter 200, batch loss 1.5788, batch acc 0.8780
15:44:15.872   Training iter 250, batch loss 1.5851, batch acc 0.8634
15:44:16.453   Training iter 300, batch loss 1.5798, batch acc 0.8734
15:44:17.084   Training iter 350, batch loss 1.5865, batch acc 0.8702
15:44:17.727   Training iter 400, batch loss 1.5850, batch acc 0.8694
15:44:18.333   Training iter 450, batch loss 1.5834, batch acc 0.8718
15:44:18.884   Training iter 500, batch loss 1.5881, batch acc 0.8630
15:44:19.408   Training iter 550, batch loss 1.5798, batch acc 0.8748
15:44:19.936   Training iter 600, batch loss 1.5813, batch acc 0.8728
15:44:19.938 Training @ 93 epoch...
15:44:20.491   Training iter 50, batch loss 1.5832, batch acc 0.8706
15:44:21.018   Training iter 100, batch loss 1.5824, batch acc 0.8704
15:44:21.532   Training iter 150, batch loss 1.5842, batch acc 0.8724
15:44:22.053   Training iter 200, batch loss 1.5850, batch acc 0.8700
15:44:22.592   Training iter 250, batch loss 1.5812, batch acc 0.8754
15:44:23.127   Training iter 300, batch loss 1.5819, batch acc 0.8708
15:44:23.664   Training iter 350, batch loss 1.5785, batch acc 0.8688
15:44:24.182   Training iter 400, batch loss 1.5837, batch acc 0.8678
15:44:24.711   Training iter 450, batch loss 1.5833, batch acc 0.8726
15:44:25.267   Training iter 500, batch loss 1.5846, batch acc 0.8684
15:44:25.817   Training iter 550, batch loss 1.5768, batch acc 0.8806
15:44:26.348   Training iter 600, batch loss 1.5861, batch acc 0.8682
15:44:26.350 Training @ 94 epoch...
15:44:26.878   Training iter 50, batch loss 1.5803, batch acc 0.8728
15:44:27.418   Training iter 100, batch loss 1.5818, batch acc 0.8744
15:44:27.952   Training iter 150, batch loss 1.5851, batch acc 0.8662
15:44:28.480   Training iter 200, batch loss 1.5820, batch acc 0.8764
15:44:29.008   Training iter 250, batch loss 1.5804, batch acc 0.8722
15:44:29.527   Training iter 300, batch loss 1.5833, batch acc 0.8730
15:44:30.035   Training iter 350, batch loss 1.5780, batch acc 0.8778
15:44:30.549   Training iter 400, batch loss 1.5842, batch acc 0.8700
15:44:31.060   Training iter 450, batch loss 1.5855, batch acc 0.8688
15:44:31.570   Training iter 500, batch loss 1.5804, batch acc 0.8754
15:44:32.084   Training iter 550, batch loss 1.5816, batch acc 0.8730
15:44:32.597   Training iter 600, batch loss 1.5863, batch acc 0.8740
15:44:32.599 Training @ 95 epoch...
15:44:33.123   Training iter 50, batch loss 1.5838, batch acc 0.8730
15:44:33.640   Training iter 100, batch loss 1.5812, batch acc 0.8708
15:44:34.158   Training iter 150, batch loss 1.5834, batch acc 0.8672
15:44:34.683   Training iter 200, batch loss 1.5845, batch acc 0.8718
15:44:35.197   Training iter 250, batch loss 1.5791, batch acc 0.8732
15:44:35.722   Training iter 300, batch loss 1.5850, batch acc 0.8766
15:44:36.240   Training iter 350, batch loss 1.5803, batch acc 0.8770
15:44:36.751   Training iter 400, batch loss 1.5872, batch acc 0.8696
15:44:37.267   Training iter 450, batch loss 1.5774, batch acc 0.8798
15:44:37.782   Training iter 500, batch loss 1.5814, batch acc 0.8700
15:44:38.315   Training iter 550, batch loss 1.5794, batch acc 0.8768
15:44:38.835   Training iter 600, batch loss 1.5833, batch acc 0.8748
15:44:38.836 Testing @ 95 epoch...
15:44:38.884     Testing, total mean loss 1.58167, total acc 0.87380
15:44:38.884 Training @ 96 epoch...
15:44:39.424   Training iter 50, batch loss 1.5799, batch acc 0.8764
15:44:39.955   Training iter 100, batch loss 1.5825, batch acc 0.8784
15:44:40.500   Training iter 150, batch loss 1.5805, batch acc 0.8770
15:44:41.024   Training iter 200, batch loss 1.5820, batch acc 0.8774
15:44:41.554   Training iter 250, batch loss 1.5828, batch acc 0.8706
15:44:42.077   Training iter 300, batch loss 1.5856, batch acc 0.8702
15:44:42.610   Training iter 350, batch loss 1.5777, batch acc 0.8740
15:44:43.143   Training iter 400, batch loss 1.5826, batch acc 0.8674
15:44:43.675   Training iter 450, batch loss 1.5866, batch acc 0.8724
15:44:44.199   Training iter 500, batch loss 1.5815, batch acc 0.8784
15:44:44.724   Training iter 550, batch loss 1.5792, batch acc 0.8790
15:44:45.241   Training iter 600, batch loss 1.5831, batch acc 0.8768
15:44:45.243 Training @ 97 epoch...
15:44:45.769   Training iter 50, batch loss 1.5757, batch acc 0.8840
15:44:46.292   Training iter 100, batch loss 1.5832, batch acc 0.8734
15:44:46.808   Training iter 150, batch loss 1.5884, batch acc 0.8678
15:44:47.329   Training iter 200, batch loss 1.5800, batch acc 0.8776
15:44:47.871   Training iter 250, batch loss 1.5851, batch acc 0.8732
15:44:48.429   Training iter 300, batch loss 1.5831, batch acc 0.8724
15:44:48.989   Training iter 350, batch loss 1.5803, batch acc 0.8808
15:44:49.542   Training iter 400, batch loss 1.5808, batch acc 0.8776
15:44:50.089   Training iter 450, batch loss 1.5808, batch acc 0.8762
15:44:50.630   Training iter 500, batch loss 1.5837, batch acc 0.8758
15:44:51.164   Training iter 550, batch loss 1.5806, batch acc 0.8754
15:44:51.691   Training iter 600, batch loss 1.5802, batch acc 0.8784
15:44:51.692 Training @ 98 epoch...
15:44:52.250   Training iter 50, batch loss 1.5910, batch acc 0.8626
15:44:52.799   Training iter 100, batch loss 1.5810, batch acc 0.8718
15:44:53.358   Training iter 150, batch loss 1.5800, batch acc 0.8786
15:44:53.865   Training iter 200, batch loss 1.5844, batch acc 0.8696
15:44:54.388   Training iter 250, batch loss 1.5819, batch acc 0.8770
15:44:54.902   Training iter 300, batch loss 1.5832, batch acc 0.8756
15:44:55.423   Training iter 350, batch loss 1.5799, batch acc 0.8772
15:44:55.923   Training iter 400, batch loss 1.5820, batch acc 0.8770
15:44:56.427   Training iter 450, batch loss 1.5829, batch acc 0.8714
15:44:56.940   Training iter 500, batch loss 1.5800, batch acc 0.8844
15:44:57.462   Training iter 550, batch loss 1.5768, batch acc 0.8852
15:44:57.989   Training iter 600, batch loss 1.5768, batch acc 0.8836
15:44:57.991 Training @ 99 epoch...
15:44:58.525   Training iter 50, batch loss 1.5835, batch acc 0.8714
15:44:59.043   Training iter 100, batch loss 1.5880, batch acc 0.8698
15:44:59.557   Training iter 150, batch loss 1.5836, batch acc 0.8744
15:45:00.080   Training iter 200, batch loss 1.5841, batch acc 0.8724
15:45:00.617   Training iter 250, batch loss 1.5810, batch acc 0.8812
15:45:01.154   Training iter 300, batch loss 1.5793, batch acc 0.8758
15:45:01.725   Training iter 350, batch loss 1.5789, batch acc 0.8822
15:45:02.272   Training iter 400, batch loss 1.5770, batch acc 0.8842
15:45:02.783   Training iter 450, batch loss 1.5779, batch acc 0.8804
15:45:03.268   Training iter 500, batch loss 1.5794, batch acc 0.8808
15:45:03.796   Training iter 550, batch loss 1.5834, batch acc 0.8826
15:45:04.362   Training iter 600, batch loss 1.5823, batch acc 0.8794
15:45:04.364 Training @ 100 epoch...
15:45:04.920   Training iter 50, batch loss 1.5840, batch acc 0.8690
15:45:05.444   Training iter 100, batch loss 1.5780, batch acc 0.8844
15:45:05.951   Training iter 150, batch loss 1.5814, batch acc 0.8774
15:45:06.421   Training iter 200, batch loss 1.5815, batch acc 0.8814
15:45:06.898   Training iter 250, batch loss 1.5811, batch acc 0.8786
15:45:07.391   Training iter 300, batch loss 1.5805, batch acc 0.8806
15:45:07.881   Training iter 350, batch loss 1.5837, batch acc 0.8704
15:45:08.375   Training iter 400, batch loss 1.5827, batch acc 0.8728
15:45:08.851   Training iter 450, batch loss 1.5815, batch acc 0.8762
15:45:09.338   Training iter 500, batch loss 1.5804, batch acc 0.8802
15:45:09.806   Training iter 550, batch loss 1.5804, batch acc 0.8814
15:45:10.269   Training iter 600, batch loss 1.5810, batch acc 0.8860
15:45:10.271 Testing @ 100 epoch...
15:45:10.318     Testing, total mean loss 1.58088, total acc 0.87320
15:45:10.318 Plot @ 100 epoch...
15:45:10.318 Training @ 101 epoch...
15:45:10.798   Training iter 50, batch loss 1.5800, batch acc 0.8770
15:45:11.268   Training iter 100, batch loss 1.5847, batch acc 0.8720
15:45:11.743   Training iter 150, batch loss 1.5848, batch acc 0.8796
15:45:12.221   Training iter 200, batch loss 1.5763, batch acc 0.8794
15:45:12.690   Training iter 250, batch loss 1.5821, batch acc 0.8796
15:45:13.171   Training iter 300, batch loss 1.5775, batch acc 0.8834
15:45:13.672   Training iter 350, batch loss 1.5840, batch acc 0.8726
15:45:14.170   Training iter 400, batch loss 1.5819, batch acc 0.8806
15:45:14.665   Training iter 450, batch loss 1.5794, batch acc 0.8820
15:45:15.147   Training iter 500, batch loss 1.5856, batch acc 0.8746
15:45:15.633   Training iter 550, batch loss 1.5793, batch acc 0.8830
15:45:16.119   Training iter 600, batch loss 1.5787, batch acc 0.8798
15:45:16.120 Training @ 102 epoch...
15:45:16.602   Training iter 50, batch loss 1.5774, batch acc 0.8852
15:45:17.084   Training iter 100, batch loss 1.5812, batch acc 0.8790
15:45:17.572   Training iter 150, batch loss 1.5815, batch acc 0.8784
15:45:18.072   Training iter 200, batch loss 1.5807, batch acc 0.8790
15:45:18.572   Training iter 250, batch loss 1.5813, batch acc 0.8756
15:45:19.079   Training iter 300, batch loss 1.5862, batch acc 0.8738
15:45:19.598   Training iter 350, batch loss 1.5847, batch acc 0.8786
15:45:20.140   Training iter 400, batch loss 1.5780, batch acc 0.8912
15:45:20.666   Training iter 450, batch loss 1.5811, batch acc 0.8806
15:45:21.185   Training iter 500, batch loss 1.5802, batch acc 0.8866
15:45:21.686   Training iter 550, batch loss 1.5814, batch acc 0.8744
15:45:22.210   Training iter 600, batch loss 1.5789, batch acc 0.8852
15:45:22.212 Training @ 103 epoch...
15:45:22.740   Training iter 50, batch loss 1.5790, batch acc 0.8854
15:45:23.280   Training iter 100, batch loss 1.5801, batch acc 0.8828
15:45:23.815   Training iter 150, batch loss 1.5856, batch acc 0.8770
15:45:24.372   Training iter 200, batch loss 1.5800, batch acc 0.8816
15:45:24.895   Training iter 250, batch loss 1.5860, batch acc 0.8682
15:45:25.412   Training iter 300, batch loss 1.5779, batch acc 0.8860
15:45:25.948   Training iter 350, batch loss 1.5818, batch acc 0.8722
15:45:26.487   Training iter 400, batch loss 1.5755, batch acc 0.8868
15:45:26.977   Training iter 450, batch loss 1.5817, batch acc 0.8842
15:45:27.464   Training iter 500, batch loss 1.5833, batch acc 0.8806
15:45:27.954   Training iter 550, batch loss 1.5811, batch acc 0.8786
15:45:28.452   Training iter 600, batch loss 1.5786, batch acc 0.8832
15:45:28.454 Training @ 104 epoch...
15:45:28.947   Training iter 50, batch loss 1.5807, batch acc 0.8840
15:45:29.447   Training iter 100, batch loss 1.5846, batch acc 0.8714
15:45:29.936   Training iter 150, batch loss 1.5810, batch acc 0.8800
15:45:30.455   Training iter 200, batch loss 1.5811, batch acc 0.8780
15:45:30.951   Training iter 250, batch loss 1.5774, batch acc 0.8886
15:45:31.448   Training iter 300, batch loss 1.5802, batch acc 0.8846
15:45:31.979   Training iter 350, batch loss 1.5803, batch acc 0.8784
15:45:32.506   Training iter 400, batch loss 1.5832, batch acc 0.8780
15:45:33.043   Training iter 450, batch loss 1.5770, batch acc 0.8868
15:45:33.574   Training iter 500, batch loss 1.5862, batch acc 0.8808
15:45:34.102   Training iter 550, batch loss 1.5816, batch acc 0.8778
15:45:34.629   Training iter 600, batch loss 1.5754, batch acc 0.8934
15:45:34.631 Training @ 105 epoch...
15:45:35.175   Training iter 50, batch loss 1.5841, batch acc 0.8824
15:45:35.718   Training iter 100, batch loss 1.5793, batch acc 0.8850
15:45:36.245   Training iter 150, batch loss 1.5797, batch acc 0.8816
15:45:36.789   Training iter 200, batch loss 1.5828, batch acc 0.8816
15:45:37.351   Training iter 250, batch loss 1.5796, batch acc 0.8806
15:45:37.884   Training iter 300, batch loss 1.5835, batch acc 0.8780
15:45:38.411   Training iter 350, batch loss 1.5794, batch acc 0.8872
15:45:38.934   Training iter 400, batch loss 1.5786, batch acc 0.8870
15:45:39.469   Training iter 450, batch loss 1.5788, batch acc 0.8836
15:45:39.992   Training iter 500, batch loss 1.5797, batch acc 0.8822
15:45:40.517   Training iter 550, batch loss 1.5791, batch acc 0.8866
15:45:41.039   Training iter 600, batch loss 1.5828, batch acc 0.8764
15:45:41.040 Testing @ 105 epoch...
15:45:41.088     Testing, total mean loss 1.58020, total acc 0.87720
15:45:41.088 Training @ 106 epoch...
15:45:41.622   Training iter 50, batch loss 1.5816, batch acc 0.8744
15:45:42.145   Training iter 100, batch loss 1.5777, batch acc 0.8872
15:45:42.664   Training iter 150, batch loss 1.5782, batch acc 0.8812
15:45:43.202   Training iter 200, batch loss 1.5858, batch acc 0.8742
15:45:43.715   Training iter 250, batch loss 1.5820, batch acc 0.8832
15:45:44.226   Training iter 300, batch loss 1.5770, batch acc 0.8896
15:45:44.744   Training iter 350, batch loss 1.5801, batch acc 0.8850
15:45:45.286   Training iter 400, batch loss 1.5834, batch acc 0.8832
15:45:45.809   Training iter 450, batch loss 1.5839, batch acc 0.8792
15:45:46.333   Training iter 500, batch loss 1.5775, batch acc 0.8920
15:45:46.856   Training iter 550, batch loss 1.5795, batch acc 0.8800
15:45:47.383   Training iter 600, batch loss 1.5791, batch acc 0.8840
15:45:47.384 Training @ 107 epoch...
15:45:47.917   Training iter 50, batch loss 1.5813, batch acc 0.8786
15:45:48.465   Training iter 100, batch loss 1.5776, batch acc 0.8908
15:45:48.987   Training iter 150, batch loss 1.5789, batch acc 0.8824
15:45:49.495   Training iter 200, batch loss 1.5844, batch acc 0.8826
15:45:50.035   Training iter 250, batch loss 1.5805, batch acc 0.8806
15:45:50.599   Training iter 300, batch loss 1.5765, batch acc 0.8878
15:45:51.151   Training iter 350, batch loss 1.5806, batch acc 0.8894
15:45:51.697   Training iter 400, batch loss 1.5785, batch acc 0.8848
15:45:52.271   Training iter 450, batch loss 1.5826, batch acc 0.8830
15:45:52.830   Training iter 500, batch loss 1.5807, batch acc 0.8798
15:45:53.392   Training iter 550, batch loss 1.5804, batch acc 0.8816
15:45:53.931   Training iter 600, batch loss 1.5817, batch acc 0.8868
15:45:53.933 Training @ 108 epoch...
15:45:54.482   Training iter 50, batch loss 1.5799, batch acc 0.8804
15:45:55.039   Training iter 100, batch loss 1.5816, batch acc 0.8848
15:45:55.581   Training iter 150, batch loss 1.5818, batch acc 0.8844
15:45:56.123   Training iter 200, batch loss 1.5782, batch acc 0.8880
15:45:56.670   Training iter 250, batch loss 1.5786, batch acc 0.8844
15:45:57.227   Training iter 300, batch loss 1.5816, batch acc 0.8874
15:45:57.767   Training iter 350, batch loss 1.5791, batch acc 0.8878
15:45:58.293   Training iter 400, batch loss 1.5782, batch acc 0.8846
15:45:58.812   Training iter 450, batch loss 1.5784, batch acc 0.8874
15:45:59.350   Training iter 500, batch loss 1.5817, batch acc 0.8844
15:45:59.869   Training iter 550, batch loss 1.5795, batch acc 0.8916
15:46:00.397   Training iter 600, batch loss 1.5836, batch acc 0.8824
15:46:00.398 Training @ 109 epoch...
15:46:00.915   Training iter 50, batch loss 1.5784, batch acc 0.8896
15:46:01.448   Training iter 100, batch loss 1.5825, batch acc 0.8838
15:46:02.038   Training iter 150, batch loss 1.5803, batch acc 0.8890
15:46:02.595   Training iter 200, batch loss 1.5811, batch acc 0.8808
15:46:03.113   Training iter 250, batch loss 1.5795, batch acc 0.8860
15:46:03.640   Training iter 300, batch loss 1.5783, batch acc 0.8874
15:46:04.159   Training iter 350, batch loss 1.5792, batch acc 0.8880
15:46:04.685   Training iter 400, batch loss 1.5752, batch acc 0.8892
15:46:05.218   Training iter 450, batch loss 1.5781, batch acc 0.8888
15:46:05.760   Training iter 500, batch loss 1.5773, batch acc 0.8868
15:46:06.300   Training iter 550, batch loss 1.5842, batch acc 0.8820
15:46:06.774   Training iter 600, batch loss 1.5863, batch acc 0.8778
15:46:06.776 Training @ 110 epoch...
15:46:07.248   Training iter 50, batch loss 1.5809, batch acc 0.8832
15:46:07.715   Training iter 100, batch loss 1.5794, batch acc 0.8882
15:46:08.198   Training iter 150, batch loss 1.5804, batch acc 0.8844
15:46:08.680   Training iter 200, batch loss 1.5815, batch acc 0.8856
15:46:09.175   Training iter 250, batch loss 1.5793, batch acc 0.8862
15:46:09.719   Training iter 300, batch loss 1.5819, batch acc 0.8860
15:46:10.282   Training iter 350, batch loss 1.5809, batch acc 0.8868
15:46:10.811   Training iter 400, batch loss 1.5803, batch acc 0.8878
15:46:11.314   Training iter 450, batch loss 1.5751, batch acc 0.8924
15:46:11.809   Training iter 500, batch loss 1.5734, batch acc 0.8928
15:46:12.322   Training iter 550, batch loss 1.5823, batch acc 0.8866
15:46:12.814   Training iter 600, batch loss 1.5835, batch acc 0.8760
15:46:12.816 Testing @ 110 epoch...
15:46:12.863     Testing, total mean loss 1.57974, total acc 0.88380
15:46:12.863 Training @ 111 epoch...
15:46:13.372   Training iter 50, batch loss 1.5801, batch acc 0.8880
15:46:13.871   Training iter 100, batch loss 1.5769, batch acc 0.8876
15:46:14.372   Training iter 150, batch loss 1.5787, batch acc 0.8874
15:46:14.897   Training iter 200, batch loss 1.5781, batch acc 0.8934
15:46:15.429   Training iter 250, batch loss 1.5778, batch acc 0.8898
15:46:15.954   Training iter 300, batch loss 1.5812, batch acc 0.8854
15:46:16.474   Training iter 350, batch loss 1.5803, batch acc 0.8910
15:46:16.988   Training iter 400, batch loss 1.5805, batch acc 0.8822
15:46:17.522   Training iter 450, batch loss 1.5817, batch acc 0.8810
15:46:18.045   Training iter 500, batch loss 1.5813, batch acc 0.8888
15:46:18.579   Training iter 550, batch loss 1.5800, batch acc 0.8872
15:46:19.101   Training iter 600, batch loss 1.5809, batch acc 0.8846
15:46:19.103 Training @ 112 epoch...
15:46:19.623   Training iter 50, batch loss 1.5784, batch acc 0.8928
15:46:20.135   Training iter 100, batch loss 1.5826, batch acc 0.8846
15:46:20.646   Training iter 150, batch loss 1.5848, batch acc 0.8750
15:46:21.167   Training iter 200, batch loss 1.5751, batch acc 0.8982
15:46:21.677   Training iter 250, batch loss 1.5783, batch acc 0.8886
15:46:22.214   Training iter 300, batch loss 1.5754, batch acc 0.8928
15:46:22.748   Training iter 350, batch loss 1.5784, batch acc 0.8862
15:46:23.268   Training iter 400, batch loss 1.5765, batch acc 0.8898
15:46:23.757   Training iter 450, batch loss 1.5795, batch acc 0.8900
15:46:24.245   Training iter 500, batch loss 1.5812, batch acc 0.8904
15:46:24.734   Training iter 550, batch loss 1.5832, batch acc 0.8788
15:46:25.222   Training iter 600, batch loss 1.5827, batch acc 0.8868
15:46:25.224 Training @ 113 epoch...
15:46:25.732   Training iter 50, batch loss 1.5816, batch acc 0.8854
15:46:26.202   Training iter 100, batch loss 1.5752, batch acc 0.8900
15:46:26.665   Training iter 150, batch loss 1.5748, batch acc 0.8934
15:46:27.186   Training iter 200, batch loss 1.5769, batch acc 0.8958
15:46:27.693   Training iter 250, batch loss 1.5786, batch acc 0.8910
15:46:28.219   Training iter 300, batch loss 1.5852, batch acc 0.8880
15:46:28.742   Training iter 350, batch loss 1.5784, batch acc 0.8874
15:46:29.270   Training iter 400, batch loss 1.5802, batch acc 0.8844
15:46:29.783   Training iter 450, batch loss 1.5825, batch acc 0.8870
15:46:30.355   Training iter 500, batch loss 1.5784, batch acc 0.8896
15:46:30.893   Training iter 550, batch loss 1.5798, batch acc 0.8854
15:46:31.473   Training iter 600, batch loss 1.5827, batch acc 0.8824
15:46:31.476 Training @ 114 epoch...
15:46:32.058   Training iter 50, batch loss 1.5893, batch acc 0.8772
15:46:32.628   Training iter 100, batch loss 1.5812, batch acc 0.8864
15:46:33.376   Training iter 150, batch loss 1.5752, batch acc 0.8928
15:46:34.123   Training iter 200, batch loss 1.5823, batch acc 0.8892
15:46:34.844   Training iter 250, batch loss 1.5748, batch acc 0.8954
15:46:35.555   Training iter 300, batch loss 1.5782, batch acc 0.8926
15:46:36.264   Training iter 350, batch loss 1.5787, batch acc 0.8890
15:46:36.799   Training iter 400, batch loss 1.5730, batch acc 0.8998
15:46:37.323   Training iter 450, batch loss 1.5807, batch acc 0.8872
15:46:37.806   Training iter 500, batch loss 1.5840, batch acc 0.8816
15:46:38.293   Training iter 550, batch loss 1.5805, batch acc 0.8842
15:46:38.786   Training iter 600, batch loss 1.5753, batch acc 0.8934
15:46:38.788 Training @ 115 epoch...
15:46:39.301   Training iter 50, batch loss 1.5768, batch acc 0.8944
15:46:39.806   Training iter 100, batch loss 1.5809, batch acc 0.8886
15:46:40.329   Training iter 150, batch loss 1.5815, batch acc 0.8856
15:46:40.859   Training iter 200, batch loss 1.5804, batch acc 0.8886
15:46:41.385   Training iter 250, batch loss 1.5761, batch acc 0.8926
15:46:41.920   Training iter 300, batch loss 1.5789, batch acc 0.8886
15:46:42.471   Training iter 350, batch loss 1.5831, batch acc 0.8832
15:46:43.010   Training iter 400, batch loss 1.5786, batch acc 0.8914
15:46:43.539   Training iter 450, batch loss 1.5753, batch acc 0.8926
15:46:44.051   Training iter 500, batch loss 1.5798, batch acc 0.8868
15:46:44.556   Training iter 550, batch loss 1.5792, batch acc 0.8950
15:46:45.079   Training iter 600, batch loss 1.5808, batch acc 0.8890
15:46:45.081 Testing @ 115 epoch...
15:46:45.128     Testing, total mean loss 1.57917, total acc 0.88840
15:46:45.128 Training @ 116 epoch...
15:46:45.650   Training iter 50, batch loss 1.5816, batch acc 0.8846
15:46:46.167   Training iter 100, batch loss 1.5799, batch acc 0.8888
15:46:46.676   Training iter 150, batch loss 1.5761, batch acc 0.8970
15:46:47.192   Training iter 200, batch loss 1.5747, batch acc 0.8980
15:46:47.697   Training iter 250, batch loss 1.5783, batch acc 0.8924
15:46:48.225   Training iter 300, batch loss 1.5794, batch acc 0.8928
15:46:48.739   Training iter 350, batch loss 1.5837, batch acc 0.8832
15:46:49.257   Training iter 400, batch loss 1.5770, batch acc 0.8944
15:46:49.784   Training iter 450, batch loss 1.5793, batch acc 0.8884
15:46:50.310   Training iter 500, batch loss 1.5790, batch acc 0.8918
15:46:50.818   Training iter 550, batch loss 1.5780, batch acc 0.8906
15:46:51.341   Training iter 600, batch loss 1.5829, batch acc 0.8846
15:46:51.343 Training @ 117 epoch...
15:46:51.877   Training iter 50, batch loss 1.5776, batch acc 0.8926
15:46:52.409   Training iter 100, batch loss 1.5776, batch acc 0.8924
15:46:52.930   Training iter 150, batch loss 1.5844, batch acc 0.8786
15:46:53.417   Training iter 200, batch loss 1.5798, batch acc 0.8910
15:46:53.882   Training iter 250, batch loss 1.5805, batch acc 0.8904
15:46:54.347   Training iter 300, batch loss 1.5796, batch acc 0.8920
15:46:54.821   Training iter 350, batch loss 1.5787, batch acc 0.8936
15:46:55.302   Training iter 400, batch loss 1.5758, batch acc 0.8980
15:46:55.794   Training iter 450, batch loss 1.5762, batch acc 0.8934
15:46:56.284   Training iter 500, batch loss 1.5807, batch acc 0.8870
15:46:56.768   Training iter 550, batch loss 1.5799, batch acc 0.8916
15:46:57.258   Training iter 600, batch loss 1.5780, batch acc 0.8910
15:46:57.260 Training @ 118 epoch...
15:46:57.750   Training iter 50, batch loss 1.5779, batch acc 0.8874
15:46:58.256   Training iter 100, batch loss 1.5809, batch acc 0.8876
15:46:58.756   Training iter 150, batch loss 1.5818, batch acc 0.8908
15:46:59.269   Training iter 200, batch loss 1.5772, batch acc 0.8944
15:46:59.797   Training iter 250, batch loss 1.5785, batch acc 0.8902
15:47:00.336   Training iter 300, batch loss 1.5758, batch acc 0.8946
15:47:00.876   Training iter 350, batch loss 1.5781, batch acc 0.8920
15:47:01.433   Training iter 400, batch loss 1.5790, batch acc 0.8910
15:47:02.009   Training iter 450, batch loss 1.5797, batch acc 0.8904
15:47:02.581   Training iter 500, batch loss 1.5818, batch acc 0.8886
15:47:03.157   Training iter 550, batch loss 1.5765, batch acc 0.8972
15:47:03.731   Training iter 600, batch loss 1.5806, batch acc 0.8898
15:47:03.733 Training @ 119 epoch...
15:47:04.298   Training iter 50, batch loss 1.5813, batch acc 0.8874
15:47:04.842   Training iter 100, batch loss 1.5804, batch acc 0.8864
15:47:05.377   Training iter 150, batch loss 1.5795, batch acc 0.8910
15:47:05.913   Training iter 200, batch loss 1.5790, batch acc 0.8920
15:47:06.457   Training iter 250, batch loss 1.5752, batch acc 0.8970
15:47:07.014   Training iter 300, batch loss 1.5781, batch acc 0.8922
15:47:07.575   Training iter 350, batch loss 1.5756, batch acc 0.8952
15:47:08.099   Training iter 400, batch loss 1.5809, batch acc 0.8894
15:47:08.612   Training iter 450, batch loss 1.5806, batch acc 0.8850
15:47:09.137   Training iter 500, batch loss 1.5782, batch acc 0.8942
15:47:09.667   Training iter 550, batch loss 1.5787, batch acc 0.8972
15:47:10.217   Training iter 600, batch loss 1.5789, batch acc 0.8932
15:47:10.219 Training @ 120 epoch...
15:47:10.764   Training iter 50, batch loss 1.5808, batch acc 0.8898
15:47:11.302   Training iter 100, batch loss 1.5784, batch acc 0.8882
15:47:11.846   Training iter 150, batch loss 1.5774, batch acc 0.8950
15:47:12.387   Training iter 200, batch loss 1.5776, batch acc 0.8934
15:47:12.940   Training iter 250, batch loss 1.5806, batch acc 0.8906
15:47:13.497   Training iter 300, batch loss 1.5760, batch acc 0.8974
15:47:14.063   Training iter 350, batch loss 1.5771, batch acc 0.8934
15:47:14.611   Training iter 400, batch loss 1.5838, batch acc 0.8856
15:47:15.161   Training iter 450, batch loss 1.5813, batch acc 0.8960
15:47:15.684   Training iter 500, batch loss 1.5746, batch acc 0.8942
15:47:16.209   Training iter 550, batch loss 1.5827, batch acc 0.8858
15:47:16.727   Training iter 600, batch loss 1.5751, batch acc 0.9006
15:47:16.729 Testing @ 120 epoch...
15:47:16.778     Testing, total mean loss 1.57867, total acc 0.89030
15:47:16.778 Training @ 121 epoch...
15:47:17.290   Training iter 50, batch loss 1.5742, batch acc 0.8990
15:47:17.804   Training iter 100, batch loss 1.5710, batch acc 0.9008
15:47:18.331   Training iter 150, batch loss 1.5800, batch acc 0.8930
15:47:18.835   Training iter 200, batch loss 1.5795, batch acc 0.8938
15:47:19.331   Training iter 250, batch loss 1.5856, batch acc 0.8848
15:47:19.843   Training iter 300, batch loss 1.5798, batch acc 0.8916
15:47:20.344   Training iter 350, batch loss 1.5785, batch acc 0.8934
15:47:20.828   Training iter 400, batch loss 1.5814, batch acc 0.8936
15:47:21.313   Training iter 450, batch loss 1.5768, batch acc 0.8928
15:47:21.807   Training iter 500, batch loss 1.5780, batch acc 0.8912
15:47:22.346   Training iter 550, batch loss 1.5785, batch acc 0.8930
15:47:22.897   Training iter 600, batch loss 1.5804, batch acc 0.8896
15:47:22.899 Training @ 122 epoch...
15:47:23.465   Training iter 50, batch loss 1.5778, batch acc 0.8952
15:47:24.012   Training iter 100, batch loss 1.5785, batch acc 0.8926
15:47:24.542   Training iter 150, batch loss 1.5825, batch acc 0.8902
15:47:25.071   Training iter 200, batch loss 1.5739, batch acc 0.8976
15:47:25.589   Training iter 250, batch loss 1.5777, batch acc 0.8970
15:47:26.078   Training iter 300, batch loss 1.5767, batch acc 0.8954
15:47:26.570   Training iter 350, batch loss 1.5787, batch acc 0.8894
15:47:27.061   Training iter 400, batch loss 1.5794, batch acc 0.8898
15:47:27.547   Training iter 450, batch loss 1.5750, batch acc 0.8940
15:47:28.084   Training iter 500, batch loss 1.5796, batch acc 0.8958
15:47:28.636   Training iter 550, batch loss 1.5811, batch acc 0.8940
15:47:29.188   Training iter 600, batch loss 1.5814, batch acc 0.8894
15:47:29.190 Training @ 123 epoch...
15:47:29.749   Training iter 50, batch loss 1.5782, batch acc 0.8906
15:47:30.325   Training iter 100, batch loss 1.5785, batch acc 0.8958
15:47:30.890   Training iter 150, batch loss 1.5756, batch acc 0.8964
15:47:31.459   Training iter 200, batch loss 1.5762, batch acc 0.8954
15:47:32.047   Training iter 250, batch loss 1.5747, batch acc 0.8944
15:47:32.642   Training iter 300, batch loss 1.5806, batch acc 0.8912
15:47:33.234   Training iter 350, batch loss 1.5836, batch acc 0.8894
15:47:33.785   Training iter 400, batch loss 1.5776, batch acc 0.8976
15:47:34.308   Training iter 450, batch loss 1.5799, batch acc 0.8914
15:47:34.842   Training iter 500, batch loss 1.5798, batch acc 0.8906
15:47:35.377   Training iter 550, batch loss 1.5797, batch acc 0.8926
15:47:35.915   Training iter 600, batch loss 1.5771, batch acc 0.8960
15:47:35.917 Training @ 124 epoch...
15:47:36.462   Training iter 50, batch loss 1.5785, batch acc 0.9002
15:47:36.997   Training iter 100, batch loss 1.5762, batch acc 0.8988
15:47:37.524   Training iter 150, batch loss 1.5775, batch acc 0.8932
15:47:38.061   Training iter 200, batch loss 1.5795, batch acc 0.8948
15:47:38.601   Training iter 250, batch loss 1.5801, batch acc 0.8926
15:47:39.127   Training iter 300, batch loss 1.5775, batch acc 0.8974
15:47:39.658   Training iter 350, batch loss 1.5766, batch acc 0.8930
15:47:40.192   Training iter 400, batch loss 1.5788, batch acc 0.8960
15:47:40.711   Training iter 450, batch loss 1.5785, batch acc 0.8892
15:47:41.227   Training iter 500, batch loss 1.5826, batch acc 0.8870
15:47:41.770   Training iter 550, batch loss 1.5723, batch acc 0.9022
15:47:42.331   Training iter 600, batch loss 1.5817, batch acc 0.8886
15:47:42.333 Training @ 125 epoch...
15:47:42.867   Training iter 50, batch loss 1.5789, batch acc 0.8950
15:47:43.385   Training iter 100, batch loss 1.5800, batch acc 0.8894
15:47:43.923   Training iter 150, batch loss 1.5792, batch acc 0.8910
15:47:44.476   Training iter 200, batch loss 1.5741, batch acc 0.9018
15:47:45.034   Training iter 250, batch loss 1.5782, batch acc 0.8942
15:47:45.594   Training iter 300, batch loss 1.5775, batch acc 0.8976
15:47:46.180   Training iter 350, batch loss 1.5795, batch acc 0.8918
15:47:46.741   Training iter 400, batch loss 1.5765, batch acc 0.8960
15:47:47.290   Training iter 450, batch loss 1.5784, batch acc 0.8954
15:47:47.852   Training iter 500, batch loss 1.5762, batch acc 0.9022
15:47:48.401   Training iter 550, batch loss 1.5815, batch acc 0.8936
15:47:48.970   Training iter 600, batch loss 1.5789, batch acc 0.8940
15:47:48.972 Testing @ 125 epoch...
15:47:49.022     Testing, total mean loss 1.57823, total acc 0.89260
15:47:49.022 Training @ 126 epoch...
15:47:49.584   Training iter 50, batch loss 1.5762, batch acc 0.8906
15:47:50.111   Training iter 100, batch loss 1.5767, batch acc 0.8986
15:47:50.634   Training iter 150, batch loss 1.5784, batch acc 0.8960
15:47:51.145   Training iter 200, batch loss 1.5817, batch acc 0.8892
15:47:51.670   Training iter 250, batch loss 1.5724, batch acc 0.9030
15:47:52.189   Training iter 300, batch loss 1.5777, batch acc 0.8942
15:47:52.726   Training iter 350, batch loss 1.5773, batch acc 0.8970
15:47:53.247   Training iter 400, batch loss 1.5823, batch acc 0.8912
15:47:53.763   Training iter 450, batch loss 1.5806, batch acc 0.8950
15:47:54.271   Training iter 500, batch loss 1.5785, batch acc 0.8888
15:47:54.780   Training iter 550, batch loss 1.5776, batch acc 0.9016
15:47:55.296   Training iter 600, batch loss 1.5780, batch acc 0.8938
15:47:55.297 Training @ 127 epoch...
15:47:55.810   Training iter 50, batch loss 1.5741, batch acc 0.8994
15:47:56.296   Training iter 100, batch loss 1.5808, batch acc 0.8912
15:47:56.786   Training iter 150, batch loss 1.5782, batch acc 0.8958
15:47:57.289   Training iter 200, batch loss 1.5774, batch acc 0.8954
15:47:57.780   Training iter 250, batch loss 1.5779, batch acc 0.8958
15:47:58.286   Training iter 300, batch loss 1.5801, batch acc 0.8946
15:47:58.787   Training iter 350, batch loss 1.5741, batch acc 0.9046
15:47:59.277   Training iter 400, batch loss 1.5762, batch acc 0.8988
15:47:59.790   Training iter 450, batch loss 1.5784, batch acc 0.8956
15:48:00.341   Training iter 500, batch loss 1.5783, batch acc 0.8982
15:48:00.892   Training iter 550, batch loss 1.5829, batch acc 0.8932
15:48:01.431   Training iter 600, batch loss 1.5783, batch acc 0.8920
15:48:01.433 Training @ 128 epoch...
15:48:02.016   Training iter 50, batch loss 1.5770, batch acc 0.9012
15:48:02.598   Training iter 100, batch loss 1.5734, batch acc 0.9016
15:48:03.172   Training iter 150, batch loss 1.5833, batch acc 0.8910
15:48:03.750   Training iter 200, batch loss 1.5741, batch acc 0.9016
15:48:04.324   Training iter 250, batch loss 1.5834, batch acc 0.8914
15:48:04.895   Training iter 300, batch loss 1.5790, batch acc 0.8878
15:48:05.470   Training iter 350, batch loss 1.5780, batch acc 0.9016
15:48:06.037   Training iter 400, batch loss 1.5749, batch acc 0.8974
15:48:06.607   Training iter 450, batch loss 1.5784, batch acc 0.8936
15:48:07.172   Training iter 500, batch loss 1.5777, batch acc 0.8976
15:48:07.718   Training iter 550, batch loss 1.5765, batch acc 0.8990
15:48:08.212   Training iter 600, batch loss 1.5794, batch acc 0.8970
15:48:08.214 Training @ 129 epoch...
15:48:08.696   Training iter 50, batch loss 1.5791, batch acc 0.8938
15:48:09.163   Training iter 100, batch loss 1.5792, batch acc 0.8912
15:48:09.645   Training iter 150, batch loss 1.5720, batch acc 0.9012
15:48:10.146   Training iter 200, batch loss 1.5825, batch acc 0.8942
15:48:10.629   Training iter 250, batch loss 1.5762, batch acc 0.9002
15:48:11.098   Training iter 300, batch loss 1.5764, batch acc 0.9028
15:48:11.580   Training iter 350, batch loss 1.5788, batch acc 0.8962
15:48:12.063   Training iter 400, batch loss 1.5778, batch acc 0.8968
15:48:12.551   Training iter 450, batch loss 1.5746, batch acc 0.9006
15:48:13.030   Training iter 500, batch loss 1.5774, batch acc 0.9016
15:48:13.519   Training iter 550, batch loss 1.5784, batch acc 0.8944
15:48:14.018   Training iter 600, batch loss 1.5822, batch acc 0.8914
15:48:14.020 Training @ 130 epoch...
15:48:14.516   Training iter 50, batch loss 1.5750, batch acc 0.9034
15:48:15.025   Training iter 100, batch loss 1.5775, batch acc 0.9026
15:48:15.524   Training iter 150, batch loss 1.5774, batch acc 0.8990
15:48:16.030   Training iter 200, batch loss 1.5773, batch acc 0.8974
15:48:16.518   Training iter 250, batch loss 1.5818, batch acc 0.8924
15:48:17.020   Training iter 300, batch loss 1.5855, batch acc 0.8890
15:48:17.529   Training iter 350, batch loss 1.5805, batch acc 0.8908
15:48:18.019   Training iter 400, batch loss 1.5746, batch acc 0.9030
15:48:18.527   Training iter 450, batch loss 1.5763, batch acc 0.8968
15:48:19.057   Training iter 500, batch loss 1.5741, batch acc 0.9014
15:48:19.596   Training iter 550, batch loss 1.5775, batch acc 0.8946
15:48:20.138   Training iter 600, batch loss 1.5760, batch acc 0.8964
15:48:20.140 Testing @ 130 epoch...
15:48:20.188     Testing, total mean loss 1.57809, total acc 0.89510
15:48:20.188 Training @ 131 epoch...
15:48:20.737   Training iter 50, batch loss 1.5797, batch acc 0.8974
15:48:21.266   Training iter 100, batch loss 1.5778, batch acc 0.8972
15:48:21.820   Training iter 150, batch loss 1.5762, batch acc 0.8996
15:48:22.363   Training iter 200, batch loss 1.5764, batch acc 0.8990
15:48:22.908   Training iter 250, batch loss 1.5742, batch acc 0.8984
15:48:23.466   Training iter 300, batch loss 1.5759, batch acc 0.9014
15:48:24.007   Training iter 350, batch loss 1.5782, batch acc 0.8988
15:48:24.557   Training iter 400, batch loss 1.5839, batch acc 0.8854
15:48:25.111   Training iter 450, batch loss 1.5770, batch acc 0.8958
15:48:25.634   Training iter 500, batch loss 1.5785, batch acc 0.8988
15:48:26.156   Training iter 550, batch loss 1.5788, batch acc 0.8998
15:48:26.669   Training iter 600, batch loss 1.5760, batch acc 0.8988
15:48:26.670 Training @ 132 epoch...
15:48:27.207   Training iter 50, batch loss 1.5782, batch acc 0.8998
15:48:27.769   Training iter 100, batch loss 1.5793, batch acc 0.8948
15:48:28.347   Training iter 150, batch loss 1.5766, batch acc 0.9012
15:48:28.934   Training iter 200, batch loss 1.5756, batch acc 0.9028
15:48:29.504   Training iter 250, batch loss 1.5787, batch acc 0.8948
15:48:30.036   Training iter 300, batch loss 1.5755, batch acc 0.8972
15:48:30.550   Training iter 350, batch loss 1.5757, batch acc 0.9000
15:48:31.085   Training iter 400, batch loss 1.5753, batch acc 0.9028
15:48:31.641   Training iter 450, batch loss 1.5780, batch acc 0.9008
15:48:32.195   Training iter 500, batch loss 1.5813, batch acc 0.8920
15:48:32.756   Training iter 550, batch loss 1.5784, batch acc 0.8926
15:48:33.294   Training iter 600, batch loss 1.5786, batch acc 0.8942
15:48:33.296 Training @ 133 epoch...
15:48:33.845   Training iter 50, batch loss 1.5736, batch acc 0.9026
15:48:34.381   Training iter 100, batch loss 1.5767, batch acc 0.9016
15:48:34.911   Training iter 150, batch loss 1.5789, batch acc 0.8966
15:48:35.434   Training iter 200, batch loss 1.5753, batch acc 0.9032
15:48:35.963   Training iter 250, batch loss 1.5820, batch acc 0.8916
15:48:36.444   Training iter 300, batch loss 1.5778, batch acc 0.8970
15:48:36.917   Training iter 350, batch loss 1.5723, batch acc 0.9042
15:48:37.397   Training iter 400, batch loss 1.5797, batch acc 0.8968
15:48:37.922   Training iter 450, batch loss 1.5811, batch acc 0.8918
15:48:38.423   Training iter 500, batch loss 1.5745, batch acc 0.8988
15:48:38.917   Training iter 550, batch loss 1.5813, batch acc 0.9004
15:48:39.462   Training iter 600, batch loss 1.5772, batch acc 0.8976
15:48:39.464 Training @ 134 epoch...
15:48:39.990   Training iter 50, batch loss 1.5769, batch acc 0.9012
15:48:40.537   Training iter 100, batch loss 1.5740, batch acc 0.9060
15:48:41.067   Training iter 150, batch loss 1.5765, batch acc 0.8978
15:48:41.599   Training iter 200, batch loss 1.5791, batch acc 0.8948
15:48:42.154   Training iter 250, batch loss 1.5795, batch acc 0.8964
15:48:42.705   Training iter 300, batch loss 1.5781, batch acc 0.8974
15:48:43.266   Training iter 350, batch loss 1.5802, batch acc 0.8970
15:48:43.830   Training iter 400, batch loss 1.5752, batch acc 0.9034
15:48:44.456   Training iter 450, batch loss 1.5772, batch acc 0.8978
15:48:44.981   Training iter 500, batch loss 1.5752, batch acc 0.9020
15:48:45.497   Training iter 550, batch loss 1.5809, batch acc 0.8936
15:48:46.010   Training iter 600, batch loss 1.5765, batch acc 0.9000
15:48:46.012 Training @ 135 epoch...
15:48:46.533   Training iter 50, batch loss 1.5802, batch acc 0.8930
15:48:47.053   Training iter 100, batch loss 1.5759, batch acc 0.9028
15:48:47.585   Training iter 150, batch loss 1.5776, batch acc 0.8998
15:48:48.108   Training iter 200, batch loss 1.5795, batch acc 0.8940
15:48:48.631   Training iter 250, batch loss 1.5767, batch acc 0.8984
15:48:49.151   Training iter 300, batch loss 1.5819, batch acc 0.8944
15:48:49.681   Training iter 350, batch loss 1.5788, batch acc 0.8976
15:48:50.221   Training iter 400, batch loss 1.5756, batch acc 0.9060
15:48:50.754   Training iter 450, batch loss 1.5778, batch acc 0.8982
15:48:51.275   Training iter 500, batch loss 1.5752, batch acc 0.9068
15:48:51.807   Training iter 550, batch loss 1.5704, batch acc 0.9088
15:48:52.349   Training iter 600, batch loss 1.5786, batch acc 0.8952
15:48:52.351 Testing @ 135 epoch...
15:48:52.398     Testing, total mean loss 1.57754, total acc 0.89790
15:48:52.398 Training @ 136 epoch...
15:48:52.923   Training iter 50, batch loss 1.5789, batch acc 0.8984
15:48:53.467   Training iter 100, batch loss 1.5766, batch acc 0.9000
15:48:53.991   Training iter 150, batch loss 1.5748, batch acc 0.9016
15:48:54.536   Training iter 200, batch loss 1.5796, batch acc 0.8972
15:48:55.084   Training iter 250, batch loss 1.5769, batch acc 0.8998
15:48:55.623   Training iter 300, batch loss 1.5789, batch acc 0.9000
15:48:56.148   Training iter 350, batch loss 1.5741, batch acc 0.9010
15:48:56.661   Training iter 400, batch loss 1.5826, batch acc 0.8948
15:48:57.178   Training iter 450, batch loss 1.5753, batch acc 0.9018
15:48:57.703   Training iter 500, batch loss 1.5759, batch acc 0.9002
15:48:58.226   Training iter 550, batch loss 1.5749, batch acc 0.9002
15:48:58.740   Training iter 600, batch loss 1.5786, batch acc 0.9030
15:48:58.742 Training @ 137 epoch...
15:48:59.263   Training iter 50, batch loss 1.5741, batch acc 0.9058
15:48:59.784   Training iter 100, batch loss 1.5784, batch acc 0.9004
15:49:00.312   Training iter 150, batch loss 1.5766, batch acc 0.8952
15:49:00.828   Training iter 200, batch loss 1.5768, batch acc 0.8996
15:49:01.341   Training iter 250, batch loss 1.5767, batch acc 0.9034
15:49:01.912   Training iter 300, batch loss 1.5722, batch acc 0.9044
15:49:02.483   Training iter 350, batch loss 1.5809, batch acc 0.8980
15:49:03.038   Training iter 400, batch loss 1.5776, batch acc 0.8974
15:49:03.593   Training iter 450, batch loss 1.5745, batch acc 0.9036
15:49:04.133   Training iter 500, batch loss 1.5810, batch acc 0.8994
15:49:04.674   Training iter 550, batch loss 1.5770, batch acc 0.8982
15:49:05.218   Training iter 600, batch loss 1.5805, batch acc 0.9004
15:49:05.220 Training @ 138 epoch...
15:49:05.761   Training iter 50, batch loss 1.5782, batch acc 0.8990
15:49:06.246   Training iter 100, batch loss 1.5780, batch acc 0.9020
15:49:06.724   Training iter 150, batch loss 1.5755, batch acc 0.9016
15:49:07.236   Training iter 200, batch loss 1.5761, batch acc 0.9032
15:49:07.718   Training iter 250, batch loss 1.5765, batch acc 0.8996
15:49:08.200   Training iter 300, batch loss 1.5737, batch acc 0.9060
15:49:08.671   Training iter 350, batch loss 1.5722, batch acc 0.9032
15:49:09.150   Training iter 400, batch loss 1.5822, batch acc 0.8950
15:49:09.628   Training iter 450, batch loss 1.5746, batch acc 0.9042
15:49:10.131   Training iter 500, batch loss 1.5766, batch acc 0.9066
15:49:10.654   Training iter 550, batch loss 1.5805, batch acc 0.8894
15:49:11.180   Training iter 600, batch loss 1.5809, batch acc 0.8950
15:49:11.182 Training @ 139 epoch...
15:49:11.710   Training iter 50, batch loss 1.5815, batch acc 0.8974
15:49:12.231   Training iter 100, batch loss 1.5756, batch acc 0.9050
15:49:12.752   Training iter 150, batch loss 1.5750, batch acc 0.9026
15:49:13.281   Training iter 200, batch loss 1.5772, batch acc 0.9020
15:49:13.795   Training iter 250, batch loss 1.5753, batch acc 0.9028
15:49:14.312   Training iter 300, batch loss 1.5735, batch acc 0.9040
15:49:14.837   Training iter 350, batch loss 1.5773, batch acc 0.9018
15:49:15.361   Training iter 400, batch loss 1.5734, batch acc 0.9080
15:49:15.883   Training iter 450, batch loss 1.5757, batch acc 0.9028
15:49:16.403   Training iter 500, batch loss 1.5828, batch acc 0.8922
15:49:16.937   Training iter 550, batch loss 1.5794, batch acc 0.8964
15:49:17.454   Training iter 600, batch loss 1.5773, batch acc 0.9006
15:49:17.455 Training @ 140 epoch...
15:49:17.977   Training iter 50, batch loss 1.5816, batch acc 0.8968
15:49:18.505   Training iter 100, batch loss 1.5754, batch acc 0.9002
15:49:19.024   Training iter 150, batch loss 1.5817, batch acc 0.8918
15:49:19.541   Training iter 200, batch loss 1.5735, batch acc 0.9042
15:49:20.064   Training iter 250, batch loss 1.5789, batch acc 0.8980
15:49:20.585   Training iter 300, batch loss 1.5804, batch acc 0.9002
15:49:21.104   Training iter 350, batch loss 1.5723, batch acc 0.9100
15:49:21.612   Training iter 400, batch loss 1.5775, batch acc 0.9008
15:49:22.108   Training iter 450, batch loss 1.5746, batch acc 0.9032
15:49:22.618   Training iter 500, batch loss 1.5749, batch acc 0.9090
15:49:23.198   Training iter 550, batch loss 1.5752, batch acc 0.9022
15:49:23.802   Training iter 600, batch loss 1.5774, batch acc 0.8986
15:49:23.804 Testing @ 140 epoch...
15:49:23.855     Testing, total mean loss 1.57709, total acc 0.89970
15:49:23.855 Training @ 141 epoch...
15:49:24.532   Training iter 50, batch loss 1.5791, batch acc 0.9028
15:49:25.242   Training iter 100, batch loss 1.5760, batch acc 0.8982
15:49:25.888   Training iter 150, batch loss 1.5732, batch acc 0.9050
15:49:26.487   Training iter 200, batch loss 1.5746, batch acc 0.9062
15:49:27.071   Training iter 250, batch loss 1.5795, batch acc 0.8966
15:49:27.693   Training iter 300, batch loss 1.5793, batch acc 0.8962
15:49:28.256   Training iter 350, batch loss 1.5739, batch acc 0.9052
15:49:28.816   Training iter 400, batch loss 1.5764, batch acc 0.9014
15:49:29.371   Training iter 450, batch loss 1.5754, batch acc 0.9054
15:49:29.895   Training iter 500, batch loss 1.5791, batch acc 0.9018
15:49:30.452   Training iter 550, batch loss 1.5807, batch acc 0.8954
15:49:31.002   Training iter 600, batch loss 1.5751, batch acc 0.9058
15:49:31.004 Training @ 142 epoch...
15:49:31.512   Training iter 50, batch loss 1.5739, batch acc 0.9054
15:49:31.988   Training iter 100, batch loss 1.5802, batch acc 0.9018
15:49:32.460   Training iter 150, batch loss 1.5787, batch acc 0.8950
15:49:32.937   Training iter 200, batch loss 1.5761, batch acc 0.9030
15:49:33.430   Training iter 250, batch loss 1.5749, batch acc 0.9042
15:49:33.927   Training iter 300, batch loss 1.5750, batch acc 0.9086
15:49:34.423   Training iter 350, batch loss 1.5724, batch acc 0.9116
15:49:34.911   Training iter 400, batch loss 1.5769, batch acc 0.8966
15:49:35.397   Training iter 450, batch loss 1.5784, batch acc 0.9010
15:49:35.871   Training iter 500, batch loss 1.5792, batch acc 0.8912
15:49:36.351   Training iter 550, batch loss 1.5771, batch acc 0.8976
15:49:36.830   Training iter 600, batch loss 1.5783, batch acc 0.9042
15:49:36.832 Training @ 143 epoch...
15:49:37.328   Training iter 50, batch loss 1.5785, batch acc 0.8986
15:49:37.854   Training iter 100, batch loss 1.5777, batch acc 0.9004
15:49:38.391   Training iter 150, batch loss 1.5777, batch acc 0.9050
15:49:38.913   Training iter 200, batch loss 1.5761, batch acc 0.9000
15:49:39.420   Training iter 250, batch loss 1.5728, batch acc 0.9072
15:49:39.932   Training iter 300, batch loss 1.5789, batch acc 0.8994
15:49:40.489   Training iter 350, batch loss 1.5754, batch acc 0.9036
15:49:41.035   Training iter 400, batch loss 1.5781, batch acc 0.9028
15:49:41.581   Training iter 450, batch loss 1.5759, batch acc 0.9020
15:49:42.143   Training iter 500, batch loss 1.5795, batch acc 0.9018
15:49:42.693   Training iter 550, batch loss 1.5765, batch acc 0.9020
15:49:43.230   Training iter 600, batch loss 1.5734, batch acc 0.9102
15:49:43.232 Training @ 144 epoch...
15:49:43.767   Training iter 50, batch loss 1.5760, batch acc 0.9028
15:49:44.299   Training iter 100, batch loss 1.5727, batch acc 0.9088
15:49:44.820   Training iter 150, batch loss 1.5768, batch acc 0.9010
15:49:45.334   Training iter 200, batch loss 1.5798, batch acc 0.8924
15:49:45.824   Training iter 250, batch loss 1.5786, batch acc 0.8990
15:49:46.382   Training iter 300, batch loss 1.5773, batch acc 0.9046
15:49:47.042   Training iter 350, batch loss 1.5766, batch acc 0.9052
15:49:47.561   Training iter 400, batch loss 1.5748, batch acc 0.9082
15:49:48.090   Training iter 450, batch loss 1.5742, batch acc 0.9044
15:49:48.635   Training iter 500, batch loss 1.5794, batch acc 0.8994
15:49:49.157   Training iter 550, batch loss 1.5755, batch acc 0.9052
15:49:49.706   Training iter 600, batch loss 1.5776, batch acc 0.9056
15:49:49.707 Training @ 145 epoch...
15:49:50.265   Training iter 50, batch loss 1.5779, batch acc 0.8996
15:49:50.810   Training iter 100, batch loss 1.5767, batch acc 0.9002
15:49:51.351   Training iter 150, batch loss 1.5746, batch acc 0.9034
15:49:51.900   Training iter 200, batch loss 1.5780, batch acc 0.9066
15:49:52.444   Training iter 250, batch loss 1.5796, batch acc 0.8988
15:49:52.977   Training iter 300, batch loss 1.5777, batch acc 0.9016
15:49:53.517   Training iter 350, batch loss 1.5725, batch acc 0.9108
15:49:54.054   Training iter 400, batch loss 1.5742, batch acc 0.9056
15:49:54.613   Training iter 450, batch loss 1.5763, batch acc 0.9020
15:49:55.159   Training iter 500, batch loss 1.5765, batch acc 0.9046
15:49:55.685   Training iter 550, batch loss 1.5764, batch acc 0.9064
15:49:56.205   Training iter 600, batch loss 1.5782, batch acc 0.8972
15:49:56.206 Testing @ 145 epoch...
15:49:56.254     Testing, total mean loss 1.57694, total acc 0.90240
15:49:56.254 Training @ 146 epoch...
15:49:56.764   Training iter 50, batch loss 1.5788, batch acc 0.9036
15:49:57.271   Training iter 100, batch loss 1.5758, batch acc 0.9048
15:49:57.800   Training iter 150, batch loss 1.5732, batch acc 0.9036
15:49:58.332   Training iter 200, batch loss 1.5726, batch acc 0.9066
15:49:58.861   Training iter 250, batch loss 1.5791, batch acc 0.9022
15:49:59.407   Training iter 300, batch loss 1.5759, batch acc 0.9002
15:49:59.955   Training iter 350, batch loss 1.5769, batch acc 0.9024
15:50:00.487   Training iter 400, batch loss 1.5738, batch acc 0.9062
15:50:01.001   Training iter 450, batch loss 1.5762, batch acc 0.9078
15:50:01.534   Training iter 500, batch loss 1.5779, batch acc 0.9006
15:50:02.079   Training iter 550, batch loss 1.5813, batch acc 0.8948
15:50:02.592   Training iter 600, batch loss 1.5760, batch acc 0.9078
15:50:02.594 Training @ 147 epoch...
15:50:03.093   Training iter 50, batch loss 1.5774, batch acc 0.9028
15:50:03.606   Training iter 100, batch loss 1.5762, batch acc 0.9028
15:50:04.115   Training iter 150, batch loss 1.5763, batch acc 0.9050
15:50:04.635   Training iter 200, batch loss 1.5781, batch acc 0.9056
15:50:05.155   Training iter 250, batch loss 1.5704, batch acc 0.9110
15:50:05.656   Training iter 300, batch loss 1.5763, batch acc 0.9042
15:50:06.186   Training iter 350, batch loss 1.5789, batch acc 0.9048
15:50:06.722   Training iter 400, batch loss 1.5732, batch acc 0.9064
15:50:07.270   Training iter 450, batch loss 1.5787, batch acc 0.8994
15:50:07.804   Training iter 500, batch loss 1.5776, batch acc 0.9018
15:50:08.348   Training iter 550, batch loss 1.5783, batch acc 0.9040
15:50:08.871   Training iter 600, batch loss 1.5748, batch acc 0.9040
15:50:08.873 Training @ 148 epoch...
15:50:09.420   Training iter 50, batch loss 1.5783, batch acc 0.9030
15:50:09.942   Training iter 100, batch loss 1.5754, batch acc 0.9116
15:50:10.472   Training iter 150, batch loss 1.5763, batch acc 0.8976
15:50:10.991   Training iter 200, batch loss 1.5798, batch acc 0.9038
15:50:11.501   Training iter 250, batch loss 1.5793, batch acc 0.9032
15:50:12.014   Training iter 300, batch loss 1.5772, batch acc 0.9022
15:50:12.517   Training iter 350, batch loss 1.5742, batch acc 0.9106
15:50:13.021   Training iter 400, batch loss 1.5778, batch acc 0.9018
15:50:13.584   Training iter 450, batch loss 1.5701, batch acc 0.9120
15:50:14.168   Training iter 500, batch loss 1.5788, batch acc 0.8998
15:50:14.748   Training iter 550, batch loss 1.5755, batch acc 0.9014
15:50:15.280   Training iter 600, batch loss 1.5727, batch acc 0.9080
15:50:15.282 Training @ 149 epoch...
15:50:15.799   Training iter 50, batch loss 1.5721, batch acc 0.9102
15:50:16.294   Training iter 100, batch loss 1.5783, batch acc 0.9018
15:50:16.796   Training iter 150, batch loss 1.5739, batch acc 0.9092
15:50:17.303   Training iter 200, batch loss 1.5740, batch acc 0.9058
15:50:17.816   Training iter 250, batch loss 1.5746, batch acc 0.9040
15:50:18.326   Training iter 300, batch loss 1.5839, batch acc 0.8922
15:50:18.824   Training iter 350, batch loss 1.5785, batch acc 0.9030
15:50:19.322   Training iter 400, batch loss 1.5761, batch acc 0.9062
15:50:19.836   Training iter 450, batch loss 1.5759, batch acc 0.9040
15:50:20.343   Training iter 500, batch loss 1.5770, batch acc 0.9038
15:50:20.846   Training iter 550, batch loss 1.5760, batch acc 0.9050
15:50:21.356   Training iter 600, batch loss 1.5740, batch acc 0.9110
15:50:21.358 Training @ 150 epoch...
15:50:21.863   Training iter 50, batch loss 1.5784, batch acc 0.9056
15:50:22.383   Training iter 100, batch loss 1.5765, batch acc 0.9036
15:50:22.909   Training iter 150, batch loss 1.5780, batch acc 0.9022
15:50:23.459   Training iter 200, batch loss 1.5738, batch acc 0.9088
15:50:23.999   Training iter 250, batch loss 1.5743, batch acc 0.9096
15:50:24.554   Training iter 300, batch loss 1.5766, batch acc 0.8980
15:50:25.124   Training iter 350, batch loss 1.5746, batch acc 0.9080
15:50:25.727   Training iter 400, batch loss 1.5745, batch acc 0.9096
15:50:26.311   Training iter 450, batch loss 1.5739, batch acc 0.9086
15:50:26.880   Training iter 500, batch loss 1.5739, batch acc 0.9064
15:50:27.459   Training iter 550, batch loss 1.5792, batch acc 0.9014
15:50:28.034   Training iter 600, batch loss 1.5799, batch acc 0.9028
15:50:28.036 Testing @ 150 epoch...
15:50:28.093     Testing, total mean loss 1.57626, total acc 0.90290
15:50:28.093 Training @ 151 epoch...
15:50:28.660   Training iter 50, batch loss 1.5757, batch acc 0.9078
15:50:29.223   Training iter 100, batch loss 1.5752, batch acc 0.9106
15:50:29.826   Training iter 150, batch loss 1.5756, batch acc 0.9072
15:50:30.392   Training iter 200, batch loss 1.5798, batch acc 0.9014
15:50:30.956   Training iter 250, batch loss 1.5816, batch acc 0.8964
15:50:31.530   Training iter 300, batch loss 1.5808, batch acc 0.8952
15:50:32.120   Training iter 350, batch loss 1.5763, batch acc 0.9032
15:50:32.712   Training iter 400, batch loss 1.5743, batch acc 0.9006
15:50:33.316   Training iter 450, batch loss 1.5730, batch acc 0.9076
15:50:33.862   Training iter 500, batch loss 1.5730, batch acc 0.9124
15:50:34.387   Training iter 550, batch loss 1.5743, batch acc 0.9082
15:50:34.915   Training iter 600, batch loss 1.5730, batch acc 0.9106
15:50:34.916 Training @ 152 epoch...
15:50:35.440   Training iter 50, batch loss 1.5738, batch acc 0.9090
15:50:35.969   Training iter 100, batch loss 1.5790, batch acc 0.9050
15:50:36.514   Training iter 150, batch loss 1.5762, batch acc 0.9056
15:50:37.055   Training iter 200, batch loss 1.5753, batch acc 0.9046
15:50:37.617   Training iter 250, batch loss 1.5716, batch acc 0.9104
15:50:38.169   Training iter 300, batch loss 1.5757, batch acc 0.9048
15:50:38.743   Training iter 350, batch loss 1.5745, batch acc 0.9086
15:50:39.322   Training iter 400, batch loss 1.5783, batch acc 0.9036
15:50:39.897   Training iter 450, batch loss 1.5805, batch acc 0.8982
15:50:40.465   Training iter 500, batch loss 1.5684, batch acc 0.9134
15:50:41.031   Training iter 550, batch loss 1.5801, batch acc 0.9020
15:50:41.595   Training iter 600, batch loss 1.5777, batch acc 0.9056
15:50:41.597 Training @ 153 epoch...
15:50:42.167   Training iter 50, batch loss 1.5733, batch acc 0.9082
15:50:42.727   Training iter 100, batch loss 1.5794, batch acc 0.9040
15:50:43.295   Training iter 150, batch loss 1.5753, batch acc 0.9052
15:50:43.854   Training iter 200, batch loss 1.5752, batch acc 0.9098
15:50:44.381   Training iter 250, batch loss 1.5788, batch acc 0.9048
15:50:44.912   Training iter 300, batch loss 1.5776, batch acc 0.9018
15:50:45.451   Training iter 350, batch loss 1.5735, batch acc 0.9092
15:50:45.961   Training iter 400, batch loss 1.5744, batch acc 0.9058
15:50:46.475   Training iter 450, batch loss 1.5731, batch acc 0.9052
15:50:46.986   Training iter 500, batch loss 1.5773, batch acc 0.9024
15:50:47.519   Training iter 550, batch loss 1.5766, batch acc 0.9044
15:50:48.039   Training iter 600, batch loss 1.5751, batch acc 0.9082
15:50:48.040 Training @ 154 epoch...
15:50:48.568   Training iter 50, batch loss 1.5773, batch acc 0.9026
15:50:49.086   Training iter 100, batch loss 1.5765, batch acc 0.9062
15:50:49.603   Training iter 150, batch loss 1.5783, batch acc 0.9050
15:50:50.120   Training iter 200, batch loss 1.5742, batch acc 0.9078
15:50:50.656   Training iter 250, batch loss 1.5769, batch acc 0.9030
15:50:51.168   Training iter 300, batch loss 1.5748, batch acc 0.9104
15:50:51.684   Training iter 350, batch loss 1.5732, batch acc 0.9100
15:50:52.207   Training iter 400, batch loss 1.5754, batch acc 0.9078
15:50:52.713   Training iter 450, batch loss 1.5715, batch acc 0.9098
15:50:53.220   Training iter 500, batch loss 1.5775, batch acc 0.9048
15:50:53.737   Training iter 550, batch loss 1.5790, batch acc 0.9006
15:50:54.257   Training iter 600, batch loss 1.5745, batch acc 0.9086
15:50:54.259 Training @ 155 epoch...
15:50:54.791   Training iter 50, batch loss 1.5720, batch acc 0.9094
15:50:55.331   Training iter 100, batch loss 1.5784, batch acc 0.8988
15:50:55.855   Training iter 150, batch loss 1.5795, batch acc 0.9040
15:50:56.371   Training iter 200, batch loss 1.5740, batch acc 0.9094
15:50:56.901   Training iter 250, batch loss 1.5817, batch acc 0.8954
15:50:57.445   Training iter 300, batch loss 1.5711, batch acc 0.9120
15:50:57.993   Training iter 350, batch loss 1.5743, batch acc 0.9072
15:50:58.525   Training iter 400, batch loss 1.5766, batch acc 0.9048
15:50:59.067   Training iter 450, batch loss 1.5762, batch acc 0.9070
15:50:59.588   Training iter 500, batch loss 1.5749, batch acc 0.9074
15:51:00.107   Training iter 550, batch loss 1.5732, batch acc 0.9128
15:51:00.610   Training iter 600, batch loss 1.5760, batch acc 0.9060
15:51:00.612 Testing @ 155 epoch...
15:51:00.659     Testing, total mean loss 1.57615, total acc 0.90350
15:51:00.659 Training @ 156 epoch...
15:51:01.175   Training iter 50, batch loss 1.5739, batch acc 0.9076
15:51:01.718   Training iter 100, batch loss 1.5742, batch acc 0.9070
15:51:02.293   Training iter 150, batch loss 1.5783, batch acc 0.9044
15:51:02.816   Training iter 200, batch loss 1.5771, batch acc 0.9082
15:51:03.332   Training iter 250, batch loss 1.5777, batch acc 0.9042
15:51:03.883   Training iter 300, batch loss 1.5756, batch acc 0.9032
15:51:04.409   Training iter 350, batch loss 1.5737, batch acc 0.9120
15:51:04.942   Training iter 400, batch loss 1.5770, batch acc 0.9024
15:51:05.446   Training iter 450, batch loss 1.5721, batch acc 0.9084
15:51:05.958   Training iter 500, batch loss 1.5767, batch acc 0.9074
15:51:06.475   Training iter 550, batch loss 1.5777, batch acc 0.9054
15:51:06.931   Training iter 600, batch loss 1.5729, batch acc 0.9142
15:51:06.933 Training @ 157 epoch...
15:51:07.442   Training iter 50, batch loss 1.5736, batch acc 0.9070
15:51:07.947   Training iter 100, batch loss 1.5765, batch acc 0.9032
15:51:08.457   Training iter 150, batch loss 1.5712, batch acc 0.9142
15:51:08.975   Training iter 200, batch loss 1.5736, batch acc 0.9094
15:51:09.482   Training iter 250, batch loss 1.5761, batch acc 0.9100
15:51:09.984   Training iter 300, batch loss 1.5785, batch acc 0.9056
15:51:10.484   Training iter 350, batch loss 1.5762, batch acc 0.9064
15:51:10.970   Training iter 400, batch loss 1.5752, batch acc 0.9076
15:51:11.530   Training iter 450, batch loss 1.5816, batch acc 0.8988
15:51:12.105   Training iter 500, batch loss 1.5767, batch acc 0.9066
15:51:12.693   Training iter 550, batch loss 1.5743, batch acc 0.9050
15:51:13.252   Training iter 600, batch loss 1.5724, batch acc 0.9124
15:51:13.253 Training @ 158 epoch...
15:51:13.778   Training iter 50, batch loss 1.5752, batch acc 0.9072
15:51:14.292   Training iter 100, batch loss 1.5723, batch acc 0.9134
15:51:14.818   Training iter 150, batch loss 1.5780, batch acc 0.9092
15:51:15.359   Training iter 200, batch loss 1.5730, batch acc 0.9070
15:51:15.893   Training iter 250, batch loss 1.5758, batch acc 0.9044
15:51:16.410   Training iter 300, batch loss 1.5776, batch acc 0.9022
15:51:16.927   Training iter 350, batch loss 1.5779, batch acc 0.9026
15:51:17.442   Training iter 400, batch loss 1.5757, batch acc 0.9082
15:51:17.961   Training iter 450, batch loss 1.5751, batch acc 0.9080
15:51:18.470   Training iter 500, batch loss 1.5750, batch acc 0.9136
15:51:18.982   Training iter 550, batch loss 1.5779, batch acc 0.9030
15:51:19.495   Training iter 600, batch loss 1.5703, batch acc 0.9136
15:51:19.496 Training @ 159 epoch...
15:51:20.023   Training iter 50, batch loss 1.5714, batch acc 0.9138
15:51:20.531   Training iter 100, batch loss 1.5763, batch acc 0.9090
15:51:21.046   Training iter 150, batch loss 1.5680, batch acc 0.9170
15:51:21.550   Training iter 200, batch loss 1.5755, batch acc 0.9070
15:51:22.055   Training iter 250, batch loss 1.5761, batch acc 0.9072
15:51:22.582   Training iter 300, batch loss 1.5721, batch acc 0.9128
15:51:23.114   Training iter 350, batch loss 1.5692, batch acc 0.9188
15:51:23.651   Training iter 400, batch loss 1.5820, batch acc 0.9004
15:51:24.181   Training iter 450, batch loss 1.5745, batch acc 0.9100
15:51:24.712   Training iter 500, batch loss 1.5793, batch acc 0.9008
15:51:25.235   Training iter 550, batch loss 1.5804, batch acc 0.8970
15:51:25.745   Training iter 600, batch loss 1.5785, batch acc 0.9034
15:51:25.747 Training @ 160 epoch...
15:51:26.243   Training iter 50, batch loss 1.5779, batch acc 0.9028
15:51:26.749   Training iter 100, batch loss 1.5770, batch acc 0.9036
15:51:27.273   Training iter 150, batch loss 1.5735, batch acc 0.9116
15:51:27.800   Training iter 200, batch loss 1.5726, batch acc 0.9108
15:51:28.328   Training iter 250, batch loss 1.5751, batch acc 0.9070
15:51:28.875   Training iter 300, batch loss 1.5783, batch acc 0.9050
15:51:29.408   Training iter 350, batch loss 1.5779, batch acc 0.9040
15:51:29.939   Training iter 400, batch loss 1.5781, batch acc 0.9044
15:51:30.470   Training iter 450, batch loss 1.5725, batch acc 0.9146
15:51:30.985   Training iter 500, batch loss 1.5721, batch acc 0.9120
15:51:31.480   Training iter 550, batch loss 1.5724, batch acc 0.9134
15:51:31.993   Training iter 600, batch loss 1.5739, batch acc 0.9102
15:51:31.995 Testing @ 160 epoch...
15:51:32.043     Testing, total mean loss 1.57550, total acc 0.90580
15:51:32.044 Training @ 161 epoch...
15:51:32.557   Training iter 50, batch loss 1.5780, batch acc 0.9058
15:51:33.068   Training iter 100, batch loss 1.5733, batch acc 0.9096
15:51:33.582   Training iter 150, batch loss 1.5761, batch acc 0.9084
15:51:34.186   Training iter 200, batch loss 1.5755, batch acc 0.9088
15:51:34.751   Training iter 250, batch loss 1.5763, batch acc 0.9066
15:51:35.397   Training iter 300, batch loss 1.5735, batch acc 0.9084
15:51:35.924   Training iter 350, batch loss 1.5765, batch acc 0.9068
15:51:36.390   Training iter 400, batch loss 1.5753, batch acc 0.9070
15:51:36.853   Training iter 450, batch loss 1.5766, batch acc 0.9096
15:51:37.350   Training iter 500, batch loss 1.5743, batch acc 0.9116
15:51:37.847   Training iter 550, batch loss 1.5696, batch acc 0.9130
15:51:38.311   Training iter 600, batch loss 1.5753, batch acc 0.9102
15:51:38.313 Training @ 162 epoch...
15:51:38.776   Training iter 50, batch loss 1.5753, batch acc 0.9072
15:51:39.273   Training iter 100, batch loss 1.5712, batch acc 0.9144
15:51:39.755   Training iter 150, batch loss 1.5754, batch acc 0.9068
15:51:40.223   Training iter 200, batch loss 1.5722, batch acc 0.9136
15:51:40.690   Training iter 250, batch loss 1.5722, batch acc 0.9106
15:51:41.170   Training iter 300, batch loss 1.5809, batch acc 0.9056
15:51:41.657   Training iter 350, batch loss 1.5742, batch acc 0.9118
15:51:42.177   Training iter 400, batch loss 1.5786, batch acc 0.9032
15:51:42.700   Training iter 450, batch loss 1.5738, batch acc 0.9076
15:51:43.242   Training iter 500, batch loss 1.5714, batch acc 0.9146
15:51:43.769   Training iter 550, batch loss 1.5778, batch acc 0.9028
15:51:44.321   Training iter 600, batch loss 1.5756, batch acc 0.9060
15:51:44.323 Training @ 163 epoch...
15:51:44.884   Training iter 50, batch loss 1.5738, batch acc 0.9116
15:51:45.453   Training iter 100, batch loss 1.5751, batch acc 0.9038
15:51:46.036   Training iter 150, batch loss 1.5737, batch acc 0.9098
15:51:46.625   Training iter 200, batch loss 1.5739, batch acc 0.9100
15:51:47.195   Training iter 250, batch loss 1.5753, batch acc 0.9052
15:51:47.750   Training iter 300, batch loss 1.5725, batch acc 0.9156
15:51:48.313   Training iter 350, batch loss 1.5748, batch acc 0.9086
15:51:48.883   Training iter 400, batch loss 1.5784, batch acc 0.9042
15:51:49.430   Training iter 450, batch loss 1.5753, batch acc 0.9126
15:51:49.953   Training iter 500, batch loss 1.5736, batch acc 0.9082
15:51:50.444   Training iter 550, batch loss 1.5771, batch acc 0.9074
15:51:50.933   Training iter 600, batch loss 1.5736, batch acc 0.9114
15:51:50.934 Training @ 164 epoch...
15:51:51.433   Training iter 50, batch loss 1.5771, batch acc 0.9050
15:51:51.937   Training iter 100, batch loss 1.5761, batch acc 0.9042
15:51:52.450   Training iter 150, batch loss 1.5706, batch acc 0.9172
15:51:52.968   Training iter 200, batch loss 1.5747, batch acc 0.9108
15:51:53.477   Training iter 250, batch loss 1.5735, batch acc 0.9094
15:51:54.007   Training iter 300, batch loss 1.5758, batch acc 0.9076
15:51:54.546   Training iter 350, batch loss 1.5719, batch acc 0.9150
15:51:55.090   Training iter 400, batch loss 1.5755, batch acc 0.9104
15:51:55.635   Training iter 450, batch loss 1.5739, batch acc 0.9138
15:51:56.129   Training iter 500, batch loss 1.5751, batch acc 0.9086
15:51:56.631   Training iter 550, batch loss 1.5775, batch acc 0.9048
15:51:57.140   Training iter 600, batch loss 1.5739, batch acc 0.9078
15:51:57.142 Training @ 165 epoch...
15:51:57.678   Training iter 50, batch loss 1.5736, batch acc 0.9088
15:51:58.226   Training iter 100, batch loss 1.5771, batch acc 0.9052
15:51:58.772   Training iter 150, batch loss 1.5726, batch acc 0.9098
15:51:59.310   Training iter 200, batch loss 1.5743, batch acc 0.9100
15:51:59.859   Training iter 250, batch loss 1.5756, batch acc 0.9084
15:52:00.409   Training iter 300, batch loss 1.5735, batch acc 0.9110
15:52:00.945   Training iter 350, batch loss 1.5753, batch acc 0.9076
15:52:01.492   Training iter 400, batch loss 1.5721, batch acc 0.9158
15:52:02.076   Training iter 450, batch loss 1.5767, batch acc 0.9098
15:52:02.662   Training iter 500, batch loss 1.5760, batch acc 0.9076
15:52:03.250   Training iter 550, batch loss 1.5747, batch acc 0.9110
15:52:03.840   Training iter 600, batch loss 1.5721, batch acc 0.9140
15:52:03.841 Testing @ 165 epoch...
15:52:03.888     Testing, total mean loss 1.57478, total acc 0.90700
15:52:03.888 Training @ 166 epoch...
15:52:04.428   Training iter 50, batch loss 1.5757, batch acc 0.9108
15:52:04.939   Training iter 100, batch loss 1.5781, batch acc 0.9062
15:52:05.468   Training iter 150, batch loss 1.5697, batch acc 0.9156
15:52:06.019   Training iter 200, batch loss 1.5692, batch acc 0.9100
15:52:06.557   Training iter 250, batch loss 1.5810, batch acc 0.8992
15:52:07.136   Training iter 300, batch loss 1.5698, batch acc 0.9180
15:52:07.687   Training iter 350, batch loss 1.5779, batch acc 0.9064
15:52:08.202   Training iter 400, batch loss 1.5747, batch acc 0.9114
15:52:08.689   Training iter 450, batch loss 1.5734, batch acc 0.9082
15:52:09.197   Training iter 500, batch loss 1.5688, batch acc 0.9166
15:52:09.715   Training iter 550, batch loss 1.5783, batch acc 0.9038
15:52:10.241   Training iter 600, batch loss 1.5755, batch acc 0.9104
15:52:10.242 Training @ 167 epoch...
15:52:10.799   Training iter 50, batch loss 1.5731, batch acc 0.9120
15:52:11.328   Training iter 100, batch loss 1.5729, batch acc 0.9098
15:52:11.823   Training iter 150, batch loss 1.5738, batch acc 0.9092
15:52:12.333   Training iter 200, batch loss 1.5745, batch acc 0.9124
15:52:12.850   Training iter 250, batch loss 1.5722, batch acc 0.9160
15:52:13.361   Training iter 300, batch loss 1.5728, batch acc 0.9108
15:52:13.884   Training iter 350, batch loss 1.5753, batch acc 0.9074
15:52:14.425   Training iter 400, batch loss 1.5738, batch acc 0.9154
15:52:14.945   Training iter 450, batch loss 1.5742, batch acc 0.9114
15:52:15.469   Training iter 500, batch loss 1.5760, batch acc 0.9046
15:52:15.984   Training iter 550, batch loss 1.5757, batch acc 0.9062
15:52:16.524   Training iter 600, batch loss 1.5758, batch acc 0.9084
15:52:16.526 Training @ 168 epoch...
15:52:17.052   Training iter 50, batch loss 1.5736, batch acc 0.9102
15:52:17.550   Training iter 100, batch loss 1.5727, batch acc 0.9112
15:52:18.048   Training iter 150, batch loss 1.5758, batch acc 0.9078
15:52:18.565   Training iter 200, batch loss 1.5746, batch acc 0.9084
15:52:19.074   Training iter 250, batch loss 1.5738, batch acc 0.9098
15:52:19.637   Training iter 300, batch loss 1.5754, batch acc 0.9122
15:52:20.188   Training iter 350, batch loss 1.5763, batch acc 0.9080
15:52:20.738   Training iter 400, batch loss 1.5740, batch acc 0.9110
15:52:21.284   Training iter 450, batch loss 1.5730, batch acc 0.9110
15:52:21.823   Training iter 500, batch loss 1.5722, batch acc 0.9156
15:52:22.376   Training iter 550, batch loss 1.5711, batch acc 0.9142
15:52:22.935   Training iter 600, batch loss 1.5761, batch acc 0.9084
15:52:22.936 Training @ 169 epoch...
15:52:23.491   Training iter 50, batch loss 1.5738, batch acc 0.9110
15:52:24.018   Training iter 100, batch loss 1.5717, batch acc 0.9156
15:52:24.557   Training iter 150, batch loss 1.5783, batch acc 0.9056
15:52:25.099   Training iter 200, batch loss 1.5740, batch acc 0.9098
15:52:25.641   Training iter 250, batch loss 1.5731, batch acc 0.9112
15:52:26.173   Training iter 300, batch loss 1.5702, batch acc 0.9158
15:52:26.728   Training iter 350, batch loss 1.5738, batch acc 0.9100
15:52:27.279   Training iter 400, batch loss 1.5754, batch acc 0.9082
15:52:27.823   Training iter 450, batch loss 1.5767, batch acc 0.9082
15:52:28.370   Training iter 500, batch loss 1.5703, batch acc 0.9116
15:52:28.909   Training iter 550, batch loss 1.5756, batch acc 0.9074
15:52:29.446   Training iter 600, batch loss 1.5735, batch acc 0.9178
15:52:29.447 Training @ 170 epoch...
15:52:30.012   Training iter 50, batch loss 1.5755, batch acc 0.9116
15:52:30.581   Training iter 100, batch loss 1.5727, batch acc 0.9100
15:52:31.153   Training iter 150, batch loss 1.5722, batch acc 0.9168
15:52:31.734   Training iter 200, batch loss 1.5729, batch acc 0.9130
15:52:32.317   Training iter 250, batch loss 1.5695, batch acc 0.9130
15:52:32.897   Training iter 300, batch loss 1.5728, batch acc 0.9128
15:52:33.470   Training iter 350, batch loss 1.5773, batch acc 0.9070
15:52:34.037   Training iter 400, batch loss 1.5731, batch acc 0.9116
15:52:34.587   Training iter 450, batch loss 1.5726, batch acc 0.9136
15:52:35.138   Training iter 500, batch loss 1.5733, batch acc 0.9132
15:52:35.694   Training iter 550, batch loss 1.5786, batch acc 0.9066
15:52:36.216   Training iter 600, batch loss 1.5746, batch acc 0.9074
15:52:36.217 Testing @ 170 epoch...
15:52:36.264     Testing, total mean loss 1.57415, total acc 0.90840
15:52:36.264 Training @ 171 epoch...
15:52:36.798   Training iter 50, batch loss 1.5740, batch acc 0.9106
15:52:37.316   Training iter 100, batch loss 1.5675, batch acc 0.9202
15:52:37.824   Training iter 150, batch loss 1.5762, batch acc 0.9072
15:52:38.371   Training iter 200, batch loss 1.5740, batch acc 0.9104
15:52:38.922   Training iter 250, batch loss 1.5736, batch acc 0.9140
15:52:39.480   Training iter 300, batch loss 1.5743, batch acc 0.9132
15:52:40.034   Training iter 350, batch loss 1.5717, batch acc 0.9122
15:52:40.576   Training iter 400, batch loss 1.5723, batch acc 0.9140
15:52:41.114   Training iter 450, batch loss 1.5688, batch acc 0.9192
15:52:41.636   Training iter 500, batch loss 1.5762, batch acc 0.9054
15:52:42.133   Training iter 550, batch loss 1.5765, batch acc 0.9082
15:52:42.605   Training iter 600, batch loss 1.5779, batch acc 0.9042
15:52:42.607 Training @ 172 epoch...
15:52:43.087   Training iter 50, batch loss 1.5730, batch acc 0.9120
15:52:43.593   Training iter 100, batch loss 1.5707, batch acc 0.9136
15:52:44.079   Training iter 150, batch loss 1.5739, batch acc 0.9136
15:52:44.570   Training iter 200, batch loss 1.5700, batch acc 0.9144
15:52:45.057   Training iter 250, batch loss 1.5752, batch acc 0.9068
15:52:45.563   Training iter 300, batch loss 1.5746, batch acc 0.9100
15:52:46.054   Training iter 350, batch loss 1.5793, batch acc 0.9088
15:52:46.531   Training iter 400, batch loss 1.5712, batch acc 0.9134
15:52:47.023   Training iter 450, batch loss 1.5699, batch acc 0.9176
15:52:47.509   Training iter 500, batch loss 1.5744, batch acc 0.9088
15:52:47.996   Training iter 550, batch loss 1.5760, batch acc 0.9080
15:52:48.483   Training iter 600, batch loss 1.5725, batch acc 0.9142
15:52:48.485 Training @ 173 epoch...
15:52:48.976   Training iter 50, batch loss 1.5751, batch acc 0.9064
15:52:49.478   Training iter 100, batch loss 1.5748, batch acc 0.9098
15:52:49.979   Training iter 150, batch loss 1.5728, batch acc 0.9140
15:52:50.493   Training iter 200, batch loss 1.5680, batch acc 0.9160
15:52:50.989   Training iter 250, batch loss 1.5752, batch acc 0.9074
15:52:51.500   Training iter 300, batch loss 1.5722, batch acc 0.9086
15:52:51.982   Training iter 350, batch loss 1.5726, batch acc 0.9106
15:52:52.469   Training iter 400, batch loss 1.5741, batch acc 0.9148
15:52:52.965   Training iter 450, batch loss 1.5731, batch acc 0.9130
15:52:53.481   Training iter 500, batch loss 1.5747, batch acc 0.9116
15:52:54.003   Training iter 550, batch loss 1.5709, batch acc 0.9164
15:52:54.555   Training iter 600, batch loss 1.5754, batch acc 0.9130
15:52:54.557 Training @ 174 epoch...
15:52:55.123   Training iter 50, batch loss 1.5731, batch acc 0.9078
15:52:55.661   Training iter 100, batch loss 1.5706, batch acc 0.9164
15:52:56.181   Training iter 150, batch loss 1.5734, batch acc 0.9146
15:52:56.697   Training iter 200, batch loss 1.5729, batch acc 0.9114
15:52:57.215   Training iter 250, batch loss 1.5777, batch acc 0.9040
15:52:57.737   Training iter 300, batch loss 1.5747, batch acc 0.9098
15:52:58.245   Training iter 350, batch loss 1.5713, batch acc 0.9178
15:52:58.736   Training iter 400, batch loss 1.5725, batch acc 0.9090
15:52:59.244   Training iter 450, batch loss 1.5704, batch acc 0.9162
15:52:59.760   Training iter 500, batch loss 1.5738, batch acc 0.9108
15:53:00.292   Training iter 550, batch loss 1.5754, batch acc 0.9102
15:53:00.806   Training iter 600, batch loss 1.5715, batch acc 0.9110
15:53:00.808 Training @ 175 epoch...
15:53:01.350   Training iter 50, batch loss 1.5697, batch acc 0.9168
15:53:01.938   Training iter 100, batch loss 1.5716, batch acc 0.9146
15:53:02.502   Training iter 150, batch loss 1.5734, batch acc 0.9122
15:53:03.070   Training iter 200, batch loss 1.5711, batch acc 0.9130
15:53:03.643   Training iter 250, batch loss 1.5751, batch acc 0.9106
15:53:04.198   Training iter 300, batch loss 1.5744, batch acc 0.9142
15:53:04.776   Training iter 350, batch loss 1.5754, batch acc 0.9080
15:53:05.346   Training iter 400, batch loss 1.5748, batch acc 0.9110
15:53:05.916   Training iter 450, batch loss 1.5755, batch acc 0.9086
15:53:06.616   Training iter 500, batch loss 1.5732, batch acc 0.9112
15:53:07.383   Training iter 550, batch loss 1.5666, batch acc 0.9172
15:53:08.143   Training iter 600, batch loss 1.5742, batch acc 0.9076
15:53:08.146 Testing @ 175 epoch...
15:53:08.225     Testing, total mean loss 1.57328, total acc 0.90950
15:53:08.225 Training @ 176 epoch...
15:53:08.941   Training iter 50, batch loss 1.5745, batch acc 0.9114
15:53:09.480   Training iter 100, batch loss 1.5747, batch acc 0.9120
15:53:09.978   Training iter 150, batch loss 1.5721, batch acc 0.9084
15:53:10.437   Training iter 200, batch loss 1.5762, batch acc 0.9078
15:53:10.888   Training iter 250, batch loss 1.5672, batch acc 0.9168
15:53:11.354   Training iter 300, batch loss 1.5729, batch acc 0.9088
15:53:11.831   Training iter 350, batch loss 1.5678, batch acc 0.9152
15:53:12.306   Training iter 400, batch loss 1.5722, batch acc 0.9128
15:53:12.786   Training iter 450, batch loss 1.5762, batch acc 0.9102
15:53:13.282   Training iter 500, batch loss 1.5710, batch acc 0.9166
15:53:13.766   Training iter 550, batch loss 1.5708, batch acc 0.9146
15:53:14.239   Training iter 600, batch loss 1.5772, batch acc 0.9080
15:53:14.241 Training @ 177 epoch...
15:53:14.724   Training iter 50, batch loss 1.5704, batch acc 0.9146
15:53:15.209   Training iter 100, batch loss 1.5738, batch acc 0.9114
15:53:15.687   Training iter 150, batch loss 1.5679, batch acc 0.9262
15:53:16.179   Training iter 200, batch loss 1.5754, batch acc 0.9052
15:53:16.669   Training iter 250, batch loss 1.5749, batch acc 0.9092
15:53:17.151   Training iter 300, batch loss 1.5714, batch acc 0.9142
15:53:17.640   Training iter 350, batch loss 1.5716, batch acc 0.9138
15:53:18.140   Training iter 400, batch loss 1.5709, batch acc 0.9144
15:53:18.651   Training iter 450, batch loss 1.5771, batch acc 0.9072
15:53:19.174   Training iter 500, batch loss 1.5706, batch acc 0.9160
15:53:19.727   Training iter 550, batch loss 1.5765, batch acc 0.9020
15:53:20.278   Training iter 600, batch loss 1.5710, batch acc 0.9124
15:53:20.280 Training @ 178 epoch...
15:53:20.822   Training iter 50, batch loss 1.5705, batch acc 0.9186
15:53:21.346   Training iter 100, batch loss 1.5731, batch acc 0.9114
15:53:21.885   Training iter 150, batch loss 1.5690, batch acc 0.9162
15:53:22.465   Training iter 200, batch loss 1.5752, batch acc 0.9086
15:53:23.037   Training iter 250, batch loss 1.5710, batch acc 0.9154
15:53:23.632   Training iter 300, batch loss 1.5775, batch acc 0.9048
15:53:24.265   Training iter 350, batch loss 1.5744, batch acc 0.9106
15:53:24.911   Training iter 400, batch loss 1.5711, batch acc 0.9154
15:53:25.566   Training iter 450, batch loss 1.5714, batch acc 0.9134
15:53:26.190   Training iter 500, batch loss 1.5709, batch acc 0.9136
15:53:26.789   Training iter 550, batch loss 1.5695, batch acc 0.9136
15:53:27.399   Training iter 600, batch loss 1.5758, batch acc 0.9102
15:53:27.402 Training @ 179 epoch...
15:53:28.017   Training iter 50, batch loss 1.5730, batch acc 0.9138
15:53:28.695   Training iter 100, batch loss 1.5727, batch acc 0.9114
15:53:29.478   Training iter 150, batch loss 1.5734, batch acc 0.9116
15:53:30.232   Training iter 200, batch loss 1.5717, batch acc 0.9162
15:53:30.946   Training iter 250, batch loss 1.5720, batch acc 0.9096
15:53:31.556   Training iter 300, batch loss 1.5737, batch acc 0.9112
15:53:32.119   Training iter 350, batch loss 1.5717, batch acc 0.9140
15:53:32.655   Training iter 400, batch loss 1.5739, batch acc 0.9126
15:53:33.198   Training iter 450, batch loss 1.5713, batch acc 0.9108
15:53:33.714   Training iter 500, batch loss 1.5696, batch acc 0.9162
15:53:34.234   Training iter 550, batch loss 1.5750, batch acc 0.9100
15:53:34.758   Training iter 600, batch loss 1.5694, batch acc 0.9154
15:53:34.760 Training @ 180 epoch...
15:53:35.310   Training iter 50, batch loss 1.5719, batch acc 0.9142
15:53:35.859   Training iter 100, batch loss 1.5777, batch acc 0.9056
15:53:36.403   Training iter 150, batch loss 1.5676, batch acc 0.9184
15:53:36.942   Training iter 200, batch loss 1.5713, batch acc 0.9152
15:53:37.507   Training iter 250, batch loss 1.5741, batch acc 0.9102
15:53:38.075   Training iter 300, batch loss 1.5684, batch acc 0.9162
15:53:38.639   Training iter 350, batch loss 1.5726, batch acc 0.9134
15:53:39.175   Training iter 400, batch loss 1.5709, batch acc 0.9136
15:53:39.693   Training iter 450, batch loss 1.5751, batch acc 0.9082
15:53:40.219   Training iter 500, batch loss 1.5705, batch acc 0.9140
15:53:40.721   Training iter 550, batch loss 1.5744, batch acc 0.9090
15:53:41.208   Training iter 600, batch loss 1.5710, batch acc 0.9128
15:53:41.210 Testing @ 180 epoch...
15:53:41.256     Testing, total mean loss 1.57246, total acc 0.90970
15:53:41.256 Training @ 181 epoch...
15:53:41.737   Training iter 50, batch loss 1.5697, batch acc 0.9142
15:53:42.236   Training iter 100, batch loss 1.5724, batch acc 0.9136
15:53:42.727   Training iter 150, batch loss 1.5725, batch acc 0.9116
15:53:43.218   Training iter 200, batch loss 1.5767, batch acc 0.9086
15:53:43.714   Training iter 250, batch loss 1.5688, batch acc 0.9144
15:53:44.206   Training iter 300, batch loss 1.5742, batch acc 0.9106
15:53:44.711   Training iter 350, batch loss 1.5676, batch acc 0.9202
15:53:45.203   Training iter 400, batch loss 1.5735, batch acc 0.9138
15:53:45.701   Training iter 450, batch loss 1.5707, batch acc 0.9184
15:53:46.218   Training iter 500, batch loss 1.5742, batch acc 0.9102
15:53:46.747   Training iter 550, batch loss 1.5696, batch acc 0.9118
15:53:47.237   Training iter 600, batch loss 1.5739, batch acc 0.9064
15:53:47.239 Training @ 182 epoch...
15:53:47.740   Training iter 50, batch loss 1.5676, batch acc 0.9182
15:53:48.232   Training iter 100, batch loss 1.5683, batch acc 0.9116
15:53:48.734   Training iter 150, batch loss 1.5712, batch acc 0.9138
15:53:49.229   Training iter 200, batch loss 1.5754, batch acc 0.9064
15:53:49.743   Training iter 250, batch loss 1.5774, batch acc 0.9102
15:53:50.240   Training iter 300, batch loss 1.5700, batch acc 0.9122
15:53:50.769   Training iter 350, batch loss 1.5684, batch acc 0.9220
15:53:51.315   Training iter 400, batch loss 1.5730, batch acc 0.9106
15:53:51.875   Training iter 450, batch loss 1.5758, batch acc 0.9116
15:53:52.429   Training iter 500, batch loss 1.5722, batch acc 0.9122
15:53:52.985   Training iter 550, batch loss 1.5719, batch acc 0.9124
15:53:53.547   Training iter 600, batch loss 1.5712, batch acc 0.9140
15:53:53.549 Training @ 183 epoch...
15:53:54.112   Training iter 50, batch loss 1.5687, batch acc 0.9184
15:53:54.659   Training iter 100, batch loss 1.5730, batch acc 0.9112
15:53:55.215   Training iter 150, batch loss 1.5760, batch acc 0.9108
15:53:55.771   Training iter 200, batch loss 1.5719, batch acc 0.9112
15:53:56.323   Training iter 250, batch loss 1.5687, batch acc 0.9170
15:53:56.857   Training iter 300, batch loss 1.5718, batch acc 0.9102
15:53:57.382   Training iter 350, batch loss 1.5740, batch acc 0.9084
15:53:57.910   Training iter 400, batch loss 1.5703, batch acc 0.9126
15:53:58.435   Training iter 450, batch loss 1.5726, batch acc 0.9110
15:53:58.953   Training iter 500, batch loss 1.5704, batch acc 0.9164
15:53:59.468   Training iter 550, batch loss 1.5720, batch acc 0.9128
15:54:00.007   Training iter 600, batch loss 1.5712, batch acc 0.9126
15:54:00.009 Training @ 184 epoch...
15:54:00.567   Training iter 50, batch loss 1.5712, batch acc 0.9106
15:54:01.119   Training iter 100, batch loss 1.5685, batch acc 0.9166
15:54:01.676   Training iter 150, batch loss 1.5714, batch acc 0.9112
15:54:02.222   Training iter 200, batch loss 1.5714, batch acc 0.9126
15:54:02.748   Training iter 250, batch loss 1.5745, batch acc 0.9100
15:54:03.268   Training iter 300, batch loss 1.5789, batch acc 0.9048
15:54:03.807   Training iter 350, batch loss 1.5695, batch acc 0.9156
15:54:04.344   Training iter 400, batch loss 1.5716, batch acc 0.9132
15:54:04.889   Training iter 450, batch loss 1.5708, batch acc 0.9126
15:54:05.423   Training iter 500, batch loss 1.5727, batch acc 0.9124
15:54:05.924   Training iter 550, batch loss 1.5671, batch acc 0.9166
15:54:06.459   Training iter 600, batch loss 1.5716, batch acc 0.9134
15:54:06.460 Training @ 185 epoch...
15:54:06.981   Training iter 50, batch loss 1.5735, batch acc 0.9140
15:54:07.509   Training iter 100, batch loss 1.5708, batch acc 0.9124
15:54:08.034   Training iter 150, batch loss 1.5686, batch acc 0.9170
15:54:08.577   Training iter 200, batch loss 1.5655, batch acc 0.9200
15:54:09.120   Training iter 250, batch loss 1.5719, batch acc 0.9124
15:54:09.680   Training iter 300, batch loss 1.5776, batch acc 0.9018
15:54:10.244   Training iter 350, batch loss 1.5740, batch acc 0.9102
15:54:10.791   Training iter 400, batch loss 1.5723, batch acc 0.9120
15:54:11.341   Training iter 450, batch loss 1.5704, batch acc 0.9150
15:54:11.881   Training iter 500, batch loss 1.5743, batch acc 0.9092
15:54:12.439   Training iter 550, batch loss 1.5663, batch acc 0.9210
15:54:12.982   Training iter 600, batch loss 1.5726, batch acc 0.9100
15:54:12.983 Testing @ 185 epoch...
15:54:13.031     Testing, total mean loss 1.57203, total acc 0.91100
15:54:13.031 Training @ 186 epoch...
15:54:13.539   Training iter 50, batch loss 1.5685, batch acc 0.9158
15:54:14.056   Training iter 100, batch loss 1.5688, batch acc 0.9152
15:54:14.591   Training iter 150, batch loss 1.5674, batch acc 0.9164
15:54:15.117   Training iter 200, batch loss 1.5736, batch acc 0.9132
15:54:15.642   Training iter 250, batch loss 1.5711, batch acc 0.9134
15:54:16.176   Training iter 300, batch loss 1.5727, batch acc 0.9090
15:54:16.686   Training iter 350, batch loss 1.5718, batch acc 0.9140
15:54:17.184   Training iter 400, batch loss 1.5692, batch acc 0.9158
15:54:17.718   Training iter 450, batch loss 1.5717, batch acc 0.9102
15:54:18.254   Training iter 500, batch loss 1.5749, batch acc 0.9102
15:54:18.769   Training iter 550, batch loss 1.5736, batch acc 0.9086
15:54:19.269   Training iter 600, batch loss 1.5723, batch acc 0.9130
15:54:19.271 Training @ 187 epoch...
15:54:19.786   Training iter 50, batch loss 1.5706, batch acc 0.9138
15:54:20.315   Training iter 100, batch loss 1.5701, batch acc 0.9148
15:54:20.830   Training iter 150, batch loss 1.5693, batch acc 0.9152
15:54:21.333   Training iter 200, batch loss 1.5679, batch acc 0.9194
15:54:21.839   Training iter 250, batch loss 1.5703, batch acc 0.9140
15:54:22.361   Training iter 300, batch loss 1.5756, batch acc 0.9082
15:54:22.884   Training iter 350, batch loss 1.5762, batch acc 0.9114
15:54:23.434   Training iter 400, batch loss 1.5761, batch acc 0.9070
15:54:23.990   Training iter 450, batch loss 1.5703, batch acc 0.9092
15:54:24.540   Training iter 500, batch loss 1.5747, batch acc 0.9070
15:54:25.103   Training iter 550, batch loss 1.5635, batch acc 0.9204
15:54:25.682   Training iter 600, batch loss 1.5696, batch acc 0.9148
15:54:25.684 Training @ 188 epoch...
15:54:26.267   Training iter 50, batch loss 1.5747, batch acc 0.9072
15:54:26.855   Training iter 100, batch loss 1.5700, batch acc 0.9138
15:54:27.428   Training iter 150, batch loss 1.5709, batch acc 0.9116
15:54:28.005   Training iter 200, batch loss 1.5711, batch acc 0.9146
15:54:28.562   Training iter 250, batch loss 1.5706, batch acc 0.9156
15:54:29.130   Training iter 300, batch loss 1.5705, batch acc 0.9150
15:54:29.680   Training iter 350, batch loss 1.5729, batch acc 0.9120
15:54:30.210   Training iter 400, batch loss 1.5743, batch acc 0.9062
15:54:30.727   Training iter 450, batch loss 1.5684, batch acc 0.9162
15:54:31.231   Training iter 500, batch loss 1.5676, batch acc 0.9176
15:54:31.734   Training iter 550, batch loss 1.5733, batch acc 0.9156
15:54:32.233   Training iter 600, batch loss 1.5680, batch acc 0.9142
15:54:32.235 Training @ 189 epoch...
15:54:32.757   Training iter 50, batch loss 1.5748, batch acc 0.9082
15:54:33.269   Training iter 100, batch loss 1.5701, batch acc 0.9142
15:54:33.795   Training iter 150, batch loss 1.5672, batch acc 0.9162
15:54:34.323   Training iter 200, batch loss 1.5726, batch acc 0.9086
15:54:34.889   Training iter 250, batch loss 1.5720, batch acc 0.9104
15:54:35.465   Training iter 300, batch loss 1.5724, batch acc 0.9146
15:54:35.992   Training iter 350, batch loss 1.5737, batch acc 0.9114
15:54:36.523   Training iter 400, batch loss 1.5750, batch acc 0.9084
15:54:37.057   Training iter 450, batch loss 1.5673, batch acc 0.9202
15:54:37.584   Training iter 500, batch loss 1.5683, batch acc 0.9160
15:54:38.102   Training iter 550, batch loss 1.5666, batch acc 0.9166
15:54:38.622   Training iter 600, batch loss 1.5709, batch acc 0.9140
15:54:38.624 Training @ 190 epoch...
15:54:39.112   Training iter 50, batch loss 1.5677, batch acc 0.9168
15:54:39.624   Training iter 100, batch loss 1.5721, batch acc 0.9100
15:54:40.170   Training iter 150, batch loss 1.5698, batch acc 0.9122
15:54:40.693   Training iter 200, batch loss 1.5730, batch acc 0.9092
15:54:41.219   Training iter 250, batch loss 1.5739, batch acc 0.9084
15:54:41.749   Training iter 300, batch loss 1.5717, batch acc 0.9110
15:54:42.301   Training iter 350, batch loss 1.5727, batch acc 0.9120
15:54:42.861   Training iter 400, batch loss 1.5701, batch acc 0.9160
15:54:43.427   Training iter 450, batch loss 1.5706, batch acc 0.9172
15:54:43.985   Training iter 500, batch loss 1.5700, batch acc 0.9140
15:54:44.549   Training iter 550, batch loss 1.5707, batch acc 0.9136
15:54:45.107   Training iter 600, batch loss 1.5677, batch acc 0.9190
15:54:45.109 Testing @ 190 epoch...
15:54:45.156     Testing, total mean loss 1.57126, total acc 0.91080
15:54:45.156 Training @ 191 epoch...
15:54:45.712   Training iter 50, batch loss 1.5688, batch acc 0.9140
15:54:46.259   Training iter 100, batch loss 1.5735, batch acc 0.9116
15:54:46.795   Training iter 150, batch loss 1.5719, batch acc 0.9074
15:54:47.315   Training iter 200, batch loss 1.5692, batch acc 0.9106
15:54:47.845   Training iter 250, batch loss 1.5708, batch acc 0.9132
15:54:48.380   Training iter 300, batch loss 1.5680, batch acc 0.9160
15:54:48.906   Training iter 350, batch loss 1.5706, batch acc 0.9110
15:54:49.432   Training iter 400, batch loss 1.5681, batch acc 0.9174
15:54:49.961   Training iter 450, batch loss 1.5744, batch acc 0.9058
15:54:50.470   Training iter 500, batch loss 1.5732, batch acc 0.9126
15:54:50.974   Training iter 550, batch loss 1.5686, batch acc 0.9170
15:54:51.479   Training iter 600, batch loss 1.5712, batch acc 0.9162
15:54:51.481 Training @ 192 epoch...
15:54:51.993   Training iter 50, batch loss 1.5657, batch acc 0.9206
15:54:52.521   Training iter 100, batch loss 1.5698, batch acc 0.9158
15:54:53.046   Training iter 150, batch loss 1.5750, batch acc 0.9070
15:54:53.593   Training iter 200, batch loss 1.5702, batch acc 0.9128
15:54:54.121   Training iter 250, batch loss 1.5669, batch acc 0.9190
15:54:54.650   Training iter 300, batch loss 1.5723, batch acc 0.9116
15:54:55.206   Training iter 350, batch loss 1.5712, batch acc 0.9138
15:54:55.752   Training iter 400, batch loss 1.5695, batch acc 0.9124
15:54:56.304   Training iter 450, batch loss 1.5693, batch acc 0.9126
15:54:56.856   Training iter 500, batch loss 1.5678, batch acc 0.9170
15:54:57.422   Training iter 550, batch loss 1.5754, batch acc 0.9100
15:54:57.970   Training iter 600, batch loss 1.5736, batch acc 0.9098
15:54:57.971 Training @ 193 epoch...
15:54:58.526   Training iter 50, batch loss 1.5697, batch acc 0.9146
15:54:59.058   Training iter 100, batch loss 1.5738, batch acc 0.9070
15:54:59.594   Training iter 150, batch loss 1.5733, batch acc 0.9110
15:55:00.162   Training iter 200, batch loss 1.5728, batch acc 0.9126
15:55:00.735   Training iter 250, batch loss 1.5725, batch acc 0.9136
15:55:01.312   Training iter 300, batch loss 1.5661, batch acc 0.9190
15:55:01.890   Training iter 350, batch loss 1.5719, batch acc 0.9132
15:55:02.456   Training iter 400, batch loss 1.5681, batch acc 0.9154
15:55:02.984   Training iter 450, batch loss 1.5683, batch acc 0.9142
15:55:03.494   Training iter 500, batch loss 1.5657, batch acc 0.9206
15:55:04.027   Training iter 550, batch loss 1.5765, batch acc 0.9054
15:55:04.564   Training iter 600, batch loss 1.5669, batch acc 0.9156
15:55:04.566 Training @ 194 epoch...
15:55:05.126   Training iter 50, batch loss 1.5713, batch acc 0.9106
15:55:05.692   Training iter 100, batch loss 1.5719, batch acc 0.9144
15:55:06.234   Training iter 150, batch loss 1.5729, batch acc 0.9128
15:55:06.766   Training iter 200, batch loss 1.5701, batch acc 0.9104
15:55:07.266   Training iter 250, batch loss 1.5740, batch acc 0.9118
15:55:07.768   Training iter 300, batch loss 1.5673, batch acc 0.9192
15:55:08.258   Training iter 350, batch loss 1.5684, batch acc 0.9126
15:55:08.742   Training iter 400, batch loss 1.5680, batch acc 0.9184
15:55:09.232   Training iter 450, batch loss 1.5721, batch acc 0.9128
15:55:09.716   Training iter 500, batch loss 1.5726, batch acc 0.9114
15:55:10.194   Training iter 550, batch loss 1.5669, batch acc 0.9190
15:55:10.663   Training iter 600, batch loss 1.5690, batch acc 0.9140
15:55:10.664 Training @ 195 epoch...
15:55:11.123   Training iter 50, batch loss 1.5736, batch acc 0.9090
15:55:11.574   Training iter 100, batch loss 1.5699, batch acc 0.9146
15:55:12.063   Training iter 150, batch loss 1.5722, batch acc 0.9130
15:55:12.561   Training iter 200, batch loss 1.5667, batch acc 0.9190
15:55:13.061   Training iter 250, batch loss 1.5686, batch acc 0.9142
15:55:13.563   Training iter 300, batch loss 1.5714, batch acc 0.9154
15:55:14.065   Training iter 350, batch loss 1.5668, batch acc 0.9166
15:55:14.564   Training iter 400, batch loss 1.5709, batch acc 0.9104
15:55:15.061   Training iter 450, batch loss 1.5699, batch acc 0.9142
15:55:15.634   Training iter 500, batch loss 1.5735, batch acc 0.9066
15:55:16.214   Training iter 550, batch loss 1.5692, batch acc 0.9150
15:55:16.798   Training iter 600, batch loss 1.5707, batch acc 0.9144
15:55:16.800 Testing @ 195 epoch...
15:55:16.847     Testing, total mean loss 1.57068, total acc 0.91080
15:55:16.847 Training @ 196 epoch...
15:55:17.399   Training iter 50, batch loss 1.5676, batch acc 0.9176
15:55:17.906   Training iter 100, batch loss 1.5707, batch acc 0.9140
15:55:18.419   Training iter 150, batch loss 1.5690, batch acc 0.9162
15:55:18.944   Training iter 200, batch loss 1.5711, batch acc 0.9114
15:55:19.488   Training iter 250, batch loss 1.5729, batch acc 0.9070
15:55:20.033   Training iter 300, batch loss 1.5716, batch acc 0.9096
15:55:20.566   Training iter 350, batch loss 1.5693, batch acc 0.9122
15:55:21.095   Training iter 400, batch loss 1.5698, batch acc 0.9160
15:55:21.630   Training iter 450, batch loss 1.5692, batch acc 0.9158
15:55:22.180   Training iter 500, batch loss 1.5736, batch acc 0.9086
15:55:22.725   Training iter 550, batch loss 1.5661, batch acc 0.9204
15:55:23.248   Training iter 600, batch loss 1.5707, batch acc 0.9140
15:55:23.250 Training @ 197 epoch...
15:55:23.772   Training iter 50, batch loss 1.5680, batch acc 0.9174
15:55:24.287   Training iter 100, batch loss 1.5697, batch acc 0.9148
15:55:24.794   Training iter 150, batch loss 1.5676, batch acc 0.9180
15:55:25.316   Training iter 200, batch loss 1.5706, batch acc 0.9138
15:55:25.823   Training iter 250, batch loss 1.5691, batch acc 0.9124
15:55:26.337   Training iter 300, batch loss 1.5722, batch acc 0.9082
15:55:26.851   Training iter 350, batch loss 1.5711, batch acc 0.9132
15:55:27.361   Training iter 400, batch loss 1.5697, batch acc 0.9166
15:55:27.878   Training iter 450, batch loss 1.5716, batch acc 0.9100
15:55:28.430   Training iter 500, batch loss 1.5737, batch acc 0.9114
15:55:28.977   Training iter 550, batch loss 1.5639, batch acc 0.9200
15:55:29.520   Training iter 600, batch loss 1.5729, batch acc 0.9100
15:55:29.522 Training @ 198 epoch...
15:55:30.060   Training iter 50, batch loss 1.5695, batch acc 0.9116
15:55:30.594   Training iter 100, batch loss 1.5722, batch acc 0.9066
15:55:31.137   Training iter 150, batch loss 1.5729, batch acc 0.9128
15:55:31.678   Training iter 200, batch loss 1.5753, batch acc 0.9068
15:55:32.202   Training iter 250, batch loss 1.5698, batch acc 0.9194
15:55:32.702   Training iter 300, batch loss 1.5695, batch acc 0.9162
15:55:33.217   Training iter 350, batch loss 1.5694, batch acc 0.9134
15:55:33.695   Training iter 400, batch loss 1.5713, batch acc 0.9114
15:55:34.156   Training iter 450, batch loss 1.5699, batch acc 0.9156
15:55:34.642   Training iter 500, batch loss 1.5651, batch acc 0.9184
15:55:35.102   Training iter 550, batch loss 1.5659, batch acc 0.9174
15:55:35.559   Training iter 600, batch loss 1.5680, batch acc 0.9170
15:55:35.561 Training @ 199 epoch...
15:55:36.024   Training iter 50, batch loss 1.5700, batch acc 0.9116
15:55:36.480   Training iter 100, batch loss 1.5700, batch acc 0.9148
15:55:36.959   Training iter 150, batch loss 1.5658, batch acc 0.9160
15:55:37.454   Training iter 200, batch loss 1.5733, batch acc 0.9126
15:55:37.938   Training iter 250, batch loss 1.5672, batch acc 0.9138
15:55:38.441   Training iter 300, batch loss 1.5709, batch acc 0.9172
15:55:38.945   Training iter 350, batch loss 1.5711, batch acc 0.9122
15:55:39.501   Training iter 400, batch loss 1.5695, batch acc 0.9156
15:55:40.049   Training iter 450, batch loss 1.5704, batch acc 0.9116
15:55:40.547   Training iter 500, batch loss 1.5689, batch acc 0.9164
15:55:41.040   Training iter 550, batch loss 1.5712, batch acc 0.9096
15:55:41.516   Training iter 600, batch loss 1.5699, batch acc 0.9146
15:55:41.518 Training @ 200 epoch...
15:55:42.007   Training iter 50, batch loss 1.5692, batch acc 0.9128
15:55:42.453   Training iter 100, batch loss 1.5747, batch acc 0.9086
15:55:42.908   Training iter 150, batch loss 1.5709, batch acc 0.9128
15:55:43.385   Training iter 200, batch loss 1.5668, batch acc 0.9176
15:55:43.920   Training iter 250, batch loss 1.5705, batch acc 0.9156
15:55:44.467   Training iter 300, batch loss 1.5700, batch acc 0.9116
15:55:45.011   Training iter 350, batch loss 1.5699, batch acc 0.9132
15:55:45.556   Training iter 400, batch loss 1.5700, batch acc 0.9148
15:55:46.106   Training iter 450, batch loss 1.5722, batch acc 0.9110
15:55:46.654   Training iter 500, batch loss 1.5691, batch acc 0.9144
15:55:47.204   Training iter 550, batch loss 1.5679, batch acc 0.9142
15:55:47.756   Training iter 600, batch loss 1.5658, batch acc 0.9198
15:55:47.758 Testing @ 200 epoch...
15:55:47.804     Testing, total mean loss 1.57038, total acc 0.91180
15:55:47.804 Plot @ 200 epoch...
15:55:47.804 Training @ 201 epoch...
15:55:48.380   Training iter 50, batch loss 1.5689, batch acc 0.9096
15:55:48.930   Training iter 100, batch loss 1.5705, batch acc 0.9120
15:55:49.481   Training iter 150, batch loss 1.5656, batch acc 0.9194
15:55:50.016   Training iter 200, batch loss 1.5720, batch acc 0.9108
15:55:50.525   Training iter 250, batch loss 1.5740, batch acc 0.9078
15:55:51.047   Training iter 300, batch loss 1.5669, batch acc 0.9202
15:55:51.578   Training iter 350, batch loss 1.5703, batch acc 0.9128
15:55:52.117   Training iter 400, batch loss 1.5708, batch acc 0.9150
15:55:52.668   Training iter 450, batch loss 1.5690, batch acc 0.9170
15:55:53.226   Training iter 500, batch loss 1.5642, batch acc 0.9196
15:55:53.771   Training iter 550, batch loss 1.5705, batch acc 0.9144
15:55:54.296   Training iter 600, batch loss 1.5734, batch acc 0.9072
15:55:54.298 Training @ 202 epoch...
15:55:54.813   Training iter 50, batch loss 1.5705, batch acc 0.9118
15:55:55.345   Training iter 100, batch loss 1.5703, batch acc 0.9098
15:55:55.857   Training iter 150, batch loss 1.5690, batch acc 0.9138
15:55:56.386   Training iter 200, batch loss 1.5696, batch acc 0.9102
15:55:56.909   Training iter 250, batch loss 1.5672, batch acc 0.9158
15:55:57.445   Training iter 300, batch loss 1.5682, batch acc 0.9150
15:55:58.014   Training iter 350, batch loss 1.5695, batch acc 0.9164
15:55:58.578   Training iter 400, batch loss 1.5699, batch acc 0.9150
15:55:59.143   Training iter 450, batch loss 1.5695, batch acc 0.9116
15:55:59.731   Training iter 500, batch loss 1.5699, batch acc 0.9146
15:56:00.330   Training iter 550, batch loss 1.5718, batch acc 0.9168
15:56:00.913   Training iter 600, batch loss 1.5696, batch acc 0.9142
15:56:00.915 Training @ 203 epoch...
15:56:01.502   Training iter 50, batch loss 1.5677, batch acc 0.9164
15:56:02.104   Training iter 100, batch loss 1.5699, batch acc 0.9146
15:56:02.686   Training iter 150, batch loss 1.5689, batch acc 0.9110
15:56:03.247   Training iter 200, batch loss 1.5691, batch acc 0.9156
15:56:03.809   Training iter 250, batch loss 1.5711, batch acc 0.9142
15:56:04.370   Training iter 300, batch loss 1.5689, batch acc 0.9142
15:56:04.936   Training iter 350, batch loss 1.5703, batch acc 0.9112
15:56:05.498   Training iter 400, batch loss 1.5725, batch acc 0.9146
15:56:06.034   Training iter 450, batch loss 1.5703, batch acc 0.9112
15:56:06.560   Training iter 500, batch loss 1.5687, batch acc 0.9174
15:56:07.112   Training iter 550, batch loss 1.5668, batch acc 0.9134
15:56:07.651   Training iter 600, batch loss 1.5692, batch acc 0.9126
15:56:07.653 Training @ 204 epoch...
15:56:08.215   Training iter 50, batch loss 1.5671, batch acc 0.9184
15:56:08.761   Training iter 100, batch loss 1.5707, batch acc 0.9116
15:56:09.301   Training iter 150, batch loss 1.5731, batch acc 0.9076
15:56:09.832   Training iter 200, batch loss 1.5669, batch acc 0.9182
15:56:10.394   Training iter 250, batch loss 1.5639, batch acc 0.9218
15:56:10.955   Training iter 300, batch loss 1.5693, batch acc 0.9144
15:56:11.536   Training iter 350, batch loss 1.5707, batch acc 0.9124
15:56:12.085   Training iter 400, batch loss 1.5661, batch acc 0.9172
15:56:12.607   Training iter 450, batch loss 1.5685, batch acc 0.9152
15:56:13.150   Training iter 500, batch loss 1.5701, batch acc 0.9122
15:56:13.681   Training iter 550, batch loss 1.5713, batch acc 0.9082
15:56:14.213   Training iter 600, batch loss 1.5747, batch acc 0.9084
15:56:14.215 Training @ 205 epoch...
15:56:14.749   Training iter 50, batch loss 1.5683, batch acc 0.9104
15:56:15.253   Training iter 100, batch loss 1.5679, batch acc 0.9136
15:56:15.766   Training iter 150, batch loss 1.5752, batch acc 0.9052
15:56:16.251   Training iter 200, batch loss 1.5664, batch acc 0.9192
15:56:16.747   Training iter 250, batch loss 1.5723, batch acc 0.9070
15:56:17.248   Training iter 300, batch loss 1.5655, batch acc 0.9178
15:56:17.751   Training iter 350, batch loss 1.5694, batch acc 0.9158
15:56:18.264   Training iter 400, batch loss 1.5607, batch acc 0.9258
15:56:18.769   Training iter 450, batch loss 1.5680, batch acc 0.9196
15:56:19.269   Training iter 500, batch loss 1.5738, batch acc 0.9114
15:56:19.755   Training iter 550, batch loss 1.5704, batch acc 0.9136
15:56:20.260   Training iter 600, batch loss 1.5737, batch acc 0.9086
15:56:20.262 Testing @ 205 epoch...
15:56:20.308     Testing, total mean loss 1.56995, total acc 0.91170
15:56:20.308 Training @ 206 epoch...
15:56:20.817   Training iter 50, batch loss 1.5711, batch acc 0.9108
15:56:21.313   Training iter 100, batch loss 1.5664, batch acc 0.9168
15:56:21.817   Training iter 150, batch loss 1.5687, batch acc 0.9174
15:56:22.348   Training iter 200, batch loss 1.5704, batch acc 0.9120
15:56:22.857   Training iter 250, batch loss 1.5643, batch acc 0.9188
15:56:23.333   Training iter 300, batch loss 1.5715, batch acc 0.9120
15:56:23.793   Training iter 350, batch loss 1.5686, batch acc 0.9146
15:56:24.290   Training iter 400, batch loss 1.5708, batch acc 0.9132
15:56:24.790   Training iter 450, batch loss 1.5668, batch acc 0.9190
15:56:25.300   Training iter 500, batch loss 1.5720, batch acc 0.9108
15:56:25.798   Training iter 550, batch loss 1.5707, batch acc 0.9116
15:56:26.300   Training iter 600, batch loss 1.5692, batch acc 0.9112
15:56:26.302 Training @ 207 epoch...
15:56:26.819   Training iter 50, batch loss 1.5634, batch acc 0.9220
15:56:27.316   Training iter 100, batch loss 1.5670, batch acc 0.9204
15:56:27.795   Training iter 150, batch loss 1.5682, batch acc 0.9146
15:56:28.286   Training iter 200, batch loss 1.5720, batch acc 0.9056
15:56:28.778   Training iter 250, batch loss 1.5705, batch acc 0.9136
15:56:29.359   Training iter 300, batch loss 1.5717, batch acc 0.9116
15:56:29.896   Training iter 350, batch loss 1.5622, batch acc 0.9208
15:56:30.417   Training iter 400, batch loss 1.5680, batch acc 0.9116
15:56:30.929   Training iter 450, batch loss 1.5746, batch acc 0.9050
15:56:31.444   Training iter 500, batch loss 1.5738, batch acc 0.9116
15:56:31.982   Training iter 550, batch loss 1.5701, batch acc 0.9130
15:56:32.520   Training iter 600, batch loss 1.5681, batch acc 0.9182
15:56:32.522 Training @ 208 epoch...
15:56:33.062   Training iter 50, batch loss 1.5677, batch acc 0.9148
15:56:33.592   Training iter 100, batch loss 1.5670, batch acc 0.9166
15:56:34.126   Training iter 150, batch loss 1.5686, batch acc 0.9180
15:56:34.655   Training iter 200, batch loss 1.5720, batch acc 0.9126
15:56:35.179   Training iter 250, batch loss 1.5681, batch acc 0.9144
15:56:35.693   Training iter 300, batch loss 1.5681, batch acc 0.9130
15:56:36.204   Training iter 350, batch loss 1.5698, batch acc 0.9100
15:56:36.716   Training iter 400, batch loss 1.5651, batch acc 0.9206
15:56:37.247   Training iter 450, batch loss 1.5747, batch acc 0.9068
15:56:37.772   Training iter 500, batch loss 1.5699, batch acc 0.9142
15:56:38.300   Training iter 550, batch loss 1.5690, batch acc 0.9094
15:56:38.825   Training iter 600, batch loss 1.5688, batch acc 0.9160
15:56:38.826 Training @ 209 epoch...
15:56:39.376   Training iter 50, batch loss 1.5695, batch acc 0.9146
15:56:39.933   Training iter 100, batch loss 1.5676, batch acc 0.9122
15:56:40.478   Training iter 150, batch loss 1.5658, batch acc 0.9190
15:56:40.995   Training iter 200, batch loss 1.5660, batch acc 0.9184
15:56:41.509   Training iter 250, batch loss 1.5664, batch acc 0.9134
15:56:42.028   Training iter 300, batch loss 1.5708, batch acc 0.9110
15:56:42.565   Training iter 350, batch loss 1.5701, batch acc 0.9128
15:56:43.088   Training iter 400, batch loss 1.5687, batch acc 0.9134
15:56:43.596   Training iter 450, batch loss 1.5755, batch acc 0.9036
15:56:44.108   Training iter 500, batch loss 1.5723, batch acc 0.9124
15:56:44.632   Training iter 550, batch loss 1.5676, batch acc 0.9186
15:56:45.158   Training iter 600, batch loss 1.5675, batch acc 0.9166
15:56:45.160 Training @ 210 epoch...
15:56:45.686   Training iter 50, batch loss 1.5648, batch acc 0.9192
15:56:46.200   Training iter 100, batch loss 1.5705, batch acc 0.9096
15:56:46.719   Training iter 150, batch loss 1.5663, batch acc 0.9180
15:56:47.234   Training iter 200, batch loss 1.5658, batch acc 0.9170
15:56:47.746   Training iter 250, batch loss 1.5715, batch acc 0.9130
15:56:48.267   Training iter 300, batch loss 1.5737, batch acc 0.9074
15:56:48.788   Training iter 350, batch loss 1.5676, batch acc 0.9138
15:56:49.321   Training iter 400, batch loss 1.5706, batch acc 0.9164
15:56:49.845   Training iter 450, batch loss 1.5668, batch acc 0.9194
15:56:50.374   Training iter 500, batch loss 1.5678, batch acc 0.9158
15:56:50.891   Training iter 550, batch loss 1.5720, batch acc 0.9030
15:56:51.401   Training iter 600, batch loss 1.5691, batch acc 0.9176
15:56:51.403 Testing @ 210 epoch...
15:56:51.449     Testing, total mean loss 1.56973, total acc 0.91210
15:56:51.449 Training @ 211 epoch...
15:56:51.966   Training iter 50, batch loss 1.5678, batch acc 0.9160
15:56:52.490   Training iter 100, batch loss 1.5716, batch acc 0.9114
15:56:53.012   Training iter 150, batch loss 1.5690, batch acc 0.9112
15:56:53.522   Training iter 200, batch loss 1.5680, batch acc 0.9162
15:56:53.996   Training iter 250, batch loss 1.5721, batch acc 0.9094
15:56:54.485   Training iter 300, batch loss 1.5663, batch acc 0.9146
15:56:54.961   Training iter 350, batch loss 1.5657, batch acc 0.9162
15:56:55.460   Training iter 400, batch loss 1.5718, batch acc 0.9128
15:56:55.941   Training iter 450, batch loss 1.5677, batch acc 0.9192
15:56:56.425   Training iter 500, batch loss 1.5678, batch acc 0.9174
15:56:56.897   Training iter 550, batch loss 1.5699, batch acc 0.9118
15:56:57.357   Training iter 600, batch loss 1.5685, batch acc 0.9154
15:56:57.358 Training @ 212 epoch...
15:56:57.826   Training iter 50, batch loss 1.5694, batch acc 0.9132
15:56:58.293   Training iter 100, batch loss 1.5738, batch acc 0.9058
15:56:58.753   Training iter 150, batch loss 1.5676, batch acc 0.9160
15:56:59.219   Training iter 200, batch loss 1.5680, batch acc 0.9172
15:56:59.681   Training iter 250, batch loss 1.5676, batch acc 0.9132
15:57:00.153   Training iter 300, batch loss 1.5647, batch acc 0.9198
15:57:00.632   Training iter 350, batch loss 1.5651, batch acc 0.9176
15:57:01.107   Training iter 400, batch loss 1.5718, batch acc 0.9108
15:57:01.632   Training iter 450, batch loss 1.5703, batch acc 0.9114
15:57:02.178   Training iter 500, batch loss 1.5681, batch acc 0.9138
15:57:02.718   Training iter 550, batch loss 1.5663, batch acc 0.9202
15:57:03.255   Training iter 600, batch loss 1.5724, batch acc 0.9118
15:57:03.257 Training @ 213 epoch...
15:57:03.809   Training iter 50, batch loss 1.5674, batch acc 0.9152
15:57:04.343   Training iter 100, batch loss 1.5684, batch acc 0.9110
15:57:04.870   Training iter 150, batch loss 1.5689, batch acc 0.9132
15:57:05.377   Training iter 200, batch loss 1.5683, batch acc 0.9122
15:57:05.884   Training iter 250, batch loss 1.5713, batch acc 0.9134
15:57:06.398   Training iter 300, batch loss 1.5682, batch acc 0.9178
15:57:06.906   Training iter 350, batch loss 1.5658, batch acc 0.9206
15:57:07.417   Training iter 400, batch loss 1.5716, batch acc 0.9116
15:57:07.925   Training iter 450, batch loss 1.5724, batch acc 0.9090
15:57:08.445   Training iter 500, batch loss 1.5686, batch acc 0.9128
15:57:08.963   Training iter 550, batch loss 1.5668, batch acc 0.9176
15:57:09.481   Training iter 600, batch loss 1.5664, batch acc 0.9166
15:57:09.482 Training @ 214 epoch...
15:57:10.020   Training iter 50, batch loss 1.5760, batch acc 0.9076
15:57:10.542   Training iter 100, batch loss 1.5705, batch acc 0.9142
15:57:11.061   Training iter 150, batch loss 1.5674, batch acc 0.9166
15:57:11.583   Training iter 200, batch loss 1.5685, batch acc 0.9170
15:57:12.110   Training iter 250, batch loss 1.5692, batch acc 0.9120
15:57:12.623   Training iter 300, batch loss 1.5668, batch acc 0.9140
15:57:13.147   Training iter 350, batch loss 1.5698, batch acc 0.9152
15:57:13.669   Training iter 400, batch loss 1.5687, batch acc 0.9126
15:57:14.187   Training iter 450, batch loss 1.5669, batch acc 0.9156
15:57:14.712   Training iter 500, batch loss 1.5663, batch acc 0.9136
15:57:15.228   Training iter 550, batch loss 1.5669, batch acc 0.9144
15:57:15.753   Training iter 600, batch loss 1.5667, batch acc 0.9154
15:57:15.754 Training @ 215 epoch...
15:57:16.291   Training iter 50, batch loss 1.5686, batch acc 0.9148
15:57:16.823   Training iter 100, batch loss 1.5752, batch acc 0.9020
15:57:17.332   Training iter 150, batch loss 1.5711, batch acc 0.9124
15:57:17.847   Training iter 200, batch loss 1.5677, batch acc 0.9160
15:57:18.377   Training iter 250, batch loss 1.5655, batch acc 0.9230
15:57:18.894   Training iter 300, batch loss 1.5675, batch acc 0.9132
15:57:19.443   Training iter 350, batch loss 1.5625, batch acc 0.9210
15:57:19.990   Training iter 400, batch loss 1.5692, batch acc 0.9112
15:57:20.531   Training iter 450, batch loss 1.5708, batch acc 0.9142
15:57:21.076   Training iter 500, batch loss 1.5668, batch acc 0.9160
15:57:21.628   Training iter 550, batch loss 1.5741, batch acc 0.9062
15:57:22.184   Training iter 600, batch loss 1.5632, batch acc 0.9210
15:57:22.186 Testing @ 215 epoch...
15:57:22.233     Testing, total mean loss 1.56940, total acc 0.91210
15:57:22.233 Training @ 216 epoch...
15:57:22.797   Training iter 50, batch loss 1.5702, batch acc 0.9136
15:57:23.358   Training iter 100, batch loss 1.5667, batch acc 0.9118
15:57:23.908   Training iter 150, batch loss 1.5716, batch acc 0.9086
15:57:24.463   Training iter 200, batch loss 1.5626, batch acc 0.9254
15:57:25.012   Training iter 250, batch loss 1.5691, batch acc 0.9130
15:57:25.542   Training iter 300, batch loss 1.5640, batch acc 0.9204
15:57:26.063   Training iter 350, batch loss 1.5713, batch acc 0.9116
15:57:26.587   Training iter 400, batch loss 1.5694, batch acc 0.9164
15:57:27.111   Training iter 450, batch loss 1.5673, batch acc 0.9124
15:57:27.619   Training iter 500, batch loss 1.5731, batch acc 0.9090
15:57:28.128   Training iter 550, batch loss 1.5680, batch acc 0.9162
15:57:28.626   Training iter 600, batch loss 1.5689, batch acc 0.9134
15:57:28.628 Training @ 217 epoch...
15:57:29.137   Training iter 50, batch loss 1.5684, batch acc 0.9146
15:57:29.630   Training iter 100, batch loss 1.5685, batch acc 0.9146
15:57:30.130   Training iter 150, batch loss 1.5675, batch acc 0.9190
15:57:30.636   Training iter 200, batch loss 1.5698, batch acc 0.9118
15:57:31.132   Training iter 250, batch loss 1.5675, batch acc 0.9172
15:57:31.636   Training iter 300, batch loss 1.5729, batch acc 0.9042
15:57:32.144   Training iter 350, batch loss 1.5666, batch acc 0.9174
15:57:32.674   Training iter 400, batch loss 1.5672, batch acc 0.9152
15:57:33.202   Training iter 450, batch loss 1.5653, batch acc 0.9192
15:57:33.730   Training iter 500, batch loss 1.5718, batch acc 0.9072
15:57:34.278   Training iter 550, batch loss 1.5654, batch acc 0.9172
15:57:34.825   Training iter 600, batch loss 1.5703, batch acc 0.9150
15:57:34.826 Training @ 218 epoch...
15:57:35.352   Training iter 50, batch loss 1.5664, batch acc 0.9212
15:57:35.866   Training iter 100, batch loss 1.5666, batch acc 0.9146
15:57:36.373   Training iter 150, batch loss 1.5690, batch acc 0.9120
15:57:36.879   Training iter 200, batch loss 1.5702, batch acc 0.9110
15:57:37.397   Training iter 250, batch loss 1.5697, batch acc 0.9154
15:57:37.902   Training iter 300, batch loss 1.5719, batch acc 0.9144
15:57:38.402   Training iter 350, batch loss 1.5668, batch acc 0.9150
15:57:38.921   Training iter 400, batch loss 1.5702, batch acc 0.9132
15:57:39.450   Training iter 450, batch loss 1.5691, batch acc 0.9130
15:57:39.972   Training iter 500, batch loss 1.5683, batch acc 0.9150
15:57:40.492   Training iter 550, batch loss 1.5640, batch acc 0.9210
15:57:41.006   Training iter 600, batch loss 1.5681, batch acc 0.9130
15:57:41.007 Training @ 219 epoch...
15:57:41.531   Training iter 50, batch loss 1.5646, batch acc 0.9206
15:57:42.059   Training iter 100, batch loss 1.5680, batch acc 0.9186
15:57:42.570   Training iter 150, batch loss 1.5694, batch acc 0.9142
15:57:43.083   Training iter 200, batch loss 1.5683, batch acc 0.9112
15:57:43.609   Training iter 250, batch loss 1.5710, batch acc 0.9116
15:57:44.145   Training iter 300, batch loss 1.5691, batch acc 0.9124
15:57:44.699   Training iter 350, batch loss 1.5659, batch acc 0.9140
15:57:45.240   Training iter 400, batch loss 1.5683, batch acc 0.9144
15:57:45.774   Training iter 450, batch loss 1.5737, batch acc 0.9080
15:57:46.310   Training iter 500, batch loss 1.5631, batch acc 0.9234
15:57:46.840   Training iter 550, batch loss 1.5705, batch acc 0.9106
15:57:47.361   Training iter 600, batch loss 1.5676, batch acc 0.9152
15:57:47.363 Training @ 220 epoch...
15:57:47.884   Training iter 50, batch loss 1.5693, batch acc 0.9134
15:57:48.458   Training iter 100, batch loss 1.5653, batch acc 0.9166
15:57:49.037   Training iter 150, batch loss 1.5669, batch acc 0.9130
15:57:49.616   Training iter 200, batch loss 1.5686, batch acc 0.9174
15:57:50.145   Training iter 250, batch loss 1.5680, batch acc 0.9176
15:57:50.676   Training iter 300, batch loss 1.5661, batch acc 0.9162
15:57:51.188   Training iter 350, batch loss 1.5730, batch acc 0.9086
15:57:51.698   Training iter 400, batch loss 1.5670, batch acc 0.9162
15:57:52.213   Training iter 450, batch loss 1.5689, batch acc 0.9120
15:57:52.717   Training iter 500, batch loss 1.5712, batch acc 0.9118
15:57:53.221   Training iter 550, batch loss 1.5659, batch acc 0.9180
15:57:53.709   Training iter 600, batch loss 1.5685, batch acc 0.9126
15:57:53.711 Testing @ 220 epoch...
15:57:53.758     Testing, total mean loss 1.56904, total acc 0.91380
15:57:53.758 Training @ 221 epoch...
15:57:54.303   Training iter 50, batch loss 1.5724, batch acc 0.9124
15:57:54.851   Training iter 100, batch loss 1.5659, batch acc 0.9156
15:57:55.396   Training iter 150, batch loss 1.5705, batch acc 0.9130
15:57:55.950   Training iter 200, batch loss 1.5659, batch acc 0.9200
15:57:56.510   Training iter 250, batch loss 1.5697, batch acc 0.9134
15:57:57.069   Training iter 300, batch loss 1.5689, batch acc 0.9116
15:57:57.642   Training iter 350, batch loss 1.5701, batch acc 0.9090
15:57:58.209   Training iter 400, batch loss 1.5651, batch acc 0.9204
15:57:58.747   Training iter 450, batch loss 1.5665, batch acc 0.9160
15:57:59.285   Training iter 500, batch loss 1.5700, batch acc 0.9086
15:57:59.828   Training iter 550, batch loss 1.5666, batch acc 0.9170
15:58:00.372   Training iter 600, batch loss 1.5666, batch acc 0.9170
15:58:00.373 Training @ 222 epoch...
15:58:00.931   Training iter 50, batch loss 1.5714, batch acc 0.9126
15:58:01.562   Training iter 100, batch loss 1.5734, batch acc 0.9062
15:58:02.279   Training iter 150, batch loss 1.5636, batch acc 0.9190
15:58:02.944   Training iter 200, batch loss 1.5685, batch acc 0.9138
15:58:03.552   Training iter 250, batch loss 1.5693, batch acc 0.9154
15:58:04.099   Training iter 300, batch loss 1.5670, batch acc 0.9128
15:58:04.647   Training iter 350, batch loss 1.5691, batch acc 0.9116
15:58:05.218   Training iter 400, batch loss 1.5702, batch acc 0.9160
15:58:05.791   Training iter 450, batch loss 1.5661, batch acc 0.9172
15:58:06.373   Training iter 500, batch loss 1.5650, batch acc 0.9180
15:58:06.927   Training iter 550, batch loss 1.5703, batch acc 0.9090
15:58:07.469   Training iter 600, batch loss 1.5633, batch acc 0.9206
15:58:07.471 Training @ 223 epoch...
15:58:07.999   Training iter 50, batch loss 1.5667, batch acc 0.9166
15:58:08.522   Training iter 100, batch loss 1.5656, batch acc 0.9142
15:58:09.025   Training iter 150, batch loss 1.5644, batch acc 0.9204
15:58:09.541   Training iter 200, batch loss 1.5669, batch acc 0.9156
15:58:10.055   Training iter 250, batch loss 1.5697, batch acc 0.9136
15:58:10.602   Training iter 300, batch loss 1.5646, batch acc 0.9192
15:58:11.125   Training iter 350, batch loss 1.5718, batch acc 0.9106
15:58:11.637   Training iter 400, batch loss 1.5672, batch acc 0.9126
15:58:12.165   Training iter 450, batch loss 1.5689, batch acc 0.9142
15:58:12.683   Training iter 500, batch loss 1.5698, batch acc 0.9138
15:58:13.203   Training iter 550, batch loss 1.5733, batch acc 0.9106
15:58:13.693   Training iter 600, batch loss 1.5679, batch acc 0.9132
15:58:13.694 Training @ 224 epoch...
15:58:14.213   Training iter 50, batch loss 1.5676, batch acc 0.9170
15:58:14.727   Training iter 100, batch loss 1.5661, batch acc 0.9134
15:58:15.230   Training iter 150, batch loss 1.5697, batch acc 0.9114
15:58:15.741   Training iter 200, batch loss 1.5688, batch acc 0.9106
15:58:16.243   Training iter 250, batch loss 1.5695, batch acc 0.9136
15:58:16.745   Training iter 300, batch loss 1.5697, batch acc 0.9094
15:58:17.257   Training iter 350, batch loss 1.5649, batch acc 0.9206
15:58:17.755   Training iter 400, batch loss 1.5636, batch acc 0.9178
15:58:18.263   Training iter 450, batch loss 1.5721, batch acc 0.9106
15:58:18.764   Training iter 500, batch loss 1.5673, batch acc 0.9164
15:58:19.259   Training iter 550, batch loss 1.5693, batch acc 0.9138
15:58:19.753   Training iter 600, batch loss 1.5672, batch acc 0.9178
15:58:19.755 Training @ 225 epoch...
15:58:20.276   Training iter 50, batch loss 1.5704, batch acc 0.9128
15:58:20.761   Training iter 100, batch loss 1.5656, batch acc 0.9216
15:58:21.227   Training iter 150, batch loss 1.5681, batch acc 0.9132
15:58:21.695   Training iter 200, batch loss 1.5671, batch acc 0.9176
15:58:22.170   Training iter 250, batch loss 1.5706, batch acc 0.9076
15:58:22.677   Training iter 300, batch loss 1.5676, batch acc 0.9170
15:58:23.168   Training iter 350, batch loss 1.5694, batch acc 0.9146
15:58:23.666   Training iter 400, batch loss 1.5594, batch acc 0.9242
15:58:24.150   Training iter 450, batch loss 1.5712, batch acc 0.9098
15:58:24.626   Training iter 500, batch loss 1.5666, batch acc 0.9170
15:58:25.106   Training iter 550, batch loss 1.5713, batch acc 0.9072
15:58:25.590   Training iter 600, batch loss 1.5679, batch acc 0.9148
15:58:25.591 Testing @ 225 epoch...
15:58:25.638     Testing, total mean loss 1.56862, total acc 0.91380
15:58:25.638 Training @ 226 epoch...
15:58:26.125   Training iter 50, batch loss 1.5649, batch acc 0.9210
15:58:26.599   Training iter 100, batch loss 1.5684, batch acc 0.9168
15:58:27.082   Training iter 150, batch loss 1.5701, batch acc 0.9124
15:58:27.641   Training iter 200, batch loss 1.5652, batch acc 0.9178
15:58:28.218   Training iter 250, batch loss 1.5668, batch acc 0.9160
15:58:28.778   Training iter 300, batch loss 1.5695, batch acc 0.9122
15:58:29.268   Training iter 350, batch loss 1.5684, batch acc 0.9150
15:58:29.739   Training iter 400, batch loss 1.5676, batch acc 0.9112
15:58:30.213   Training iter 450, batch loss 1.5696, batch acc 0.9124
15:58:30.697   Training iter 500, batch loss 1.5672, batch acc 0.9150
15:58:31.186   Training iter 550, batch loss 1.5679, batch acc 0.9172
15:58:31.699   Training iter 600, batch loss 1.5692, batch acc 0.9106
15:58:31.701 Training @ 227 epoch...
15:58:32.211   Training iter 50, batch loss 1.5657, batch acc 0.9176
15:58:32.729   Training iter 100, batch loss 1.5654, batch acc 0.9142
15:58:33.245   Training iter 150, batch loss 1.5697, batch acc 0.9130
15:58:33.754   Training iter 200, batch loss 1.5702, batch acc 0.9102
15:58:34.282   Training iter 250, batch loss 1.5710, batch acc 0.9102
15:58:34.815   Training iter 300, batch loss 1.5705, batch acc 0.9144
15:58:35.346   Training iter 350, batch loss 1.5652, batch acc 0.9178
15:58:35.874   Training iter 400, batch loss 1.5693, batch acc 0.9158
15:58:36.385   Training iter 450, batch loss 1.5652, batch acc 0.9156
15:58:36.892   Training iter 500, batch loss 1.5685, batch acc 0.9132
15:58:37.391   Training iter 550, batch loss 1.5661, batch acc 0.9156
15:58:37.882   Training iter 600, batch loss 1.5675, batch acc 0.9156
15:58:37.883 Training @ 228 epoch...
15:58:38.382   Training iter 50, batch loss 1.5643, batch acc 0.9202
15:58:38.884   Training iter 100, batch loss 1.5706, batch acc 0.9096
15:58:39.390   Training iter 150, batch loss 1.5670, batch acc 0.9176
15:58:39.899   Training iter 200, batch loss 1.5679, batch acc 0.9178
15:58:40.414   Training iter 250, batch loss 1.5683, batch acc 0.9116
15:58:40.930   Training iter 300, batch loss 1.5661, batch acc 0.9100
15:58:41.448   Training iter 350, batch loss 1.5708, batch acc 0.9088
15:58:41.957   Training iter 400, batch loss 1.5702, batch acc 0.9124
15:58:42.486   Training iter 450, batch loss 1.5707, batch acc 0.9164
15:58:43.009   Training iter 500, batch loss 1.5659, batch acc 0.9174
15:58:43.530   Training iter 550, batch loss 1.5638, batch acc 0.9222
15:58:44.046   Training iter 600, batch loss 1.5676, batch acc 0.9138
15:58:44.048 Training @ 229 epoch...
15:58:44.561   Training iter 50, batch loss 1.5663, batch acc 0.9134
15:58:45.070   Training iter 100, batch loss 1.5621, batch acc 0.9192
15:58:45.599   Training iter 150, batch loss 1.5659, batch acc 0.9170
15:58:46.145   Training iter 200, batch loss 1.5704, batch acc 0.9118
15:58:46.680   Training iter 250, batch loss 1.5681, batch acc 0.9106
15:58:47.190   Training iter 300, batch loss 1.5695, batch acc 0.9162
15:58:47.695   Training iter 350, batch loss 1.5762, batch acc 0.9056
15:58:48.209   Training iter 400, batch loss 1.5690, batch acc 0.9124
15:58:48.717   Training iter 450, batch loss 1.5623, batch acc 0.9224
15:58:49.222   Training iter 500, batch loss 1.5702, batch acc 0.9106
15:58:49.738   Training iter 550, batch loss 1.5664, batch acc 0.9148
15:58:50.268   Training iter 600, batch loss 1.5661, batch acc 0.9192
15:58:50.269 Training @ 230 epoch...
15:58:50.799   Training iter 50, batch loss 1.5651, batch acc 0.9194
15:58:51.319   Training iter 100, batch loss 1.5666, batch acc 0.9140
15:58:51.845   Training iter 150, batch loss 1.5657, batch acc 0.9146
15:58:52.353   Training iter 200, batch loss 1.5708, batch acc 0.9138
15:58:52.856   Training iter 250, batch loss 1.5711, batch acc 0.9098
15:58:53.356   Training iter 300, batch loss 1.5686, batch acc 0.9126
15:58:53.850   Training iter 350, batch loss 1.5683, batch acc 0.9148
15:58:54.369   Training iter 400, batch loss 1.5648, batch acc 0.9178
15:58:54.875   Training iter 450, batch loss 1.5718, batch acc 0.9096
15:58:55.391   Training iter 500, batch loss 1.5653, batch acc 0.9156
15:58:55.890   Training iter 550, batch loss 1.5639, batch acc 0.9172
15:58:56.413   Training iter 600, batch loss 1.5700, batch acc 0.9130
15:58:56.415 Testing @ 230 epoch...
15:58:56.464     Testing, total mean loss 1.56861, total acc 0.91190
15:58:56.464 Training @ 231 epoch...
15:58:56.994   Training iter 50, batch loss 1.5635, batch acc 0.9192
15:58:57.556   Training iter 100, batch loss 1.5670, batch acc 0.9128
15:58:58.100   Training iter 150, batch loss 1.5650, batch acc 0.9206
15:58:58.642   Training iter 200, batch loss 1.5669, batch acc 0.9182
15:58:59.203   Training iter 250, batch loss 1.5674, batch acc 0.9144
15:58:59.763   Training iter 300, batch loss 1.5709, batch acc 0.9104
15:59:00.341   Training iter 350, batch loss 1.5659, batch acc 0.9168
15:59:00.920   Training iter 400, batch loss 1.5676, batch acc 0.9162
15:59:01.518   Training iter 450, batch loss 1.5704, batch acc 0.9118
15:59:02.196   Training iter 500, batch loss 1.5695, batch acc 0.9098
15:59:02.785   Training iter 550, batch loss 1.5681, batch acc 0.9120
15:59:03.352   Training iter 600, batch loss 1.5699, batch acc 0.9106
15:59:03.354 Training @ 232 epoch...
15:59:03.929   Training iter 50, batch loss 1.5639, batch acc 0.9142
15:59:04.454   Training iter 100, batch loss 1.5611, batch acc 0.9222
15:59:05.005   Training iter 150, batch loss 1.5663, batch acc 0.9174
15:59:05.547   Training iter 200, batch loss 1.5696, batch acc 0.9136
15:59:06.095   Training iter 250, batch loss 1.5682, batch acc 0.9160
15:59:06.655   Training iter 300, batch loss 1.5638, batch acc 0.9182
15:59:07.202   Training iter 350, batch loss 1.5698, batch acc 0.9108
15:59:07.707   Training iter 400, batch loss 1.5674, batch acc 0.9138
15:59:08.207   Training iter 450, batch loss 1.5686, batch acc 0.9106
15:59:08.708   Training iter 500, batch loss 1.5698, batch acc 0.9128
15:59:09.215   Training iter 550, batch loss 1.5670, batch acc 0.9160
15:59:09.733   Training iter 600, batch loss 1.5755, batch acc 0.9070
15:59:09.735 Training @ 233 epoch...
15:59:10.269   Training iter 50, batch loss 1.5689, batch acc 0.9136
15:59:10.797   Training iter 100, batch loss 1.5667, batch acc 0.9160
15:59:11.323   Training iter 150, batch loss 1.5736, batch acc 0.9086
15:59:11.849   Training iter 200, batch loss 1.5676, batch acc 0.9154
15:59:12.378   Training iter 250, batch loss 1.5707, batch acc 0.9080
15:59:12.905   Training iter 300, batch loss 1.5640, batch acc 0.9188
15:59:13.412   Training iter 350, batch loss 1.5686, batch acc 0.9112
15:59:13.939   Training iter 400, batch loss 1.5652, batch acc 0.9170
15:59:14.499   Training iter 450, batch loss 1.5670, batch acc 0.9106
15:59:15.059   Training iter 500, batch loss 1.5661, batch acc 0.9170
15:59:15.629   Training iter 550, batch loss 1.5649, batch acc 0.9212
15:59:16.178   Training iter 600, batch loss 1.5672, batch acc 0.9150
15:59:16.180 Training @ 234 epoch...
15:59:16.731   Training iter 50, batch loss 1.5684, batch acc 0.9154
15:59:17.272   Training iter 100, batch loss 1.5656, batch acc 0.9174
15:59:17.812   Training iter 150, batch loss 1.5674, batch acc 0.9128
15:59:18.357   Training iter 200, batch loss 1.5640, batch acc 0.9178
15:59:18.915   Training iter 250, batch loss 1.5673, batch acc 0.9138
15:59:19.450   Training iter 300, batch loss 1.5691, batch acc 0.9156
15:59:19.980   Training iter 350, batch loss 1.5691, batch acc 0.9102
15:59:20.480   Training iter 400, batch loss 1.5681, batch acc 0.9148
15:59:20.987   Training iter 450, batch loss 1.5694, batch acc 0.9120
15:59:21.495   Training iter 500, batch loss 1.5696, batch acc 0.9130
15:59:22.008   Training iter 550, batch loss 1.5653, batch acc 0.9186
15:59:22.534   Training iter 600, batch loss 1.5672, batch acc 0.9124
15:59:22.536 Training @ 235 epoch...
15:59:23.102   Training iter 50, batch loss 1.5646, batch acc 0.9168
15:59:23.675   Training iter 100, batch loss 1.5644, batch acc 0.9142
15:59:24.260   Training iter 150, batch loss 1.5682, batch acc 0.9152
15:59:24.805   Training iter 200, batch loss 1.5737, batch acc 0.9064
15:59:25.293   Training iter 250, batch loss 1.5655, batch acc 0.9198
15:59:25.781   Training iter 300, batch loss 1.5687, batch acc 0.9138
15:59:26.272   Training iter 350, batch loss 1.5672, batch acc 0.9116
15:59:26.762   Training iter 400, batch loss 1.5679, batch acc 0.9174
15:59:27.249   Training iter 450, batch loss 1.5672, batch acc 0.9160
15:59:27.742   Training iter 500, batch loss 1.5673, batch acc 0.9148
15:59:28.274   Training iter 550, batch loss 1.5677, batch acc 0.9138
15:59:28.785   Training iter 600, batch loss 1.5668, batch acc 0.9142
15:59:28.787 Testing @ 235 epoch...
15:59:28.833     Testing, total mean loss 1.56815, total acc 0.91260
15:59:28.833 Training @ 236 epoch...
15:59:29.344   Training iter 50, batch loss 1.5671, batch acc 0.9144
15:59:29.823   Training iter 100, batch loss 1.5636, batch acc 0.9174
15:59:30.339   Training iter 150, batch loss 1.5695, batch acc 0.9092
15:59:30.882   Training iter 200, batch loss 1.5687, batch acc 0.9124
15:59:31.426   Training iter 250, batch loss 1.5661, batch acc 0.9156
15:59:31.992   Training iter 300, batch loss 1.5659, batch acc 0.9176
15:59:32.544   Training iter 350, batch loss 1.5677, batch acc 0.9138
15:59:33.060   Training iter 400, batch loss 1.5701, batch acc 0.9120
15:59:33.578   Training iter 450, batch loss 1.5644, batch acc 0.9224
15:59:34.118   Training iter 500, batch loss 1.5721, batch acc 0.9098
15:59:34.627   Training iter 550, batch loss 1.5649, batch acc 0.9168
15:59:35.133   Training iter 600, batch loss 1.5692, batch acc 0.9158
15:59:35.134 Training @ 237 epoch...
15:59:35.628   Training iter 50, batch loss 1.5716, batch acc 0.9076
15:59:36.134   Training iter 100, batch loss 1.5664, batch acc 0.9152
15:59:36.653   Training iter 150, batch loss 1.5702, batch acc 0.9100
15:59:37.166   Training iter 200, batch loss 1.5678, batch acc 0.9158
15:59:37.669   Training iter 250, batch loss 1.5662, batch acc 0.9158
15:59:38.176   Training iter 300, batch loss 1.5687, batch acc 0.9122
15:59:38.685   Training iter 350, batch loss 1.5659, batch acc 0.9178
15:59:39.189   Training iter 400, batch loss 1.5690, batch acc 0.9156
15:59:39.655   Training iter 450, batch loss 1.5652, batch acc 0.9150
15:59:40.151   Training iter 500, batch loss 1.5677, batch acc 0.9146
15:59:40.625   Training iter 550, batch loss 1.5664, batch acc 0.9172
15:59:41.093   Training iter 600, batch loss 1.5635, batch acc 0.9218
15:59:41.095 Training @ 238 epoch...
15:59:41.567   Training iter 50, batch loss 1.5692, batch acc 0.9124
15:59:42.044   Training iter 100, batch loss 1.5648, batch acc 0.9138
15:59:42.523   Training iter 150, batch loss 1.5657, batch acc 0.9152
15:59:43.007   Training iter 200, batch loss 1.5673, batch acc 0.9174
15:59:43.498   Training iter 250, batch loss 1.5744, batch acc 0.9076
15:59:43.986   Training iter 300, batch loss 1.5661, batch acc 0.9174
15:59:44.456   Training iter 350, batch loss 1.5653, batch acc 0.9164
15:59:44.932   Training iter 400, batch loss 1.5649, batch acc 0.9192
15:59:45.427   Training iter 450, batch loss 1.5643, batch acc 0.9180
15:59:45.938   Training iter 500, batch loss 1.5653, batch acc 0.9164
15:59:46.467   Training iter 550, batch loss 1.5705, batch acc 0.9114
15:59:46.999   Training iter 600, batch loss 1.5703, batch acc 0.9118
15:59:47.002 Training @ 239 epoch...
15:59:47.519   Training iter 50, batch loss 1.5706, batch acc 0.9098
15:59:48.026   Training iter 100, batch loss 1.5662, batch acc 0.9162
15:59:48.518   Training iter 150, batch loss 1.5704, batch acc 0.9110
15:59:49.014   Training iter 200, batch loss 1.5660, batch acc 0.9172
15:59:49.509   Training iter 250, batch loss 1.5680, batch acc 0.9132
15:59:50.014   Training iter 300, batch loss 1.5660, batch acc 0.9188
15:59:50.529   Training iter 350, batch loss 1.5694, batch acc 0.9122
15:59:51.040   Training iter 400, batch loss 1.5645, batch acc 0.9182
15:59:51.533   Training iter 450, batch loss 1.5649, batch acc 0.9198
15:59:52.022   Training iter 500, batch loss 1.5700, batch acc 0.9088
15:59:52.512   Training iter 550, batch loss 1.5639, batch acc 0.9156
15:59:53.004   Training iter 600, batch loss 1.5677, batch acc 0.9170
15:59:53.006 Training @ 240 epoch...
15:59:53.504   Training iter 50, batch loss 1.5651, batch acc 0.9204
15:59:54.014   Training iter 100, batch loss 1.5654, batch acc 0.9194
15:59:54.528   Training iter 150, batch loss 1.5654, batch acc 0.9148
15:59:55.048   Training iter 200, batch loss 1.5675, batch acc 0.9140
15:59:55.579   Training iter 250, batch loss 1.5673, batch acc 0.9160
15:59:56.088   Training iter 300, batch loss 1.5671, batch acc 0.9160
15:59:56.589   Training iter 350, batch loss 1.5647, batch acc 0.9174
15:59:57.086   Training iter 400, batch loss 1.5676, batch acc 0.9170
15:59:57.596   Training iter 450, batch loss 1.5708, batch acc 0.9096
15:59:58.111   Training iter 500, batch loss 1.5651, batch acc 0.9134
15:59:58.628   Training iter 550, batch loss 1.5706, batch acc 0.9110
15:59:59.135   Training iter 600, batch loss 1.5705, batch acc 0.9118
15:59:59.136 Testing @ 240 epoch...
15:59:59.183     Testing, total mean loss 1.56804, total acc 0.91180
15:59:59.183 Training @ 241 epoch...
15:59:59.696   Training iter 50, batch loss 1.5649, batch acc 0.9126
16:00:00.247   Training iter 100, batch loss 1.5712, batch acc 0.9110
16:00:00.824   Training iter 150, batch loss 1.5695, batch acc 0.9138
16:00:01.431   Training iter 200, batch loss 1.5686, batch acc 0.9110
16:00:02.121   Training iter 250, batch loss 1.5633, batch acc 0.9184
16:00:02.748   Training iter 300, batch loss 1.5627, batch acc 0.9228
16:00:03.364   Training iter 350, batch loss 1.5684, batch acc 0.9142
16:00:03.998   Training iter 400, batch loss 1.5676, batch acc 0.9128
16:00:04.632   Training iter 450, batch loss 1.5693, batch acc 0.9134
16:00:05.248   Training iter 500, batch loss 1.5634, batch acc 0.9184
16:00:05.861   Training iter 550, batch loss 1.5683, batch acc 0.9138
16:00:06.502   Training iter 600, batch loss 1.5691, batch acc 0.9122
16:00:06.505 Training @ 242 epoch...
16:00:07.141   Training iter 50, batch loss 1.5689, batch acc 0.9144
16:00:07.771   Training iter 100, batch loss 1.5684, batch acc 0.9122
16:00:08.390   Training iter 150, batch loss 1.5713, batch acc 0.9098
16:00:09.012   Training iter 200, batch loss 1.5684, batch acc 0.9166
16:00:09.615   Training iter 250, batch loss 1.5632, batch acc 0.9162
16:00:10.237   Training iter 300, batch loss 1.5613, batch acc 0.9228
16:00:10.879   Training iter 350, batch loss 1.5674, batch acc 0.9122
16:00:11.534   Training iter 400, batch loss 1.5667, batch acc 0.9142
16:00:12.174   Training iter 450, batch loss 1.5691, batch acc 0.9140
16:00:12.819   Training iter 500, batch loss 1.5697, batch acc 0.9086
16:00:13.409   Training iter 550, batch loss 1.5655, batch acc 0.9200
16:00:14.016   Training iter 600, batch loss 1.5663, batch acc 0.9168
16:00:14.018 Training @ 243 epoch...
16:00:14.630   Training iter 50, batch loss 1.5642, batch acc 0.9156
16:00:15.240   Training iter 100, batch loss 1.5643, batch acc 0.9178
16:00:15.847   Training iter 150, batch loss 1.5644, batch acc 0.9164
16:00:16.448   Training iter 200, batch loss 1.5698, batch acc 0.9100
16:00:17.052   Training iter 250, batch loss 1.5666, batch acc 0.9138
16:00:17.649   Training iter 300, batch loss 1.5690, batch acc 0.9120
16:00:18.251   Training iter 350, batch loss 1.5676, batch acc 0.9130
16:00:18.862   Training iter 400, batch loss 1.5662, batch acc 0.9180
16:00:19.453   Training iter 450, batch loss 1.5692, batch acc 0.9118
16:00:20.045   Training iter 500, batch loss 1.5684, batch acc 0.9136
16:00:20.640   Training iter 550, batch loss 1.5684, batch acc 0.9160
16:00:21.219   Training iter 600, batch loss 1.5673, batch acc 0.9192
16:00:21.220 Training @ 244 epoch...
16:00:21.820   Training iter 50, batch loss 1.5664, batch acc 0.9174
16:00:22.417   Training iter 100, batch loss 1.5687, batch acc 0.9126
16:00:23.013   Training iter 150, batch loss 1.5661, batch acc 0.9128
16:00:23.608   Training iter 200, batch loss 1.5626, batch acc 0.9178
16:00:24.206   Training iter 250, batch loss 1.5666, batch acc 0.9152
16:00:24.765   Training iter 300, batch loss 1.5675, batch acc 0.9160
16:00:25.335   Training iter 350, batch loss 1.5684, batch acc 0.9136
16:00:25.891   Training iter 400, batch loss 1.5694, batch acc 0.9136
16:00:26.464   Training iter 450, batch loss 1.5663, batch acc 0.9164
16:00:27.028   Training iter 500, batch loss 1.5675, batch acc 0.9138
16:00:27.580   Training iter 550, batch loss 1.5683, batch acc 0.9116
16:00:28.133   Training iter 600, batch loss 1.5669, batch acc 0.9114
16:00:28.135 Training @ 245 epoch...
16:00:28.682   Training iter 50, batch loss 1.5718, batch acc 0.9036
16:00:29.221   Training iter 100, batch loss 1.5682, batch acc 0.9138
16:00:29.761   Training iter 150, batch loss 1.5699, batch acc 0.9108
16:00:30.332   Training iter 200, batch loss 1.5662, batch acc 0.9128
16:00:30.886   Training iter 250, batch loss 1.5680, batch acc 0.9144
16:00:31.403   Training iter 300, batch loss 1.5670, batch acc 0.9146
16:00:31.960   Training iter 350, batch loss 1.5650, batch acc 0.9208
16:00:32.470   Training iter 400, batch loss 1.5671, batch acc 0.9130
16:00:32.955   Training iter 450, batch loss 1.5665, batch acc 0.9134
16:00:33.446   Training iter 500, batch loss 1.5630, batch acc 0.9220
16:00:33.920   Training iter 550, batch loss 1.5658, batch acc 0.9178
16:00:34.418   Training iter 600, batch loss 1.5661, batch acc 0.9170
16:00:34.420 Testing @ 245 epoch...
16:00:34.467     Testing, total mean loss 1.56791, total acc 0.91280
16:00:34.467 Training @ 246 epoch...
16:00:34.954   Training iter 50, batch loss 1.5663, batch acc 0.9162
16:00:35.466   Training iter 100, batch loss 1.5662, batch acc 0.9154
16:00:35.963   Training iter 150, batch loss 1.5689, batch acc 0.9112
16:00:36.469   Training iter 200, batch loss 1.5684, batch acc 0.9168
16:00:36.985   Training iter 250, batch loss 1.5650, batch acc 0.9196
16:00:37.514   Training iter 300, batch loss 1.5705, batch acc 0.9074
16:00:38.045   Training iter 350, batch loss 1.5639, batch acc 0.9174
16:00:38.561   Training iter 400, batch loss 1.5677, batch acc 0.9142
16:00:39.075   Training iter 450, batch loss 1.5681, batch acc 0.9136
16:00:39.586   Training iter 500, batch loss 1.5673, batch acc 0.9100
16:00:40.089   Training iter 550, batch loss 1.5655, batch acc 0.9202
16:00:40.576   Training iter 600, batch loss 1.5660, batch acc 0.9176
16:00:40.577 Training @ 247 epoch...
16:00:41.050   Training iter 50, batch loss 1.5630, batch acc 0.9184
16:00:41.542   Training iter 100, batch loss 1.5686, batch acc 0.9134
16:00:42.052   Training iter 150, batch loss 1.5678, batch acc 0.9154
16:00:42.557   Training iter 200, batch loss 1.5673, batch acc 0.9164
16:00:43.055   Training iter 250, batch loss 1.5645, batch acc 0.9184
16:00:43.555   Training iter 300, batch loss 1.5666, batch acc 0.9112
16:00:44.072   Training iter 350, batch loss 1.5714, batch acc 0.9138
16:00:44.602   Training iter 400, batch loss 1.5639, batch acc 0.9204
16:00:45.136   Training iter 450, batch loss 1.5636, batch acc 0.9194
16:00:45.675   Training iter 500, batch loss 1.5644, batch acc 0.9160
16:00:46.199   Training iter 550, batch loss 1.5719, batch acc 0.9078
16:00:46.737   Training iter 600, batch loss 1.5707, batch acc 0.9092
16:00:46.738 Training @ 248 epoch...
16:00:47.185   Training iter 50, batch loss 1.5650, batch acc 0.9176
16:00:47.617   Training iter 100, batch loss 1.5660, batch acc 0.9136
16:00:48.085   Training iter 150, batch loss 1.5679, batch acc 0.9146
16:00:48.545   Training iter 200, batch loss 1.5708, batch acc 0.9086
16:00:49.005   Training iter 250, batch loss 1.5682, batch acc 0.9140
16:00:49.496   Training iter 300, batch loss 1.5646, batch acc 0.9206
16:00:50.018   Training iter 350, batch loss 1.5651, batch acc 0.9162
16:00:50.542   Training iter 400, batch loss 1.5679, batch acc 0.9154
16:00:51.064   Training iter 450, batch loss 1.5646, batch acc 0.9162
16:00:51.594   Training iter 500, batch loss 1.5713, batch acc 0.9126
16:00:52.113   Training iter 550, batch loss 1.5674, batch acc 0.9140
16:00:52.620   Training iter 600, batch loss 1.5646, batch acc 0.9164
16:00:52.622 Training @ 249 epoch...
16:00:53.138   Training iter 50, batch loss 1.5650, batch acc 0.9180
16:00:53.688   Training iter 100, batch loss 1.5674, batch acc 0.9152
16:00:54.249   Training iter 150, batch loss 1.5689, batch acc 0.9128
16:00:54.801   Training iter 200, batch loss 1.5631, batch acc 0.9200
16:00:55.350   Training iter 250, batch loss 1.5662, batch acc 0.9178
16:00:55.876   Training iter 300, batch loss 1.5694, batch acc 0.9074
16:00:56.404   Training iter 350, batch loss 1.5704, batch acc 0.9088
16:00:56.924   Training iter 400, batch loss 1.5665, batch acc 0.9174
16:00:57.436   Training iter 450, batch loss 1.5618, batch acc 0.9194
16:00:57.931   Training iter 500, batch loss 1.5677, batch acc 0.9162
16:00:58.481   Training iter 550, batch loss 1.5675, batch acc 0.9178
16:00:59.039   Training iter 600, batch loss 1.5693, batch acc 0.9110
16:00:59.041 Training @ 250 epoch...
16:00:59.644   Training iter 50, batch loss 1.5635, batch acc 0.9182
16:01:00.187   Training iter 100, batch loss 1.5637, batch acc 0.9184
16:01:00.714   Training iter 150, batch loss 1.5640, batch acc 0.9184
16:01:01.266   Training iter 200, batch loss 1.5701, batch acc 0.9110
16:01:01.827   Training iter 250, batch loss 1.5712, batch acc 0.9102
16:01:02.410   Training iter 300, batch loss 1.5646, batch acc 0.9172
16:01:02.940   Training iter 350, batch loss 1.5640, batch acc 0.9162
16:01:03.458   Training iter 400, batch loss 1.5668, batch acc 0.9176
16:01:04.006   Training iter 450, batch loss 1.5670, batch acc 0.9166
16:01:04.555   Training iter 500, batch loss 1.5655, batch acc 0.9170
16:01:05.096   Training iter 550, batch loss 1.5720, batch acc 0.9086
16:01:05.642   Training iter 600, batch loss 1.5702, batch acc 0.9104
16:01:05.643 Testing @ 250 epoch...
16:01:05.690     Testing, total mean loss 1.56799, total acc 0.91360
16:01:05.690 Training @ 251 epoch...
16:01:06.253   Training iter 50, batch loss 1.5677, batch acc 0.9130
16:01:06.805   Training iter 100, batch loss 1.5656, batch acc 0.9136
16:01:07.358   Training iter 150, batch loss 1.5688, batch acc 0.9118
16:01:07.914   Training iter 200, batch loss 1.5691, batch acc 0.9120
16:01:08.438   Training iter 250, batch loss 1.5637, batch acc 0.9204
16:01:08.966   Training iter 300, batch loss 1.5705, batch acc 0.9148
16:01:09.484   Training iter 350, batch loss 1.5617, batch acc 0.9218
16:01:09.981   Training iter 400, batch loss 1.5673, batch acc 0.9128
16:01:10.500   Training iter 450, batch loss 1.5638, batch acc 0.9226
16:01:11.045   Training iter 500, batch loss 1.5681, batch acc 0.9142
16:01:11.567   Training iter 550, batch loss 1.5651, batch acc 0.9136
16:01:12.092   Training iter 600, batch loss 1.5707, batch acc 0.9118
16:01:12.094 Training @ 252 epoch...
16:01:12.622   Training iter 50, batch loss 1.5684, batch acc 0.9140
16:01:13.147   Training iter 100, batch loss 1.5689, batch acc 0.9110
16:01:13.670   Training iter 150, batch loss 1.5636, batch acc 0.9202
16:01:14.212   Training iter 200, batch loss 1.5681, batch acc 0.9128
16:01:14.736   Training iter 250, batch loss 1.5658, batch acc 0.9144
16:01:15.278   Training iter 300, batch loss 1.5679, batch acc 0.9094
16:01:15.818   Training iter 350, batch loss 1.5645, batch acc 0.9212
16:01:16.368   Training iter 400, batch loss 1.5665, batch acc 0.9192
16:01:16.899   Training iter 450, batch loss 1.5645, batch acc 0.9166
16:01:17.431   Training iter 500, batch loss 1.5660, batch acc 0.9138
16:01:17.964   Training iter 550, batch loss 1.5686, batch acc 0.9132
16:01:18.545   Training iter 600, batch loss 1.5681, batch acc 0.9122
16:01:18.547 Training @ 253 epoch...
16:01:19.090   Training iter 50, batch loss 1.5646, batch acc 0.9220
16:01:19.614   Training iter 100, batch loss 1.5690, batch acc 0.9112
16:01:20.146   Training iter 150, batch loss 1.5635, batch acc 0.9208
16:01:20.645   Training iter 200, batch loss 1.5682, batch acc 0.9120
16:01:21.148   Training iter 250, batch loss 1.5680, batch acc 0.9120
16:01:21.670   Training iter 300, batch loss 1.5679, batch acc 0.9120
16:01:22.236   Training iter 350, batch loss 1.5595, batch acc 0.9218
16:01:22.801   Training iter 400, batch loss 1.5637, batch acc 0.9190
16:01:23.377   Training iter 450, batch loss 1.5735, batch acc 0.9080
16:01:23.947   Training iter 500, batch loss 1.5700, batch acc 0.9086
16:01:24.523   Training iter 550, batch loss 1.5617, batch acc 0.9218
16:01:25.095   Training iter 600, batch loss 1.5715, batch acc 0.9100
16:01:25.096 Training @ 254 epoch...
16:01:25.679   Training iter 50, batch loss 1.5633, batch acc 0.9188
16:01:26.255   Training iter 100, batch loss 1.5625, batch acc 0.9206
16:01:26.834   Training iter 150, batch loss 1.5692, batch acc 0.9106
16:01:27.406   Training iter 200, batch loss 1.5685, batch acc 0.9114
16:01:27.962   Training iter 250, batch loss 1.5702, batch acc 0.9108
16:01:28.526   Training iter 300, batch loss 1.5650, batch acc 0.9218
16:01:29.089   Training iter 350, batch loss 1.5641, batch acc 0.9200
16:01:29.654   Training iter 400, batch loss 1.5706, batch acc 0.9134
16:01:30.215   Training iter 450, batch loss 1.5656, batch acc 0.9140
16:01:30.784   Training iter 500, batch loss 1.5690, batch acc 0.9124
16:01:31.332   Training iter 550, batch loss 1.5682, batch acc 0.9164
16:01:31.884   Training iter 600, batch loss 1.5643, batch acc 0.9164
16:01:31.885 Training @ 255 epoch...
16:01:32.440   Training iter 50, batch loss 1.5714, batch acc 0.9092
16:01:32.984   Training iter 100, batch loss 1.5638, batch acc 0.9178
16:01:33.525   Training iter 150, batch loss 1.5707, batch acc 0.9114
16:01:34.067   Training iter 200, batch loss 1.5614, batch acc 0.9176
16:01:34.595   Training iter 250, batch loss 1.5704, batch acc 0.9096
16:01:35.127   Training iter 300, batch loss 1.5670, batch acc 0.9140
16:01:35.650   Training iter 350, batch loss 1.5661, batch acc 0.9174
16:01:36.190   Training iter 400, batch loss 1.5670, batch acc 0.9144
16:01:36.734   Training iter 450, batch loss 1.5659, batch acc 0.9162
16:01:37.270   Training iter 500, batch loss 1.5611, batch acc 0.9238
16:01:37.815   Training iter 550, batch loss 1.5675, batch acc 0.9154
16:01:38.381   Training iter 600, batch loss 1.5686, batch acc 0.9140
16:01:38.383 Testing @ 255 epoch...
16:01:38.430     Testing, total mean loss 1.56752, total acc 0.91330
16:01:38.430 Training @ 256 epoch...
16:01:38.991   Training iter 50, batch loss 1.5645, batch acc 0.9198
16:01:39.550   Training iter 100, batch loss 1.5647, batch acc 0.9162
16:01:40.116   Training iter 150, batch loss 1.5682, batch acc 0.9134
16:01:40.676   Training iter 200, batch loss 1.5683, batch acc 0.9084
16:01:41.236   Training iter 250, batch loss 1.5695, batch acc 0.9136
16:01:41.800   Training iter 300, batch loss 1.5645, batch acc 0.9190
16:01:42.374   Training iter 350, batch loss 1.5689, batch acc 0.9110
16:01:42.935   Training iter 400, batch loss 1.5634, batch acc 0.9208
16:01:43.494   Training iter 450, batch loss 1.5683, batch acc 0.9132
16:01:44.034   Training iter 500, batch loss 1.5649, batch acc 0.9188
16:01:44.569   Training iter 550, batch loss 1.5679, batch acc 0.9172
16:01:45.122   Training iter 600, batch loss 1.5666, batch acc 0.9128
16:01:45.124 Training @ 257 epoch...
16:01:45.673   Training iter 50, batch loss 1.5652, batch acc 0.9184
16:01:46.222   Training iter 100, batch loss 1.5653, batch acc 0.9140
16:01:46.743   Training iter 150, batch loss 1.5629, batch acc 0.9168
16:01:47.237   Training iter 200, batch loss 1.5640, batch acc 0.9186
16:01:47.734   Training iter 250, batch loss 1.5666, batch acc 0.9154
16:01:48.228   Training iter 300, batch loss 1.5716, batch acc 0.9092
16:01:48.733   Training iter 350, batch loss 1.5706, batch acc 0.9070
16:01:49.236   Training iter 400, batch loss 1.5674, batch acc 0.9140
16:01:49.725   Training iter 450, batch loss 1.5655, batch acc 0.9166
16:01:50.218   Training iter 500, batch loss 1.5650, batch acc 0.9176
16:01:50.737   Training iter 550, batch loss 1.5696, batch acc 0.9116
16:01:51.226   Training iter 600, batch loss 1.5659, batch acc 0.9188
16:01:51.228 Training @ 258 epoch...
16:01:51.734   Training iter 50, batch loss 1.5650, batch acc 0.9180
16:01:52.231   Training iter 100, batch loss 1.5686, batch acc 0.9110
16:01:52.694   Training iter 150, batch loss 1.5654, batch acc 0.9172
16:01:53.152   Training iter 200, batch loss 1.5675, batch acc 0.9176
16:01:53.699   Training iter 250, batch loss 1.5637, batch acc 0.9174
16:01:54.245   Training iter 300, batch loss 1.5631, batch acc 0.9194
16:01:54.802   Training iter 350, batch loss 1.5652, batch acc 0.9170
16:01:55.292   Training iter 400, batch loss 1.5709, batch acc 0.9134
16:01:55.764   Training iter 450, batch loss 1.5674, batch acc 0.9170
16:01:56.270   Training iter 500, batch loss 1.5691, batch acc 0.9086
16:01:56.774   Training iter 550, batch loss 1.5683, batch acc 0.9110
16:01:57.281   Training iter 600, batch loss 1.5649, batch acc 0.9176
16:01:57.282 Training @ 259 epoch...
16:01:57.789   Training iter 50, batch loss 1.5717, batch acc 0.9064
16:01:58.273   Training iter 100, batch loss 1.5650, batch acc 0.9190
16:01:58.760   Training iter 150, batch loss 1.5662, batch acc 0.9184
16:01:59.260   Training iter 200, batch loss 1.5670, batch acc 0.9132
16:01:59.757   Training iter 250, batch loss 1.5619, batch acc 0.9234
16:02:00.273   Training iter 300, batch loss 1.5662, batch acc 0.9184
16:02:00.788   Training iter 350, batch loss 1.5626, batch acc 0.9214
16:02:01.313   Training iter 400, batch loss 1.5685, batch acc 0.9086
16:02:01.847   Training iter 450, batch loss 1.5661, batch acc 0.9150
16:02:02.389   Training iter 500, batch loss 1.5682, batch acc 0.9100
16:02:02.904   Training iter 550, batch loss 1.5712, batch acc 0.9082
16:02:03.415   Training iter 600, batch loss 1.5644, batch acc 0.9176
16:02:03.417 Training @ 260 epoch...
16:02:03.942   Training iter 50, batch loss 1.5730, batch acc 0.9138
16:02:04.482   Training iter 100, batch loss 1.5650, batch acc 0.9180
16:02:05.019   Training iter 150, batch loss 1.5698, batch acc 0.9104
16:02:05.547   Training iter 200, batch loss 1.5657, batch acc 0.9166
16:02:06.089   Training iter 250, batch loss 1.5661, batch acc 0.9146
16:02:06.592   Training iter 300, batch loss 1.5669, batch acc 0.9124
16:02:07.072   Training iter 350, batch loss 1.5696, batch acc 0.9092
16:02:07.547   Training iter 400, batch loss 1.5659, batch acc 0.9138
16:02:08.031   Training iter 450, batch loss 1.5637, batch acc 0.9190
16:02:08.498   Training iter 500, batch loss 1.5648, batch acc 0.9164
16:02:08.961   Training iter 550, batch loss 1.5647, batch acc 0.9154
16:02:09.428   Training iter 600, batch loss 1.5633, batch acc 0.9202
16:02:09.429 Testing @ 260 epoch...
16:02:09.476     Testing, total mean loss 1.56759, total acc 0.91270
16:02:09.476 Training @ 261 epoch...
16:02:09.947   Training iter 50, batch loss 1.5653, batch acc 0.9170
16:02:10.435   Training iter 100, batch loss 1.5646, batch acc 0.9198
16:02:10.930   Training iter 150, batch loss 1.5692, batch acc 0.9138
16:02:11.417   Training iter 200, batch loss 1.5640, batch acc 0.9186
16:02:11.888   Training iter 250, batch loss 1.5633, batch acc 0.9188
16:02:12.389   Training iter 300, batch loss 1.5641, batch acc 0.9158
16:02:12.899   Training iter 350, batch loss 1.5707, batch acc 0.9138
16:02:13.372   Training iter 400, batch loss 1.5716, batch acc 0.9092
16:02:13.854   Training iter 450, batch loss 1.5622, batch acc 0.9172
16:02:14.357   Training iter 500, batch loss 1.5672, batch acc 0.9126
16:02:14.862   Training iter 550, batch loss 1.5654, batch acc 0.9142
16:02:15.366   Training iter 600, batch loss 1.5706, batch acc 0.9096
16:02:15.367 Training @ 262 epoch...
16:02:15.875   Training iter 50, batch loss 1.5654, batch acc 0.9138
16:02:16.369   Training iter 100, batch loss 1.5655, batch acc 0.9154
16:02:16.856   Training iter 150, batch loss 1.5634, batch acc 0.9206
16:02:17.349   Training iter 200, batch loss 1.5670, batch acc 0.9138
16:02:17.835   Training iter 250, batch loss 1.5692, batch acc 0.9112
16:02:18.333   Training iter 300, batch loss 1.5640, batch acc 0.9152
16:02:18.860   Training iter 350, batch loss 1.5654, batch acc 0.9178
16:02:19.372   Training iter 400, batch loss 1.5675, batch acc 0.9142
16:02:19.869   Training iter 450, batch loss 1.5705, batch acc 0.9116
16:02:20.371   Training iter 500, batch loss 1.5692, batch acc 0.9128
16:02:20.882   Training iter 550, batch loss 1.5641, batch acc 0.9210
16:02:21.408   Training iter 600, batch loss 1.5666, batch acc 0.9152
16:02:21.410 Training @ 263 epoch...
16:02:21.959   Training iter 50, batch loss 1.5672, batch acc 0.9168
16:02:22.486   Training iter 100, batch loss 1.5663, batch acc 0.9132
16:02:23.043   Training iter 150, batch loss 1.5655, batch acc 0.9196
16:02:23.613   Training iter 200, batch loss 1.5640, batch acc 0.9178
16:02:24.175   Training iter 250, batch loss 1.5682, batch acc 0.9138
16:02:24.744   Training iter 300, batch loss 1.5682, batch acc 0.9114
16:02:25.361   Training iter 350, batch loss 1.5688, batch acc 0.9116
16:02:25.959   Training iter 400, batch loss 1.5665, batch acc 0.9160
16:02:26.560   Training iter 450, batch loss 1.5695, batch acc 0.9098
16:02:27.205   Training iter 500, batch loss 1.5617, batch acc 0.9232
16:02:27.814   Training iter 550, batch loss 1.5636, batch acc 0.9218
16:02:28.432   Training iter 600, batch loss 1.5683, batch acc 0.9124
16:02:28.434 Training @ 264 epoch...
16:02:29.015   Training iter 50, batch loss 1.5704, batch acc 0.9110
16:02:29.642   Training iter 100, batch loss 1.5669, batch acc 0.9156
16:02:30.282   Training iter 150, batch loss 1.5598, batch acc 0.9226
16:02:30.887   Training iter 200, batch loss 1.5685, batch acc 0.9134
16:02:31.467   Training iter 250, batch loss 1.5689, batch acc 0.9120
16:02:32.045   Training iter 300, batch loss 1.5684, batch acc 0.9106
16:02:32.618   Training iter 350, batch loss 1.5673, batch acc 0.9138
16:02:33.169   Training iter 400, batch loss 1.5694, batch acc 0.9138
16:02:33.729   Training iter 450, batch loss 1.5653, batch acc 0.9170
16:02:34.312   Training iter 500, batch loss 1.5665, batch acc 0.9140
16:02:34.874   Training iter 550, batch loss 1.5655, batch acc 0.9174
16:02:35.429   Training iter 600, batch loss 1.5606, batch acc 0.9248
16:02:35.431 Training @ 265 epoch...
16:02:36.010   Training iter 50, batch loss 1.5593, batch acc 0.9236
16:02:36.556   Training iter 100, batch loss 1.5650, batch acc 0.9164
16:02:37.095   Training iter 150, batch loss 1.5691, batch acc 0.9082
16:02:37.609   Training iter 200, batch loss 1.5666, batch acc 0.9186
16:02:38.118   Training iter 250, batch loss 1.5665, batch acc 0.9144
16:02:38.618   Training iter 300, batch loss 1.5689, batch acc 0.9104
16:02:39.126   Training iter 350, batch loss 1.5650, batch acc 0.9190
16:02:39.677   Training iter 400, batch loss 1.5705, batch acc 0.9108
16:02:40.257   Training iter 450, batch loss 1.5673, batch acc 0.9142
16:02:40.834   Training iter 500, batch loss 1.5666, batch acc 0.9138
16:02:41.418   Training iter 550, batch loss 1.5633, batch acc 0.9192
16:02:41.998   Training iter 600, batch loss 1.5686, batch acc 0.9136
16:02:42.001 Testing @ 265 epoch...
16:02:42.054     Testing, total mean loss 1.56748, total acc 0.91280
16:02:42.054 Training @ 266 epoch...
16:02:42.643   Training iter 50, batch loss 1.5665, batch acc 0.9162
16:02:43.237   Training iter 100, batch loss 1.5667, batch acc 0.9122
16:02:43.838   Training iter 150, batch loss 1.5669, batch acc 0.9156
16:02:44.443   Training iter 200, batch loss 1.5654, batch acc 0.9152
16:02:45.041   Training iter 250, batch loss 1.5703, batch acc 0.9100
16:02:45.630   Training iter 300, batch loss 1.5680, batch acc 0.9120
16:02:46.222   Training iter 350, batch loss 1.5639, batch acc 0.9174
16:02:46.811   Training iter 400, batch loss 1.5633, batch acc 0.9218
16:02:47.384   Training iter 450, batch loss 1.5653, batch acc 0.9150
16:02:47.952   Training iter 500, batch loss 1.5629, batch acc 0.9212
16:02:48.509   Training iter 550, batch loss 1.5715, batch acc 0.9088
16:02:49.056   Training iter 600, batch loss 1.5660, batch acc 0.9192
16:02:49.058 Training @ 267 epoch...
16:02:49.597   Training iter 50, batch loss 1.5666, batch acc 0.9166
16:02:50.121   Training iter 100, batch loss 1.5643, batch acc 0.9224
16:02:50.637   Training iter 150, batch loss 1.5637, batch acc 0.9200
16:02:51.148   Training iter 200, batch loss 1.5668, batch acc 0.9166
16:02:51.663   Training iter 250, batch loss 1.5695, batch acc 0.9118
16:02:52.212   Training iter 300, batch loss 1.5664, batch acc 0.9114
16:02:52.772   Training iter 350, batch loss 1.5628, batch acc 0.9234
16:02:53.316   Training iter 400, batch loss 1.5673, batch acc 0.9100
16:02:53.864   Training iter 450, batch loss 1.5619, batch acc 0.9206
16:02:54.425   Training iter 500, batch loss 1.5686, batch acc 0.9122
16:02:54.986   Training iter 550, batch loss 1.5650, batch acc 0.9170
16:02:55.536   Training iter 600, batch loss 1.5735, batch acc 0.9078
16:02:55.538 Training @ 268 epoch...
16:02:56.098   Training iter 50, batch loss 1.5640, batch acc 0.9216
16:02:56.663   Training iter 100, batch loss 1.5690, batch acc 0.9114
16:02:57.234   Training iter 150, batch loss 1.5661, batch acc 0.9140
16:02:57.808   Training iter 200, batch loss 1.5655, batch acc 0.9144
16:02:58.372   Training iter 250, batch loss 1.5678, batch acc 0.9158
16:02:58.936   Training iter 300, batch loss 1.5646, batch acc 0.9208
16:02:59.509   Training iter 350, batch loss 1.5690, batch acc 0.9134
16:03:00.071   Training iter 400, batch loss 1.5673, batch acc 0.9094
16:03:00.633   Training iter 450, batch loss 1.5684, batch acc 0.9136
16:03:01.199   Training iter 500, batch loss 1.5644, batch acc 0.9160
16:03:01.786   Training iter 550, batch loss 1.5637, batch acc 0.9196
16:03:02.395   Training iter 600, batch loss 1.5667, batch acc 0.9162
16:03:02.397 Training @ 269 epoch...
16:03:03.007   Training iter 50, batch loss 1.5637, batch acc 0.9208
16:03:03.559   Training iter 100, batch loss 1.5615, batch acc 0.9214
16:03:04.128   Training iter 150, batch loss 1.5668, batch acc 0.9134
16:03:04.690   Training iter 200, batch loss 1.5671, batch acc 0.9138
16:03:05.230   Training iter 250, batch loss 1.5672, batch acc 0.9162
16:03:05.758   Training iter 300, batch loss 1.5684, batch acc 0.9118
16:03:06.305   Training iter 350, batch loss 1.5687, batch acc 0.9138
16:03:06.855   Training iter 400, batch loss 1.5666, batch acc 0.9118
16:03:07.417   Training iter 450, batch loss 1.5655, batch acc 0.9158
16:03:07.983   Training iter 500, batch loss 1.5670, batch acc 0.9160
16:03:08.505   Training iter 550, batch loss 1.5666, batch acc 0.9146
16:03:09.021   Training iter 600, batch loss 1.5664, batch acc 0.9132
16:03:09.023 Training @ 270 epoch...
16:03:09.563   Training iter 50, batch loss 1.5674, batch acc 0.9146
16:03:10.111   Training iter 100, batch loss 1.5656, batch acc 0.9154
16:03:10.645   Training iter 150, batch loss 1.5632, batch acc 0.9214
16:03:11.165   Training iter 200, batch loss 1.5685, batch acc 0.9148
16:03:11.683   Training iter 250, batch loss 1.5651, batch acc 0.9138
16:03:12.211   Training iter 300, batch loss 1.5696, batch acc 0.9106
16:03:12.740   Training iter 350, batch loss 1.5701, batch acc 0.9092
16:03:13.253   Training iter 400, batch loss 1.5649, batch acc 0.9202
16:03:13.781   Training iter 450, batch loss 1.5652, batch acc 0.9194
16:03:14.298   Training iter 500, batch loss 1.5644, batch acc 0.9186
16:03:14.822   Training iter 550, batch loss 1.5629, batch acc 0.9188
16:03:15.350   Training iter 600, batch loss 1.5679, batch acc 0.9100
16:03:15.352 Testing @ 270 epoch...
16:03:15.401     Testing, total mean loss 1.56753, total acc 0.91150
16:03:15.401 Training @ 271 epoch...
16:03:15.935   Training iter 50, batch loss 1.5621, batch acc 0.9216
16:03:16.466   Training iter 100, batch loss 1.5661, batch acc 0.9140
16:03:16.981   Training iter 150, batch loss 1.5716, batch acc 0.9090
16:03:17.536   Training iter 200, batch loss 1.5639, batch acc 0.9192
16:03:18.143   Training iter 250, batch loss 1.5678, batch acc 0.9122
16:03:18.761   Training iter 300, batch loss 1.5645, batch acc 0.9182
16:03:19.357   Training iter 350, batch loss 1.5665, batch acc 0.9126
16:03:19.965   Training iter 400, batch loss 1.5630, batch acc 0.9174
16:03:20.591   Training iter 450, batch loss 1.5665, batch acc 0.9196
16:03:21.226   Training iter 500, batch loss 1.5652, batch acc 0.9158
16:03:21.894   Training iter 550, batch loss 1.5694, batch acc 0.9090
16:03:22.564   Training iter 600, batch loss 1.5685, batch acc 0.9164
16:03:22.566 Training @ 272 epoch...
16:03:23.228   Training iter 50, batch loss 1.5644, batch acc 0.9166
16:03:23.850   Training iter 100, batch loss 1.5633, batch acc 0.9210
16:03:24.476   Training iter 150, batch loss 1.5640, batch acc 0.9200
16:03:25.092   Training iter 200, batch loss 1.5696, batch acc 0.9110
16:03:25.701   Training iter 250, batch loss 1.5665, batch acc 0.9126
16:03:26.327   Training iter 300, batch loss 1.5666, batch acc 0.9138
16:03:26.932   Training iter 350, batch loss 1.5669, batch acc 0.9140
16:03:27.554   Training iter 400, batch loss 1.5686, batch acc 0.9124
16:03:28.166   Training iter 450, batch loss 1.5697, batch acc 0.9094
16:03:28.761   Training iter 500, batch loss 1.5626, batch acc 0.9198
16:03:29.371   Training iter 550, batch loss 1.5681, batch acc 0.9194
16:03:29.981   Training iter 600, batch loss 1.5639, batch acc 0.9188
16:03:29.983 Training @ 273 epoch...
16:03:30.573   Training iter 50, batch loss 1.5702, batch acc 0.9136
16:03:31.168   Training iter 100, batch loss 1.5684, batch acc 0.9082
16:03:31.771   Training iter 150, batch loss 1.5643, batch acc 0.9214
16:03:32.417   Training iter 200, batch loss 1.5686, batch acc 0.9120
16:03:33.045   Training iter 250, batch loss 1.5661, batch acc 0.9132
16:03:33.657   Training iter 300, batch loss 1.5663, batch acc 0.9152
16:03:34.284   Training iter 350, batch loss 1.5645, batch acc 0.9128
16:03:34.911   Training iter 400, batch loss 1.5641, batch acc 0.9192
16:03:35.527   Training iter 450, batch loss 1.5662, batch acc 0.9184
16:03:36.134   Training iter 500, batch loss 1.5674, batch acc 0.9114
16:03:36.753   Training iter 550, batch loss 1.5643, batch acc 0.9206
16:03:37.360   Training iter 600, batch loss 1.5641, batch acc 0.9168
16:03:37.363 Training @ 274 epoch...
16:03:37.951   Training iter 50, batch loss 1.5653, batch acc 0.9166
16:03:38.514   Training iter 100, batch loss 1.5638, batch acc 0.9196
16:03:39.052   Training iter 150, batch loss 1.5662, batch acc 0.9144
16:03:39.605   Training iter 200, batch loss 1.5671, batch acc 0.9150
16:03:40.165   Training iter 250, batch loss 1.5656, batch acc 0.9170
16:03:40.713   Training iter 300, batch loss 1.5690, batch acc 0.9116
16:03:41.255   Training iter 350, batch loss 1.5694, batch acc 0.9156
16:03:41.794   Training iter 400, batch loss 1.5655, batch acc 0.9144
16:03:42.344   Training iter 450, batch loss 1.5689, batch acc 0.9112
16:03:42.916   Training iter 500, batch loss 1.5648, batch acc 0.9152
16:03:43.462   Training iter 550, batch loss 1.5599, batch acc 0.9244
16:03:44.010   Training iter 600, batch loss 1.5693, batch acc 0.9120
16:03:44.012 Training @ 275 epoch...
16:03:44.562   Training iter 50, batch loss 1.5692, batch acc 0.9112
16:03:45.099   Training iter 100, batch loss 1.5636, batch acc 0.9210
16:03:45.631   Training iter 150, batch loss 1.5653, batch acc 0.9166
16:03:46.165   Training iter 200, batch loss 1.5684, batch acc 0.9124
16:03:46.712   Training iter 250, batch loss 1.5649, batch acc 0.9164
16:03:47.248   Training iter 300, batch loss 1.5690, batch acc 0.9140
16:03:47.781   Training iter 350, batch loss 1.5649, batch acc 0.9136
16:03:48.315   Training iter 400, batch loss 1.5666, batch acc 0.9122
16:03:48.846   Training iter 450, batch loss 1.5638, batch acc 0.9160
16:03:49.387   Training iter 500, batch loss 1.5692, batch acc 0.9128
16:03:49.928   Training iter 550, batch loss 1.5672, batch acc 0.9182
16:03:50.468   Training iter 600, batch loss 1.5617, batch acc 0.9194
16:03:50.470 Testing @ 275 epoch...
16:03:50.519     Testing, total mean loss 1.56704, total acc 0.91240
16:03:50.519 Training @ 276 epoch...
16:03:51.059   Training iter 50, batch loss 1.5670, batch acc 0.9128
16:03:51.615   Training iter 100, batch loss 1.5690, batch acc 0.9118
16:03:52.150   Training iter 150, batch loss 1.5629, batch acc 0.9172
16:03:52.651   Training iter 200, batch loss 1.5656, batch acc 0.9176
16:03:53.144   Training iter 250, batch loss 1.5668, batch acc 0.9172
16:03:53.642   Training iter 300, batch loss 1.5629, batch acc 0.9194
16:03:54.146   Training iter 350, batch loss 1.5633, batch acc 0.9212
16:03:54.664   Training iter 400, batch loss 1.5683, batch acc 0.9124
16:03:55.170   Training iter 450, batch loss 1.5716, batch acc 0.9080
16:03:55.674   Training iter 500, batch loss 1.5652, batch acc 0.9142
16:03:56.168   Training iter 550, batch loss 1.5683, batch acc 0.9090
16:03:56.673   Training iter 600, batch loss 1.5627, batch acc 0.9214
16:03:56.675 Training @ 277 epoch...
16:03:57.203   Training iter 50, batch loss 1.5682, batch acc 0.9144
16:03:57.722   Training iter 100, batch loss 1.5712, batch acc 0.9060
16:03:58.241   Training iter 150, batch loss 1.5683, batch acc 0.9146
16:03:58.731   Training iter 200, batch loss 1.5646, batch acc 0.9172
16:03:59.246   Training iter 250, batch loss 1.5654, batch acc 0.9156
16:03:59.764   Training iter 300, batch loss 1.5637, batch acc 0.9200
16:04:00.305   Training iter 350, batch loss 1.5628, batch acc 0.9156
16:04:00.823   Training iter 400, batch loss 1.5676, batch acc 0.9146
16:04:01.337   Training iter 450, batch loss 1.5658, batch acc 0.9180
16:04:01.895   Training iter 500, batch loss 1.5611, batch acc 0.9216
16:04:02.465   Training iter 550, batch loss 1.5654, batch acc 0.9156
16:04:03.046   Training iter 600, batch loss 1.5687, batch acc 0.9114
16:04:03.048 Training @ 278 epoch...
16:04:03.644   Training iter 50, batch loss 1.5679, batch acc 0.9134
16:04:04.243   Training iter 100, batch loss 1.5689, batch acc 0.9088
16:04:04.827   Training iter 150, batch loss 1.5675, batch acc 0.9156
16:04:05.410   Training iter 200, batch loss 1.5680, batch acc 0.9114
16:04:05.987   Training iter 250, batch loss 1.5654, batch acc 0.9182
16:04:06.554   Training iter 300, batch loss 1.5601, batch acc 0.9274
16:04:07.142   Training iter 350, batch loss 1.5699, batch acc 0.9088
16:04:07.733   Training iter 400, batch loss 1.5673, batch acc 0.9100
16:04:08.304   Training iter 450, batch loss 1.5641, batch acc 0.9162
16:04:08.829   Training iter 500, batch loss 1.5637, batch acc 0.9184
16:04:09.370   Training iter 550, batch loss 1.5663, batch acc 0.9156
16:04:09.912   Training iter 600, batch loss 1.5638, batch acc 0.9174
16:04:09.914 Training @ 279 epoch...
16:04:10.461   Training iter 50, batch loss 1.5681, batch acc 0.9162
16:04:11.008   Training iter 100, batch loss 1.5682, batch acc 0.9124
16:04:11.548   Training iter 150, batch loss 1.5643, batch acc 0.9152
16:04:12.094   Training iter 200, batch loss 1.5652, batch acc 0.9150
16:04:12.676   Training iter 250, batch loss 1.5657, batch acc 0.9178
16:04:13.267   Training iter 300, batch loss 1.5707, batch acc 0.9094
16:04:13.855   Training iter 350, batch loss 1.5646, batch acc 0.9170
16:04:14.426   Training iter 400, batch loss 1.5662, batch acc 0.9188
16:04:14.944   Training iter 450, batch loss 1.5671, batch acc 0.9168
16:04:15.466   Training iter 500, batch loss 1.5649, batch acc 0.9150
16:04:16.005   Training iter 550, batch loss 1.5631, batch acc 0.9190
16:04:16.545   Training iter 600, batch loss 1.5649, batch acc 0.9188
16:04:16.547 Training @ 280 epoch...
16:04:17.101   Training iter 50, batch loss 1.5693, batch acc 0.9132
16:04:17.638   Training iter 100, batch loss 1.5666, batch acc 0.9104
16:04:18.167   Training iter 150, batch loss 1.5662, batch acc 0.9170
16:04:18.719   Training iter 200, batch loss 1.5630, batch acc 0.9188
16:04:19.280   Training iter 250, batch loss 1.5629, batch acc 0.9218
16:04:19.835   Training iter 300, batch loss 1.5659, batch acc 0.9172
16:04:20.400   Training iter 350, batch loss 1.5706, batch acc 0.9104
16:04:20.954   Training iter 400, batch loss 1.5631, batch acc 0.9196
16:04:21.507   Training iter 450, batch loss 1.5664, batch acc 0.9164
16:04:22.073   Training iter 500, batch loss 1.5648, batch acc 0.9134
16:04:22.632   Training iter 550, batch loss 1.5666, batch acc 0.9166
16:04:23.196   Training iter 600, batch loss 1.5668, batch acc 0.9136
16:04:23.197 Testing @ 280 epoch...
16:04:23.246     Testing, total mean loss 1.56699, total acc 0.91310
16:04:23.246 Training @ 281 epoch...
16:04:23.833   Training iter 50, batch loss 1.5665, batch acc 0.9160
16:04:24.411   Training iter 100, batch loss 1.5609, batch acc 0.9206
16:04:24.978   Training iter 150, batch loss 1.5653, batch acc 0.9162
16:04:25.518   Training iter 200, batch loss 1.5675, batch acc 0.9114
16:04:26.034   Training iter 250, batch loss 1.5705, batch acc 0.9088
16:04:26.556   Training iter 300, batch loss 1.5641, batch acc 0.9170
16:04:27.081   Training iter 350, batch loss 1.5671, batch acc 0.9134
16:04:27.617   Training iter 400, batch loss 1.5679, batch acc 0.9142
16:04:28.140   Training iter 450, batch loss 1.5657, batch acc 0.9182
16:04:28.658   Training iter 500, batch loss 1.5629, batch acc 0.9216
16:04:29.212   Training iter 550, batch loss 1.5666, batch acc 0.9130
16:04:29.764   Training iter 600, batch loss 1.5674, batch acc 0.9170
16:04:29.766 Training @ 282 epoch...
16:04:30.327   Training iter 50, batch loss 1.5650, batch acc 0.9180
16:04:30.860   Training iter 100, batch loss 1.5654, batch acc 0.9132
16:04:31.411   Training iter 150, batch loss 1.5642, batch acc 0.9190
16:04:31.957   Training iter 200, batch loss 1.5665, batch acc 0.9168
16:04:32.505   Training iter 250, batch loss 1.5682, batch acc 0.9108
16:04:33.066   Training iter 300, batch loss 1.5668, batch acc 0.9154
16:04:33.591   Training iter 350, batch loss 1.5671, batch acc 0.9118
16:04:34.126   Training iter 400, batch loss 1.5651, batch acc 0.9142
16:04:34.670   Training iter 450, batch loss 1.5667, batch acc 0.9140
16:04:35.235   Training iter 500, batch loss 1.5678, batch acc 0.9164
16:04:35.804   Training iter 550, batch loss 1.5659, batch acc 0.9154
16:04:36.364   Training iter 600, batch loss 1.5639, batch acc 0.9186
16:04:36.366 Training @ 283 epoch...
16:04:36.936   Training iter 50, batch loss 1.5616, batch acc 0.9218
16:04:37.505   Training iter 100, batch loss 1.5709, batch acc 0.9118
16:04:38.073   Training iter 150, batch loss 1.5699, batch acc 0.9104
16:04:38.626   Training iter 200, batch loss 1.5670, batch acc 0.9164
16:04:39.190   Training iter 250, batch loss 1.5632, batch acc 0.9174
16:04:39.742   Training iter 300, batch loss 1.5637, batch acc 0.9204
16:04:40.290   Training iter 350, batch loss 1.5672, batch acc 0.9138
16:04:40.816   Training iter 400, batch loss 1.5651, batch acc 0.9188
16:04:41.322   Training iter 450, batch loss 1.5637, batch acc 0.9154
16:04:41.831   Training iter 500, batch loss 1.5630, batch acc 0.9222
16:04:42.355   Training iter 550, batch loss 1.5696, batch acc 0.9100
16:04:42.880   Training iter 600, batch loss 1.5668, batch acc 0.9124
16:04:42.882 Training @ 284 epoch...
16:04:43.392   Training iter 50, batch loss 1.5646, batch acc 0.9164
16:04:43.909   Training iter 100, batch loss 1.5661, batch acc 0.9152
16:04:44.413   Training iter 150, batch loss 1.5677, batch acc 0.9158
16:04:44.900   Training iter 200, batch loss 1.5708, batch acc 0.9092
16:04:45.386   Training iter 250, batch loss 1.5628, batch acc 0.9220
16:04:45.885   Training iter 300, batch loss 1.5666, batch acc 0.9154
16:04:46.379   Training iter 350, batch loss 1.5646, batch acc 0.9144
16:04:46.872   Training iter 400, batch loss 1.5665, batch acc 0.9158
16:04:47.361   Training iter 450, batch loss 1.5674, batch acc 0.9120
16:04:47.845   Training iter 500, batch loss 1.5665, batch acc 0.9178
16:04:48.342   Training iter 550, batch loss 1.5664, batch acc 0.9154
16:04:48.835   Training iter 600, batch loss 1.5611, batch acc 0.9214
16:04:48.837 Training @ 285 epoch...
16:04:49.345   Training iter 50, batch loss 1.5671, batch acc 0.9124
16:04:49.850   Training iter 100, batch loss 1.5677, batch acc 0.9122
16:04:50.351   Training iter 150, batch loss 1.5685, batch acc 0.9104
16:04:50.892   Training iter 200, batch loss 1.5685, batch acc 0.9106
16:04:51.435   Training iter 250, batch loss 1.5643, batch acc 0.9182
16:04:51.998   Training iter 300, batch loss 1.5630, batch acc 0.9244
16:04:52.567   Training iter 350, batch loss 1.5605, batch acc 0.9202
16:04:53.141   Training iter 400, batch loss 1.5667, batch acc 0.9148
16:04:53.717   Training iter 450, batch loss 1.5649, batch acc 0.9192
16:04:54.299   Training iter 500, batch loss 1.5730, batch acc 0.9094
16:04:54.879   Training iter 550, batch loss 1.5598, batch acc 0.9230
16:04:55.460   Training iter 600, batch loss 1.5674, batch acc 0.9146
16:04:55.461 Testing @ 285 epoch...
16:04:55.510     Testing, total mean loss 1.56695, total acc 0.91140
16:04:55.510 Training @ 286 epoch...
16:04:56.089   Training iter 50, batch loss 1.5718, batch acc 0.9086
16:04:56.666   Training iter 100, batch loss 1.5666, batch acc 0.9164
16:04:57.221   Training iter 150, batch loss 1.5670, batch acc 0.9152
16:04:57.754   Training iter 200, batch loss 1.5658, batch acc 0.9162
16:04:58.291   Training iter 250, batch loss 1.5657, batch acc 0.9150
16:04:58.829   Training iter 300, batch loss 1.5610, batch acc 0.9250
16:04:59.381   Training iter 350, batch loss 1.5644, batch acc 0.9134
16:04:59.910   Training iter 400, batch loss 1.5631, batch acc 0.9204
16:05:00.454   Training iter 450, batch loss 1.5697, batch acc 0.9138
16:05:00.985   Training iter 500, batch loss 1.5671, batch acc 0.9122
16:05:01.563   Training iter 550, batch loss 1.5607, batch acc 0.9198
16:05:02.169   Training iter 600, batch loss 1.5677, batch acc 0.9120
16:05:02.171 Training @ 287 epoch...
16:05:02.762   Training iter 50, batch loss 1.5684, batch acc 0.9146
16:05:03.363   Training iter 100, batch loss 1.5660, batch acc 0.9176
16:05:03.953   Training iter 150, batch loss 1.5647, batch acc 0.9132
16:05:04.507   Training iter 200, batch loss 1.5602, batch acc 0.9234
16:05:05.068   Training iter 250, batch loss 1.5702, batch acc 0.9108
16:05:05.620   Training iter 300, batch loss 1.5648, batch acc 0.9170
16:05:06.197   Training iter 350, batch loss 1.5636, batch acc 0.9156
16:05:06.768   Training iter 400, batch loss 1.5695, batch acc 0.9110
16:05:07.342   Training iter 450, batch loss 1.5642, batch acc 0.9152
16:05:07.885   Training iter 500, batch loss 1.5641, batch acc 0.9194
16:05:08.405   Training iter 550, batch loss 1.5665, batch acc 0.9150
16:05:08.934   Training iter 600, batch loss 1.5681, batch acc 0.9128
16:05:08.936 Training @ 288 epoch...
16:05:09.474   Training iter 50, batch loss 1.5630, batch acc 0.9222
16:05:10.005   Training iter 100, batch loss 1.5710, batch acc 0.9080
16:05:10.540   Training iter 150, batch loss 1.5650, batch acc 0.9200
16:05:11.061   Training iter 200, batch loss 1.5676, batch acc 0.9136
16:05:11.573   Training iter 250, batch loss 1.5696, batch acc 0.9084
16:05:12.108   Training iter 300, batch loss 1.5652, batch acc 0.9164
16:05:12.648   Training iter 350, batch loss 1.5665, batch acc 0.9158
16:05:13.182   Training iter 400, batch loss 1.5642, batch acc 0.9186
16:05:13.699   Training iter 450, batch loss 1.5642, batch acc 0.9156
16:05:14.244   Training iter 500, batch loss 1.5634, batch acc 0.9146
16:05:14.779   Training iter 550, batch loss 1.5661, batch acc 0.9172
16:05:15.310   Training iter 600, batch loss 1.5649, batch acc 0.9178
16:05:15.312 Training @ 289 epoch...
16:05:15.854   Training iter 50, batch loss 1.5629, batch acc 0.9218
16:05:16.384   Training iter 100, batch loss 1.5664, batch acc 0.9190
16:05:16.927   Training iter 150, batch loss 1.5693, batch acc 0.9106
16:05:17.451   Training iter 200, batch loss 1.5692, batch acc 0.9162
16:05:17.973   Training iter 250, batch loss 1.5646, batch acc 0.9146
16:05:18.499   Training iter 300, batch loss 1.5625, batch acc 0.9184
16:05:19.041   Training iter 350, batch loss 1.5657, batch acc 0.9122
16:05:19.559   Training iter 400, batch loss 1.5723, batch acc 0.9048
16:05:20.082   Training iter 450, batch loss 1.5640, batch acc 0.9204
16:05:20.610   Training iter 500, batch loss 1.5595, batch acc 0.9236
16:05:21.136   Training iter 550, batch loss 1.5659, batch acc 0.9132
16:05:21.660   Training iter 600, batch loss 1.5675, batch acc 0.9128
16:05:21.661 Training @ 290 epoch...
16:05:22.189   Training iter 50, batch loss 1.5672, batch acc 0.9124
16:05:22.723   Training iter 100, batch loss 1.5637, batch acc 0.9188
16:05:23.264   Training iter 150, batch loss 1.5675, batch acc 0.9154
16:05:23.801   Training iter 200, batch loss 1.5632, batch acc 0.9164
16:05:24.349   Training iter 250, batch loss 1.5642, batch acc 0.9132
16:05:24.896   Training iter 300, batch loss 1.5649, batch acc 0.9146
16:05:25.441   Training iter 350, batch loss 1.5694, batch acc 0.9122
16:05:25.978   Training iter 400, batch loss 1.5645, batch acc 0.9188
16:05:26.515   Training iter 450, batch loss 1.5668, batch acc 0.9116
16:05:27.045   Training iter 500, batch loss 1.5643, batch acc 0.9180
16:05:27.579   Training iter 550, batch loss 1.5662, batch acc 0.9152
16:05:28.126   Training iter 600, batch loss 1.5676, batch acc 0.9182
16:05:28.128 Testing @ 290 epoch...
16:05:28.176     Testing, total mean loss 1.56745, total acc 0.91300
16:05:28.176 Training @ 291 epoch...
16:05:28.723   Training iter 50, batch loss 1.5685, batch acc 0.9116
16:05:29.265   Training iter 100, batch loss 1.5625, batch acc 0.9178
16:05:29.820   Training iter 150, batch loss 1.5652, batch acc 0.9170
16:05:30.384   Training iter 200, batch loss 1.5635, batch acc 0.9198
16:05:30.925   Training iter 250, batch loss 1.5638, batch acc 0.9206
16:05:31.459   Training iter 300, batch loss 1.5693, batch acc 0.9108
16:05:31.995   Training iter 350, batch loss 1.5734, batch acc 0.9110
16:05:32.531   Training iter 400, batch loss 1.5630, batch acc 0.9238
16:05:33.084   Training iter 450, batch loss 1.5632, batch acc 0.9168
16:05:33.621   Training iter 500, batch loss 1.5663, batch acc 0.9138
16:05:34.156   Training iter 550, batch loss 1.5651, batch acc 0.9164
16:05:34.707   Training iter 600, batch loss 1.5661, batch acc 0.9134
16:05:34.709 Training @ 292 epoch...
16:05:35.247   Training iter 50, batch loss 1.5699, batch acc 0.9102
16:05:35.775   Training iter 100, batch loss 1.5646, batch acc 0.9184
16:05:36.313   Training iter 150, batch loss 1.5604, batch acc 0.9218
16:05:36.830   Training iter 200, batch loss 1.5682, batch acc 0.9108
16:05:37.324   Training iter 250, batch loss 1.5670, batch acc 0.9120
16:05:37.816   Training iter 300, batch loss 1.5649, batch acc 0.9190
16:05:38.310   Training iter 350, batch loss 1.5688, batch acc 0.9098
16:05:38.803   Training iter 400, batch loss 1.5607, batch acc 0.9194
16:05:39.293   Training iter 450, batch loss 1.5703, batch acc 0.9144
16:05:39.771   Training iter 500, batch loss 1.5644, batch acc 0.9168
16:05:40.254   Training iter 550, batch loss 1.5637, batch acc 0.9206
16:05:40.752   Training iter 600, batch loss 1.5664, batch acc 0.9170
16:05:40.753 Training @ 293 epoch...
16:05:41.267   Training iter 50, batch loss 1.5650, batch acc 0.9160
16:05:41.767   Training iter 100, batch loss 1.5689, batch acc 0.9074
16:05:42.270   Training iter 150, batch loss 1.5677, batch acc 0.9134
16:05:42.780   Training iter 200, batch loss 1.5644, batch acc 0.9202
16:05:43.264   Training iter 250, batch loss 1.5659, batch acc 0.9162
16:05:43.752   Training iter 300, batch loss 1.5666, batch acc 0.9154
16:05:44.231   Training iter 350, batch loss 1.5638, batch acc 0.9188
16:05:44.718   Training iter 400, batch loss 1.5641, batch acc 0.9166
16:05:45.216   Training iter 450, batch loss 1.5671, batch acc 0.9142
16:05:45.705   Training iter 500, batch loss 1.5661, batch acc 0.9158
16:05:46.200   Training iter 550, batch loss 1.5661, batch acc 0.9164
16:05:46.693   Training iter 600, batch loss 1.5635, batch acc 0.9172
16:05:46.695 Training @ 294 epoch...
16:05:47.198   Training iter 50, batch loss 1.5685, batch acc 0.9112
16:05:47.683   Training iter 100, batch loss 1.5692, batch acc 0.9096
16:05:48.170   Training iter 150, batch loss 1.5686, batch acc 0.9122
16:05:48.648   Training iter 200, batch loss 1.5625, batch acc 0.9176
16:05:49.137   Training iter 250, batch loss 1.5678, batch acc 0.9152
16:05:49.638   Training iter 300, batch loss 1.5671, batch acc 0.9128
16:05:50.148   Training iter 350, batch loss 1.5612, batch acc 0.9208
16:05:50.641   Training iter 400, batch loss 1.5628, batch acc 0.9184
16:05:51.149   Training iter 450, batch loss 1.5617, batch acc 0.9244
16:05:51.656   Training iter 500, batch loss 1.5661, batch acc 0.9168
16:05:52.171   Training iter 550, batch loss 1.5664, batch acc 0.9132
16:05:52.646   Training iter 600, batch loss 1.5672, batch acc 0.9140
16:05:52.648 Training @ 295 epoch...
16:05:53.149   Training iter 50, batch loss 1.5620, batch acc 0.9206
16:05:53.647   Training iter 100, batch loss 1.5672, batch acc 0.9142
16:05:54.157   Training iter 150, batch loss 1.5646, batch acc 0.9176
16:05:54.691   Training iter 200, batch loss 1.5666, batch acc 0.9166
16:05:55.228   Training iter 250, batch loss 1.5651, batch acc 0.9180
16:05:55.742   Training iter 300, batch loss 1.5662, batch acc 0.9114
16:05:56.273   Training iter 350, batch loss 1.5652, batch acc 0.9150
16:05:56.819   Training iter 400, batch loss 1.5692, batch acc 0.9126
16:05:57.380   Training iter 450, batch loss 1.5691, batch acc 0.9112
16:05:57.955   Training iter 500, batch loss 1.5649, batch acc 0.9176
16:05:58.522   Training iter 550, batch loss 1.5668, batch acc 0.9136
16:05:59.052   Training iter 600, batch loss 1.5615, batch acc 0.9202
16:05:59.054 Testing @ 295 epoch...
16:05:59.103     Testing, total mean loss 1.56682, total acc 0.91210
16:05:59.103 Training @ 296 epoch...
16:05:59.650   Training iter 50, batch loss 1.5671, batch acc 0.9150
16:06:00.195   Training iter 100, batch loss 1.5657, batch acc 0.9132
16:06:00.719   Training iter 150, batch loss 1.5689, batch acc 0.9136
16:06:01.208   Training iter 200, batch loss 1.5666, batch acc 0.9164
16:06:01.738   Training iter 250, batch loss 1.5663, batch acc 0.9128
16:06:02.292   Training iter 300, batch loss 1.5681, batch acc 0.9118
16:06:02.838   Training iter 350, batch loss 1.5652, batch acc 0.9168
16:06:03.397   Training iter 400, batch loss 1.5660, batch acc 0.9190
16:06:03.965   Training iter 450, batch loss 1.5605, batch acc 0.9246
16:06:04.533   Training iter 500, batch loss 1.5681, batch acc 0.9130
16:06:05.100   Training iter 550, batch loss 1.5626, batch acc 0.9180
16:06:05.648   Training iter 600, batch loss 1.5630, batch acc 0.9172
16:06:05.650 Training @ 297 epoch...
16:06:06.229   Training iter 50, batch loss 1.5637, batch acc 0.9214
16:06:06.785   Training iter 100, batch loss 1.5680, batch acc 0.9130
16:06:07.321   Training iter 150, batch loss 1.5637, batch acc 0.9180
16:06:07.830   Training iter 200, batch loss 1.5687, batch acc 0.9148
16:06:08.347   Training iter 250, batch loss 1.5659, batch acc 0.9170
16:06:08.864   Training iter 300, batch loss 1.5606, batch acc 0.9214
16:06:09.417   Training iter 350, batch loss 1.5670, batch acc 0.9116
16:06:09.934   Training iter 400, batch loss 1.5675, batch acc 0.9136
16:06:10.480   Training iter 450, batch loss 1.5691, batch acc 0.9092
16:06:11.018   Training iter 500, batch loss 1.5656, batch acc 0.9144
16:06:11.546   Training iter 550, batch loss 1.5657, batch acc 0.9154
16:06:12.092   Training iter 600, batch loss 1.5625, batch acc 0.9210
16:06:12.093 Training @ 298 epoch...
16:06:12.643   Training iter 50, batch loss 1.5655, batch acc 0.9176
16:06:13.185   Training iter 100, batch loss 1.5682, batch acc 0.9118
16:06:13.721   Training iter 150, batch loss 1.5665, batch acc 0.9144
16:06:14.277   Training iter 200, batch loss 1.5623, batch acc 0.9184
16:06:14.817   Training iter 250, batch loss 1.5668, batch acc 0.9130
16:06:15.367   Training iter 300, batch loss 1.5623, batch acc 0.9214
16:06:15.917   Training iter 350, batch loss 1.5664, batch acc 0.9132
16:06:16.458   Training iter 400, batch loss 1.5658, batch acc 0.9162
16:06:16.997   Training iter 450, batch loss 1.5637, batch acc 0.9198
16:06:17.534   Training iter 500, batch loss 1.5666, batch acc 0.9108
16:06:18.070   Training iter 550, batch loss 1.5686, batch acc 0.9144
16:06:18.602   Training iter 600, batch loss 1.5647, batch acc 0.9178
16:06:18.604 Training @ 299 epoch...
16:06:19.142   Training iter 50, batch loss 1.5651, batch acc 0.9176
16:06:19.672   Training iter 100, batch loss 1.5640, batch acc 0.9176
16:06:20.212   Training iter 150, batch loss 1.5651, batch acc 0.9168
16:06:20.760   Training iter 200, batch loss 1.5700, batch acc 0.9156
16:06:21.302   Training iter 250, batch loss 1.5616, batch acc 0.9178
16:06:21.806   Training iter 300, batch loss 1.5644, batch acc 0.9172
16:06:22.319   Training iter 350, batch loss 1.5716, batch acc 0.9098
16:06:22.824   Training iter 400, batch loss 1.5626, batch acc 0.9186
16:06:23.371   Training iter 450, batch loss 1.5690, batch acc 0.9112
16:06:23.920   Training iter 500, batch loss 1.5603, batch acc 0.9240
16:06:24.468   Training iter 550, batch loss 1.5668, batch acc 0.9098
16:06:25.016   Training iter 600, batch loss 1.5670, batch acc 0.9144
16:06:25.018 Training @ 300 epoch...
16:06:25.561   Training iter 50, batch loss 1.5743, batch acc 0.9092
16:06:26.125   Training iter 100, batch loss 1.5622, batch acc 0.9194
16:06:26.675   Training iter 150, batch loss 1.5631, batch acc 0.9178
16:06:27.231   Training iter 200, batch loss 1.5652, batch acc 0.9192
16:06:27.788   Training iter 250, batch loss 1.5639, batch acc 0.9194
16:06:28.354   Training iter 300, batch loss 1.5700, batch acc 0.9110
16:06:28.920   Training iter 350, batch loss 1.5659, batch acc 0.9138
16:06:29.476   Training iter 400, batch loss 1.5645, batch acc 0.9154
16:06:29.984   Training iter 450, batch loss 1.5628, batch acc 0.9222
16:06:30.503   Training iter 500, batch loss 1.5681, batch acc 0.9106
16:06:31.031   Training iter 550, batch loss 1.5651, batch acc 0.9134
16:06:31.560   Training iter 600, batch loss 1.5622, batch acc 0.9194
16:06:31.562 Testing @ 300 epoch...
16:06:31.611     Testing, total mean loss 1.56677, total acc 0.91200
16:06:31.611 Plot @ 300 epoch...
16:06:31.611 Training @ 301 epoch...
16:06:32.195   Training iter 50, batch loss 1.5659, batch acc 0.9156
16:06:32.766   Training iter 100, batch loss 1.5609, batch acc 0.9216
16:06:33.317   Training iter 150, batch loss 1.5614, batch acc 0.9206
16:06:33.859   Training iter 200, batch loss 1.5660, batch acc 0.9154
16:06:34.386   Training iter 250, batch loss 1.5613, batch acc 0.9186
16:06:34.906   Training iter 300, batch loss 1.5680, batch acc 0.9122
16:06:35.429   Training iter 350, batch loss 1.5688, batch acc 0.9088
16:06:35.946   Training iter 400, batch loss 1.5679, batch acc 0.9134
16:06:36.466   Training iter 450, batch loss 1.5674, batch acc 0.9148
16:06:36.971   Training iter 500, batch loss 1.5649, batch acc 0.9172
16:06:37.472   Training iter 550, batch loss 1.5652, batch acc 0.9182
16:06:37.975   Training iter 600, batch loss 1.5691, batch acc 0.9156
16:06:37.977 Training @ 302 epoch...
16:06:38.472   Training iter 50, batch loss 1.5660, batch acc 0.9134
16:06:38.959   Training iter 100, batch loss 1.5628, batch acc 0.9192
16:06:39.445   Training iter 150, batch loss 1.5654, batch acc 0.9164
16:06:39.930   Training iter 200, batch loss 1.5632, batch acc 0.9214
16:06:40.434   Training iter 250, batch loss 1.5651, batch acc 0.9164
16:06:40.920   Training iter 300, batch loss 1.5699, batch acc 0.9112
16:06:41.415   Training iter 350, batch loss 1.5674, batch acc 0.9166
16:06:41.906   Training iter 400, batch loss 1.5615, batch acc 0.9236
16:06:42.432   Training iter 450, batch loss 1.5655, batch acc 0.9118
16:06:42.987   Training iter 500, batch loss 1.5652, batch acc 0.9174
16:06:43.554   Training iter 550, batch loss 1.5677, batch acc 0.9114
16:06:44.125   Training iter 600, batch loss 1.5668, batch acc 0.9154
16:06:44.127 Training @ 303 epoch...
16:06:44.690   Training iter 50, batch loss 1.5669, batch acc 0.9166
16:06:45.254   Training iter 100, batch loss 1.5690, batch acc 0.9112
16:06:45.799   Training iter 150, batch loss 1.5603, batch acc 0.9216
16:06:46.377   Training iter 200, batch loss 1.5677, batch acc 0.9140
16:06:46.972   Training iter 250, batch loss 1.5676, batch acc 0.9106
16:06:47.564   Training iter 300, batch loss 1.5659, batch acc 0.9162
16:06:48.155   Training iter 350, batch loss 1.5638, batch acc 0.9200
16:06:48.663   Training iter 400, batch loss 1.5638, batch acc 0.9198
16:06:49.158   Training iter 450, batch loss 1.5633, batch acc 0.9186
16:06:49.663   Training iter 500, batch loss 1.5688, batch acc 0.9128
16:06:50.164   Training iter 550, batch loss 1.5631, batch acc 0.9182
16:06:50.661   Training iter 600, batch loss 1.5666, batch acc 0.9144
16:06:50.662 Training @ 304 epoch...
16:06:51.171   Training iter 50, batch loss 1.5622, batch acc 0.9212
16:06:51.690   Training iter 100, batch loss 1.5639, batch acc 0.9196
16:06:52.182   Training iter 150, batch loss 1.5665, batch acc 0.9146
16:06:52.690   Training iter 200, batch loss 1.5703, batch acc 0.9082
16:06:53.163   Training iter 250, batch loss 1.5651, batch acc 0.9134
16:06:53.666   Training iter 300, batch loss 1.5632, batch acc 0.9262
16:06:54.199   Training iter 350, batch loss 1.5666, batch acc 0.9146
16:06:54.736   Training iter 400, batch loss 1.5617, batch acc 0.9214
16:06:55.282   Training iter 450, batch loss 1.5670, batch acc 0.9168
16:06:55.805   Training iter 500, batch loss 1.5701, batch acc 0.9078
16:06:56.355   Training iter 550, batch loss 1.5653, batch acc 0.9138
16:06:56.894   Training iter 600, batch loss 1.5647, batch acc 0.9144
16:06:56.896 Training @ 305 epoch...
16:06:57.434   Training iter 50, batch loss 1.5644, batch acc 0.9136
16:06:57.934   Training iter 100, batch loss 1.5672, batch acc 0.9164
16:06:58.473   Training iter 150, batch loss 1.5645, batch acc 0.9166
16:06:59.013   Training iter 200, batch loss 1.5679, batch acc 0.9182
16:06:59.546   Training iter 250, batch loss 1.5692, batch acc 0.9142
16:07:00.088   Training iter 300, batch loss 1.5666, batch acc 0.9136
16:07:00.613   Training iter 350, batch loss 1.5690, batch acc 0.9098
16:07:01.144   Training iter 400, batch loss 1.5600, batch acc 0.9250
16:07:01.719   Training iter 450, batch loss 1.5645, batch acc 0.9196
16:07:02.298   Training iter 500, batch loss 1.5637, batch acc 0.9166
16:07:02.826   Training iter 550, batch loss 1.5635, batch acc 0.9150
16:07:03.361   Training iter 600, batch loss 1.5655, batch acc 0.9130
16:07:03.363 Testing @ 305 epoch...
16:07:03.412     Testing, total mean loss 1.56658, total acc 0.91220
16:07:03.412 Training @ 306 epoch...
16:07:03.952   Training iter 50, batch loss 1.5664, batch acc 0.9132
16:07:04.468   Training iter 100, batch loss 1.5668, batch acc 0.9138
16:07:04.961   Training iter 150, batch loss 1.5652, batch acc 0.9170
16:07:05.474   Training iter 200, batch loss 1.5662, batch acc 0.9162
16:07:05.972   Training iter 250, batch loss 1.5654, batch acc 0.9166
16:07:06.471   Training iter 300, batch loss 1.5653, batch acc 0.9202
16:07:06.954   Training iter 350, batch loss 1.5603, batch acc 0.9224
16:07:07.464   Training iter 400, batch loss 1.5652, batch acc 0.9164
16:07:07.974   Training iter 450, batch loss 1.5677, batch acc 0.9148
16:07:08.515   Training iter 500, batch loss 1.5619, batch acc 0.9194
16:07:09.064   Training iter 550, batch loss 1.5686, batch acc 0.9108
16:07:09.592   Training iter 600, batch loss 1.5667, batch acc 0.9152
16:07:09.594 Training @ 307 epoch...
16:07:10.136   Training iter 50, batch loss 1.5670, batch acc 0.9162
16:07:10.684   Training iter 100, batch loss 1.5631, batch acc 0.9168
16:07:11.217   Training iter 150, batch loss 1.5641, batch acc 0.9138
16:07:11.744   Training iter 200, batch loss 1.5654, batch acc 0.9208
16:07:12.283   Training iter 250, batch loss 1.5652, batch acc 0.9160
16:07:12.809   Training iter 300, batch loss 1.5659, batch acc 0.9138
16:07:13.326   Training iter 350, batch loss 1.5628, batch acc 0.9230
16:07:13.837   Training iter 400, batch loss 1.5628, batch acc 0.9212
16:07:14.374   Training iter 450, batch loss 1.5707, batch acc 0.9054
16:07:14.941   Training iter 500, batch loss 1.5651, batch acc 0.9190
16:07:15.496   Training iter 550, batch loss 1.5651, batch acc 0.9180
16:07:16.052   Training iter 600, batch loss 1.5684, batch acc 0.9142
16:07:16.054 Training @ 308 epoch...
16:07:16.620   Training iter 50, batch loss 1.5618, batch acc 0.9206
16:07:17.188   Training iter 100, batch loss 1.5686, batch acc 0.9140
16:07:17.747   Training iter 150, batch loss 1.5646, batch acc 0.9192
16:07:18.314   Training iter 200, batch loss 1.5654, batch acc 0.9158
16:07:18.878   Training iter 250, batch loss 1.5670, batch acc 0.9120
16:07:19.451   Training iter 300, batch loss 1.5645, batch acc 0.9174
16:07:20.029   Training iter 350, batch loss 1.5612, batch acc 0.9220
16:07:20.598   Training iter 400, batch loss 1.5651, batch acc 0.9152
16:07:21.148   Training iter 450, batch loss 1.5666, batch acc 0.9142
16:07:21.687   Training iter 500, batch loss 1.5677, batch acc 0.9136
16:07:22.243   Training iter 550, batch loss 1.5667, batch acc 0.9146
16:07:22.779   Training iter 600, batch loss 1.5661, batch acc 0.9166
16:07:22.781 Training @ 309 epoch...
16:07:23.335   Training iter 50, batch loss 1.5619, batch acc 0.9250
16:07:23.871   Training iter 100, batch loss 1.5572, batch acc 0.9270
16:07:24.423   Training iter 150, batch loss 1.5698, batch acc 0.9116
16:07:24.980   Training iter 200, batch loss 1.5684, batch acc 0.9114
16:07:25.536   Training iter 250, batch loss 1.5641, batch acc 0.9152
16:07:26.081   Training iter 300, batch loss 1.5691, batch acc 0.9110
16:07:26.638   Training iter 350, batch loss 1.5613, batch acc 0.9238
16:07:27.214   Training iter 400, batch loss 1.5672, batch acc 0.9126
16:07:27.760   Training iter 450, batch loss 1.5625, batch acc 0.9188
16:07:28.314   Training iter 500, batch loss 1.5679, batch acc 0.9142
16:07:28.869   Training iter 550, batch loss 1.5684, batch acc 0.9116
16:07:29.433   Training iter 600, batch loss 1.5673, batch acc 0.9126
16:07:29.434 Training @ 310 epoch...
16:07:30.002   Training iter 50, batch loss 1.5651, batch acc 0.9178
16:07:30.566   Training iter 100, batch loss 1.5669, batch acc 0.9150
16:07:31.148   Training iter 150, batch loss 1.5651, batch acc 0.9172
16:07:31.710   Training iter 200, batch loss 1.5686, batch acc 0.9106
16:07:32.268   Training iter 250, batch loss 1.5667, batch acc 0.9162
16:07:32.821   Training iter 300, batch loss 1.5657, batch acc 0.9134
16:07:33.390   Training iter 350, batch loss 1.5639, batch acc 0.9186
16:07:33.973   Training iter 400, batch loss 1.5642, batch acc 0.9184
16:07:34.558   Training iter 450, batch loss 1.5641, batch acc 0.9180
16:07:35.141   Training iter 500, batch loss 1.5618, batch acc 0.9220
16:07:35.748   Training iter 550, batch loss 1.5647, batch acc 0.9158
16:07:36.358   Training iter 600, batch loss 1.5680, batch acc 0.9122
16:07:36.360 Testing @ 310 epoch...
16:07:36.411     Testing, total mean loss 1.56637, total acc 0.91290
16:07:36.411 Training @ 311 epoch...
16:07:37.026   Training iter 50, batch loss 1.5680, batch acc 0.9100
16:07:37.640   Training iter 100, batch loss 1.5645, batch acc 0.9208
16:07:38.226   Training iter 150, batch loss 1.5698, batch acc 0.9132
16:07:38.808   Training iter 200, batch loss 1.5625, batch acc 0.9198
16:07:39.402   Training iter 250, batch loss 1.5649, batch acc 0.9184
16:07:39.961   Training iter 300, batch loss 1.5624, batch acc 0.9172
16:07:40.539   Training iter 350, batch loss 1.5596, batch acc 0.9252
16:07:41.106   Training iter 400, batch loss 1.5680, batch acc 0.9142
16:07:41.659   Training iter 450, batch loss 1.5631, batch acc 0.9182
16:07:42.276   Training iter 500, batch loss 1.5648, batch acc 0.9158
16:07:43.040   Training iter 550, batch loss 1.5686, batch acc 0.9118
16:07:43.800   Training iter 600, batch loss 1.5683, batch acc 0.9118
16:07:43.802 Training @ 312 epoch...
16:07:44.399   Training iter 50, batch loss 1.5605, batch acc 0.9242
16:07:44.933   Training iter 100, batch loss 1.5619, batch acc 0.9192
16:07:45.469   Training iter 150, batch loss 1.5678, batch acc 0.9122
16:07:46.006   Training iter 200, batch loss 1.5665, batch acc 0.9124
16:07:46.551   Training iter 250, batch loss 1.5670, batch acc 0.9102
16:07:47.114   Training iter 300, batch loss 1.5680, batch acc 0.9142
16:07:47.648   Training iter 350, batch loss 1.5665, batch acc 0.9148
16:07:48.177   Training iter 400, batch loss 1.5708, batch acc 0.9110
16:07:48.696   Training iter 450, batch loss 1.5635, batch acc 0.9190
16:07:49.223   Training iter 500, batch loss 1.5675, batch acc 0.9146
16:07:49.780   Training iter 550, batch loss 1.5601, batch acc 0.9216
16:07:50.339   Training iter 600, batch loss 1.5643, batch acc 0.9188
16:07:50.341 Training @ 313 epoch...
16:07:50.891   Training iter 50, batch loss 1.5648, batch acc 0.9184
16:07:51.453   Training iter 100, batch loss 1.5645, batch acc 0.9182
16:07:52.025   Training iter 150, batch loss 1.5675, batch acc 0.9148
16:07:52.581   Training iter 200, batch loss 1.5684, batch acc 0.9170
16:07:53.132   Training iter 250, batch loss 1.5635, batch acc 0.9186
16:07:53.672   Training iter 300, batch loss 1.5648, batch acc 0.9194
16:07:54.209   Training iter 350, batch loss 1.5637, batch acc 0.9160
16:07:54.752   Training iter 400, batch loss 1.5630, batch acc 0.9188
16:07:55.325   Training iter 450, batch loss 1.5659, batch acc 0.9160
16:07:55.878   Training iter 500, batch loss 1.5637, batch acc 0.9152
16:07:56.437   Training iter 550, batch loss 1.5691, batch acc 0.9106
16:07:56.977   Training iter 600, batch loss 1.5651, batch acc 0.9126
16:07:56.979 Training @ 314 epoch...
16:07:57.538   Training iter 50, batch loss 1.5660, batch acc 0.9134
16:07:58.078   Training iter 100, batch loss 1.5694, batch acc 0.9142
16:07:58.623   Training iter 150, batch loss 1.5648, batch acc 0.9190
16:07:59.154   Training iter 200, batch loss 1.5654, batch acc 0.9146
16:07:59.691   Training iter 250, batch loss 1.5634, batch acc 0.9186
16:08:00.232   Training iter 300, batch loss 1.5632, batch acc 0.9188
16:08:00.778   Training iter 350, batch loss 1.5675, batch acc 0.9156
16:08:01.338   Training iter 400, batch loss 1.5640, batch acc 0.9174
16:08:01.918   Training iter 450, batch loss 1.5695, batch acc 0.9128
16:08:02.485   Training iter 500, batch loss 1.5665, batch acc 0.9116
16:08:03.062   Training iter 550, batch loss 1.5626, batch acc 0.9184
16:08:03.633   Training iter 600, batch loss 1.5615, batch acc 0.9206
16:08:03.635 Training @ 315 epoch...
16:08:04.224   Training iter 50, batch loss 1.5696, batch acc 0.9104
16:08:04.782   Training iter 100, batch loss 1.5641, batch acc 0.9180
16:08:05.332   Training iter 150, batch loss 1.5622, batch acc 0.9208
16:08:05.897   Training iter 200, batch loss 1.5685, batch acc 0.9116
16:08:06.478   Training iter 250, batch loss 1.5640, batch acc 0.9170
16:08:07.021   Training iter 300, batch loss 1.5672, batch acc 0.9144
16:08:07.537   Training iter 350, batch loss 1.5679, batch acc 0.9132
16:08:08.081   Training iter 400, batch loss 1.5606, batch acc 0.9222
16:08:08.615   Training iter 450, batch loss 1.5684, batch acc 0.9090
16:08:09.157   Training iter 500, batch loss 1.5671, batch acc 0.9146
16:08:09.681   Training iter 550, batch loss 1.5589, batch acc 0.9278
16:08:10.233   Training iter 600, batch loss 1.5652, batch acc 0.9196
16:08:10.235 Testing @ 315 epoch...
16:08:10.284     Testing, total mean loss 1.56643, total acc 0.91320
16:08:10.284 Training @ 316 epoch...
16:08:10.837   Training iter 50, batch loss 1.5660, batch acc 0.9170
16:08:11.382   Training iter 100, batch loss 1.5677, batch acc 0.9154
16:08:11.921   Training iter 150, batch loss 1.5679, batch acc 0.9150
16:08:12.466   Training iter 200, batch loss 1.5732, batch acc 0.9094
16:08:13.027   Training iter 250, batch loss 1.5614, batch acc 0.9216
16:08:13.605   Training iter 300, batch loss 1.5689, batch acc 0.9116
16:08:14.188   Training iter 350, batch loss 1.5645, batch acc 0.9126
16:08:14.748   Training iter 400, batch loss 1.5611, batch acc 0.9246
16:08:15.298   Training iter 450, batch loss 1.5696, batch acc 0.9132
16:08:15.844   Training iter 500, batch loss 1.5632, batch acc 0.9130
16:08:16.403   Training iter 550, batch loss 1.5604, batch acc 0.9204
16:08:16.966   Training iter 600, batch loss 1.5596, batch acc 0.9224
16:08:16.968 Training @ 317 epoch...
16:08:17.527   Training iter 50, batch loss 1.5652, batch acc 0.9168
16:08:18.152   Training iter 100, batch loss 1.5669, batch acc 0.9148
16:08:19.177   Training iter 150, batch loss 1.5651, batch acc 0.9168
16:08:19.731   Training iter 200, batch loss 1.5634, batch acc 0.9200
16:08:20.268   Training iter 250, batch loss 1.5633, batch acc 0.9194
16:08:20.792   Training iter 300, batch loss 1.5654, batch acc 0.9136
16:08:21.302   Training iter 350, batch loss 1.5643, batch acc 0.9184
16:08:21.822   Training iter 400, batch loss 1.5645, batch acc 0.9168
16:08:22.345   Training iter 450, batch loss 1.5644, batch acc 0.9168
16:08:22.859   Training iter 500, batch loss 1.5690, batch acc 0.9118
16:08:23.373   Training iter 550, batch loss 1.5657, batch acc 0.9186
16:08:23.887   Training iter 600, batch loss 1.5660, batch acc 0.9114
16:08:23.888 Training @ 318 epoch...
16:08:24.444   Training iter 50, batch loss 1.5630, batch acc 0.9196
16:08:24.987   Training iter 100, batch loss 1.5639, batch acc 0.9198
16:08:25.522   Training iter 150, batch loss 1.5646, batch acc 0.9160
16:08:26.035   Training iter 200, batch loss 1.5665, batch acc 0.9130
16:08:26.559   Training iter 250, batch loss 1.5663, batch acc 0.9134
16:08:27.091   Training iter 300, batch loss 1.5685, batch acc 0.9160
16:08:27.624   Training iter 350, batch loss 1.5641, batch acc 0.9190
16:08:28.169   Training iter 400, batch loss 1.5640, batch acc 0.9164
16:08:28.708   Training iter 450, batch loss 1.5636, batch acc 0.9158
16:08:29.266   Training iter 500, batch loss 1.5684, batch acc 0.9100
16:08:29.829   Training iter 550, batch loss 1.5665, batch acc 0.9148
16:08:30.388   Training iter 600, batch loss 1.5633, batch acc 0.9200
16:08:30.390 Training @ 319 epoch...
16:08:30.951   Training iter 50, batch loss 1.5635, batch acc 0.9150
16:08:31.491   Training iter 100, batch loss 1.5678, batch acc 0.9126
16:08:31.998   Training iter 150, batch loss 1.5658, batch acc 0.9154
16:08:32.512   Training iter 200, batch loss 1.5657, batch acc 0.9206
16:08:33.030   Training iter 250, batch loss 1.5605, batch acc 0.9166
16:08:33.551   Training iter 300, batch loss 1.5647, batch acc 0.9170
16:08:34.088   Training iter 350, batch loss 1.5702, batch acc 0.9144
16:08:34.654   Training iter 400, batch loss 1.5639, batch acc 0.9190
16:08:35.210   Training iter 450, batch loss 1.5646, batch acc 0.9144
16:08:35.765   Training iter 500, batch loss 1.5682, batch acc 0.9154
16:08:36.318   Training iter 550, batch loss 1.5649, batch acc 0.9174
16:08:36.869   Training iter 600, batch loss 1.5625, batch acc 0.9216
16:08:36.871 Training @ 320 epoch...
16:08:37.438   Training iter 50, batch loss 1.5627, batch acc 0.9206
16:08:37.982   Training iter 100, batch loss 1.5619, batch acc 0.9236
16:08:38.520   Training iter 150, batch loss 1.5631, batch acc 0.9204
16:08:39.035   Training iter 200, batch loss 1.5677, batch acc 0.9136
16:08:39.558   Training iter 250, batch loss 1.5617, batch acc 0.9188
16:08:40.080   Training iter 300, batch loss 1.5669, batch acc 0.9116
16:08:40.562   Training iter 350, batch loss 1.5680, batch acc 0.9152
16:08:41.049   Training iter 400, batch loss 1.5661, batch acc 0.9192
16:08:41.535   Training iter 450, batch loss 1.5666, batch acc 0.9166
16:08:42.034   Training iter 500, batch loss 1.5651, batch acc 0.9156
16:08:42.545   Training iter 550, batch loss 1.5663, batch acc 0.9086
16:08:43.046   Training iter 600, batch loss 1.5660, batch acc 0.9144
16:08:43.047 Testing @ 320 epoch...
16:08:43.096     Testing, total mean loss 1.56636, total acc 0.91200
16:08:43.096 Training @ 321 epoch...
16:08:43.591   Training iter 50, batch loss 1.5669, batch acc 0.9124
16:08:44.094   Training iter 100, batch loss 1.5663, batch acc 0.9160
16:08:44.598   Training iter 150, batch loss 1.5609, batch acc 0.9186
16:08:45.099   Training iter 200, batch loss 1.5672, batch acc 0.9118
16:08:45.592   Training iter 250, batch loss 1.5614, batch acc 0.9250
16:08:46.089   Training iter 300, batch loss 1.5654, batch acc 0.9190
16:08:46.612   Training iter 350, batch loss 1.5684, batch acc 0.9138
16:08:47.120   Training iter 400, batch loss 1.5666, batch acc 0.9118
16:08:47.627   Training iter 450, batch loss 1.5670, batch acc 0.9146
16:08:48.133   Training iter 500, batch loss 1.5657, batch acc 0.9114
16:08:48.653   Training iter 550, batch loss 1.5628, batch acc 0.9200
16:08:49.183   Training iter 600, batch loss 1.5642, batch acc 0.9196
16:08:49.185 Training @ 322 epoch...
16:08:49.733   Training iter 50, batch loss 1.5599, batch acc 0.9234
16:08:50.271   Training iter 100, batch loss 1.5684, batch acc 0.9138
16:08:50.801   Training iter 150, batch loss 1.5602, batch acc 0.9214
16:08:51.336   Training iter 200, batch loss 1.5623, batch acc 0.9198
16:08:51.881   Training iter 250, batch loss 1.5598, batch acc 0.9234
16:08:52.426   Training iter 300, batch loss 1.5637, batch acc 0.9210
16:08:52.955   Training iter 350, batch loss 1.5662, batch acc 0.9140
16:08:53.473   Training iter 400, batch loss 1.5665, batch acc 0.9156
16:08:54.002   Training iter 450, batch loss 1.5653, batch acc 0.9152
16:08:54.539   Training iter 500, batch loss 1.5688, batch acc 0.9080
16:08:55.077   Training iter 550, batch loss 1.5737, batch acc 0.9086
16:08:55.617   Training iter 600, batch loss 1.5668, batch acc 0.9160
16:08:55.619 Training @ 323 epoch...
16:08:56.163   Training iter 50, batch loss 1.5657, batch acc 0.9158
16:08:56.696   Training iter 100, batch loss 1.5590, batch acc 0.9244
16:08:57.243   Training iter 150, batch loss 1.5686, batch acc 0.9120
16:08:57.803   Training iter 200, batch loss 1.5645, batch acc 0.9184
16:08:58.350   Training iter 250, batch loss 1.5688, batch acc 0.9140
16:08:58.876   Training iter 300, batch loss 1.5629, batch acc 0.9166
16:08:59.411   Training iter 350, batch loss 1.5697, batch acc 0.9120
16:08:59.944   Training iter 400, batch loss 1.5627, batch acc 0.9178
16:09:00.476   Training iter 450, batch loss 1.5671, batch acc 0.9144
16:09:01.012   Training iter 500, batch loss 1.5613, batch acc 0.9234
16:09:01.565   Training iter 550, batch loss 1.5680, batch acc 0.9122
16:09:02.137   Training iter 600, batch loss 1.5634, batch acc 0.9184
16:09:02.138 Training @ 324 epoch...
16:09:02.706   Training iter 50, batch loss 1.5674, batch acc 0.9126
16:09:03.251   Training iter 100, batch loss 1.5679, batch acc 0.9138
16:09:03.806   Training iter 150, batch loss 1.5632, batch acc 0.9190
16:09:04.386   Training iter 200, batch loss 1.5589, batch acc 0.9234
16:09:04.967   Training iter 250, batch loss 1.5587, batch acc 0.9250
16:09:05.544   Training iter 300, batch loss 1.5668, batch acc 0.9162
16:09:06.097   Training iter 350, batch loss 1.5683, batch acc 0.9114
16:09:06.647   Training iter 400, batch loss 1.5666, batch acc 0.9182
16:09:07.174   Training iter 450, batch loss 1.5673, batch acc 0.9116
16:09:07.766   Training iter 500, batch loss 1.5685, batch acc 0.9138
16:09:08.352   Training iter 550, batch loss 1.5636, batch acc 0.9196
16:09:08.935   Training iter 600, batch loss 1.5641, batch acc 0.9186
16:09:08.936 Training @ 325 epoch...
16:09:09.434   Training iter 50, batch loss 1.5642, batch acc 0.9176
16:09:09.959   Training iter 100, batch loss 1.5662, batch acc 0.9140
16:09:10.478   Training iter 150, batch loss 1.5657, batch acc 0.9150
16:09:10.987   Training iter 200, batch loss 1.5664, batch acc 0.9164
16:09:11.501   Training iter 250, batch loss 1.5665, batch acc 0.9200
16:09:12.016   Training iter 300, batch loss 1.5676, batch acc 0.9164
16:09:12.537   Training iter 350, batch loss 1.5615, batch acc 0.9174
16:09:13.072   Training iter 400, batch loss 1.5674, batch acc 0.9124
16:09:13.604   Training iter 450, batch loss 1.5618, batch acc 0.9226
16:09:14.141   Training iter 500, batch loss 1.5682, batch acc 0.9084
16:09:14.676   Training iter 550, batch loss 1.5642, batch acc 0.9192
16:09:15.206   Training iter 600, batch loss 1.5611, batch acc 0.9220
16:09:15.207 Testing @ 325 epoch...
16:09:15.256     Testing, total mean loss 1.56611, total acc 0.91300
16:09:15.256 Training @ 326 epoch...
16:09:15.784   Training iter 50, batch loss 1.5659, batch acc 0.9142
16:09:16.306   Training iter 100, batch loss 1.5589, batch acc 0.9252
16:09:16.826   Training iter 150, batch loss 1.5665, batch acc 0.9140
16:09:17.338   Training iter 200, batch loss 1.5665, batch acc 0.9148
16:09:17.804   Training iter 250, batch loss 1.5656, batch acc 0.9182
16:09:18.287   Training iter 300, batch loss 1.5668, batch acc 0.9158
16:09:18.766   Training iter 350, batch loss 1.5677, batch acc 0.9136
16:09:19.259   Training iter 400, batch loss 1.5680, batch acc 0.9116
16:09:19.764   Training iter 450, batch loss 1.5633, batch acc 0.9192
16:09:20.259   Training iter 500, batch loss 1.5589, batch acc 0.9252
16:09:20.751   Training iter 550, batch loss 1.5702, batch acc 0.9076
16:09:21.229   Training iter 600, batch loss 1.5627, batch acc 0.9188
16:09:21.231 Training @ 327 epoch...
16:09:21.729   Training iter 50, batch loss 1.5722, batch acc 0.9078
16:09:22.266   Training iter 100, batch loss 1.5597, batch acc 0.9236
16:09:22.831   Training iter 150, batch loss 1.5615, batch acc 0.9200
16:09:23.399   Training iter 200, batch loss 1.5666, batch acc 0.9144
16:09:23.960   Training iter 250, batch loss 1.5663, batch acc 0.9160
16:09:24.532   Training iter 300, batch loss 1.5689, batch acc 0.9090
16:09:25.125   Training iter 350, batch loss 1.5613, batch acc 0.9192
16:09:25.720   Training iter 400, batch loss 1.5626, batch acc 0.9232
16:09:26.313   Training iter 450, batch loss 1.5696, batch acc 0.9118
16:09:26.903   Training iter 500, batch loss 1.5652, batch acc 0.9166
16:09:27.503   Training iter 550, batch loss 1.5638, batch acc 0.9186
16:09:28.086   Training iter 600, batch loss 1.5627, batch acc 0.9224
16:09:28.088 Training @ 328 epoch...
16:09:28.637   Training iter 50, batch loss 1.5685, batch acc 0.9062
16:09:29.193   Training iter 100, batch loss 1.5649, batch acc 0.9190
16:09:29.744   Training iter 150, batch loss 1.5700, batch acc 0.9136
16:09:30.287   Training iter 200, batch loss 1.5658, batch acc 0.9144
16:09:30.821   Training iter 250, batch loss 1.5644, batch acc 0.9178
16:09:31.364   Training iter 300, batch loss 1.5612, batch acc 0.9154
16:09:31.903   Training iter 350, batch loss 1.5691, batch acc 0.9132
16:09:32.460   Training iter 400, batch loss 1.5666, batch acc 0.9140
16:09:33.006   Training iter 450, batch loss 1.5619, batch acc 0.9218
16:09:33.541   Training iter 500, batch loss 1.5656, batch acc 0.9192
16:09:34.085   Training iter 550, batch loss 1.5610, batch acc 0.9236
16:09:34.629   Training iter 600, batch loss 1.5610, batch acc 0.9228
16:09:34.631 Training @ 329 epoch...
16:09:35.173   Training iter 50, batch loss 1.5666, batch acc 0.9162
16:09:35.703   Training iter 100, batch loss 1.5655, batch acc 0.9164
16:09:36.249   Training iter 150, batch loss 1.5646, batch acc 0.9194
16:09:36.792   Training iter 200, batch loss 1.5683, batch acc 0.9110
16:09:37.343   Training iter 250, batch loss 1.5619, batch acc 0.9222
16:09:37.883   Training iter 300, batch loss 1.5620, batch acc 0.9186
16:09:38.424   Training iter 350, batch loss 1.5656, batch acc 0.9162
16:09:38.969   Training iter 400, batch loss 1.5674, batch acc 0.9118
16:09:39.577   Training iter 450, batch loss 1.5663, batch acc 0.9148
16:09:40.177   Training iter 500, batch loss 1.5712, batch acc 0.9072
16:09:40.722   Training iter 550, batch loss 1.5632, batch acc 0.9202
16:09:41.250   Training iter 600, batch loss 1.5578, batch acc 0.9240
16:09:41.252 Training @ 330 epoch...
16:09:41.771   Training iter 50, batch loss 1.5631, batch acc 0.9176
16:09:42.305   Training iter 100, batch loss 1.5640, batch acc 0.9158
16:09:42.847   Training iter 150, batch loss 1.5709, batch acc 0.9092
16:09:43.382   Training iter 200, batch loss 1.5707, batch acc 0.9110
16:09:43.919   Training iter 250, batch loss 1.5607, batch acc 0.9210
16:09:44.459   Training iter 300, batch loss 1.5630, batch acc 0.9194
16:09:44.990   Training iter 350, batch loss 1.5672, batch acc 0.9154
16:09:45.536   Training iter 400, batch loss 1.5641, batch acc 0.9210
16:09:46.072   Training iter 450, batch loss 1.5570, batch acc 0.9204
16:09:46.587   Training iter 500, batch loss 1.5671, batch acc 0.9132
16:09:47.081   Training iter 550, batch loss 1.5668, batch acc 0.9144
16:09:47.601   Training iter 600, batch loss 1.5649, batch acc 0.9210
16:09:47.602 Testing @ 330 epoch...
16:09:47.651     Testing, total mean loss 1.56610, total acc 0.91340
16:09:47.651 Training @ 331 epoch...
16:09:48.183   Training iter 50, batch loss 1.5595, batch acc 0.9198
16:09:48.701   Training iter 100, batch loss 1.5655, batch acc 0.9156
16:09:49.222   Training iter 150, batch loss 1.5653, batch acc 0.9156
16:09:49.749   Training iter 200, batch loss 1.5618, batch acc 0.9258
16:09:50.272   Training iter 250, batch loss 1.5648, batch acc 0.9172
16:09:50.789   Training iter 300, batch loss 1.5653, batch acc 0.9164
16:09:51.299   Training iter 350, batch loss 1.5695, batch acc 0.9112
16:09:51.824   Training iter 400, batch loss 1.5681, batch acc 0.9152
16:09:52.358   Training iter 450, batch loss 1.5658, batch acc 0.9146
16:09:52.887   Training iter 500, batch loss 1.5643, batch acc 0.9182
16:09:53.418   Training iter 550, batch loss 1.5650, batch acc 0.9162
16:09:53.971   Training iter 600, batch loss 1.5643, batch acc 0.9172
16:09:53.973 Training @ 332 epoch...
16:09:54.516   Training iter 50, batch loss 1.5638, batch acc 0.9188
16:09:55.043   Training iter 100, batch loss 1.5663, batch acc 0.9132
16:09:55.572   Training iter 150, batch loss 1.5661, batch acc 0.9150
16:09:56.125   Training iter 200, batch loss 1.5676, batch acc 0.9128
16:09:56.690   Training iter 250, batch loss 1.5653, batch acc 0.9144
16:09:57.255   Training iter 300, batch loss 1.5687, batch acc 0.9106
16:09:57.800   Training iter 350, batch loss 1.5682, batch acc 0.9104
16:09:58.373   Training iter 400, batch loss 1.5636, batch acc 0.9208
16:09:58.978   Training iter 450, batch loss 1.5642, batch acc 0.9194
16:09:59.574   Training iter 500, batch loss 1.5578, batch acc 0.9268
16:10:00.128   Training iter 550, batch loss 1.5615, batch acc 0.9224
16:10:00.613   Training iter 600, batch loss 1.5664, batch acc 0.9134
16:10:00.614 Training @ 333 epoch...
16:10:01.131   Training iter 50, batch loss 1.5651, batch acc 0.9166
16:10:01.728   Training iter 100, batch loss 1.5661, batch acc 0.9158
16:10:02.326   Training iter 150, batch loss 1.5655, batch acc 0.9156
16:10:02.904   Training iter 200, batch loss 1.5695, batch acc 0.9112
16:10:03.472   Training iter 250, batch loss 1.5656, batch acc 0.9210
16:10:04.047   Training iter 300, batch loss 1.5685, batch acc 0.9112
16:10:04.616   Training iter 350, batch loss 1.5637, batch acc 0.9192
16:10:05.204   Training iter 400, batch loss 1.5616, batch acc 0.9218
16:10:05.786   Training iter 450, batch loss 1.5618, batch acc 0.9222
16:10:06.350   Training iter 500, batch loss 1.5660, batch acc 0.9148
16:10:06.896   Training iter 550, batch loss 1.5625, batch acc 0.9212
16:10:07.433   Training iter 600, batch loss 1.5635, batch acc 0.9172
16:10:07.434 Training @ 334 epoch...
16:10:07.959   Training iter 50, batch loss 1.5667, batch acc 0.9140
16:10:08.498   Training iter 100, batch loss 1.5655, batch acc 0.9162
16:10:09.033   Training iter 150, batch loss 1.5677, batch acc 0.9152
16:10:09.582   Training iter 200, batch loss 1.5646, batch acc 0.9150
16:10:10.175   Training iter 250, batch loss 1.5656, batch acc 0.9152
16:10:10.751   Training iter 300, batch loss 1.5623, batch acc 0.9212
16:10:11.336   Training iter 350, batch loss 1.5682, batch acc 0.9136
16:10:11.896   Training iter 400, batch loss 1.5661, batch acc 0.9122
16:10:12.471   Training iter 450, batch loss 1.5624, batch acc 0.9190
16:10:13.049   Training iter 500, batch loss 1.5659, batch acc 0.9166
16:10:13.615   Training iter 550, batch loss 1.5631, batch acc 0.9202
16:10:14.178   Training iter 600, batch loss 1.5605, batch acc 0.9248
16:10:14.180 Training @ 335 epoch...
16:10:14.753   Training iter 50, batch loss 1.5619, batch acc 0.9200
16:10:15.302   Training iter 100, batch loss 1.5691, batch acc 0.9112
16:10:15.808   Training iter 150, batch loss 1.5626, batch acc 0.9188
16:10:16.343   Training iter 200, batch loss 1.5670, batch acc 0.9152
16:10:16.864   Training iter 250, batch loss 1.5627, batch acc 0.9190
16:10:17.358   Training iter 300, batch loss 1.5694, batch acc 0.9124
16:10:17.842   Training iter 350, batch loss 1.5618, batch acc 0.9174
16:10:18.316   Training iter 400, batch loss 1.5658, batch acc 0.9154
16:10:18.786   Training iter 450, batch loss 1.5624, batch acc 0.9214
16:10:19.274   Training iter 500, batch loss 1.5624, batch acc 0.9184
16:10:19.758   Training iter 550, batch loss 1.5671, batch acc 0.9154
16:10:20.240   Training iter 600, batch loss 1.5662, batch acc 0.9174
16:10:20.242 Testing @ 335 epoch...
16:10:20.291     Testing, total mean loss 1.56608, total acc 0.91280
16:10:20.291 Training @ 336 epoch...
16:10:20.768   Training iter 50, batch loss 1.5597, batch acc 0.9246
16:10:21.246   Training iter 100, batch loss 1.5676, batch acc 0.9130
16:10:21.734   Training iter 150, batch loss 1.5685, batch acc 0.9146
16:10:22.228   Training iter 200, batch loss 1.5676, batch acc 0.9138
16:10:22.734   Training iter 250, batch loss 1.5654, batch acc 0.9162
16:10:23.228   Training iter 300, batch loss 1.5683, batch acc 0.9082
16:10:23.767   Training iter 350, batch loss 1.5658, batch acc 0.9150
16:10:24.319   Training iter 400, batch loss 1.5597, batch acc 0.9226
16:10:24.856   Training iter 450, batch loss 1.5617, batch acc 0.9212
16:10:25.394   Training iter 500, batch loss 1.5649, batch acc 0.9152
16:10:25.941   Training iter 550, batch loss 1.5643, batch acc 0.9170
16:10:26.508   Training iter 600, batch loss 1.5646, batch acc 0.9176
16:10:26.510 Training @ 337 epoch...
16:10:27.087   Training iter 50, batch loss 1.5683, batch acc 0.9128
16:10:27.616   Training iter 100, batch loss 1.5618, batch acc 0.9166
16:10:28.139   Training iter 150, batch loss 1.5702, batch acc 0.9118
16:10:28.668   Training iter 200, batch loss 1.5671, batch acc 0.9142
16:10:29.202   Training iter 250, batch loss 1.5664, batch acc 0.9160
16:10:29.746   Training iter 300, batch loss 1.5615, batch acc 0.9238
16:10:30.297   Training iter 350, batch loss 1.5599, batch acc 0.9234
16:10:30.871   Training iter 400, batch loss 1.5645, batch acc 0.9188
16:10:31.441   Training iter 450, batch loss 1.5642, batch acc 0.9170
16:10:32.007   Training iter 500, batch loss 1.5607, batch acc 0.9202
16:10:32.568   Training iter 550, batch loss 1.5668, batch acc 0.9188
16:10:33.116   Training iter 600, batch loss 1.5663, batch acc 0.9116
16:10:33.117 Training @ 338 epoch...
16:10:33.659   Training iter 50, batch loss 1.5640, batch acc 0.9168
16:10:34.200   Training iter 100, batch loss 1.5669, batch acc 0.9134
16:10:34.762   Training iter 150, batch loss 1.5608, batch acc 0.9206
16:10:35.316   Training iter 200, batch loss 1.5637, batch acc 0.9186
16:10:35.873   Training iter 250, batch loss 1.5673, batch acc 0.9112
16:10:36.457   Training iter 300, batch loss 1.5666, batch acc 0.9166
16:10:37.017   Training iter 350, batch loss 1.5596, batch acc 0.9254
16:10:37.564   Training iter 400, batch loss 1.5651, batch acc 0.9196
16:10:38.080   Training iter 450, batch loss 1.5657, batch acc 0.9162
16:10:38.612   Training iter 500, batch loss 1.5639, batch acc 0.9180
16:10:39.184   Training iter 550, batch loss 1.5675, batch acc 0.9124
16:10:39.729   Training iter 600, batch loss 1.5662, batch acc 0.9152
16:10:39.731 Training @ 339 epoch...
16:10:40.264   Training iter 50, batch loss 1.5699, batch acc 0.9120
16:10:40.776   Training iter 100, batch loss 1.5628, batch acc 0.9192
16:10:41.285   Training iter 150, batch loss 1.5641, batch acc 0.9214
16:10:41.810   Training iter 200, batch loss 1.5649, batch acc 0.9148
16:10:42.355   Training iter 250, batch loss 1.5697, batch acc 0.9110
16:10:42.882   Training iter 300, batch loss 1.5647, batch acc 0.9136
16:10:43.425   Training iter 350, batch loss 1.5636, batch acc 0.9178
16:10:43.960   Training iter 400, batch loss 1.5668, batch acc 0.9154
16:10:44.503   Training iter 450, batch loss 1.5619, batch acc 0.9212
16:10:45.048   Training iter 500, batch loss 1.5641, batch acc 0.9206
16:10:45.584   Training iter 550, batch loss 1.5668, batch acc 0.9136
16:10:46.125   Training iter 600, batch loss 1.5583, batch acc 0.9248
16:10:46.127 Training @ 340 epoch...
16:10:46.662   Training iter 50, batch loss 1.5643, batch acc 0.9172
16:10:47.191   Training iter 100, batch loss 1.5630, batch acc 0.9222
16:10:47.713   Training iter 150, batch loss 1.5648, batch acc 0.9184
16:10:48.221   Training iter 200, batch loss 1.5629, batch acc 0.9188
16:10:48.746   Training iter 250, batch loss 1.5693, batch acc 0.9124
16:10:49.274   Training iter 300, batch loss 1.5583, batch acc 0.9228
16:10:49.785   Training iter 350, batch loss 1.5650, batch acc 0.9154
16:10:50.307   Training iter 400, batch loss 1.5652, batch acc 0.9172
16:10:50.837   Training iter 450, batch loss 1.5666, batch acc 0.9184
16:10:51.379   Training iter 500, batch loss 1.5642, batch acc 0.9148
16:10:51.961   Training iter 550, batch loss 1.5664, batch acc 0.9146
16:10:52.511   Training iter 600, batch loss 1.5670, batch acc 0.9122
16:10:52.513 Testing @ 340 epoch...
16:10:52.564     Testing, total mean loss 1.56592, total acc 0.91320
16:10:52.564 Training @ 341 epoch...
16:10:53.112   Training iter 50, batch loss 1.5629, batch acc 0.9200
16:10:53.655   Training iter 100, batch loss 1.5601, batch acc 0.9260
16:10:54.211   Training iter 150, batch loss 1.5677, batch acc 0.9134
16:10:54.816   Training iter 200, batch loss 1.5641, batch acc 0.9180
16:10:55.438   Training iter 250, batch loss 1.5628, batch acc 0.9220
16:10:56.058   Training iter 300, batch loss 1.5603, batch acc 0.9222
16:10:56.605   Training iter 350, batch loss 1.5659, batch acc 0.9156
16:10:57.170   Training iter 400, batch loss 1.5654, batch acc 0.9162
16:10:57.686   Training iter 450, batch loss 1.5638, batch acc 0.9164
16:10:58.219   Training iter 500, batch loss 1.5683, batch acc 0.9098
16:10:58.781   Training iter 550, batch loss 1.5717, batch acc 0.9074
16:10:59.317   Training iter 600, batch loss 1.5636, batch acc 0.9196
16:10:59.318 Training @ 342 epoch...
16:10:59.865   Training iter 50, batch loss 1.5620, batch acc 0.9192
16:11:00.409   Training iter 100, batch loss 1.5621, batch acc 0.9224
16:11:00.955   Training iter 150, batch loss 1.5621, batch acc 0.9218
16:11:01.518   Training iter 200, batch loss 1.5659, batch acc 0.9172
16:11:02.152   Training iter 250, batch loss 1.5671, batch acc 0.9158
16:11:02.802   Training iter 300, batch loss 1.5590, batch acc 0.9236
16:11:03.359   Training iter 350, batch loss 1.5660, batch acc 0.9138
16:11:03.904   Training iter 400, batch loss 1.5671, batch acc 0.9144
16:11:04.447   Training iter 450, batch loss 1.5653, batch acc 0.9122
16:11:04.983   Training iter 500, batch loss 1.5648, batch acc 0.9194
16:11:05.522   Training iter 550, batch loss 1.5655, batch acc 0.9166
16:11:06.092   Training iter 600, batch loss 1.5695, batch acc 0.9108
16:11:06.094 Training @ 343 epoch...
16:11:06.655   Training iter 50, batch loss 1.5624, batch acc 0.9190
16:11:07.222   Training iter 100, batch loss 1.5617, batch acc 0.9224
16:11:07.795   Training iter 150, batch loss 1.5653, batch acc 0.9140
16:11:08.380   Training iter 200, batch loss 1.5694, batch acc 0.9104
16:11:08.975   Training iter 250, batch loss 1.5662, batch acc 0.9152
16:11:09.577   Training iter 300, batch loss 1.5638, batch acc 0.9168
16:11:10.169   Training iter 350, batch loss 1.5649, batch acc 0.9182
16:11:10.723   Training iter 400, batch loss 1.5662, batch acc 0.9182
16:11:11.254   Training iter 450, batch loss 1.5637, batch acc 0.9182
16:11:11.811   Training iter 500, batch loss 1.5624, batch acc 0.9194
16:11:12.382   Training iter 550, batch loss 1.5653, batch acc 0.9126
16:11:12.961   Training iter 600, batch loss 1.5645, batch acc 0.9182
16:11:12.963 Training @ 344 epoch...
16:11:13.510   Training iter 50, batch loss 1.5657, batch acc 0.9138
16:11:14.040   Training iter 100, batch loss 1.5659, batch acc 0.9146
16:11:14.575   Training iter 150, batch loss 1.5666, batch acc 0.9142
16:11:15.125   Training iter 200, batch loss 1.5691, batch acc 0.9146
16:11:15.644   Training iter 250, batch loss 1.5573, batch acc 0.9264
16:11:16.171   Training iter 300, batch loss 1.5594, batch acc 0.9228
16:11:16.718   Training iter 350, batch loss 1.5655, batch acc 0.9156
16:11:17.281   Training iter 400, batch loss 1.5659, batch acc 0.9150
16:11:17.803   Training iter 450, batch loss 1.5627, batch acc 0.9214
16:11:18.316   Training iter 500, batch loss 1.5680, batch acc 0.9142
16:11:18.849   Training iter 550, batch loss 1.5674, batch acc 0.9126
16:11:19.383   Training iter 600, batch loss 1.5627, batch acc 0.9220
16:11:19.385 Training @ 345 epoch...
16:11:19.928   Training iter 50, batch loss 1.5652, batch acc 0.9198
16:11:20.457   Training iter 100, batch loss 1.5640, batch acc 0.9174
16:11:20.972   Training iter 150, batch loss 1.5605, batch acc 0.9210
16:11:21.477   Training iter 200, batch loss 1.5697, batch acc 0.9094
16:11:21.996   Training iter 250, batch loss 1.5643, batch acc 0.9150
16:11:22.514   Training iter 300, batch loss 1.5669, batch acc 0.9124
16:11:23.015   Training iter 350, batch loss 1.5610, batch acc 0.9212
16:11:23.536   Training iter 400, batch loss 1.5637, batch acc 0.9190
16:11:24.060   Training iter 450, batch loss 1.5646, batch acc 0.9182
16:11:24.565   Training iter 500, batch loss 1.5613, batch acc 0.9206
16:11:25.065   Training iter 550, batch loss 1.5695, batch acc 0.9118
16:11:25.572   Training iter 600, batch loss 1.5651, batch acc 0.9194
16:11:25.574 Testing @ 345 epoch...
16:11:25.625     Testing, total mean loss 1.56591, total acc 0.91370
16:11:25.625 Training @ 346 epoch...
16:11:26.137   Training iter 50, batch loss 1.5650, batch acc 0.9214
16:11:26.629   Training iter 100, batch loss 1.5613, batch acc 0.9176
16:11:27.121   Training iter 150, batch loss 1.5623, batch acc 0.9190
16:11:27.625   Training iter 200, batch loss 1.5637, batch acc 0.9168
16:11:28.115   Training iter 250, batch loss 1.5679, batch acc 0.9124
16:11:28.626   Training iter 300, batch loss 1.5680, batch acc 0.9128
16:11:29.168   Training iter 350, batch loss 1.5684, batch acc 0.9162
16:11:29.701   Training iter 400, batch loss 1.5685, batch acc 0.9112
16:11:30.222   Training iter 450, batch loss 1.5598, batch acc 0.9242
16:11:30.756   Training iter 500, batch loss 1.5611, batch acc 0.9218
16:11:31.328   Training iter 550, batch loss 1.5638, batch acc 0.9172
16:11:31.912   Training iter 600, batch loss 1.5656, batch acc 0.9164
16:11:31.913 Training @ 347 epoch...
16:11:32.486   Training iter 50, batch loss 1.5652, batch acc 0.9162
16:11:33.048   Training iter 100, batch loss 1.5624, batch acc 0.9232
16:11:33.602   Training iter 150, batch loss 1.5621, batch acc 0.9188
16:11:34.164   Training iter 200, batch loss 1.5647, batch acc 0.9150
16:11:34.742   Training iter 250, batch loss 1.5670, batch acc 0.9158
16:11:35.320   Training iter 300, batch loss 1.5636, batch acc 0.9168
16:11:35.885   Training iter 350, batch loss 1.5671, batch acc 0.9144
16:11:36.451   Training iter 400, batch loss 1.5641, batch acc 0.9178
16:11:36.996   Training iter 450, batch loss 1.5627, batch acc 0.9182
16:11:37.543   Training iter 500, batch loss 1.5647, batch acc 0.9186
16:11:38.078   Training iter 550, batch loss 1.5682, batch acc 0.9112
16:11:38.647   Training iter 600, batch loss 1.5630, batch acc 0.9220
16:11:38.649 Training @ 348 epoch...
16:11:39.224   Training iter 50, batch loss 1.5585, batch acc 0.9252
16:11:39.800   Training iter 100, batch loss 1.5633, batch acc 0.9210
16:11:40.368   Training iter 150, batch loss 1.5708, batch acc 0.9108
16:11:40.925   Training iter 200, batch loss 1.5648, batch acc 0.9174
16:11:41.489   Training iter 250, batch loss 1.5675, batch acc 0.9114
16:11:42.074   Training iter 300, batch loss 1.5686, batch acc 0.9150
16:11:42.670   Training iter 350, batch loss 1.5626, batch acc 0.9184
16:11:43.255   Training iter 400, batch loss 1.5690, batch acc 0.9124
16:11:43.841   Training iter 450, batch loss 1.5626, batch acc 0.9166
16:11:44.435   Training iter 500, batch loss 1.5646, batch acc 0.9182
16:11:45.020   Training iter 550, batch loss 1.5613, batch acc 0.9228
16:11:45.628   Training iter 600, batch loss 1.5618, batch acc 0.9190
16:11:45.629 Training @ 349 epoch...
16:11:46.261   Training iter 50, batch loss 1.5609, batch acc 0.9232
16:11:46.941   Training iter 100, batch loss 1.5655, batch acc 0.9154
16:11:47.611   Training iter 150, batch loss 1.5621, batch acc 0.9228
16:11:48.182   Training iter 200, batch loss 1.5671, batch acc 0.9166
16:11:48.712   Training iter 250, batch loss 1.5621, batch acc 0.9186
16:11:49.248   Training iter 300, batch loss 1.5643, batch acc 0.9148
16:11:49.782   Training iter 350, batch loss 1.5636, batch acc 0.9190
16:11:50.326   Training iter 400, batch loss 1.5653, batch acc 0.9184
16:11:50.874   Training iter 450, batch loss 1.5655, batch acc 0.9138
16:11:51.419   Training iter 500, batch loss 1.5682, batch acc 0.9128
16:11:51.959   Training iter 550, batch loss 1.5646, batch acc 0.9174
16:11:52.511   Training iter 600, batch loss 1.5656, batch acc 0.9154
16:11:52.513 Training @ 350 epoch...
16:11:53.024   Training iter 50, batch loss 1.5632, batch acc 0.9142
16:11:53.509   Training iter 100, batch loss 1.5686, batch acc 0.9146
16:11:53.996   Training iter 150, batch loss 1.5644, batch acc 0.9180
16:11:54.516   Training iter 200, batch loss 1.5609, batch acc 0.9234
16:11:55.009   Training iter 250, batch loss 1.5640, batch acc 0.9190
16:11:55.533   Training iter 300, batch loss 1.5669, batch acc 0.9156
16:11:56.046   Training iter 350, batch loss 1.5683, batch acc 0.9134
16:11:56.554   Training iter 400, batch loss 1.5652, batch acc 0.9156
16:11:57.078   Training iter 450, batch loss 1.5650, batch acc 0.9174
16:11:57.586   Training iter 500, batch loss 1.5601, batch acc 0.9276
16:11:58.082   Training iter 550, batch loss 1.5664, batch acc 0.9130
16:11:58.621   Training iter 600, batch loss 1.5617, batch acc 0.9214
16:11:58.623 Testing @ 350 epoch...
16:11:58.670     Testing, total mean loss 1.56576, total acc 0.91330
16:11:58.670 Training @ 351 epoch...
16:11:59.190   Training iter 50, batch loss 1.5631, batch acc 0.9190
16:11:59.706   Training iter 100, batch loss 1.5651, batch acc 0.9150
16:12:00.222   Training iter 150, batch loss 1.5643, batch acc 0.9176
16:12:00.727   Training iter 200, batch loss 1.5648, batch acc 0.9170
16:12:01.247   Training iter 250, batch loss 1.5616, batch acc 0.9208
16:12:01.794   Training iter 300, batch loss 1.5658, batch acc 0.9162
16:12:02.377   Training iter 350, batch loss 1.5639, batch acc 0.9192
16:12:02.965   Training iter 400, batch loss 1.5675, batch acc 0.9130
16:12:03.542   Training iter 450, batch loss 1.5667, batch acc 0.9162
16:12:04.130   Training iter 500, batch loss 1.5617, batch acc 0.9222
16:12:04.701   Training iter 550, batch loss 1.5686, batch acc 0.9112
16:12:05.278   Training iter 600, batch loss 1.5606, batch acc 0.9200
16:12:05.280 Training @ 352 epoch...
16:12:05.861   Training iter 50, batch loss 1.5659, batch acc 0.9166
16:12:06.454   Training iter 100, batch loss 1.5672, batch acc 0.9146
16:12:07.041   Training iter 150, batch loss 1.5657, batch acc 0.9158
16:12:07.612   Training iter 200, batch loss 1.5667, batch acc 0.9188
16:12:08.179   Training iter 250, batch loss 1.5616, batch acc 0.9218
16:12:08.730   Training iter 300, batch loss 1.5651, batch acc 0.9146
16:12:09.272   Training iter 350, batch loss 1.5635, batch acc 0.9180
16:12:09.812   Training iter 400, batch loss 1.5656, batch acc 0.9170
16:12:10.399   Training iter 450, batch loss 1.5607, batch acc 0.9200
16:12:10.938   Training iter 500, batch loss 1.5629, batch acc 0.9170
16:12:11.472   Training iter 550, batch loss 1.5673, batch acc 0.9120
16:12:11.981   Training iter 600, batch loss 1.5611, batch acc 0.9202
16:12:11.983 Training @ 353 epoch...
16:12:12.510   Training iter 50, batch loss 1.5651, batch acc 0.9190
16:12:13.017   Training iter 100, batch loss 1.5655, batch acc 0.9168
16:12:13.524   Training iter 150, batch loss 1.5667, batch acc 0.9176
16:12:14.035   Training iter 200, batch loss 1.5669, batch acc 0.9150
16:12:14.561   Training iter 250, batch loss 1.5638, batch acc 0.9162
16:12:15.076   Training iter 300, batch loss 1.5646, batch acc 0.9168
16:12:15.584   Training iter 350, batch loss 1.5657, batch acc 0.9144
16:12:16.099   Training iter 400, batch loss 1.5618, batch acc 0.9206
16:12:16.627   Training iter 450, batch loss 1.5677, batch acc 0.9120
16:12:17.130   Training iter 500, batch loss 1.5601, batch acc 0.9216
16:12:17.623   Training iter 550, batch loss 1.5622, batch acc 0.9206
16:12:18.103   Training iter 600, batch loss 1.5630, batch acc 0.9204
16:12:18.104 Training @ 354 epoch...
16:12:18.578   Training iter 50, batch loss 1.5604, batch acc 0.9224
16:12:19.056   Training iter 100, batch loss 1.5633, batch acc 0.9178
16:12:19.545   Training iter 150, batch loss 1.5627, batch acc 0.9164
16:12:20.031   Training iter 200, batch loss 1.5637, batch acc 0.9194
16:12:20.523   Training iter 250, batch loss 1.5643, batch acc 0.9126
16:12:21.004   Training iter 300, batch loss 1.5653, batch acc 0.9184
16:12:21.501   Training iter 350, batch loss 1.5641, batch acc 0.9236
16:12:22.009   Training iter 400, batch loss 1.5654, batch acc 0.9142
16:12:22.529   Training iter 450, batch loss 1.5637, batch acc 0.9208
16:12:23.033   Training iter 500, batch loss 1.5672, batch acc 0.9136
16:12:23.527   Training iter 550, batch loss 1.5664, batch acc 0.9136
16:12:24.049   Training iter 600, batch loss 1.5668, batch acc 0.9138
16:12:24.051 Training @ 355 epoch...
16:12:24.586   Training iter 50, batch loss 1.5678, batch acc 0.9154
16:12:25.107   Training iter 100, batch loss 1.5647, batch acc 0.9184
16:12:25.620   Training iter 150, batch loss 1.5635, batch acc 0.9182
16:12:26.147   Training iter 200, batch loss 1.5639, batch acc 0.9128
16:12:26.664   Training iter 250, batch loss 1.5627, batch acc 0.9216
16:12:27.189   Training iter 300, batch loss 1.5634, batch acc 0.9202
16:12:27.690   Training iter 350, batch loss 1.5648, batch acc 0.9158
16:12:28.209   Training iter 400, batch loss 1.5640, batch acc 0.9150
16:12:28.710   Training iter 450, batch loss 1.5660, batch acc 0.9186
16:12:29.229   Training iter 500, batch loss 1.5612, batch acc 0.9232
16:12:29.740   Training iter 550, batch loss 1.5656, batch acc 0.9146
16:12:30.252   Training iter 600, batch loss 1.5657, batch acc 0.9126
16:12:30.254 Testing @ 355 epoch...
16:12:30.305     Testing, total mean loss 1.56575, total acc 0.91440
16:12:30.305 Training @ 356 epoch...
16:12:30.835   Training iter 50, batch loss 1.5647, batch acc 0.9210
16:12:31.372   Training iter 100, batch loss 1.5666, batch acc 0.9118
16:12:31.906   Training iter 150, batch loss 1.5621, batch acc 0.9212
16:12:32.454   Training iter 200, batch loss 1.5635, batch acc 0.9204
16:12:32.997   Training iter 250, batch loss 1.5634, batch acc 0.9168
16:12:33.512   Training iter 300, batch loss 1.5659, batch acc 0.9148
16:12:34.017   Training iter 350, batch loss 1.5674, batch acc 0.9116
16:12:34.533   Training iter 400, batch loss 1.5641, batch acc 0.9182
16:12:35.071   Training iter 450, batch loss 1.5660, batch acc 0.9120
16:12:35.608   Training iter 500, batch loss 1.5646, batch acc 0.9204
16:12:36.135   Training iter 550, batch loss 1.5609, batch acc 0.9222
16:12:36.668   Training iter 600, batch loss 1.5635, batch acc 0.9172
16:12:36.670 Training @ 357 epoch...
16:12:37.235   Training iter 50, batch loss 1.5647, batch acc 0.9170
16:12:37.791   Training iter 100, batch loss 1.5628, batch acc 0.9188
16:12:38.333   Training iter 150, batch loss 1.5672, batch acc 0.9156
16:12:38.895   Training iter 200, batch loss 1.5669, batch acc 0.9128
16:12:39.435   Training iter 250, batch loss 1.5652, batch acc 0.9226
16:12:39.962   Training iter 300, batch loss 1.5616, batch acc 0.9226
16:12:40.498   Training iter 350, batch loss 1.5631, batch acc 0.9168
16:12:40.977   Training iter 400, batch loss 1.5624, batch acc 0.9166
16:12:41.467   Training iter 450, batch loss 1.5613, batch acc 0.9218
16:12:41.964   Training iter 500, batch loss 1.5641, batch acc 0.9150
16:12:42.518   Training iter 550, batch loss 1.5689, batch acc 0.9130
16:12:43.107   Training iter 600, batch loss 1.5641, batch acc 0.9198
16:12:43.109 Training @ 358 epoch...
16:12:43.711   Training iter 50, batch loss 1.5653, batch acc 0.9140
16:12:44.285   Training iter 100, batch loss 1.5658, batch acc 0.9180
16:12:44.812   Training iter 150, batch loss 1.5652, batch acc 0.9176
16:12:45.355   Training iter 200, batch loss 1.5664, batch acc 0.9134
16:12:45.882   Training iter 250, batch loss 1.5641, batch acc 0.9202
16:12:46.412   Training iter 300, batch loss 1.5630, batch acc 0.9218
16:12:46.962   Training iter 350, batch loss 1.5673, batch acc 0.9128
16:12:47.510   Training iter 400, batch loss 1.5609, batch acc 0.9204
16:12:48.061   Training iter 450, batch loss 1.5621, batch acc 0.9204
16:12:48.606   Training iter 500, batch loss 1.5702, batch acc 0.9102
16:12:49.135   Training iter 550, batch loss 1.5616, batch acc 0.9248
16:12:49.677   Training iter 600, batch loss 1.5602, batch acc 0.9246
16:12:49.679 Training @ 359 epoch...
16:12:50.214   Training iter 50, batch loss 1.5618, batch acc 0.9182
16:12:50.765   Training iter 100, batch loss 1.5621, batch acc 0.9192
16:12:51.317   Training iter 150, batch loss 1.5679, batch acc 0.9130
16:12:51.874   Training iter 200, batch loss 1.5621, batch acc 0.9194
16:12:52.421   Training iter 250, batch loss 1.5671, batch acc 0.9130
16:12:52.981   Training iter 300, batch loss 1.5655, batch acc 0.9176
16:12:53.544   Training iter 350, batch loss 1.5643, batch acc 0.9204
16:12:54.102   Training iter 400, batch loss 1.5608, batch acc 0.9208
16:12:54.644   Training iter 450, batch loss 1.5647, batch acc 0.9226
16:12:55.178   Training iter 500, batch loss 1.5662, batch acc 0.9172
16:12:55.712   Training iter 550, batch loss 1.5639, batch acc 0.9158
16:12:56.250   Training iter 600, batch loss 1.5653, batch acc 0.9152
16:12:56.252 Training @ 360 epoch...
16:12:56.783   Training iter 50, batch loss 1.5665, batch acc 0.9168
16:12:57.286   Training iter 100, batch loss 1.5628, batch acc 0.9164
16:12:57.788   Training iter 150, batch loss 1.5628, batch acc 0.9178
16:12:58.285   Training iter 200, batch loss 1.5635, batch acc 0.9188
16:12:58.784   Training iter 250, batch loss 1.5649, batch acc 0.9184
16:12:59.308   Training iter 300, batch loss 1.5634, batch acc 0.9176
16:12:59.810   Training iter 350, batch loss 1.5658, batch acc 0.9168
16:13:00.328   Training iter 400, batch loss 1.5639, batch acc 0.9200
16:13:00.851   Training iter 450, batch loss 1.5630, batch acc 0.9208
16:13:01.398   Training iter 500, batch loss 1.5676, batch acc 0.9174
16:13:01.971   Training iter 550, batch loss 1.5657, batch acc 0.9162
16:13:02.522   Training iter 600, batch loss 1.5613, batch acc 0.9208
16:13:02.524 Testing @ 360 epoch...
16:13:02.572     Testing, total mean loss 1.56560, total acc 0.91220
16:13:02.572 Training @ 361 epoch...
16:13:03.139   Training iter 50, batch loss 1.5635, batch acc 0.9158
16:13:03.698   Training iter 100, batch loss 1.5636, batch acc 0.9178
16:13:04.255   Training iter 150, batch loss 1.5639, batch acc 0.9172
16:13:04.817   Training iter 200, batch loss 1.5642, batch acc 0.9202
16:13:05.384   Training iter 250, batch loss 1.5689, batch acc 0.9136
16:13:05.959   Training iter 300, batch loss 1.5612, batch acc 0.9192
16:13:06.533   Training iter 350, batch loss 1.5644, batch acc 0.9172
16:13:07.059   Training iter 400, batch loss 1.5648, batch acc 0.9190
16:13:07.604   Training iter 450, batch loss 1.5650, batch acc 0.9180
16:13:08.126   Training iter 500, batch loss 1.5603, batch acc 0.9244
16:13:08.675   Training iter 550, batch loss 1.5649, batch acc 0.9166
16:13:09.229   Training iter 600, batch loss 1.5667, batch acc 0.9170
16:13:09.231 Training @ 362 epoch...
16:13:09.768   Training iter 50, batch loss 1.5596, batch acc 0.9250
16:13:10.273   Training iter 100, batch loss 1.5606, batch acc 0.9258
16:13:10.779   Training iter 150, batch loss 1.5685, batch acc 0.9140
16:13:11.298   Training iter 200, batch loss 1.5623, batch acc 0.9174
16:13:11.796   Training iter 250, batch loss 1.5658, batch acc 0.9128
16:13:12.310   Training iter 300, batch loss 1.5678, batch acc 0.9130
16:13:12.838   Training iter 350, batch loss 1.5639, batch acc 0.9192
16:13:13.346   Training iter 400, batch loss 1.5626, batch acc 0.9184
16:13:13.868   Training iter 450, batch loss 1.5658, batch acc 0.9156
16:13:14.404   Training iter 500, batch loss 1.5664, batch acc 0.9146
16:13:14.929   Training iter 550, batch loss 1.5637, batch acc 0.9208
16:13:15.479   Training iter 600, batch loss 1.5641, batch acc 0.9186
16:13:15.481 Training @ 363 epoch...
16:13:16.034   Training iter 50, batch loss 1.5636, batch acc 0.9170
16:13:16.571   Training iter 100, batch loss 1.5618, batch acc 0.9210
16:13:17.114   Training iter 150, batch loss 1.5665, batch acc 0.9150
16:13:17.638   Training iter 200, batch loss 1.5687, batch acc 0.9078
16:13:18.169   Training iter 250, batch loss 1.5611, batch acc 0.9222
16:13:18.691   Training iter 300, batch loss 1.5621, batch acc 0.9226
16:13:19.225   Training iter 350, batch loss 1.5705, batch acc 0.9094
16:13:19.747   Training iter 400, batch loss 1.5659, batch acc 0.9162
16:13:20.279   Training iter 450, batch loss 1.5632, batch acc 0.9192
16:13:20.818   Training iter 500, batch loss 1.5631, batch acc 0.9194
16:13:21.362   Training iter 550, batch loss 1.5620, batch acc 0.9190
16:13:21.903   Training iter 600, batch loss 1.5623, batch acc 0.9202
16:13:21.905 Training @ 364 epoch...
16:13:22.444   Training iter 50, batch loss 1.5652, batch acc 0.9172
16:13:22.961   Training iter 100, batch loss 1.5623, batch acc 0.9204
16:13:23.482   Training iter 150, batch loss 1.5676, batch acc 0.9132
16:13:24.011   Training iter 200, batch loss 1.5682, batch acc 0.9136
16:13:24.539   Training iter 250, batch loss 1.5610, batch acc 0.9226
16:13:25.080   Training iter 300, batch loss 1.5643, batch acc 0.9176
16:13:25.609   Training iter 350, batch loss 1.5624, batch acc 0.9202
16:13:26.154   Training iter 400, batch loss 1.5701, batch acc 0.9092
16:13:26.693   Training iter 450, batch loss 1.5648, batch acc 0.9184
16:13:27.246   Training iter 500, batch loss 1.5585, batch acc 0.9250
16:13:27.794   Training iter 550, batch loss 1.5614, batch acc 0.9222
16:13:28.524   Training iter 600, batch loss 1.5646, batch acc 0.9118
16:13:28.527 Training @ 365 epoch...
16:13:29.224   Training iter 50, batch loss 1.5684, batch acc 0.9140
16:13:29.756   Training iter 100, batch loss 1.5615, batch acc 0.9226
16:13:30.305   Training iter 150, batch loss 1.5628, batch acc 0.9204
16:13:30.838   Training iter 200, batch loss 1.5660, batch acc 0.9132
16:13:31.361   Training iter 250, batch loss 1.5622, batch acc 0.9208
16:13:31.909   Training iter 300, batch loss 1.5633, batch acc 0.9192
16:13:32.499   Training iter 350, batch loss 1.5654, batch acc 0.9172
16:13:33.116   Training iter 400, batch loss 1.5621, batch acc 0.9214
16:13:33.647   Training iter 450, batch loss 1.5646, batch acc 0.9178
16:13:34.179   Training iter 500, batch loss 1.5655, batch acc 0.9176
16:13:34.795   Training iter 550, batch loss 1.5643, batch acc 0.9196
16:13:35.341   Training iter 600, batch loss 1.5641, batch acc 0.9132
16:13:35.343 Testing @ 365 epoch...
16:13:35.390     Testing, total mean loss 1.56557, total acc 0.91320
16:13:35.390 Training @ 366 epoch...
16:13:35.918   Training iter 50, batch loss 1.5659, batch acc 0.9154
16:13:36.479   Training iter 100, batch loss 1.5623, batch acc 0.9214
16:13:37.063   Training iter 150, batch loss 1.5636, batch acc 0.9218
16:13:37.662   Training iter 200, batch loss 1.5610, batch acc 0.9224
16:13:38.229   Training iter 250, batch loss 1.5633, batch acc 0.9188
16:13:38.770   Training iter 300, batch loss 1.5648, batch acc 0.9158
16:13:39.321   Training iter 350, batch loss 1.5662, batch acc 0.9120
16:13:39.861   Training iter 400, batch loss 1.5623, batch acc 0.9206
16:13:40.415   Training iter 450, batch loss 1.5687, batch acc 0.9124
16:13:40.972   Training iter 500, batch loss 1.5646, batch acc 0.9164
16:13:41.531   Training iter 550, batch loss 1.5594, batch acc 0.9220
16:13:42.071   Training iter 600, batch loss 1.5674, batch acc 0.9146
16:13:42.072 Training @ 367 epoch...
16:13:42.596   Training iter 50, batch loss 1.5645, batch acc 0.9160
16:13:43.133   Training iter 100, batch loss 1.5673, batch acc 0.9134
16:13:43.680   Training iter 150, batch loss 1.5647, batch acc 0.9190
16:13:44.240   Training iter 200, batch loss 1.5627, batch acc 0.9172
16:13:44.789   Training iter 250, batch loss 1.5618, batch acc 0.9214
16:13:45.302   Training iter 300, batch loss 1.5620, batch acc 0.9200
16:13:45.832   Training iter 350, batch loss 1.5670, batch acc 0.9164
16:13:46.363   Training iter 400, batch loss 1.5688, batch acc 0.9094
16:13:46.904   Training iter 450, batch loss 1.5652, batch acc 0.9168
16:13:47.437   Training iter 500, batch loss 1.5660, batch acc 0.9140
16:13:47.957   Training iter 550, batch loss 1.5575, batch acc 0.9294
16:13:48.490   Training iter 600, batch loss 1.5623, batch acc 0.9224
16:13:48.492 Training @ 368 epoch...
16:13:49.025   Training iter 50, batch loss 1.5658, batch acc 0.9138
16:13:49.569   Training iter 100, batch loss 1.5689, batch acc 0.9136
16:13:50.104   Training iter 150, batch loss 1.5668, batch acc 0.9150
16:13:50.644   Training iter 200, batch loss 1.5644, batch acc 0.9168
16:13:51.166   Training iter 250, batch loss 1.5611, batch acc 0.9224
16:13:51.700   Training iter 300, batch loss 1.5650, batch acc 0.9178
16:13:52.236   Training iter 350, batch loss 1.5628, batch acc 0.9194
16:13:52.773   Training iter 400, batch loss 1.5637, batch acc 0.9172
16:13:53.305   Training iter 450, batch loss 1.5651, batch acc 0.9156
16:13:53.853   Training iter 500, batch loss 1.5648, batch acc 0.9156
16:13:54.410   Training iter 550, batch loss 1.5605, batch acc 0.9222
16:13:54.937   Training iter 600, batch loss 1.5603, batch acc 0.9262
16:13:54.939 Training @ 369 epoch...
16:13:55.497   Training iter 50, batch loss 1.5631, batch acc 0.9196
16:13:56.035   Training iter 100, batch loss 1.5651, batch acc 0.9154
16:13:56.592   Training iter 150, batch loss 1.5616, batch acc 0.9186
16:13:57.154   Training iter 200, batch loss 1.5639, batch acc 0.9204
16:13:57.717   Training iter 250, batch loss 1.5576, batch acc 0.9286
16:13:58.280   Training iter 300, batch loss 1.5634, batch acc 0.9198
16:13:58.821   Training iter 350, batch loss 1.5656, batch acc 0.9140
16:13:59.360   Training iter 400, batch loss 1.5625, batch acc 0.9184
16:13:59.886   Training iter 450, batch loss 1.5683, batch acc 0.9126
16:14:00.419   Training iter 500, batch loss 1.5646, batch acc 0.9180
16:14:00.942   Training iter 550, batch loss 1.5682, batch acc 0.9174
16:14:01.519   Training iter 600, batch loss 1.5652, batch acc 0.9142
16:14:01.521 Training @ 370 epoch...
16:14:02.134   Training iter 50, batch loss 1.5627, batch acc 0.9170
16:14:02.716   Training iter 100, batch loss 1.5620, batch acc 0.9218
16:14:03.283   Training iter 150, batch loss 1.5612, batch acc 0.9222
16:14:03.838   Training iter 200, batch loss 1.5606, batch acc 0.9194
16:14:04.386   Training iter 250, batch loss 1.5659, batch acc 0.9180
16:14:04.935   Training iter 300, batch loss 1.5619, batch acc 0.9242
16:14:05.450   Training iter 350, batch loss 1.5684, batch acc 0.9110
16:14:05.956   Training iter 400, batch loss 1.5594, batch acc 0.9248
16:14:06.470   Training iter 450, batch loss 1.5681, batch acc 0.9092
16:14:06.995   Training iter 500, batch loss 1.5665, batch acc 0.9152
16:14:07.524   Training iter 550, batch loss 1.5661, batch acc 0.9162
16:14:08.049   Training iter 600, batch loss 1.5656, batch acc 0.9164
16:14:08.051 Testing @ 370 epoch...
16:14:08.099     Testing, total mean loss 1.56528, total acc 0.91410
16:14:08.099 Training @ 371 epoch...
16:14:08.636   Training iter 50, batch loss 1.5626, batch acc 0.9188
16:14:09.178   Training iter 100, batch loss 1.5642, batch acc 0.9204
16:14:09.685   Training iter 150, batch loss 1.5633, batch acc 0.9186
16:14:10.190   Training iter 200, batch loss 1.5647, batch acc 0.9146
16:14:10.726   Training iter 250, batch loss 1.5625, batch acc 0.9186
16:14:11.304   Training iter 300, batch loss 1.5624, batch acc 0.9260
16:14:11.877   Training iter 350, batch loss 1.5639, batch acc 0.9164
16:14:12.448   Training iter 400, batch loss 1.5672, batch acc 0.9150
16:14:12.995   Training iter 450, batch loss 1.5633, batch acc 0.9182
16:14:13.534   Training iter 500, batch loss 1.5635, batch acc 0.9186
16:14:14.077   Training iter 550, batch loss 1.5619, batch acc 0.9182
16:14:14.609   Training iter 600, batch loss 1.5688, batch acc 0.9114
16:14:14.611 Training @ 372 epoch...
16:14:15.123   Training iter 50, batch loss 1.5641, batch acc 0.9186
16:14:15.647   Training iter 100, batch loss 1.5591, batch acc 0.9236
16:14:16.179   Training iter 150, batch loss 1.5629, batch acc 0.9172
16:14:16.716   Training iter 200, batch loss 1.5648, batch acc 0.9178
16:14:17.223   Training iter 250, batch loss 1.5631, batch acc 0.9218
16:14:17.717   Training iter 300, batch loss 1.5678, batch acc 0.9128
16:14:18.233   Training iter 350, batch loss 1.5645, batch acc 0.9158
16:14:18.761   Training iter 400, batch loss 1.5639, batch acc 0.9190
16:14:19.302   Training iter 450, batch loss 1.5684, batch acc 0.9118
16:14:19.831   Training iter 500, batch loss 1.5655, batch acc 0.9184
16:14:20.343   Training iter 550, batch loss 1.5623, batch acc 0.9172
16:14:20.813   Training iter 600, batch loss 1.5615, batch acc 0.9216
16:14:20.815 Training @ 373 epoch...
16:14:21.304   Training iter 50, batch loss 1.5659, batch acc 0.9156
16:14:21.815   Training iter 100, batch loss 1.5620, batch acc 0.9192
16:14:22.336   Training iter 150, batch loss 1.5605, batch acc 0.9220
16:14:22.845   Training iter 200, batch loss 1.5667, batch acc 0.9142
16:14:23.356   Training iter 250, batch loss 1.5606, batch acc 0.9228
16:14:23.840   Training iter 300, batch loss 1.5648, batch acc 0.9168
16:14:24.340   Training iter 350, batch loss 1.5700, batch acc 0.9122
16:14:24.862   Training iter 400, batch loss 1.5621, batch acc 0.9232
16:14:25.354   Training iter 450, batch loss 1.5693, batch acc 0.9090
16:14:25.849   Training iter 500, batch loss 1.5622, batch acc 0.9200
16:14:26.355   Training iter 550, batch loss 1.5610, batch acc 0.9256
16:14:26.856   Training iter 600, batch loss 1.5630, batch acc 0.9148
16:14:26.858 Training @ 374 epoch...
16:14:27.366   Training iter 50, batch loss 1.5652, batch acc 0.9160
16:14:27.874   Training iter 100, batch loss 1.5631, batch acc 0.9182
16:14:28.385   Training iter 150, batch loss 1.5593, batch acc 0.9246
16:14:28.856   Training iter 200, batch loss 1.5658, batch acc 0.9160
16:14:29.343   Training iter 250, batch loss 1.5626, batch acc 0.9182
16:14:29.822   Training iter 300, batch loss 1.5615, batch acc 0.9232
16:14:30.356   Training iter 350, batch loss 1.5648, batch acc 0.9158
16:14:30.830   Training iter 400, batch loss 1.5638, batch acc 0.9164
16:14:31.301   Training iter 450, batch loss 1.5674, batch acc 0.9132
16:14:31.790   Training iter 500, batch loss 1.5626, batch acc 0.9188
16:14:32.313   Training iter 550, batch loss 1.5674, batch acc 0.9128
16:14:32.811   Training iter 600, batch loss 1.5637, batch acc 0.9178
16:14:32.813 Training @ 375 epoch...
16:14:33.296   Training iter 50, batch loss 1.5701, batch acc 0.9104
16:14:33.770   Training iter 100, batch loss 1.5647, batch acc 0.9152
16:14:34.248   Training iter 150, batch loss 1.5637, batch acc 0.9178
16:14:34.733   Training iter 200, batch loss 1.5646, batch acc 0.9178
16:14:35.210   Training iter 250, batch loss 1.5613, batch acc 0.9238
16:14:35.707   Training iter 300, batch loss 1.5660, batch acc 0.9142
16:14:36.226   Training iter 350, batch loss 1.5591, batch acc 0.9242
16:14:36.767   Training iter 400, batch loss 1.5628, batch acc 0.9194
16:14:37.286   Training iter 450, batch loss 1.5621, batch acc 0.9204
16:14:37.801   Training iter 500, batch loss 1.5664, batch acc 0.9138
16:14:38.312   Training iter 550, batch loss 1.5646, batch acc 0.9192
16:14:38.810   Training iter 600, batch loss 1.5623, batch acc 0.9232
16:14:38.812 Testing @ 375 epoch...
16:14:38.863     Testing, total mean loss 1.56531, total acc 0.91430
16:14:38.863 Training @ 376 epoch...
16:14:39.390   Training iter 50, batch loss 1.5651, batch acc 0.9164
16:14:39.880   Training iter 100, batch loss 1.5700, batch acc 0.9092
16:14:40.401   Training iter 150, batch loss 1.5605, batch acc 0.9220
16:14:40.894   Training iter 200, batch loss 1.5637, batch acc 0.9192
16:14:41.435   Training iter 250, batch loss 1.5643, batch acc 0.9198
16:14:41.985   Training iter 300, batch loss 1.5635, batch acc 0.9182
16:14:42.551   Training iter 350, batch loss 1.5643, batch acc 0.9176
16:14:43.134   Training iter 400, batch loss 1.5577, batch acc 0.9258
16:14:43.712   Training iter 450, batch loss 1.5652, batch acc 0.9156
16:14:44.351   Training iter 500, batch loss 1.5663, batch acc 0.9170
16:14:44.920   Training iter 550, batch loss 1.5632, batch acc 0.9194
16:14:45.487   Training iter 600, batch loss 1.5635, batch acc 0.9180
16:14:45.488 Training @ 377 epoch...
16:14:46.072   Training iter 50, batch loss 1.5630, batch acc 0.9190
16:14:46.682   Training iter 100, batch loss 1.5627, batch acc 0.9188
16:14:47.245   Training iter 150, batch loss 1.5653, batch acc 0.9182
16:14:47.792   Training iter 200, batch loss 1.5590, batch acc 0.9254
16:14:48.353   Training iter 250, batch loss 1.5629, batch acc 0.9190
16:14:48.900   Training iter 300, batch loss 1.5668, batch acc 0.9134
16:14:49.427   Training iter 350, batch loss 1.5625, batch acc 0.9208
16:14:49.957   Training iter 400, batch loss 1.5651, batch acc 0.9180
16:14:50.521   Training iter 450, batch loss 1.5590, batch acc 0.9240
16:14:51.071   Training iter 500, batch loss 1.5679, batch acc 0.9134
16:14:51.631   Training iter 550, batch loss 1.5664, batch acc 0.9156
16:14:52.203   Training iter 600, batch loss 1.5661, batch acc 0.9154
16:14:52.204 Training @ 378 epoch...
16:14:52.760   Training iter 50, batch loss 1.5654, batch acc 0.9138
16:14:53.317   Training iter 100, batch loss 1.5641, batch acc 0.9174
16:14:53.863   Training iter 150, batch loss 1.5596, batch acc 0.9208
16:14:54.410   Training iter 200, batch loss 1.5646, batch acc 0.9178
16:14:54.928   Training iter 250, batch loss 1.5651, batch acc 0.9162
16:14:55.448   Training iter 300, batch loss 1.5648, batch acc 0.9190
16:14:55.951   Training iter 350, batch loss 1.5655, batch acc 0.9174
16:14:56.477   Training iter 400, batch loss 1.5628, batch acc 0.9186
16:14:57.008   Training iter 450, batch loss 1.5626, batch acc 0.9140
16:14:57.540   Training iter 500, batch loss 1.5652, batch acc 0.9170
16:14:58.052   Training iter 550, batch loss 1.5627, batch acc 0.9236
16:14:58.573   Training iter 600, batch loss 1.5641, batch acc 0.9216
16:14:58.574 Training @ 379 epoch...
16:14:59.129   Training iter 50, batch loss 1.5617, batch acc 0.9196
16:14:59.660   Training iter 100, batch loss 1.5656, batch acc 0.9150
16:15:00.199   Training iter 150, batch loss 1.5667, batch acc 0.9176
16:15:00.751   Training iter 200, batch loss 1.5652, batch acc 0.9172
16:15:01.351   Training iter 250, batch loss 1.5613, batch acc 0.9248
16:15:01.958   Training iter 300, batch loss 1.5654, batch acc 0.9162
16:15:02.559   Training iter 350, batch loss 1.5591, batch acc 0.9224
16:15:03.148   Training iter 400, batch loss 1.5645, batch acc 0.9182
16:15:03.699   Training iter 450, batch loss 1.5646, batch acc 0.9162
16:15:04.226   Training iter 500, batch loss 1.5628, batch acc 0.9190
16:15:04.758   Training iter 550, batch loss 1.5672, batch acc 0.9118
16:15:05.282   Training iter 600, batch loss 1.5623, batch acc 0.9180
16:15:05.283 Training @ 380 epoch...
16:15:05.802   Training iter 50, batch loss 1.5648, batch acc 0.9144
16:15:06.277   Training iter 100, batch loss 1.5671, batch acc 0.9136
16:15:06.770   Training iter 150, batch loss 1.5607, batch acc 0.9184
16:15:07.262   Training iter 200, batch loss 1.5645, batch acc 0.9184
16:15:07.748   Training iter 250, batch loss 1.5648, batch acc 0.9180
16:15:08.230   Training iter 300, batch loss 1.5629, batch acc 0.9218
16:15:08.722   Training iter 350, batch loss 1.5658, batch acc 0.9134
16:15:09.210   Training iter 400, batch loss 1.5607, batch acc 0.9230
16:15:09.700   Training iter 450, batch loss 1.5601, batch acc 0.9238
16:15:10.210   Training iter 500, batch loss 1.5670, batch acc 0.9148
16:15:10.706   Training iter 550, batch loss 1.5635, batch acc 0.9172
16:15:11.193   Training iter 600, batch loss 1.5642, batch acc 0.9176
16:15:11.195 Testing @ 380 epoch...
16:15:11.250     Testing, total mean loss 1.56500, total acc 0.91440
16:15:11.250 Training @ 381 epoch...
16:15:11.787   Training iter 50, batch loss 1.5636, batch acc 0.9164
16:15:12.300   Training iter 100, batch loss 1.5619, batch acc 0.9224
16:15:12.809   Training iter 150, batch loss 1.5661, batch acc 0.9136
16:15:13.303   Training iter 200, batch loss 1.5606, batch acc 0.9208
16:15:13.795   Training iter 250, batch loss 1.5653, batch acc 0.9174
16:15:14.318   Training iter 300, batch loss 1.5616, batch acc 0.9234
16:15:14.834   Training iter 350, batch loss 1.5651, batch acc 0.9186
16:15:15.335   Training iter 400, batch loss 1.5636, batch acc 0.9188
16:15:15.832   Training iter 450, batch loss 1.5653, batch acc 0.9148
16:15:16.364   Training iter 500, batch loss 1.5633, batch acc 0.9200
16:15:16.922   Training iter 550, batch loss 1.5676, batch acc 0.9092
16:15:17.515   Training iter 600, batch loss 1.5616, batch acc 0.9198
16:15:17.517 Training @ 382 epoch...
16:15:18.096   Training iter 50, batch loss 1.5620, batch acc 0.9180
16:15:18.684   Training iter 100, batch loss 1.5637, batch acc 0.9192
16:15:19.244   Training iter 150, batch loss 1.5610, batch acc 0.9212
16:15:19.800   Training iter 200, batch loss 1.5659, batch acc 0.9150
16:15:20.346   Training iter 250, batch loss 1.5657, batch acc 0.9166
16:15:20.879   Training iter 300, batch loss 1.5672, batch acc 0.9152
16:15:21.430   Training iter 350, batch loss 1.5615, batch acc 0.9216
16:15:21.966   Training iter 400, batch loss 1.5636, batch acc 0.9198
16:15:22.538   Training iter 450, batch loss 1.5666, batch acc 0.9144
16:15:23.094   Training iter 500, batch loss 1.5641, batch acc 0.9186
16:15:23.617   Training iter 550, batch loss 1.5614, batch acc 0.9198
16:15:24.162   Training iter 600, batch loss 1.5626, batch acc 0.9158
16:15:24.164 Training @ 383 epoch...
16:15:24.721   Training iter 50, batch loss 1.5618, batch acc 0.9164
16:15:25.286   Training iter 100, batch loss 1.5640, batch acc 0.9194
16:15:25.863   Training iter 150, batch loss 1.5633, batch acc 0.9210
16:15:26.417   Training iter 200, batch loss 1.5671, batch acc 0.9142
16:15:26.954   Training iter 250, batch loss 1.5623, batch acc 0.9186
16:15:27.490   Training iter 300, batch loss 1.5666, batch acc 0.9138
16:15:28.041   Training iter 350, batch loss 1.5617, batch acc 0.9230
16:15:28.591   Training iter 400, batch loss 1.5652, batch acc 0.9170
16:15:29.188   Training iter 450, batch loss 1.5663, batch acc 0.9174
16:15:29.748   Training iter 500, batch loss 1.5576, batch acc 0.9260
16:15:30.328   Training iter 550, batch loss 1.5661, batch acc 0.9160
16:15:30.893   Training iter 600, batch loss 1.5634, batch acc 0.9186
16:15:30.896 Training @ 384 epoch...
16:15:31.475   Training iter 50, batch loss 1.5645, batch acc 0.9152
16:15:32.024   Training iter 100, batch loss 1.5639, batch acc 0.9182
16:15:32.560   Training iter 150, batch loss 1.5686, batch acc 0.9168
16:15:33.132   Training iter 200, batch loss 1.5593, batch acc 0.9250
16:15:33.669   Training iter 250, batch loss 1.5678, batch acc 0.9100
16:15:34.212   Training iter 300, batch loss 1.5622, batch acc 0.9220
16:15:34.748   Training iter 350, batch loss 1.5668, batch acc 0.9170
16:15:35.270   Training iter 400, batch loss 1.5632, batch acc 0.9174
16:15:35.847   Training iter 450, batch loss 1.5614, batch acc 0.9276
16:15:36.438   Training iter 500, batch loss 1.5615, batch acc 0.9190
16:15:37.021   Training iter 550, batch loss 1.5622, batch acc 0.9184
16:15:37.619   Training iter 600, batch loss 1.5638, batch acc 0.9188
16:15:37.621 Training @ 385 epoch...
16:15:38.223   Training iter 50, batch loss 1.5646, batch acc 0.9184
16:15:38.781   Training iter 100, batch loss 1.5680, batch acc 0.9138
16:15:39.313   Training iter 150, batch loss 1.5644, batch acc 0.9146
16:15:39.851   Training iter 200, batch loss 1.5630, batch acc 0.9200
16:15:40.387   Training iter 250, batch loss 1.5614, batch acc 0.9206
16:15:40.923   Training iter 300, batch loss 1.5653, batch acc 0.9128
16:15:41.458   Training iter 350, batch loss 1.5634, batch acc 0.9202
16:15:41.987   Training iter 400, batch loss 1.5606, batch acc 0.9244
16:15:42.519   Training iter 450, batch loss 1.5644, batch acc 0.9182
16:15:43.060   Training iter 500, batch loss 1.5646, batch acc 0.9206
16:15:43.606   Training iter 550, batch loss 1.5636, batch acc 0.9194
16:15:44.140   Training iter 600, batch loss 1.5610, batch acc 0.9226
16:15:44.142 Testing @ 385 epoch...
16:15:44.198     Testing, total mean loss 1.56502, total acc 0.91410
16:15:44.198 Training @ 386 epoch...
16:15:44.714   Training iter 50, batch loss 1.5629, batch acc 0.9186
16:15:45.205   Training iter 100, batch loss 1.5599, batch acc 0.9224
16:15:45.700   Training iter 150, batch loss 1.5653, batch acc 0.9176
16:15:46.194   Training iter 200, batch loss 1.5663, batch acc 0.9146
16:15:46.726   Training iter 250, batch loss 1.5651, batch acc 0.9148
16:15:47.241   Training iter 300, batch loss 1.5686, batch acc 0.9084
16:15:47.732   Training iter 350, batch loss 1.5617, batch acc 0.9226
16:15:48.226   Training iter 400, batch loss 1.5649, batch acc 0.9172
16:15:48.778   Training iter 450, batch loss 1.5623, batch acc 0.9224
16:15:49.441   Training iter 500, batch loss 1.5617, batch acc 0.9196
16:15:50.034   Training iter 550, batch loss 1.5659, batch acc 0.9134
16:15:50.573   Training iter 600, batch loss 1.5598, batch acc 0.9264
16:15:50.575 Training @ 387 epoch...
16:15:51.141   Training iter 50, batch loss 1.5650, batch acc 0.9190
16:15:51.698   Training iter 100, batch loss 1.5647, batch acc 0.9206
16:15:52.257   Training iter 150, batch loss 1.5614, batch acc 0.9212
16:15:52.826   Training iter 200, batch loss 1.5653, batch acc 0.9170
16:15:53.381   Training iter 250, batch loss 1.5673, batch acc 0.9124
16:15:53.946   Training iter 300, batch loss 1.5607, batch acc 0.9202
16:15:54.482   Training iter 350, batch loss 1.5634, batch acc 0.9158
16:15:54.960   Training iter 400, batch loss 1.5627, batch acc 0.9224
16:15:55.443   Training iter 450, batch loss 1.5655, batch acc 0.9198
16:15:55.933   Training iter 500, batch loss 1.5615, batch acc 0.9204
16:15:56.442   Training iter 550, batch loss 1.5645, batch acc 0.9176
16:15:56.968   Training iter 600, batch loss 1.5616, batch acc 0.9178
16:15:56.969 Training @ 388 epoch...
16:15:57.524   Training iter 50, batch loss 1.5642, batch acc 0.9180
16:15:58.066   Training iter 100, batch loss 1.5664, batch acc 0.9128
16:15:58.640   Training iter 150, batch loss 1.5656, batch acc 0.9172
16:15:59.213   Training iter 200, batch loss 1.5631, batch acc 0.9194
16:15:59.731   Training iter 250, batch loss 1.5680, batch acc 0.9096
16:16:00.248   Training iter 300, batch loss 1.5643, batch acc 0.9204
16:16:00.849   Training iter 350, batch loss 1.5637, batch acc 0.9180
16:16:01.615   Training iter 400, batch loss 1.5608, batch acc 0.9232
16:16:02.378   Training iter 450, batch loss 1.5605, batch acc 0.9196
16:16:03.143   Training iter 500, batch loss 1.5623, batch acc 0.9220
16:16:03.915   Training iter 550, batch loss 1.5631, batch acc 0.9178
16:16:04.687   Training iter 600, batch loss 1.5617, batch acc 0.9204
16:16:04.691 Training @ 389 epoch...
16:16:05.340   Training iter 50, batch loss 1.5642, batch acc 0.9194
16:16:05.913   Training iter 100, batch loss 1.5641, batch acc 0.9144
16:16:06.481   Training iter 150, batch loss 1.5602, batch acc 0.9192
16:16:07.046   Training iter 200, batch loss 1.5588, batch acc 0.9224
16:16:07.596   Training iter 250, batch loss 1.5626, batch acc 0.9190
16:16:08.110   Training iter 300, batch loss 1.5619, batch acc 0.9204
16:16:08.625   Training iter 350, batch loss 1.5661, batch acc 0.9116
16:16:09.155   Training iter 400, batch loss 1.5603, batch acc 0.9242
16:16:09.666   Training iter 450, batch loss 1.5659, batch acc 0.9170
16:16:10.181   Training iter 500, batch loss 1.5703, batch acc 0.9116
16:16:10.673   Training iter 550, batch loss 1.5620, batch acc 0.9242
16:16:11.153   Training iter 600, batch loss 1.5667, batch acc 0.9150
16:16:11.155 Training @ 390 epoch...
16:16:11.633   Training iter 50, batch loss 1.5656, batch acc 0.9172
16:16:12.107   Training iter 100, batch loss 1.5657, batch acc 0.9132
16:16:12.593   Training iter 150, batch loss 1.5608, batch acc 0.9226
16:16:13.074   Training iter 200, batch loss 1.5597, batch acc 0.9210
16:16:13.580   Training iter 250, batch loss 1.5668, batch acc 0.9148
16:16:14.122   Training iter 300, batch loss 1.5620, batch acc 0.9218
16:16:14.661   Training iter 350, batch loss 1.5661, batch acc 0.9146
16:16:15.209   Training iter 400, batch loss 1.5654, batch acc 0.9160
16:16:15.791   Training iter 450, batch loss 1.5655, batch acc 0.9144
16:16:16.338   Training iter 500, batch loss 1.5611, batch acc 0.9236
16:16:16.871   Training iter 550, batch loss 1.5604, batch acc 0.9226
16:16:17.409   Training iter 600, batch loss 1.5645, batch acc 0.9188
16:16:17.410 Testing @ 390 epoch...
16:16:17.460     Testing, total mean loss 1.56494, total acc 0.91450
16:16:17.460 Training @ 391 epoch...
16:16:17.998   Training iter 50, batch loss 1.5609, batch acc 0.9208
16:16:18.523   Training iter 100, batch loss 1.5602, batch acc 0.9228
16:16:19.058   Training iter 150, batch loss 1.5620, batch acc 0.9206
16:16:19.592   Training iter 200, batch loss 1.5635, batch acc 0.9228
16:16:20.114   Training iter 250, batch loss 1.5646, batch acc 0.9150
16:16:20.642   Training iter 300, batch loss 1.5674, batch acc 0.9122
16:16:21.173   Training iter 350, batch loss 1.5667, batch acc 0.9156
16:16:21.702   Training iter 400, batch loss 1.5640, batch acc 0.9192
16:16:22.234   Training iter 450, batch loss 1.5627, batch acc 0.9184
16:16:22.772   Training iter 500, batch loss 1.5624, batch acc 0.9226
16:16:23.313   Training iter 550, batch loss 1.5582, batch acc 0.9240
16:16:23.844   Training iter 600, batch loss 1.5705, batch acc 0.9078
16:16:23.845 Training @ 392 epoch...
16:16:24.407   Training iter 50, batch loss 1.5633, batch acc 0.9170
16:16:24.974   Training iter 100, batch loss 1.5655, batch acc 0.9136
16:16:25.565   Training iter 150, batch loss 1.5628, batch acc 0.9234
16:16:26.110   Training iter 200, batch loss 1.5661, batch acc 0.9166
16:16:26.684   Training iter 250, batch loss 1.5661, batch acc 0.9162
16:16:27.237   Training iter 300, batch loss 1.5662, batch acc 0.9132
16:16:27.769   Training iter 350, batch loss 1.5621, batch acc 0.9244
16:16:28.293   Training iter 400, batch loss 1.5606, batch acc 0.9222
16:16:28.813   Training iter 450, batch loss 1.5659, batch acc 0.9164
16:16:29.333   Training iter 500, batch loss 1.5629, batch acc 0.9162
16:16:29.856   Training iter 550, batch loss 1.5626, batch acc 0.9180
16:16:30.371   Training iter 600, batch loss 1.5584, batch acc 0.9254
16:16:30.373 Training @ 393 epoch...
16:16:30.864   Training iter 50, batch loss 1.5656, batch acc 0.9180
16:16:31.334   Training iter 100, batch loss 1.5585, batch acc 0.9252
16:16:31.805   Training iter 150, batch loss 1.5630, batch acc 0.9238
16:16:32.290   Training iter 200, batch loss 1.5710, batch acc 0.9122
16:16:32.772   Training iter 250, batch loss 1.5628, batch acc 0.9158
16:16:33.266   Training iter 300, batch loss 1.5606, batch acc 0.9202
16:16:33.767   Training iter 350, batch loss 1.5642, batch acc 0.9180
16:16:34.268   Training iter 400, batch loss 1.5630, batch acc 0.9168
16:16:34.804   Training iter 450, batch loss 1.5613, batch acc 0.9232
16:16:35.318   Training iter 500, batch loss 1.5624, batch acc 0.9180
16:16:35.833   Training iter 550, batch loss 1.5613, batch acc 0.9200
16:16:36.392   Training iter 600, batch loss 1.5683, batch acc 0.9110
16:16:36.394 Training @ 394 epoch...
16:16:36.928   Training iter 50, batch loss 1.5654, batch acc 0.9134
16:16:37.419   Training iter 100, batch loss 1.5580, batch acc 0.9246
16:16:37.927   Training iter 150, batch loss 1.5652, batch acc 0.9166
16:16:38.457   Training iter 200, batch loss 1.5649, batch acc 0.9184
16:16:38.971   Training iter 250, batch loss 1.5640, batch acc 0.9182
16:16:39.481   Training iter 300, batch loss 1.5598, batch acc 0.9254
16:16:39.983   Training iter 350, batch loss 1.5636, batch acc 0.9170
16:16:40.526   Training iter 400, batch loss 1.5623, batch acc 0.9190
16:16:41.052   Training iter 450, batch loss 1.5641, batch acc 0.9172
16:16:41.577   Training iter 500, batch loss 1.5665, batch acc 0.9190
16:16:42.075   Training iter 550, batch loss 1.5644, batch acc 0.9172
16:16:42.573   Training iter 600, batch loss 1.5637, batch acc 0.9190
16:16:42.574 Training @ 395 epoch...
16:16:43.081   Training iter 50, batch loss 1.5592, batch acc 0.9232
16:16:43.572   Training iter 100, batch loss 1.5667, batch acc 0.9130
16:16:44.068   Training iter 150, batch loss 1.5667, batch acc 0.9138
16:16:44.548   Training iter 200, batch loss 1.5628, batch acc 0.9198
16:16:45.033   Training iter 250, batch loss 1.5594, batch acc 0.9266
16:16:45.587   Training iter 300, batch loss 1.5679, batch acc 0.9142
16:16:46.148   Training iter 350, batch loss 1.5638, batch acc 0.9162
16:16:46.719   Training iter 400, batch loss 1.5626, batch acc 0.9224
16:16:47.241   Training iter 450, batch loss 1.5628, batch acc 0.9172
16:16:47.719   Training iter 500, batch loss 1.5649, batch acc 0.9142
16:16:48.205   Training iter 550, batch loss 1.5634, batch acc 0.9194
16:16:48.716   Training iter 600, batch loss 1.5614, batch acc 0.9194
16:16:48.718 Testing @ 395 epoch...
16:16:48.766     Testing, total mean loss 1.56494, total acc 0.91450
16:16:48.766 Training @ 396 epoch...
16:16:49.289   Training iter 50, batch loss 1.5669, batch acc 0.9158
16:16:49.806   Training iter 100, batch loss 1.5629, batch acc 0.9184
16:16:50.303   Training iter 150, batch loss 1.5630, batch acc 0.9186
16:16:50.797   Training iter 200, batch loss 1.5665, batch acc 0.9128
16:16:51.293   Training iter 250, batch loss 1.5609, batch acc 0.9230
16:16:51.794   Training iter 300, batch loss 1.5611, batch acc 0.9222
16:16:52.338   Training iter 350, batch loss 1.5610, batch acc 0.9210
16:16:52.891   Training iter 400, batch loss 1.5636, batch acc 0.9162
16:16:53.428   Training iter 450, batch loss 1.5674, batch acc 0.9116
16:16:53.960   Training iter 500, batch loss 1.5637, batch acc 0.9150
16:16:54.495   Training iter 550, batch loss 1.5626, batch acc 0.9258
16:16:55.075   Training iter 600, batch loss 1.5619, batch acc 0.9208
16:16:55.076 Training @ 397 epoch...
16:16:55.715   Training iter 50, batch loss 1.5632, batch acc 0.9172
16:16:56.268   Training iter 100, batch loss 1.5665, batch acc 0.9162
16:16:56.901   Training iter 150, batch loss 1.5580, batch acc 0.9258
16:16:57.514   Training iter 200, batch loss 1.5652, batch acc 0.9174
16:16:58.136   Training iter 250, batch loss 1.5653, batch acc 0.9146
16:16:58.638   Training iter 300, batch loss 1.5635, batch acc 0.9202
16:16:59.142   Training iter 350, batch loss 1.5621, batch acc 0.9220
16:16:59.655   Training iter 400, batch loss 1.5656, batch acc 0.9162
16:17:00.161   Training iter 450, batch loss 1.5628, batch acc 0.9194
16:17:00.682   Training iter 500, batch loss 1.5628, batch acc 0.9174
16:17:01.209   Training iter 550, batch loss 1.5618, batch acc 0.9198
16:17:01.790   Training iter 600, batch loss 1.5644, batch acc 0.9166
16:17:01.792 Training @ 398 epoch...
16:17:02.371   Training iter 50, batch loss 1.5637, batch acc 0.9200
16:17:02.926   Training iter 100, batch loss 1.5620, batch acc 0.9230
16:17:03.472   Training iter 150, batch loss 1.5690, batch acc 0.9142
16:17:04.025   Training iter 200, batch loss 1.5639, batch acc 0.9160
16:17:04.562   Training iter 250, batch loss 1.5612, batch acc 0.9198
16:17:05.087   Training iter 300, batch loss 1.5666, batch acc 0.9172
16:17:05.616   Training iter 350, batch loss 1.5620, batch acc 0.9202
16:17:06.177   Training iter 400, batch loss 1.5645, batch acc 0.9162
16:17:06.722   Training iter 450, batch loss 1.5657, batch acc 0.9168
16:17:07.254   Training iter 500, batch loss 1.5606, batch acc 0.9216
16:17:07.772   Training iter 550, batch loss 1.5640, batch acc 0.9170
16:17:08.280   Training iter 600, batch loss 1.5582, batch acc 0.9248
16:17:08.281 Training @ 399 epoch...
16:17:08.802   Training iter 50, batch loss 1.5643, batch acc 0.9172
16:17:09.329   Training iter 100, batch loss 1.5626, batch acc 0.9202
16:17:09.849   Training iter 150, batch loss 1.5620, batch acc 0.9220
16:17:10.386   Training iter 200, batch loss 1.5601, batch acc 0.9230
16:17:10.909   Training iter 250, batch loss 1.5630, batch acc 0.9218
16:17:11.440   Training iter 300, batch loss 1.5637, batch acc 0.9182
16:17:11.964   Training iter 350, batch loss 1.5647, batch acc 0.9116
16:17:12.489   Training iter 400, batch loss 1.5608, batch acc 0.9230
16:17:13.010   Training iter 450, batch loss 1.5644, batch acc 0.9164
16:17:13.531   Training iter 500, batch loss 1.5659, batch acc 0.9160
16:17:14.031   Training iter 550, batch loss 1.5656, batch acc 0.9154
16:17:14.553   Training iter 600, batch loss 1.5637, batch acc 0.9172
16:17:14.554 Training @ 400 epoch...
16:17:15.057   Training iter 50, batch loss 1.5656, batch acc 0.9166
16:17:15.554   Training iter 100, batch loss 1.5647, batch acc 0.9142
16:17:16.073   Training iter 150, batch loss 1.5640, batch acc 0.9224
16:17:16.590   Training iter 200, batch loss 1.5607, batch acc 0.9232
16:17:17.113   Training iter 250, batch loss 1.5655, batch acc 0.9160
16:17:17.628   Training iter 300, batch loss 1.5556, batch acc 0.9248
16:17:18.150   Training iter 350, batch loss 1.5664, batch acc 0.9170
16:17:18.664   Training iter 400, batch loss 1.5640, batch acc 0.9182
16:17:19.183   Training iter 450, batch loss 1.5670, batch acc 0.9160
16:17:19.688   Training iter 500, batch loss 1.5637, batch acc 0.9172
16:17:20.210   Training iter 550, batch loss 1.5626, batch acc 0.9176
16:17:20.733   Training iter 600, batch loss 1.5608, batch acc 0.9194
16:17:20.734 Testing @ 400 epoch...
16:17:20.783     Testing, total mean loss 1.56456, total acc 0.91360
16:17:20.783 Plot @ 400 epoch...
16:17:20.783 Training @ 401 epoch...
16:17:21.306   Training iter 50, batch loss 1.5633, batch acc 0.9186
16:17:21.820   Training iter 100, batch loss 1.5636, batch acc 0.9180
16:17:22.332   Training iter 150, batch loss 1.5682, batch acc 0.9130
16:17:22.852   Training iter 200, batch loss 1.5643, batch acc 0.9178
16:17:23.380   Training iter 250, batch loss 1.5630, batch acc 0.9214
16:17:23.933   Training iter 300, batch loss 1.5600, batch acc 0.9206
16:17:24.544   Training iter 350, batch loss 1.5653, batch acc 0.9170
16:17:25.141   Training iter 400, batch loss 1.5626, batch acc 0.9160
16:17:25.728   Training iter 450, batch loss 1.5625, batch acc 0.9214
16:17:26.260   Training iter 500, batch loss 1.5618, batch acc 0.9212
16:17:26.791   Training iter 550, batch loss 1.5622, batch acc 0.9182
16:17:27.335   Training iter 600, batch loss 1.5632, batch acc 0.9246
16:17:27.336 Training @ 402 epoch...
16:17:27.873   Training iter 50, batch loss 1.5591, batch acc 0.9274
16:17:28.401   Training iter 100, batch loss 1.5646, batch acc 0.9170
16:17:29.026   Training iter 150, batch loss 1.5614, batch acc 0.9228
16:17:29.557   Training iter 200, batch loss 1.5608, batch acc 0.9234
16:17:30.052   Training iter 250, batch loss 1.5620, batch acc 0.9162
16:17:30.555   Training iter 300, batch loss 1.5581, batch acc 0.9270
16:17:31.062   Training iter 350, batch loss 1.5644, batch acc 0.9170
16:17:31.572   Training iter 400, batch loss 1.5679, batch acc 0.9122
16:17:32.090   Training iter 450, batch loss 1.5624, batch acc 0.9206
16:17:32.597   Training iter 500, batch loss 1.5665, batch acc 0.9146
16:17:33.115   Training iter 550, batch loss 1.5677, batch acc 0.9112
16:17:33.631   Training iter 600, batch loss 1.5650, batch acc 0.9146
16:17:33.633 Training @ 403 epoch...
16:17:34.150   Training iter 50, batch loss 1.5655, batch acc 0.9148
16:17:34.666   Training iter 100, batch loss 1.5619, batch acc 0.9182
16:17:35.166   Training iter 150, batch loss 1.5611, batch acc 0.9214
16:17:35.676   Training iter 200, batch loss 1.5627, batch acc 0.9208
16:17:36.177   Training iter 250, batch loss 1.5621, batch acc 0.9194
16:17:36.668   Training iter 300, batch loss 1.5652, batch acc 0.9166
16:17:37.161   Training iter 350, batch loss 1.5610, batch acc 0.9250
16:17:37.668   Training iter 400, batch loss 1.5618, batch acc 0.9228
16:17:38.194   Training iter 450, batch loss 1.5620, batch acc 0.9204
16:17:38.723   Training iter 500, batch loss 1.5644, batch acc 0.9182
16:17:39.249   Training iter 550, batch loss 1.5662, batch acc 0.9158
16:17:39.795   Training iter 600, batch loss 1.5657, batch acc 0.9146
16:17:39.796 Training @ 404 epoch...
16:17:40.369   Training iter 50, batch loss 1.5630, batch acc 0.9172
16:17:40.937   Training iter 100, batch loss 1.5592, batch acc 0.9262
16:17:41.501   Training iter 150, batch loss 1.5622, batch acc 0.9206
16:17:42.069   Training iter 200, batch loss 1.5617, batch acc 0.9214
16:17:42.657   Training iter 250, batch loss 1.5614, batch acc 0.9178
16:17:43.249   Training iter 300, batch loss 1.5610, batch acc 0.9252
16:17:43.825   Training iter 350, batch loss 1.5628, batch acc 0.9236
16:17:44.396   Training iter 400, batch loss 1.5585, batch acc 0.9230
16:17:44.972   Training iter 450, batch loss 1.5685, batch acc 0.9106
16:17:45.540   Training iter 500, batch loss 1.5637, batch acc 0.9148
16:17:46.114   Training iter 550, batch loss 1.5685, batch acc 0.9154
16:17:46.660   Training iter 600, batch loss 1.5692, batch acc 0.9120
16:17:46.661 Training @ 405 epoch...
16:17:47.205   Training iter 50, batch loss 1.5614, batch acc 0.9202
16:17:47.746   Training iter 100, batch loss 1.5585, batch acc 0.9252
16:17:48.278   Training iter 150, batch loss 1.5595, batch acc 0.9242
16:17:48.810   Training iter 200, batch loss 1.5651, batch acc 0.9174
16:17:49.356   Training iter 250, batch loss 1.5641, batch acc 0.9144
16:17:49.894   Training iter 300, batch loss 1.5616, batch acc 0.9214
16:17:50.431   Training iter 350, batch loss 1.5671, batch acc 0.9122
16:17:50.975   Training iter 400, batch loss 1.5643, batch acc 0.9152
16:17:51.527   Training iter 450, batch loss 1.5639, batch acc 0.9212
16:17:52.070   Training iter 500, batch loss 1.5685, batch acc 0.9106
16:17:52.627   Training iter 550, batch loss 1.5596, batch acc 0.9260
16:17:53.175   Training iter 600, batch loss 1.5656, batch acc 0.9194
16:17:53.177 Testing @ 405 epoch...
16:17:53.226     Testing, total mean loss 1.56444, total acc 0.91460
16:17:53.226 Training @ 406 epoch...
16:17:53.768   Training iter 50, batch loss 1.5654, batch acc 0.9172
16:17:54.321   Training iter 100, batch loss 1.5674, batch acc 0.9140
16:17:54.866   Training iter 150, batch loss 1.5595, batch acc 0.9254
16:17:55.416   Training iter 200, batch loss 1.5640, batch acc 0.9182
16:17:55.973   Training iter 250, batch loss 1.5627, batch acc 0.9230
16:17:56.546   Training iter 300, batch loss 1.5666, batch acc 0.9122
16:17:57.116   Training iter 350, batch loss 1.5604, batch acc 0.9234
16:17:57.688   Training iter 400, batch loss 1.5634, batch acc 0.9164
16:17:58.260   Training iter 450, batch loss 1.5599, batch acc 0.9240
16:17:58.828   Training iter 500, batch loss 1.5662, batch acc 0.9154
16:17:59.404   Training iter 550, batch loss 1.5610, batch acc 0.9176
16:17:59.965   Training iter 600, batch loss 1.5624, batch acc 0.9220
16:17:59.966 Training @ 407 epoch...
16:18:00.547   Training iter 50, batch loss 1.5621, batch acc 0.9198
16:18:01.103   Training iter 100, batch loss 1.5599, batch acc 0.9224
16:18:01.669   Training iter 150, batch loss 1.5635, batch acc 0.9192
16:18:02.238   Training iter 200, batch loss 1.5665, batch acc 0.9134
16:18:02.807   Training iter 250, batch loss 1.5658, batch acc 0.9170
16:18:03.352   Training iter 300, batch loss 1.5640, batch acc 0.9182
16:18:03.910   Training iter 350, batch loss 1.5616, batch acc 0.9248
16:18:04.475   Training iter 400, batch loss 1.5633, batch acc 0.9162
16:18:05.043   Training iter 450, batch loss 1.5627, batch acc 0.9180
16:18:05.598   Training iter 500, batch loss 1.5628, batch acc 0.9208
16:18:06.155   Training iter 550, batch loss 1.5638, batch acc 0.9166
16:18:06.706   Training iter 600, batch loss 1.5627, batch acc 0.9234
16:18:06.707 Training @ 408 epoch...
16:18:07.284   Training iter 50, batch loss 1.5591, batch acc 0.9254
16:18:07.839   Training iter 100, batch loss 1.5606, batch acc 0.9246
16:18:08.370   Training iter 150, batch loss 1.5638, batch acc 0.9180
16:18:08.895   Training iter 200, batch loss 1.5668, batch acc 0.9150
16:18:09.439   Training iter 250, batch loss 1.5657, batch acc 0.9170
16:18:09.970   Training iter 300, batch loss 1.5643, batch acc 0.9154
16:18:10.494   Training iter 350, batch loss 1.5630, batch acc 0.9162
16:18:11.027   Training iter 400, batch loss 1.5654, batch acc 0.9142
16:18:11.560   Training iter 450, batch loss 1.5613, batch acc 0.9214
16:18:12.115   Training iter 500, batch loss 1.5622, batch acc 0.9188
16:18:12.689   Training iter 550, batch loss 1.5617, batch acc 0.9240
16:18:13.347   Training iter 600, batch loss 1.5645, batch acc 0.9190
16:18:13.350 Training @ 409 epoch...
16:18:13.999   Training iter 50, batch loss 1.5634, batch acc 0.9176
16:18:14.593   Training iter 100, batch loss 1.5637, batch acc 0.9182
16:18:15.197   Training iter 150, batch loss 1.5671, batch acc 0.9154
16:18:15.809   Training iter 200, batch loss 1.5588, batch acc 0.9250
16:18:16.401   Training iter 250, batch loss 1.5611, batch acc 0.9202
16:18:16.978   Training iter 300, batch loss 1.5609, batch acc 0.9222
16:18:17.550   Training iter 350, batch loss 1.5639, batch acc 0.9172
16:18:18.096   Training iter 400, batch loss 1.5641, batch acc 0.9186
16:18:18.627   Training iter 450, batch loss 1.5655, batch acc 0.9134
16:18:19.165   Training iter 500, batch loss 1.5640, batch acc 0.9166
16:18:19.697   Training iter 550, batch loss 1.5638, batch acc 0.9176
16:18:20.221   Training iter 600, batch loss 1.5617, batch acc 0.9238
16:18:20.222 Training @ 410 epoch...
16:18:20.751   Training iter 50, batch loss 1.5602, batch acc 0.9242
16:18:21.284   Training iter 100, batch loss 1.5620, batch acc 0.9204
16:18:21.826   Training iter 150, batch loss 1.5597, batch acc 0.9276
16:18:22.360   Training iter 200, batch loss 1.5640, batch acc 0.9198
16:18:22.901   Training iter 250, batch loss 1.5617, batch acc 0.9190
16:18:23.442   Training iter 300, batch loss 1.5682, batch acc 0.9120
16:18:23.993   Training iter 350, batch loss 1.5644, batch acc 0.9192
16:18:24.556   Training iter 400, batch loss 1.5659, batch acc 0.9146
16:18:25.115   Training iter 450, batch loss 1.5621, batch acc 0.9178
16:18:25.672   Training iter 500, batch loss 1.5656, batch acc 0.9166
16:18:26.235   Training iter 550, batch loss 1.5670, batch acc 0.9094
16:18:26.782   Training iter 600, batch loss 1.5570, batch acc 0.9252
16:18:26.784 Testing @ 410 epoch...
16:18:26.836     Testing, total mean loss 1.56452, total acc 0.91420
16:18:26.836 Training @ 411 epoch...
16:18:27.416   Training iter 50, batch loss 1.5656, batch acc 0.9160
16:18:27.992   Training iter 100, batch loss 1.5654, batch acc 0.9180
16:18:28.582   Training iter 150, batch loss 1.5599, batch acc 0.9208
16:18:29.172   Training iter 200, batch loss 1.5644, batch acc 0.9172
16:18:29.747   Training iter 250, batch loss 1.5625, batch acc 0.9186
16:18:30.326   Training iter 300, batch loss 1.5605, batch acc 0.9218
16:18:30.904   Training iter 350, batch loss 1.5558, batch acc 0.9280
16:18:31.493   Training iter 400, batch loss 1.5660, batch acc 0.9154
16:18:32.065   Training iter 450, batch loss 1.5641, batch acc 0.9190
16:18:32.645   Training iter 500, batch loss 1.5644, batch acc 0.9186
16:18:33.234   Training iter 550, batch loss 1.5636, batch acc 0.9168
16:18:33.825   Training iter 600, batch loss 1.5653, batch acc 0.9174
16:18:33.827 Training @ 412 epoch...
16:18:34.389   Training iter 50, batch loss 1.5607, batch acc 0.9214
16:18:34.923   Training iter 100, batch loss 1.5659, batch acc 0.9142
16:18:35.450   Training iter 150, batch loss 1.5620, batch acc 0.9226
16:18:35.976   Training iter 200, batch loss 1.5678, batch acc 0.9140
16:18:36.504   Training iter 250, batch loss 1.5601, batch acc 0.9238
16:18:37.028   Training iter 300, batch loss 1.5659, batch acc 0.9122
16:18:37.546   Training iter 350, batch loss 1.5605, batch acc 0.9234
16:18:38.067   Training iter 400, batch loss 1.5649, batch acc 0.9152
16:18:38.594   Training iter 450, batch loss 1.5610, batch acc 0.9232
16:18:39.113   Training iter 500, batch loss 1.5600, batch acc 0.9230
16:18:39.631   Training iter 550, batch loss 1.5651, batch acc 0.9160
16:18:40.148   Training iter 600, batch loss 1.5632, batch acc 0.9172
16:18:40.150 Training @ 413 epoch...
16:18:40.671   Training iter 50, batch loss 1.5619, batch acc 0.9236
16:18:41.209   Training iter 100, batch loss 1.5636, batch acc 0.9204
16:18:41.741   Training iter 150, batch loss 1.5653, batch acc 0.9134
16:18:42.267   Training iter 200, batch loss 1.5586, batch acc 0.9258
16:18:42.825   Training iter 250, batch loss 1.5653, batch acc 0.9164
16:18:43.375   Training iter 300, batch loss 1.5596, batch acc 0.9254
16:18:43.918   Training iter 350, batch loss 1.5601, batch acc 0.9232
16:18:44.492   Training iter 400, batch loss 1.5632, batch acc 0.9180
16:18:45.060   Training iter 450, batch loss 1.5649, batch acc 0.9184
16:18:45.639   Training iter 500, batch loss 1.5663, batch acc 0.9158
16:18:46.226   Training iter 550, batch loss 1.5675, batch acc 0.9104
16:18:46.799   Training iter 600, batch loss 1.5608, batch acc 0.9212
16:18:46.801 Training @ 414 epoch...
16:18:47.380   Training iter 50, batch loss 1.5625, batch acc 0.9222
16:18:47.944   Training iter 100, batch loss 1.5594, batch acc 0.9266
16:18:48.523   Training iter 150, batch loss 1.5639, batch acc 0.9174
16:18:49.092   Training iter 200, batch loss 1.5620, batch acc 0.9206
16:18:49.667   Training iter 250, batch loss 1.5613, batch acc 0.9204
16:18:50.193   Training iter 300, batch loss 1.5601, batch acc 0.9238
16:18:50.723   Training iter 350, batch loss 1.5636, batch acc 0.9176
16:18:51.233   Training iter 400, batch loss 1.5619, batch acc 0.9202
16:18:51.787   Training iter 450, batch loss 1.5626, batch acc 0.9186
16:18:52.317   Training iter 500, batch loss 1.5624, batch acc 0.9192
16:18:52.855   Training iter 550, batch loss 1.5698, batch acc 0.9112
16:18:53.399   Training iter 600, batch loss 1.5668, batch acc 0.9128
16:18:53.400 Training @ 415 epoch...
16:18:53.950   Training iter 50, batch loss 1.5592, batch acc 0.9238
16:18:54.503   Training iter 100, batch loss 1.5646, batch acc 0.9170
16:18:55.041   Training iter 150, batch loss 1.5656, batch acc 0.9156
16:18:55.584   Training iter 200, batch loss 1.5637, batch acc 0.9200
16:18:56.125   Training iter 250, batch loss 1.5633, batch acc 0.9198
16:18:56.668   Training iter 300, batch loss 1.5622, batch acc 0.9208
16:18:57.209   Training iter 350, batch loss 1.5640, batch acc 0.9154
16:18:57.751   Training iter 400, batch loss 1.5605, batch acc 0.9210
16:18:58.276   Training iter 450, batch loss 1.5610, batch acc 0.9220
16:18:58.769   Training iter 500, batch loss 1.5623, batch acc 0.9218
16:18:59.275   Training iter 550, batch loss 1.5670, batch acc 0.9146
16:18:59.764   Training iter 600, batch loss 1.5634, batch acc 0.9186
16:18:59.766 Testing @ 415 epoch...
16:18:59.815     Testing, total mean loss 1.56423, total acc 0.91480
16:18:59.815 Training @ 416 epoch...
16:19:00.310   Training iter 50, batch loss 1.5634, batch acc 0.9210
16:19:00.806   Training iter 100, batch loss 1.5686, batch acc 0.9142
16:19:01.351   Training iter 150, batch loss 1.5655, batch acc 0.9140
16:19:01.924   Training iter 200, batch loss 1.5613, batch acc 0.9190
16:19:02.504   Training iter 250, batch loss 1.5598, batch acc 0.9224
16:19:03.079   Training iter 300, batch loss 1.5595, batch acc 0.9236
16:19:03.661   Training iter 350, batch loss 1.5610, batch acc 0.9214
16:19:04.230   Training iter 400, batch loss 1.5565, batch acc 0.9316
16:19:04.787   Training iter 450, batch loss 1.5664, batch acc 0.9154
16:19:05.339   Training iter 500, batch loss 1.5654, batch acc 0.9156
16:19:05.879   Training iter 550, batch loss 1.5653, batch acc 0.9134
16:19:06.410   Training iter 600, batch loss 1.5631, batch acc 0.9180
16:19:06.412 Training @ 417 epoch...
16:19:06.944   Training iter 50, batch loss 1.5597, batch acc 0.9228
16:19:07.478   Training iter 100, batch loss 1.5586, batch acc 0.9278
16:19:08.033   Training iter 150, batch loss 1.5664, batch acc 0.9134
16:19:08.574   Training iter 200, batch loss 1.5664, batch acc 0.9162
16:19:09.129   Training iter 250, batch loss 1.5641, batch acc 0.9198
16:19:09.690   Training iter 300, batch loss 1.5630, batch acc 0.9202
16:19:10.248   Training iter 350, batch loss 1.5640, batch acc 0.9160
16:19:10.770   Training iter 400, batch loss 1.5679, batch acc 0.9090
16:19:11.286   Training iter 450, batch loss 1.5646, batch acc 0.9180
16:19:11.804   Training iter 500, batch loss 1.5613, batch acc 0.9194
16:19:12.365   Training iter 550, batch loss 1.5609, batch acc 0.9204
16:19:12.918   Training iter 600, batch loss 1.5590, batch acc 0.9242
16:19:12.920 Training @ 418 epoch...
16:19:13.475   Training iter 50, batch loss 1.5655, batch acc 0.9184
16:19:14.037   Training iter 100, batch loss 1.5582, batch acc 0.9236
16:19:14.592   Training iter 150, batch loss 1.5646, batch acc 0.9174
16:19:15.136   Training iter 200, batch loss 1.5604, batch acc 0.9238
16:19:15.696   Training iter 250, batch loss 1.5626, batch acc 0.9162
16:19:16.269   Training iter 300, batch loss 1.5613, batch acc 0.9212
16:19:16.833   Training iter 350, batch loss 1.5631, batch acc 0.9178
16:19:17.402   Training iter 400, batch loss 1.5657, batch acc 0.9186
16:19:17.959   Training iter 450, batch loss 1.5600, batch acc 0.9236
16:19:18.527   Training iter 500, batch loss 1.5665, batch acc 0.9146
16:19:19.079   Training iter 550, batch loss 1.5622, batch acc 0.9222
16:19:19.635   Training iter 600, batch loss 1.5657, batch acc 0.9192
16:19:19.637 Training @ 419 epoch...
16:19:20.209   Training iter 50, batch loss 1.5586, batch acc 0.9280
16:19:20.781   Training iter 100, batch loss 1.5592, batch acc 0.9228
16:19:21.368   Training iter 150, batch loss 1.5660, batch acc 0.9138
16:19:21.910   Training iter 200, batch loss 1.5645, batch acc 0.9204
16:19:22.427   Training iter 250, batch loss 1.5625, batch acc 0.9188
16:19:22.933   Training iter 300, batch loss 1.5634, batch acc 0.9170
16:19:23.452   Training iter 350, batch loss 1.5681, batch acc 0.9128
16:19:23.980   Training iter 400, batch loss 1.5618, batch acc 0.9174
16:19:24.505   Training iter 450, batch loss 1.5624, batch acc 0.9206
16:19:25.029   Training iter 500, batch loss 1.5640, batch acc 0.9164
16:19:25.536   Training iter 550, batch loss 1.5654, batch acc 0.9144
16:19:26.059   Training iter 600, batch loss 1.5596, batch acc 0.9250
16:19:26.061 Training @ 420 epoch...
16:19:26.586   Training iter 50, batch loss 1.5625, batch acc 0.9174
16:19:27.119   Training iter 100, batch loss 1.5622, batch acc 0.9192
16:19:27.644   Training iter 150, batch loss 1.5622, batch acc 0.9236
16:19:28.167   Training iter 200, batch loss 1.5627, batch acc 0.9218
16:19:28.693   Training iter 250, batch loss 1.5609, batch acc 0.9224
16:19:29.231   Training iter 300, batch loss 1.5649, batch acc 0.9150
16:19:29.725   Training iter 350, batch loss 1.5632, batch acc 0.9166
16:19:30.199   Training iter 400, batch loss 1.5633, batch acc 0.9168
16:19:30.678   Training iter 450, batch loss 1.5638, batch acc 0.9180
16:19:31.173   Training iter 500, batch loss 1.5601, batch acc 0.9236
16:19:31.684   Training iter 550, batch loss 1.5666, batch acc 0.9170
16:19:32.194   Training iter 600, batch loss 1.5627, batch acc 0.9186
16:19:32.195 Testing @ 420 epoch...
16:19:32.244     Testing, total mean loss 1.56423, total acc 0.91490
16:19:32.244 Training @ 421 epoch...
16:19:32.776   Training iter 50, batch loss 1.5621, batch acc 0.9214
16:19:33.318   Training iter 100, batch loss 1.5667, batch acc 0.9156
16:19:33.889   Training iter 150, batch loss 1.5632, batch acc 0.9182
16:19:34.464   Training iter 200, batch loss 1.5694, batch acc 0.9110
16:19:34.998   Training iter 250, batch loss 1.5648, batch acc 0.9140
16:19:35.533   Training iter 300, batch loss 1.5665, batch acc 0.9158
16:19:36.065   Training iter 350, batch loss 1.5641, batch acc 0.9172
16:19:36.627   Training iter 400, batch loss 1.5574, batch acc 0.9276
16:19:37.201   Training iter 450, batch loss 1.5578, batch acc 0.9256
16:19:37.778   Training iter 500, batch loss 1.5628, batch acc 0.9230
16:19:38.355   Training iter 550, batch loss 1.5610, batch acc 0.9230
16:19:38.908   Training iter 600, batch loss 1.5590, batch acc 0.9242
16:19:38.910 Training @ 422 epoch...
16:19:39.459   Training iter 50, batch loss 1.5599, batch acc 0.9238
16:19:40.003   Training iter 100, batch loss 1.5648, batch acc 0.9198
16:19:40.559   Training iter 150, batch loss 1.5638, batch acc 0.9154
16:19:41.118   Training iter 200, batch loss 1.5608, batch acc 0.9208
16:19:41.682   Training iter 250, batch loss 1.5614, batch acc 0.9224
16:19:42.246   Training iter 300, batch loss 1.5625, batch acc 0.9230
16:19:42.783   Training iter 350, batch loss 1.5625, batch acc 0.9198
16:19:43.325   Training iter 400, batch loss 1.5625, batch acc 0.9198
16:19:43.875   Training iter 450, batch loss 1.5634, batch acc 0.9184
16:19:44.422   Training iter 500, batch loss 1.5629, batch acc 0.9182
16:19:44.964   Training iter 550, batch loss 1.5650, batch acc 0.9166
16:19:45.503   Training iter 600, batch loss 1.5651, batch acc 0.9162
16:19:45.505 Training @ 423 epoch...
16:19:46.064   Training iter 50, batch loss 1.5580, batch acc 0.9248
16:19:46.622   Training iter 100, batch loss 1.5630, batch acc 0.9210
16:19:47.177   Training iter 150, batch loss 1.5629, batch acc 0.9206
16:19:47.692   Training iter 200, batch loss 1.5685, batch acc 0.9140
16:19:48.221   Training iter 250, batch loss 1.5628, batch acc 0.9182
16:19:48.730   Training iter 300, batch loss 1.5662, batch acc 0.9132
16:19:49.223   Training iter 350, batch loss 1.5649, batch acc 0.9156
16:19:49.713   Training iter 400, batch loss 1.5656, batch acc 0.9158
16:19:50.216   Training iter 450, batch loss 1.5626, batch acc 0.9188
16:19:50.748   Training iter 500, batch loss 1.5582, batch acc 0.9250
16:19:51.289   Training iter 550, batch loss 1.5605, batch acc 0.9230
16:19:51.833   Training iter 600, batch loss 1.5613, batch acc 0.9238
16:19:51.835 Training @ 424 epoch...
16:19:52.379   Training iter 50, batch loss 1.5604, batch acc 0.9202
16:19:52.887   Training iter 100, batch loss 1.5649, batch acc 0.9152
16:19:53.391   Training iter 150, batch loss 1.5648, batch acc 0.9158
16:19:53.935   Training iter 200, batch loss 1.5610, batch acc 0.9218
16:19:54.463   Training iter 250, batch loss 1.5629, batch acc 0.9210
16:19:54.960   Training iter 300, batch loss 1.5695, batch acc 0.9102
16:19:55.455   Training iter 350, batch loss 1.5642, batch acc 0.9180
16:19:55.917   Training iter 400, batch loss 1.5575, batch acc 0.9270
16:19:56.397   Training iter 450, batch loss 1.5635, batch acc 0.9206
16:19:56.889   Training iter 500, batch loss 1.5622, batch acc 0.9194
16:19:57.381   Training iter 550, batch loss 1.5605, batch acc 0.9224
16:19:57.865   Training iter 600, batch loss 1.5624, batch acc 0.9226
16:19:57.866 Training @ 425 epoch...
16:19:58.359   Training iter 50, batch loss 1.5624, batch acc 0.9200
16:19:58.870   Training iter 100, batch loss 1.5660, batch acc 0.9104
16:19:59.373   Training iter 150, batch loss 1.5632, batch acc 0.9228
16:19:59.888   Training iter 200, batch loss 1.5599, batch acc 0.9212
16:20:00.403   Training iter 250, batch loss 1.5603, batch acc 0.9200
16:20:00.911   Training iter 300, batch loss 1.5675, batch acc 0.9148
16:20:01.484   Training iter 350, batch loss 1.5617, batch acc 0.9246
16:20:02.080   Training iter 400, batch loss 1.5645, batch acc 0.9218
16:20:02.651   Training iter 450, batch loss 1.5640, batch acc 0.9172
16:20:03.225   Training iter 500, batch loss 1.5608, batch acc 0.9240
16:20:03.813   Training iter 550, batch loss 1.5616, batch acc 0.9204
16:20:04.387   Training iter 600, batch loss 1.5625, batch acc 0.9184
16:20:04.389 Testing @ 425 epoch...
16:20:04.439     Testing, total mean loss 1.56405, total acc 0.91630
16:20:04.439 Training @ 426 epoch...
16:20:05.047   Training iter 50, batch loss 1.5595, batch acc 0.9234
16:20:05.640   Training iter 100, batch loss 1.5630, batch acc 0.9190
16:20:06.223   Training iter 150, batch loss 1.5653, batch acc 0.9186
16:20:06.784   Training iter 200, batch loss 1.5657, batch acc 0.9176
16:20:07.340   Training iter 250, batch loss 1.5578, batch acc 0.9276
16:20:07.893   Training iter 300, batch loss 1.5647, batch acc 0.9146
16:20:08.449   Training iter 350, batch loss 1.5609, batch acc 0.9222
16:20:09.025   Training iter 400, batch loss 1.5636, batch acc 0.9168
16:20:09.578   Training iter 450, batch loss 1.5646, batch acc 0.9164
16:20:10.129   Training iter 500, batch loss 1.5633, batch acc 0.9160
16:20:10.674   Training iter 550, batch loss 1.5623, batch acc 0.9184
16:20:11.199   Training iter 600, batch loss 1.5629, batch acc 0.9198
16:20:11.201 Training @ 427 epoch...
16:20:11.737   Training iter 50, batch loss 1.5667, batch acc 0.9168
16:20:12.254   Training iter 100, batch loss 1.5624, batch acc 0.9222
16:20:12.784   Training iter 150, batch loss 1.5591, batch acc 0.9218
16:20:13.316   Training iter 200, batch loss 1.5651, batch acc 0.9142
16:20:13.856   Training iter 250, batch loss 1.5652, batch acc 0.9170
16:20:14.385   Training iter 300, batch loss 1.5600, batch acc 0.9238
16:20:14.898   Training iter 350, batch loss 1.5629, batch acc 0.9204
16:20:15.429   Training iter 400, batch loss 1.5637, batch acc 0.9172
16:20:15.963   Training iter 450, batch loss 1.5651, batch acc 0.9156
16:20:16.482   Training iter 500, batch loss 1.5567, batch acc 0.9284
16:20:16.981   Training iter 550, batch loss 1.5620, batch acc 0.9182
16:20:17.495   Training iter 600, batch loss 1.5646, batch acc 0.9172
16:20:17.497 Training @ 428 epoch...
16:20:18.021   Training iter 50, batch loss 1.5668, batch acc 0.9174
16:20:18.521   Training iter 100, batch loss 1.5584, batch acc 0.9230
16:20:19.041   Training iter 150, batch loss 1.5640, batch acc 0.9174
16:20:19.551   Training iter 200, batch loss 1.5599, batch acc 0.9214
16:20:20.053   Training iter 250, batch loss 1.5649, batch acc 0.9174
16:20:20.541   Training iter 300, batch loss 1.5617, batch acc 0.9214
16:20:21.030   Training iter 350, batch loss 1.5623, batch acc 0.9192
16:20:21.504   Training iter 400, batch loss 1.5614, batch acc 0.9212
16:20:21.984   Training iter 450, batch loss 1.5636, batch acc 0.9180
16:20:22.477   Training iter 500, batch loss 1.5648, batch acc 0.9178
16:20:22.966   Training iter 550, batch loss 1.5598, batch acc 0.9230
16:20:23.448   Training iter 600, batch loss 1.5657, batch acc 0.9170
16:20:23.450 Training @ 429 epoch...
16:20:23.928   Training iter 50, batch loss 1.5658, batch acc 0.9182
16:20:24.412   Training iter 100, batch loss 1.5627, batch acc 0.9192
16:20:24.890   Training iter 150, batch loss 1.5616, batch acc 0.9210
16:20:25.360   Training iter 200, batch loss 1.5624, batch acc 0.9198
16:20:25.847   Training iter 250, batch loss 1.5662, batch acc 0.9174
16:20:26.331   Training iter 300, batch loss 1.5613, batch acc 0.9244
16:20:26.835   Training iter 350, batch loss 1.5594, batch acc 0.9214
16:20:27.350   Training iter 400, batch loss 1.5589, batch acc 0.9220
16:20:27.853   Training iter 450, batch loss 1.5650, batch acc 0.9162
16:20:28.334   Training iter 500, batch loss 1.5615, batch acc 0.9192
16:20:28.835   Training iter 550, batch loss 1.5646, batch acc 0.9184
16:20:29.345   Training iter 600, batch loss 1.5633, batch acc 0.9188
16:20:29.347 Training @ 430 epoch...
16:20:29.853   Training iter 50, batch loss 1.5593, batch acc 0.9220
16:20:30.376   Training iter 100, batch loss 1.5665, batch acc 0.9118
16:20:30.901   Training iter 150, batch loss 1.5601, batch acc 0.9204
16:20:31.444   Training iter 200, batch loss 1.5657, batch acc 0.9166
16:20:31.956   Training iter 250, batch loss 1.5637, batch acc 0.9140
16:20:32.466   Training iter 300, batch loss 1.5580, batch acc 0.9286
16:20:32.976   Training iter 350, batch loss 1.5659, batch acc 0.9148
16:20:33.488   Training iter 400, batch loss 1.5649, batch acc 0.9208
16:20:34.008   Training iter 450, batch loss 1.5605, batch acc 0.9244
16:20:34.528   Training iter 500, batch loss 1.5613, batch acc 0.9226
16:20:35.046   Training iter 550, batch loss 1.5625, batch acc 0.9178
16:20:35.550   Training iter 600, batch loss 1.5639, batch acc 0.9218
16:20:35.552 Testing @ 430 epoch...
16:20:35.601     Testing, total mean loss 1.56386, total acc 0.91520
16:20:35.601 Training @ 431 epoch...
16:20:36.126   Training iter 50, batch loss 1.5627, batch acc 0.9210
16:20:36.651   Training iter 100, batch loss 1.5662, batch acc 0.9178
16:20:37.188   Training iter 150, batch loss 1.5617, batch acc 0.9198
16:20:37.721   Training iter 200, batch loss 1.5634, batch acc 0.9214
16:20:38.240   Training iter 250, batch loss 1.5627, batch acc 0.9212
16:20:38.772   Training iter 300, batch loss 1.5621, batch acc 0.9186
16:20:39.302   Training iter 350, batch loss 1.5589, batch acc 0.9220
16:20:39.834   Training iter 400, batch loss 1.5627, batch acc 0.9190
16:20:40.359   Training iter 450, batch loss 1.5645, batch acc 0.9172
16:20:40.878   Training iter 500, batch loss 1.5638, batch acc 0.9202
16:20:41.402   Training iter 550, batch loss 1.5631, batch acc 0.9166
16:20:41.947   Training iter 600, batch loss 1.5604, batch acc 0.9194
16:20:41.948 Training @ 432 epoch...
16:20:42.497   Training iter 50, batch loss 1.5637, batch acc 0.9166
16:20:43.032   Training iter 100, batch loss 1.5625, batch acc 0.9224
16:20:43.545   Training iter 150, batch loss 1.5624, batch acc 0.9210
16:20:44.067   Training iter 200, batch loss 1.5673, batch acc 0.9168
16:20:44.601   Training iter 250, batch loss 1.5605, batch acc 0.9232
16:20:45.145   Training iter 300, batch loss 1.5621, batch acc 0.9202
16:20:45.676   Training iter 350, batch loss 1.5595, batch acc 0.9218
16:20:46.219   Training iter 400, batch loss 1.5669, batch acc 0.9134
16:20:46.763   Training iter 450, batch loss 1.5663, batch acc 0.9130
16:20:47.292   Training iter 500, batch loss 1.5626, batch acc 0.9188
16:20:47.825   Training iter 550, batch loss 1.5581, batch acc 0.9246
16:20:48.348   Training iter 600, batch loss 1.5603, batch acc 0.9190
16:20:48.349 Training @ 433 epoch...
16:20:48.900   Training iter 50, batch loss 1.5601, batch acc 0.9200
16:20:49.446   Training iter 100, batch loss 1.5659, batch acc 0.9156
16:20:49.968   Training iter 150, batch loss 1.5623, batch acc 0.9210
16:20:50.494   Training iter 200, batch loss 1.5653, batch acc 0.9114
16:20:51.021   Training iter 250, batch loss 1.5556, batch acc 0.9322
16:20:51.547   Training iter 300, batch loss 1.5651, batch acc 0.9162
16:20:52.066   Training iter 350, batch loss 1.5603, batch acc 0.9230
16:20:52.602   Training iter 400, batch loss 1.5664, batch acc 0.9162
16:20:53.154   Training iter 450, batch loss 1.5624, batch acc 0.9212
16:20:53.720   Training iter 500, batch loss 1.5619, batch acc 0.9226
16:20:54.268   Training iter 550, batch loss 1.5599, batch acc 0.9260
16:20:54.814   Training iter 600, batch loss 1.5664, batch acc 0.9120
16:20:54.815 Training @ 434 epoch...
16:20:55.368   Training iter 50, batch loss 1.5584, batch acc 0.9218
16:20:55.903   Training iter 100, batch loss 1.5622, batch acc 0.9188
16:20:56.447   Training iter 150, batch loss 1.5646, batch acc 0.9186
16:20:56.998   Training iter 200, batch loss 1.5615, batch acc 0.9206
16:20:57.552   Training iter 250, batch loss 1.5612, batch acc 0.9184
16:20:58.101   Training iter 300, batch loss 1.5668, batch acc 0.9112
16:20:58.654   Training iter 350, batch loss 1.5640, batch acc 0.9204
16:20:59.210   Training iter 400, batch loss 1.5620, batch acc 0.9262
16:20:59.755   Training iter 450, batch loss 1.5571, batch acc 0.9244
16:21:00.288   Training iter 500, batch loss 1.5671, batch acc 0.9140
16:21:00.822   Training iter 550, batch loss 1.5633, batch acc 0.9218
16:21:01.368   Training iter 600, batch loss 1.5635, batch acc 0.9194
16:21:01.369 Training @ 435 epoch...
16:21:01.952   Training iter 50, batch loss 1.5590, batch acc 0.9232
16:21:02.515   Training iter 100, batch loss 1.5595, batch acc 0.9252
16:21:03.074   Training iter 150, batch loss 1.5606, batch acc 0.9196
16:21:03.636   Training iter 200, batch loss 1.5681, batch acc 0.9092
16:21:04.207   Training iter 250, batch loss 1.5596, batch acc 0.9232
16:21:04.764   Training iter 300, batch loss 1.5661, batch acc 0.9164
16:21:05.314   Training iter 350, batch loss 1.5659, batch acc 0.9166
16:21:05.849   Training iter 400, batch loss 1.5652, batch acc 0.9184
16:21:06.395   Training iter 450, batch loss 1.5604, batch acc 0.9272
16:21:06.939   Training iter 500, batch loss 1.5633, batch acc 0.9176
16:21:07.504   Training iter 550, batch loss 1.5628, batch acc 0.9170
16:21:08.016   Training iter 600, batch loss 1.5607, batch acc 0.9190
16:21:08.018 Testing @ 435 epoch...
16:21:08.066     Testing, total mean loss 1.56381, total acc 0.91560
16:21:08.066 Training @ 436 epoch...
16:21:08.597   Training iter 50, batch loss 1.5628, batch acc 0.9220
16:21:09.171   Training iter 100, batch loss 1.5616, batch acc 0.9198
16:21:09.723   Training iter 150, batch loss 1.5654, batch acc 0.9172
16:21:10.278   Training iter 200, batch loss 1.5658, batch acc 0.9154
16:21:10.853   Training iter 250, batch loss 1.5677, batch acc 0.9168
16:21:11.410   Training iter 300, batch loss 1.5639, batch acc 0.9180
16:21:11.967   Training iter 350, batch loss 1.5567, batch acc 0.9222
16:21:12.522   Training iter 400, batch loss 1.5615, batch acc 0.9228
16:21:13.062   Training iter 450, batch loss 1.5585, batch acc 0.9238
16:21:13.600   Training iter 500, batch loss 1.5602, batch acc 0.9218
16:21:14.147   Training iter 550, batch loss 1.5632, batch acc 0.9192
16:21:14.699   Training iter 600, batch loss 1.5641, batch acc 0.9168
16:21:14.701 Training @ 437 epoch...
16:21:15.257   Training iter 50, batch loss 1.5592, batch acc 0.9220
16:21:15.769   Training iter 100, batch loss 1.5626, batch acc 0.9190
16:21:16.294   Training iter 150, batch loss 1.5643, batch acc 0.9172
16:21:16.832   Training iter 200, batch loss 1.5685, batch acc 0.9086
16:21:17.369   Training iter 250, batch loss 1.5610, batch acc 0.9226
16:21:17.895   Training iter 300, batch loss 1.5645, batch acc 0.9178
16:21:18.385   Training iter 350, batch loss 1.5621, batch acc 0.9188
16:21:18.865   Training iter 400, batch loss 1.5613, batch acc 0.9230
16:21:19.350   Training iter 450, batch loss 1.5589, batch acc 0.9272
16:21:19.837   Training iter 500, batch loss 1.5654, batch acc 0.9146
16:21:20.329   Training iter 550, batch loss 1.5611, batch acc 0.9206
16:21:20.839   Training iter 600, batch loss 1.5622, batch acc 0.9214
16:21:20.840 Training @ 438 epoch...
16:21:21.368   Training iter 50, batch loss 1.5632, batch acc 0.9190
16:21:21.896   Training iter 100, batch loss 1.5641, batch acc 0.9186
16:21:22.436   Training iter 150, batch loss 1.5581, batch acc 0.9236
16:21:22.957   Training iter 200, batch loss 1.5619, batch acc 0.9212
16:21:23.478   Training iter 250, batch loss 1.5626, batch acc 0.9172
16:21:23.993   Training iter 300, batch loss 1.5641, batch acc 0.9200
16:21:24.534   Training iter 350, batch loss 1.5611, batch acc 0.9222
16:21:25.066   Training iter 400, batch loss 1.5642, batch acc 0.9158
16:21:25.613   Training iter 450, batch loss 1.5546, batch acc 0.9286
16:21:26.147   Training iter 500, batch loss 1.5630, batch acc 0.9218
16:21:26.669   Training iter 550, batch loss 1.5674, batch acc 0.9124
16:21:27.196   Training iter 600, batch loss 1.5664, batch acc 0.9166
16:21:27.197 Training @ 439 epoch...
16:21:27.732   Training iter 50, batch loss 1.5636, batch acc 0.9208
16:21:28.251   Training iter 100, batch loss 1.5643, batch acc 0.9164
16:21:28.761   Training iter 150, batch loss 1.5651, batch acc 0.9160
16:21:29.288   Training iter 200, batch loss 1.5615, batch acc 0.9222
16:21:29.803   Training iter 250, batch loss 1.5598, batch acc 0.9250
16:21:30.338   Training iter 300, batch loss 1.5634, batch acc 0.9166
16:21:30.862   Training iter 350, batch loss 1.5626, batch acc 0.9190
16:21:31.390   Training iter 400, batch loss 1.5600, batch acc 0.9236
16:21:31.922   Training iter 450, batch loss 1.5632, batch acc 0.9200
16:21:32.464   Training iter 500, batch loss 1.5603, batch acc 0.9216
16:21:33.003   Training iter 550, batch loss 1.5642, batch acc 0.9182
16:21:33.536   Training iter 600, batch loss 1.5624, batch acc 0.9184
16:21:33.538 Training @ 440 epoch...
16:21:34.084   Training iter 50, batch loss 1.5616, batch acc 0.9172
16:21:34.637   Training iter 100, batch loss 1.5650, batch acc 0.9178
16:21:35.183   Training iter 150, batch loss 1.5603, batch acc 0.9236
16:21:35.726   Training iter 200, batch loss 1.5574, batch acc 0.9280
16:21:36.275   Training iter 250, batch loss 1.5651, batch acc 0.9148
16:21:36.812   Training iter 300, batch loss 1.5650, batch acc 0.9164
16:21:37.344   Training iter 350, batch loss 1.5612, batch acc 0.9226
16:21:37.867   Training iter 400, batch loss 1.5623, batch acc 0.9192
16:21:38.395   Training iter 450, batch loss 1.5606, batch acc 0.9224
16:21:38.929   Training iter 500, batch loss 1.5633, batch acc 0.9134
16:21:39.478   Training iter 550, batch loss 1.5666, batch acc 0.9192
16:21:39.984   Training iter 600, batch loss 1.5616, batch acc 0.9218
16:21:39.986 Testing @ 440 epoch...
16:21:40.037     Testing, total mean loss 1.56383, total acc 0.91540
16:21:40.037 Training @ 441 epoch...
16:21:40.546   Training iter 50, batch loss 1.5644, batch acc 0.9144
16:21:41.024   Training iter 100, batch loss 1.5621, batch acc 0.9224
16:21:41.516   Training iter 150, batch loss 1.5588, batch acc 0.9250
16:21:42.015   Training iter 200, batch loss 1.5617, batch acc 0.9232
16:21:42.501   Training iter 250, batch loss 1.5618, batch acc 0.9210
16:21:42.989   Training iter 300, batch loss 1.5596, batch acc 0.9252
16:21:43.494   Training iter 350, batch loss 1.5653, batch acc 0.9140
16:21:43.991   Training iter 400, batch loss 1.5618, batch acc 0.9206
16:21:44.485   Training iter 450, batch loss 1.5635, batch acc 0.9212
16:21:44.977   Training iter 500, batch loss 1.5644, batch acc 0.9164
16:21:45.478   Training iter 550, batch loss 1.5640, batch acc 0.9158
16:21:45.972   Training iter 600, batch loss 1.5623, batch acc 0.9174
16:21:45.974 Training @ 442 epoch...
16:21:46.486   Training iter 50, batch loss 1.5637, batch acc 0.9194
16:21:47.030   Training iter 100, batch loss 1.5607, batch acc 0.9198
16:21:47.587   Training iter 150, batch loss 1.5656, batch acc 0.9156
16:21:48.143   Training iter 200, batch loss 1.5604, batch acc 0.9226
16:21:48.700   Training iter 250, batch loss 1.5651, batch acc 0.9174
16:21:49.250   Training iter 300, batch loss 1.5602, batch acc 0.9226
16:21:49.797   Training iter 350, batch loss 1.5612, batch acc 0.9200
16:21:50.350   Training iter 400, batch loss 1.5650, batch acc 0.9172
16:21:50.881   Training iter 450, batch loss 1.5631, batch acc 0.9218
16:21:51.410   Training iter 500, batch loss 1.5603, batch acc 0.9216
16:21:51.955   Training iter 550, batch loss 1.5631, batch acc 0.9198
16:21:52.498   Training iter 600, batch loss 1.5610, batch acc 0.9212
16:21:52.499 Training @ 443 epoch...
16:21:53.034   Training iter 50, batch loss 1.5546, batch acc 0.9302
16:21:53.566   Training iter 100, batch loss 1.5627, batch acc 0.9212
16:21:54.110   Training iter 150, batch loss 1.5609, batch acc 0.9200
16:21:54.661   Training iter 200, batch loss 1.5652, batch acc 0.9152
16:21:55.227   Training iter 250, batch loss 1.5667, batch acc 0.9158
16:21:55.771   Training iter 300, batch loss 1.5656, batch acc 0.9146
16:21:56.324   Training iter 350, batch loss 1.5620, batch acc 0.9186
16:21:56.879   Training iter 400, batch loss 1.5619, batch acc 0.9190
16:21:57.446   Training iter 450, batch loss 1.5613, batch acc 0.9236
16:21:58.006   Training iter 500, batch loss 1.5622, batch acc 0.9220
16:21:58.573   Training iter 550, batch loss 1.5631, batch acc 0.9192
16:21:59.120   Training iter 600, batch loss 1.5629, batch acc 0.9188
16:21:59.122 Training @ 444 epoch...
16:21:59.651   Training iter 50, batch loss 1.5599, batch acc 0.9208
16:22:00.166   Training iter 100, batch loss 1.5624, batch acc 0.9180
16:22:00.733   Training iter 150, batch loss 1.5653, batch acc 0.9176
16:22:01.324   Training iter 200, batch loss 1.5600, batch acc 0.9224
16:22:01.929   Training iter 250, batch loss 1.5608, batch acc 0.9218
16:22:02.520   Training iter 300, batch loss 1.5652, batch acc 0.9146
16:22:03.058   Training iter 350, batch loss 1.5663, batch acc 0.9180
16:22:03.597   Training iter 400, batch loss 1.5561, batch acc 0.9304
16:22:04.155   Training iter 450, batch loss 1.5610, batch acc 0.9208
16:22:04.719   Training iter 500, batch loss 1.5634, batch acc 0.9202
16:22:05.291   Training iter 550, batch loss 1.5649, batch acc 0.9170
16:22:05.890   Training iter 600, batch loss 1.5632, batch acc 0.9206
16:22:05.892 Training @ 445 epoch...
16:22:06.500   Training iter 50, batch loss 1.5611, batch acc 0.9222
16:22:07.097   Training iter 100, batch loss 1.5621, batch acc 0.9220
16:22:07.679   Training iter 150, batch loss 1.5598, batch acc 0.9232
16:22:08.265   Training iter 200, batch loss 1.5634, batch acc 0.9170
16:22:08.830   Training iter 250, batch loss 1.5633, batch acc 0.9200
16:22:09.401   Training iter 300, batch loss 1.5651, batch acc 0.9192
16:22:09.951   Training iter 350, batch loss 1.5609, batch acc 0.9230
16:22:10.521   Training iter 400, batch loss 1.5595, batch acc 0.9204
16:22:11.056   Training iter 450, batch loss 1.5649, batch acc 0.9154
16:22:11.597   Training iter 500, batch loss 1.5650, batch acc 0.9190
16:22:12.140   Training iter 550, batch loss 1.5653, batch acc 0.9142
16:22:12.692   Training iter 600, batch loss 1.5581, batch acc 0.9268
16:22:12.694 Testing @ 445 epoch...
16:22:12.747     Testing, total mean loss 1.56363, total acc 0.91550
16:22:12.747 Training @ 446 epoch...
16:22:13.290   Training iter 50, batch loss 1.5595, batch acc 0.9228
16:22:13.844   Training iter 100, batch loss 1.5626, batch acc 0.9168
16:22:14.380   Training iter 150, batch loss 1.5609, batch acc 0.9232
16:22:14.926   Training iter 200, batch loss 1.5662, batch acc 0.9156
16:22:15.468   Training iter 250, batch loss 1.5596, batch acc 0.9210
16:22:16.006   Training iter 300, batch loss 1.5614, batch acc 0.9192
16:22:16.553   Training iter 350, batch loss 1.5646, batch acc 0.9160
16:22:17.121   Training iter 400, batch loss 1.5605, batch acc 0.9224
16:22:17.713   Training iter 450, batch loss 1.5631, batch acc 0.9212
16:22:18.263   Training iter 500, batch loss 1.5598, batch acc 0.9236
16:22:18.783   Training iter 550, batch loss 1.5635, batch acc 0.9188
16:22:19.312   Training iter 600, batch loss 1.5670, batch acc 0.9144
16:22:19.313 Training @ 447 epoch...
16:22:19.859   Training iter 50, batch loss 1.5635, batch acc 0.9186
16:22:20.404   Training iter 100, batch loss 1.5644, batch acc 0.9180
16:22:20.947   Training iter 150, batch loss 1.5624, batch acc 0.9200
16:22:21.484   Training iter 200, batch loss 1.5645, batch acc 0.9174
16:22:22.033   Training iter 250, batch loss 1.5611, batch acc 0.9220
16:22:22.575   Training iter 300, batch loss 1.5614, batch acc 0.9226
16:22:23.097   Training iter 350, batch loss 1.5599, batch acc 0.9232
16:22:23.590   Training iter 400, batch loss 1.5635, batch acc 0.9204
16:22:24.102   Training iter 450, batch loss 1.5616, batch acc 0.9176
16:22:24.624   Training iter 500, batch loss 1.5568, batch acc 0.9288
16:22:25.118   Training iter 550, batch loss 1.5654, batch acc 0.9176
16:22:25.631   Training iter 600, batch loss 1.5639, batch acc 0.9156
16:22:25.633 Training @ 448 epoch...
16:22:26.147   Training iter 50, batch loss 1.5636, batch acc 0.9164
16:22:26.663   Training iter 100, batch loss 1.5600, batch acc 0.9234
16:22:27.181   Training iter 150, batch loss 1.5577, batch acc 0.9264
16:22:27.703   Training iter 200, batch loss 1.5608, batch acc 0.9198
16:22:28.209   Training iter 250, batch loss 1.5676, batch acc 0.9146
16:22:28.729   Training iter 300, batch loss 1.5645, batch acc 0.9180
16:22:29.246   Training iter 350, batch loss 1.5647, batch acc 0.9164
16:22:29.770   Training iter 400, batch loss 1.5620, batch acc 0.9214
16:22:30.290   Training iter 450, batch loss 1.5635, batch acc 0.9198
16:22:30.839   Training iter 500, batch loss 1.5646, batch acc 0.9154
16:22:31.394   Training iter 550, batch loss 1.5597, batch acc 0.9230
16:22:31.966   Training iter 600, batch loss 1.5588, batch acc 0.9274
16:22:31.967 Training @ 449 epoch...
16:22:32.552   Training iter 50, batch loss 1.5628, batch acc 0.9222
16:22:33.131   Training iter 100, batch loss 1.5606, batch acc 0.9222
16:22:33.702   Training iter 150, batch loss 1.5655, batch acc 0.9184
16:22:34.270   Training iter 200, batch loss 1.5597, batch acc 0.9206
16:22:34.846   Training iter 250, batch loss 1.5617, batch acc 0.9182
16:22:35.399   Training iter 300, batch loss 1.5647, batch acc 0.9168
16:22:35.948   Training iter 350, batch loss 1.5656, batch acc 0.9148
16:22:36.498   Training iter 400, batch loss 1.5599, batch acc 0.9180
16:22:37.047   Training iter 450, batch loss 1.5655, batch acc 0.9180
16:22:37.591   Training iter 500, batch loss 1.5610, batch acc 0.9214
16:22:38.147   Training iter 550, batch loss 1.5603, batch acc 0.9250
16:22:38.682   Training iter 600, batch loss 1.5606, batch acc 0.9220
16:22:38.684 Training @ 450 epoch...
16:22:39.234   Training iter 50, batch loss 1.5611, batch acc 0.9278
16:22:39.788   Training iter 100, batch loss 1.5616, batch acc 0.9194
16:22:40.330   Training iter 150, batch loss 1.5583, batch acc 0.9246
16:22:40.862   Training iter 200, batch loss 1.5597, batch acc 0.9224
16:22:41.386   Training iter 250, batch loss 1.5597, batch acc 0.9268
16:22:41.923   Training iter 300, batch loss 1.5645, batch acc 0.9214
16:22:42.463   Training iter 350, batch loss 1.5608, batch acc 0.9190
16:22:43.009   Training iter 400, batch loss 1.5668, batch acc 0.9128
16:22:43.555   Training iter 450, batch loss 1.5648, batch acc 0.9148
16:22:44.103   Training iter 500, batch loss 1.5667, batch acc 0.9168
16:22:44.656   Training iter 550, batch loss 1.5616, batch acc 0.9208
16:22:45.204   Training iter 600, batch loss 1.5619, batch acc 0.9174
16:22:45.206 Testing @ 450 epoch...
16:22:45.255     Testing, total mean loss 1.56347, total acc 0.91580
16:22:45.255 Training @ 451 epoch...
16:22:45.818   Training iter 50, batch loss 1.5612, batch acc 0.9206
16:22:46.368   Training iter 100, batch loss 1.5583, batch acc 0.9236
16:22:46.934   Training iter 150, batch loss 1.5670, batch acc 0.9150
16:22:47.502   Training iter 200, batch loss 1.5635, batch acc 0.9190
16:22:48.078   Training iter 250, batch loss 1.5611, batch acc 0.9242
16:22:48.627   Training iter 300, batch loss 1.5594, batch acc 0.9230
16:22:49.214   Training iter 350, batch loss 1.5621, batch acc 0.9218
16:22:49.779   Training iter 400, batch loss 1.5668, batch acc 0.9120
16:22:50.331   Training iter 450, batch loss 1.5609, batch acc 0.9250
16:22:50.876   Training iter 500, batch loss 1.5664, batch acc 0.9146
16:22:51.420   Training iter 550, batch loss 1.5606, batch acc 0.9200
16:22:51.989   Training iter 600, batch loss 1.5599, batch acc 0.9214
16:22:51.991 Training @ 452 epoch...
16:22:52.580   Training iter 50, batch loss 1.5642, batch acc 0.9148
16:22:53.153   Training iter 100, batch loss 1.5638, batch acc 0.9192
16:22:53.689   Training iter 150, batch loss 1.5649, batch acc 0.9146
16:22:54.231   Training iter 200, batch loss 1.5577, batch acc 0.9264
16:22:54.780   Training iter 250, batch loss 1.5632, batch acc 0.9196
16:22:55.315   Training iter 300, batch loss 1.5574, batch acc 0.9268
16:22:55.837   Training iter 350, batch loss 1.5601, batch acc 0.9242
16:22:56.357   Training iter 400, batch loss 1.5635, batch acc 0.9196
16:22:56.881   Training iter 450, batch loss 1.5596, batch acc 0.9232
16:22:57.407   Training iter 500, batch loss 1.5650, batch acc 0.9170
16:22:57.929   Training iter 550, batch loss 1.5637, batch acc 0.9198
16:22:58.441   Training iter 600, batch loss 1.5637, batch acc 0.9164
16:22:58.443 Training @ 453 epoch...
16:22:58.965   Training iter 50, batch loss 1.5615, batch acc 0.9192
16:22:59.497   Training iter 100, batch loss 1.5682, batch acc 0.9144
16:23:00.052   Training iter 150, batch loss 1.5611, batch acc 0.9220
16:23:00.585   Training iter 200, batch loss 1.5619, batch acc 0.9200
16:23:01.089   Training iter 250, batch loss 1.5647, batch acc 0.9186
16:23:01.655   Training iter 300, batch loss 1.5611, batch acc 0.9230
16:23:02.220   Training iter 350, batch loss 1.5599, batch acc 0.9216
16:23:02.779   Training iter 400, batch loss 1.5651, batch acc 0.9142
16:23:03.320   Training iter 450, batch loss 1.5615, batch acc 0.9214
16:23:03.859   Training iter 500, batch loss 1.5624, batch acc 0.9208
16:23:04.385   Training iter 550, batch loss 1.5604, batch acc 0.9204
16:23:04.893   Training iter 600, batch loss 1.5582, batch acc 0.9262
16:23:04.894 Training @ 454 epoch...
16:23:05.395   Training iter 50, batch loss 1.5617, batch acc 0.9198
16:23:05.897   Training iter 100, batch loss 1.5613, batch acc 0.9236
16:23:06.415   Training iter 150, batch loss 1.5594, batch acc 0.9246
16:23:06.930   Training iter 200, batch loss 1.5614, batch acc 0.9204
16:23:07.429   Training iter 250, batch loss 1.5606, batch acc 0.9232
16:23:07.940   Training iter 300, batch loss 1.5621, batch acc 0.9220
16:23:08.448   Training iter 350, batch loss 1.5652, batch acc 0.9142
16:23:08.997   Training iter 400, batch loss 1.5639, batch acc 0.9166
16:23:09.521   Training iter 450, batch loss 1.5635, batch acc 0.9172
16:23:10.046   Training iter 500, batch loss 1.5619, batch acc 0.9206
16:23:10.575   Training iter 550, batch loss 1.5621, batch acc 0.9196
16:23:11.120   Training iter 600, batch loss 1.5630, batch acc 0.9194
16:23:11.121 Training @ 455 epoch...
16:23:11.645   Training iter 50, batch loss 1.5561, batch acc 0.9268
16:23:12.183   Training iter 100, batch loss 1.5664, batch acc 0.9142
16:23:12.708   Training iter 150, batch loss 1.5642, batch acc 0.9166
16:23:13.229   Training iter 200, batch loss 1.5610, batch acc 0.9222
16:23:13.747   Training iter 250, batch loss 1.5588, batch acc 0.9248
16:23:14.263   Training iter 300, batch loss 1.5663, batch acc 0.9146
16:23:14.778   Training iter 350, batch loss 1.5659, batch acc 0.9132
16:23:15.297   Training iter 400, batch loss 1.5632, batch acc 0.9204
16:23:15.832   Training iter 450, batch loss 1.5651, batch acc 0.9202
16:23:16.371   Training iter 500, batch loss 1.5592, batch acc 0.9246
16:23:16.930   Training iter 550, batch loss 1.5604, batch acc 0.9254
16:23:17.480   Training iter 600, batch loss 1.5593, batch acc 0.9212
16:23:17.482 Testing @ 455 epoch...
16:23:17.531     Testing, total mean loss 1.56340, total acc 0.91680
16:23:17.531 Training @ 456 epoch...
16:23:18.092   Training iter 50, batch loss 1.5614, batch acc 0.9186
16:23:18.645   Training iter 100, batch loss 1.5640, batch acc 0.9190
16:23:19.204   Training iter 150, batch loss 1.5602, batch acc 0.9228
16:23:19.767   Training iter 200, batch loss 1.5578, batch acc 0.9246
16:23:20.330   Training iter 250, batch loss 1.5656, batch acc 0.9186
16:23:20.887   Training iter 300, batch loss 1.5646, batch acc 0.9172
16:23:21.444   Training iter 350, batch loss 1.5605, batch acc 0.9198
16:23:22.019   Training iter 400, batch loss 1.5647, batch acc 0.9192
16:23:22.561   Training iter 450, batch loss 1.5571, batch acc 0.9262
16:23:23.091   Training iter 500, batch loss 1.5653, batch acc 0.9170
16:23:23.623   Training iter 550, batch loss 1.5598, batch acc 0.9248
16:23:24.170   Training iter 600, batch loss 1.5644, batch acc 0.9170
16:23:24.172 Training @ 457 epoch...
16:23:24.703   Training iter 50, batch loss 1.5654, batch acc 0.9188
16:23:25.235   Training iter 100, batch loss 1.5624, batch acc 0.9200
16:23:25.789   Training iter 150, batch loss 1.5632, batch acc 0.9192
16:23:26.358   Training iter 200, batch loss 1.5615, batch acc 0.9212
16:23:26.901   Training iter 250, batch loss 1.5615, batch acc 0.9216
16:23:27.442   Training iter 300, batch loss 1.5625, batch acc 0.9168
16:23:27.974   Training iter 350, batch loss 1.5621, batch acc 0.9216
16:23:28.522   Training iter 400, batch loss 1.5632, batch acc 0.9178
16:23:29.092   Training iter 450, batch loss 1.5578, batch acc 0.9256
16:23:29.656   Training iter 500, batch loss 1.5659, batch acc 0.9186
16:23:30.229   Training iter 550, batch loss 1.5590, batch acc 0.9264
16:23:30.821   Training iter 600, batch loss 1.5607, batch acc 0.9224
16:23:30.823 Training @ 458 epoch...
16:23:31.404   Training iter 50, batch loss 1.5625, batch acc 0.9212
16:23:31.978   Training iter 100, batch loss 1.5573, batch acc 0.9280
16:23:32.565   Training iter 150, batch loss 1.5638, batch acc 0.9190
16:23:33.156   Training iter 200, batch loss 1.5630, batch acc 0.9198
16:23:33.731   Training iter 250, batch loss 1.5638, batch acc 0.9182
16:23:34.303   Training iter 300, batch loss 1.5613, batch acc 0.9218
16:23:34.935   Training iter 350, batch loss 1.5653, batch acc 0.9170
16:23:35.612   Training iter 400, batch loss 1.5586, batch acc 0.9254
16:23:36.292   Training iter 450, batch loss 1.5649, batch acc 0.9174
16:23:36.950   Training iter 500, batch loss 1.5601, batch acc 0.9222
16:23:37.596   Training iter 550, batch loss 1.5639, batch acc 0.9146
16:23:38.239   Training iter 600, batch loss 1.5606, batch acc 0.9222
16:23:38.241 Training @ 459 epoch...
16:23:38.838   Training iter 50, batch loss 1.5633, batch acc 0.9212
16:23:39.445   Training iter 100, batch loss 1.5581, batch acc 0.9276
16:23:40.040   Training iter 150, batch loss 1.5651, batch acc 0.9164
16:23:40.654   Training iter 200, batch loss 1.5621, batch acc 0.9200
16:23:41.271   Training iter 250, batch loss 1.5617, batch acc 0.9204
16:23:41.891   Training iter 300, batch loss 1.5598, batch acc 0.9244
16:23:42.500   Training iter 350, batch loss 1.5631, batch acc 0.9210
16:23:43.116   Training iter 400, batch loss 1.5608, batch acc 0.9204
16:23:43.700   Training iter 450, batch loss 1.5604, batch acc 0.9188
16:23:44.294   Training iter 500, batch loss 1.5623, batch acc 0.9184
16:23:44.910   Training iter 550, batch loss 1.5626, batch acc 0.9206
16:23:45.504   Training iter 600, batch loss 1.5653, batch acc 0.9156
16:23:45.506 Training @ 460 epoch...
16:23:46.100   Training iter 50, batch loss 1.5598, batch acc 0.9200
16:23:46.699   Training iter 100, batch loss 1.5661, batch acc 0.9142
16:23:47.229   Training iter 150, batch loss 1.5645, batch acc 0.9192
16:23:47.762   Training iter 200, batch loss 1.5615, batch acc 0.9260
16:23:48.320   Training iter 250, batch loss 1.5635, batch acc 0.9202
16:23:48.894   Training iter 300, batch loss 1.5597, batch acc 0.9254
16:23:49.462   Training iter 350, batch loss 1.5581, batch acc 0.9264
16:23:50.026   Training iter 400, batch loss 1.5626, batch acc 0.9170
16:23:50.590   Training iter 450, batch loss 1.5601, batch acc 0.9228
16:23:51.165   Training iter 500, batch loss 1.5642, batch acc 0.9192
16:23:51.737   Training iter 550, batch loss 1.5627, batch acc 0.9186
16:23:52.306   Training iter 600, batch loss 1.5616, batch acc 0.9176
16:23:52.308 Testing @ 460 epoch...
16:23:52.357     Testing, total mean loss 1.56334, total acc 0.91630
16:23:52.357 Training @ 461 epoch...
16:23:52.939   Training iter 50, batch loss 1.5640, batch acc 0.9158
16:23:53.526   Training iter 100, batch loss 1.5611, batch acc 0.9262
16:23:54.105   Training iter 150, batch loss 1.5629, batch acc 0.9186
16:23:54.659   Training iter 200, batch loss 1.5591, batch acc 0.9256
16:23:55.212   Training iter 250, batch loss 1.5651, batch acc 0.9174
16:23:55.751   Training iter 300, batch loss 1.5635, batch acc 0.9168
16:23:56.299   Training iter 350, batch loss 1.5621, batch acc 0.9188
16:23:56.846   Training iter 400, batch loss 1.5575, batch acc 0.9250
16:23:57.375   Training iter 450, batch loss 1.5646, batch acc 0.9156
16:23:57.910   Training iter 500, batch loss 1.5621, batch acc 0.9182
16:23:58.450   Training iter 550, batch loss 1.5573, batch acc 0.9266
16:23:58.987   Training iter 600, batch loss 1.5651, batch acc 0.9172
16:23:58.989 Training @ 462 epoch...
16:23:59.520   Training iter 50, batch loss 1.5604, batch acc 0.9234
16:24:00.054   Training iter 100, batch loss 1.5576, batch acc 0.9218
16:24:00.607   Training iter 150, batch loss 1.5624, batch acc 0.9196
16:24:01.155   Training iter 200, batch loss 1.5620, batch acc 0.9198
16:24:01.747   Training iter 250, batch loss 1.5655, batch acc 0.9186
16:24:02.318   Training iter 300, batch loss 1.5611, batch acc 0.9202
16:24:02.893   Training iter 350, batch loss 1.5634, batch acc 0.9188
16:24:03.463   Training iter 400, batch loss 1.5694, batch acc 0.9118
16:24:04.047   Training iter 450, batch loss 1.5589, batch acc 0.9244
16:24:04.618   Training iter 500, batch loss 1.5650, batch acc 0.9136
16:24:05.183   Training iter 550, batch loss 1.5612, batch acc 0.9230
16:24:05.750   Training iter 600, batch loss 1.5570, batch acc 0.9296
16:24:05.751 Training @ 463 epoch...
16:24:06.317   Training iter 50, batch loss 1.5618, batch acc 0.9210
16:24:06.862   Training iter 100, batch loss 1.5605, batch acc 0.9208
16:24:07.378   Training iter 150, batch loss 1.5592, batch acc 0.9250
16:24:07.869   Training iter 200, batch loss 1.5618, batch acc 0.9222
16:24:08.365   Training iter 250, batch loss 1.5588, batch acc 0.9192
16:24:08.847   Training iter 300, batch loss 1.5611, batch acc 0.9244
16:24:09.344   Training iter 350, batch loss 1.5636, batch acc 0.9154
16:24:09.844   Training iter 400, batch loss 1.5606, batch acc 0.9204
16:24:10.349   Training iter 450, batch loss 1.5612, batch acc 0.9224
16:24:10.819   Training iter 500, batch loss 1.5675, batch acc 0.9140
16:24:11.308   Training iter 550, batch loss 1.5608, batch acc 0.9216
16:24:11.790   Training iter 600, batch loss 1.5669, batch acc 0.9174
16:24:11.792 Training @ 464 epoch...
16:24:12.292   Training iter 50, batch loss 1.5637, batch acc 0.9218
16:24:12.787   Training iter 100, batch loss 1.5594, batch acc 0.9232
16:24:13.284   Training iter 150, batch loss 1.5617, batch acc 0.9196
16:24:13.760   Training iter 200, batch loss 1.5573, batch acc 0.9228
16:24:14.260   Training iter 250, batch loss 1.5601, batch acc 0.9246
16:24:14.767   Training iter 300, batch loss 1.5620, batch acc 0.9242
16:24:15.278   Training iter 350, batch loss 1.5649, batch acc 0.9164
16:24:15.781   Training iter 400, batch loss 1.5631, batch acc 0.9182
16:24:16.284   Training iter 450, batch loss 1.5646, batch acc 0.9176
16:24:16.818   Training iter 500, batch loss 1.5596, batch acc 0.9220
16:24:17.373   Training iter 550, batch loss 1.5590, batch acc 0.9224
16:24:17.911   Training iter 600, batch loss 1.5677, batch acc 0.9164
16:24:17.913 Training @ 465 epoch...
16:24:18.466   Training iter 50, batch loss 1.5583, batch acc 0.9236
16:24:19.017   Training iter 100, batch loss 1.5636, batch acc 0.9176
16:24:19.563   Training iter 150, batch loss 1.5589, batch acc 0.9260
16:24:20.112   Training iter 200, batch loss 1.5624, batch acc 0.9240
16:24:20.687   Training iter 250, batch loss 1.5574, batch acc 0.9300
16:24:21.264   Training iter 300, batch loss 1.5612, batch acc 0.9230
16:24:21.823   Training iter 350, batch loss 1.5620, batch acc 0.9218
16:24:22.394   Training iter 400, batch loss 1.5644, batch acc 0.9188
16:24:22.960   Training iter 450, batch loss 1.5644, batch acc 0.9148
16:24:23.533   Training iter 500, batch loss 1.5633, batch acc 0.9166
16:24:24.100   Training iter 550, batch loss 1.5617, batch acc 0.9178
16:24:24.683   Training iter 600, batch loss 1.5660, batch acc 0.9140
16:24:24.685 Testing @ 465 epoch...
16:24:24.734     Testing, total mean loss 1.56319, total acc 0.91560
16:24:24.734 Training @ 466 epoch...
16:24:25.319   Training iter 50, batch loss 1.5601, batch acc 0.9252
16:24:25.887   Training iter 100, batch loss 1.5626, batch acc 0.9184
16:24:26.456   Training iter 150, batch loss 1.5611, batch acc 0.9218
16:24:26.996   Training iter 200, batch loss 1.5604, batch acc 0.9176
16:24:27.564   Training iter 250, batch loss 1.5576, batch acc 0.9242
16:24:28.153   Training iter 300, batch loss 1.5650, batch acc 0.9152
16:24:28.735   Training iter 350, batch loss 1.5656, batch acc 0.9160
16:24:29.322   Training iter 400, batch loss 1.5598, batch acc 0.9232
16:24:29.891   Training iter 450, batch loss 1.5605, batch acc 0.9230
16:24:30.455   Training iter 500, batch loss 1.5631, batch acc 0.9218
16:24:31.028   Training iter 550, batch loss 1.5631, batch acc 0.9208
16:24:31.555   Training iter 600, batch loss 1.5641, batch acc 0.9188
16:24:31.557 Training @ 467 epoch...
16:24:32.101   Training iter 50, batch loss 1.5607, batch acc 0.9228
16:24:32.670   Training iter 100, batch loss 1.5605, batch acc 0.9220
16:24:33.239   Training iter 150, batch loss 1.5636, batch acc 0.9214
16:24:33.788   Training iter 200, batch loss 1.5614, batch acc 0.9212
16:24:34.340   Training iter 250, batch loss 1.5598, batch acc 0.9232
16:24:34.888   Training iter 300, batch loss 1.5587, batch acc 0.9242
16:24:35.437   Training iter 350, batch loss 1.5646, batch acc 0.9126
16:24:35.980   Training iter 400, batch loss 1.5613, batch acc 0.9178
16:24:36.547   Training iter 450, batch loss 1.5631, batch acc 0.9206
16:24:37.128   Training iter 500, batch loss 1.5698, batch acc 0.9146
16:24:37.715   Training iter 550, batch loss 1.5613, batch acc 0.9200
16:24:38.293   Training iter 600, batch loss 1.5583, batch acc 0.9274
16:24:38.295 Training @ 468 epoch...
16:24:38.854   Training iter 50, batch loss 1.5648, batch acc 0.9166
16:24:39.410   Training iter 100, batch loss 1.5589, batch acc 0.9204
16:24:39.959   Training iter 150, batch loss 1.5642, batch acc 0.9170
16:24:40.535   Training iter 200, batch loss 1.5668, batch acc 0.9152
16:24:41.108   Training iter 250, batch loss 1.5611, batch acc 0.9212
16:24:41.671   Training iter 300, batch loss 1.5591, batch acc 0.9254
16:24:42.245   Training iter 350, batch loss 1.5596, batch acc 0.9244
16:24:42.792   Training iter 400, batch loss 1.5620, batch acc 0.9204
16:24:43.334   Training iter 450, batch loss 1.5624, batch acc 0.9216
16:24:43.866   Training iter 500, batch loss 1.5599, batch acc 0.9232
16:24:44.425   Training iter 550, batch loss 1.5598, batch acc 0.9234
16:24:44.978   Training iter 600, batch loss 1.5638, batch acc 0.9198
16:24:44.980 Training @ 469 epoch...
16:24:45.543   Training iter 50, batch loss 1.5640, batch acc 0.9166
16:24:46.100   Training iter 100, batch loss 1.5633, batch acc 0.9210
16:24:46.655   Training iter 150, batch loss 1.5605, batch acc 0.9210
16:24:47.192   Training iter 200, batch loss 1.5603, batch acc 0.9232
16:24:47.746   Training iter 250, batch loss 1.5611, batch acc 0.9186
16:24:48.285   Training iter 300, batch loss 1.5610, batch acc 0.9234
16:24:48.832   Training iter 350, batch loss 1.5607, batch acc 0.9240
16:24:49.389   Training iter 400, batch loss 1.5646, batch acc 0.9154
16:24:49.938   Training iter 450, batch loss 1.5618, batch acc 0.9198
16:24:50.468   Training iter 500, batch loss 1.5610, batch acc 0.9222
16:24:50.969   Training iter 550, batch loss 1.5630, batch acc 0.9208
16:24:51.467   Training iter 600, batch loss 1.5609, batch acc 0.9200
16:24:51.468 Training @ 470 epoch...
16:24:51.970   Training iter 50, batch loss 1.5617, batch acc 0.9224
16:24:52.473   Training iter 100, batch loss 1.5627, batch acc 0.9196
16:24:52.985   Training iter 150, batch loss 1.5608, batch acc 0.9224
16:24:53.498   Training iter 200, batch loss 1.5603, batch acc 0.9224
16:24:53.998   Training iter 250, batch loss 1.5644, batch acc 0.9150
16:24:54.522   Training iter 300, batch loss 1.5641, batch acc 0.9174
16:24:55.033   Training iter 350, batch loss 1.5568, batch acc 0.9304
16:24:55.547   Training iter 400, batch loss 1.5610, batch acc 0.9254
16:24:56.044   Training iter 450, batch loss 1.5655, batch acc 0.9150
16:24:56.539   Training iter 500, batch loss 1.5573, batch acc 0.9242
16:24:57.056   Training iter 550, batch loss 1.5638, batch acc 0.9148
16:24:57.557   Training iter 600, batch loss 1.5633, batch acc 0.9176
16:24:57.559 Testing @ 470 epoch...
16:24:57.608     Testing, total mean loss 1.56310, total acc 0.91730
16:24:57.608 Training @ 471 epoch...
16:24:58.102   Training iter 50, batch loss 1.5611, batch acc 0.9216
16:24:58.580   Training iter 100, batch loss 1.5614, batch acc 0.9204
16:24:59.066   Training iter 150, batch loss 1.5629, batch acc 0.9202
16:24:59.534   Training iter 200, batch loss 1.5616, batch acc 0.9184
16:25:00.003   Training iter 250, batch loss 1.5648, batch acc 0.9224
16:25:00.505   Training iter 300, batch loss 1.5591, batch acc 0.9256
16:25:01.037   Training iter 350, batch loss 1.5603, batch acc 0.9208
16:25:01.626   Training iter 400, batch loss 1.5612, batch acc 0.9198
16:25:02.227   Training iter 450, batch loss 1.5600, batch acc 0.9246
16:25:02.830   Training iter 500, batch loss 1.5591, batch acc 0.9266
16:25:03.369   Training iter 550, batch loss 1.5613, batch acc 0.9208
16:25:03.910   Training iter 600, batch loss 1.5688, batch acc 0.9098
16:25:03.912 Training @ 472 epoch...
16:25:04.448   Training iter 50, batch loss 1.5607, batch acc 0.9196
16:25:04.982   Training iter 100, batch loss 1.5639, batch acc 0.9230
16:25:05.529   Training iter 150, batch loss 1.5649, batch acc 0.9168
16:25:06.067   Training iter 200, batch loss 1.5662, batch acc 0.9190
16:25:06.599   Training iter 250, batch loss 1.5560, batch acc 0.9278
16:25:07.145   Training iter 300, batch loss 1.5587, batch acc 0.9234
16:25:07.683   Training iter 350, batch loss 1.5590, batch acc 0.9222
16:25:08.210   Training iter 400, batch loss 1.5604, batch acc 0.9222
16:25:08.727   Training iter 450, batch loss 1.5599, batch acc 0.9216
16:25:09.275   Training iter 500, batch loss 1.5643, batch acc 0.9144
16:25:09.804   Training iter 550, batch loss 1.5596, batch acc 0.9254
16:25:10.337   Training iter 600, batch loss 1.5678, batch acc 0.9138
16:25:10.339 Training @ 473 epoch...
16:25:10.862   Training iter 50, batch loss 1.5614, batch acc 0.9196
16:25:11.399   Training iter 100, batch loss 1.5567, batch acc 0.9286
16:25:11.946   Training iter 150, batch loss 1.5632, batch acc 0.9196
16:25:12.518   Training iter 200, batch loss 1.5649, batch acc 0.9194
16:25:13.046   Training iter 250, batch loss 1.5602, batch acc 0.9208
16:25:13.571   Training iter 300, batch loss 1.5598, batch acc 0.9188
16:25:14.107   Training iter 350, batch loss 1.5630, batch acc 0.9180
16:25:14.629   Training iter 400, batch loss 1.5613, batch acc 0.9210
16:25:15.105   Training iter 450, batch loss 1.5564, batch acc 0.9290
16:25:15.573   Training iter 500, batch loss 1.5630, batch acc 0.9184
16:25:16.052   Training iter 550, batch loss 1.5641, batch acc 0.9184
16:25:16.544   Training iter 600, batch loss 1.5671, batch acc 0.9148
16:25:16.545 Training @ 474 epoch...
16:25:17.053   Training iter 50, batch loss 1.5648, batch acc 0.9180
16:25:17.562   Training iter 100, batch loss 1.5619, batch acc 0.9206
16:25:18.050   Training iter 150, batch loss 1.5592, batch acc 0.9242
16:25:18.567   Training iter 200, batch loss 1.5632, batch acc 0.9202
16:25:19.073   Training iter 250, batch loss 1.5610, batch acc 0.9208
16:25:19.580   Training iter 300, batch loss 1.5632, batch acc 0.9190
16:25:20.072   Training iter 350, batch loss 1.5645, batch acc 0.9174
16:25:20.559   Training iter 400, batch loss 1.5603, batch acc 0.9250
16:25:21.041   Training iter 450, batch loss 1.5644, batch acc 0.9148
16:25:21.549   Training iter 500, batch loss 1.5560, batch acc 0.9306
16:25:22.081   Training iter 550, batch loss 1.5601, batch acc 0.9242
16:25:22.599   Training iter 600, batch loss 1.5622, batch acc 0.9178
16:25:22.601 Training @ 475 epoch...
16:25:23.117   Training iter 50, batch loss 1.5639, batch acc 0.9200
16:25:23.633   Training iter 100, batch loss 1.5583, batch acc 0.9242
16:25:24.160   Training iter 150, batch loss 1.5616, batch acc 0.9186
16:25:24.736   Training iter 200, batch loss 1.5643, batch acc 0.9180
16:25:25.298   Training iter 250, batch loss 1.5695, batch acc 0.9116
16:25:25.867   Training iter 300, batch loss 1.5591, batch acc 0.9244
16:25:26.434   Training iter 350, batch loss 1.5613, batch acc 0.9220
16:25:26.999   Training iter 400, batch loss 1.5609, batch acc 0.9244
16:25:27.564   Training iter 450, batch loss 1.5581, batch acc 0.9262
16:25:28.097   Training iter 500, batch loss 1.5617, batch acc 0.9222
16:25:28.656   Training iter 550, batch loss 1.5550, batch acc 0.9292
16:25:29.248   Training iter 600, batch loss 1.5671, batch acc 0.9112
16:25:29.250 Testing @ 475 epoch...
16:25:29.302     Testing, total mean loss 1.56298, total acc 0.91640
16:25:29.303 Training @ 476 epoch...
16:25:29.884   Training iter 50, batch loss 1.5638, batch acc 0.9178
16:25:30.442   Training iter 100, batch loss 1.5662, batch acc 0.9180
16:25:30.988   Training iter 150, batch loss 1.5613, batch acc 0.9220
16:25:31.521   Training iter 200, batch loss 1.5588, batch acc 0.9222
16:25:32.043   Training iter 250, batch loss 1.5628, batch acc 0.9188
16:25:32.559   Training iter 300, batch loss 1.5606, batch acc 0.9246
16:25:33.108   Training iter 350, batch loss 1.5660, batch acc 0.9156
16:25:33.651   Training iter 400, batch loss 1.5611, batch acc 0.9190
16:25:34.191   Training iter 450, batch loss 1.5588, batch acc 0.9220
16:25:34.730   Training iter 500, batch loss 1.5633, batch acc 0.9214
16:25:35.248   Training iter 550, batch loss 1.5557, batch acc 0.9278
16:25:35.754   Training iter 600, batch loss 1.5620, batch acc 0.9246
16:25:35.756 Training @ 477 epoch...
16:25:36.272   Training iter 50, batch loss 1.5648, batch acc 0.9180
16:25:36.797   Training iter 100, batch loss 1.5591, batch acc 0.9234
16:25:37.314   Training iter 150, batch loss 1.5591, batch acc 0.9242
16:25:37.820   Training iter 200, batch loss 1.5634, batch acc 0.9172
16:25:38.320   Training iter 250, batch loss 1.5634, batch acc 0.9174
16:25:38.826   Training iter 300, batch loss 1.5611, batch acc 0.9222
16:25:39.344   Training iter 350, batch loss 1.5638, batch acc 0.9180
16:25:39.871   Training iter 400, batch loss 1.5614, batch acc 0.9186
16:25:40.402   Training iter 450, batch loss 1.5595, batch acc 0.9244
16:25:40.950   Training iter 500, batch loss 1.5636, batch acc 0.9214
16:25:41.528   Training iter 550, batch loss 1.5595, batch acc 0.9262
16:25:42.114   Training iter 600, batch loss 1.5616, batch acc 0.9176
16:25:42.116 Training @ 478 epoch...
16:25:42.692   Training iter 50, batch loss 1.5612, batch acc 0.9236
16:25:43.261   Training iter 100, batch loss 1.5604, batch acc 0.9236
16:25:43.826   Training iter 150, batch loss 1.5577, batch acc 0.9240
16:25:44.413   Training iter 200, batch loss 1.5666, batch acc 0.9126
16:25:44.994   Training iter 250, batch loss 1.5571, batch acc 0.9266
16:25:45.561   Training iter 300, batch loss 1.5591, batch acc 0.9238
16:25:46.138   Training iter 350, batch loss 1.5636, batch acc 0.9168
16:25:46.706   Training iter 400, batch loss 1.5639, batch acc 0.9182
16:25:47.237   Training iter 450, batch loss 1.5653, batch acc 0.9146
16:25:47.781   Training iter 500, batch loss 1.5633, batch acc 0.9208
16:25:48.333   Training iter 550, batch loss 1.5616, batch acc 0.9234
16:25:48.887   Training iter 600, batch loss 1.5602, batch acc 0.9244
16:25:48.889 Training @ 479 epoch...
16:25:49.435   Training iter 50, batch loss 1.5625, batch acc 0.9178
16:25:49.974   Training iter 100, batch loss 1.5629, batch acc 0.9198
16:25:50.515   Training iter 150, batch loss 1.5607, batch acc 0.9216
16:25:51.073   Training iter 200, batch loss 1.5642, batch acc 0.9180
16:25:51.617   Training iter 250, batch loss 1.5606, batch acc 0.9182
16:25:52.160   Training iter 300, batch loss 1.5604, batch acc 0.9238
16:25:52.739   Training iter 350, batch loss 1.5659, batch acc 0.9154
16:25:53.298   Training iter 400, batch loss 1.5584, batch acc 0.9254
16:25:53.873   Training iter 450, batch loss 1.5613, batch acc 0.9246
16:25:54.439   Training iter 500, batch loss 1.5661, batch acc 0.9168
16:25:54.990   Training iter 550, batch loss 1.5571, batch acc 0.9260
16:25:55.551   Training iter 600, batch loss 1.5598, batch acc 0.9226
16:25:55.553 Training @ 480 epoch...
16:25:56.115   Training iter 50, batch loss 1.5611, batch acc 0.9206
16:25:56.824   Training iter 100, batch loss 1.5675, batch acc 0.9128
16:25:57.389   Training iter 150, batch loss 1.5606, batch acc 0.9216
16:25:57.964   Training iter 200, batch loss 1.5596, batch acc 0.9222
16:25:58.527   Training iter 250, batch loss 1.5577, batch acc 0.9262
16:25:59.099   Training iter 300, batch loss 1.5652, batch acc 0.9212
16:25:59.679   Training iter 350, batch loss 1.5632, batch acc 0.9184
16:26:00.247   Training iter 400, batch loss 1.5605, batch acc 0.9224
16:26:00.788   Training iter 450, batch loss 1.5597, batch acc 0.9216
16:26:01.309   Training iter 500, batch loss 1.5601, batch acc 0.9230
16:26:01.956   Training iter 550, batch loss 1.5617, batch acc 0.9214
16:26:02.522   Training iter 600, batch loss 1.5625, batch acc 0.9180
16:26:02.524 Testing @ 480 epoch...
16:26:02.572     Testing, total mean loss 1.56306, total acc 0.91840
16:26:02.572 Training @ 481 epoch...
16:26:03.102   Training iter 50, batch loss 1.5620, batch acc 0.9214
16:26:03.660   Training iter 100, batch loss 1.5661, batch acc 0.9126
16:26:04.211   Training iter 150, batch loss 1.5558, batch acc 0.9262
16:26:04.775   Training iter 200, batch loss 1.5603, batch acc 0.9264
16:26:05.334   Training iter 250, batch loss 1.5625, batch acc 0.9216
16:26:05.907   Training iter 300, batch loss 1.5588, batch acc 0.9250
16:26:06.479   Training iter 350, batch loss 1.5585, batch acc 0.9258
16:26:06.980   Training iter 400, batch loss 1.5673, batch acc 0.9154
16:26:07.477   Training iter 450, batch loss 1.5618, batch acc 0.9206
16:26:07.970   Training iter 500, batch loss 1.5609, batch acc 0.9188
16:26:08.461   Training iter 550, batch loss 1.5664, batch acc 0.9164
16:26:08.942   Training iter 600, batch loss 1.5587, batch acc 0.9248
16:26:08.944 Training @ 482 epoch...
16:26:09.440   Training iter 50, batch loss 1.5583, batch acc 0.9246
16:26:09.923   Training iter 100, batch loss 1.5630, batch acc 0.9208
16:26:10.434   Training iter 150, batch loss 1.5611, batch acc 0.9224
16:26:10.920   Training iter 200, batch loss 1.5572, batch acc 0.9236
16:26:11.416   Training iter 250, batch loss 1.5623, batch acc 0.9218
16:26:11.890   Training iter 300, batch loss 1.5602, batch acc 0.9256
16:26:12.370   Training iter 350, batch loss 1.5614, batch acc 0.9240
16:26:12.858   Training iter 400, batch loss 1.5629, batch acc 0.9152
16:26:13.339   Training iter 450, batch loss 1.5651, batch acc 0.9172
16:26:13.870   Training iter 500, batch loss 1.5634, batch acc 0.9186
16:26:14.407   Training iter 550, batch loss 1.5626, batch acc 0.9172
16:26:14.931   Training iter 600, batch loss 1.5609, batch acc 0.9234
16:26:14.933 Training @ 483 epoch...
16:26:15.464   Training iter 50, batch loss 1.5637, batch acc 0.9212
16:26:16.007   Training iter 100, batch loss 1.5632, batch acc 0.9170
16:26:16.548   Training iter 150, batch loss 1.5608, batch acc 0.9240
16:26:17.134   Training iter 200, batch loss 1.5613, batch acc 0.9214
16:26:17.701   Training iter 250, batch loss 1.5582, batch acc 0.9224
16:26:18.257   Training iter 300, batch loss 1.5644, batch acc 0.9198
16:26:18.865   Training iter 350, batch loss 1.5605, batch acc 0.9236
16:26:19.477   Training iter 400, batch loss 1.5625, batch acc 0.9214
16:26:19.992   Training iter 450, batch loss 1.5604, batch acc 0.9196
16:26:20.524   Training iter 500, batch loss 1.5590, batch acc 0.9250
16:26:21.057   Training iter 550, batch loss 1.5646, batch acc 0.9178
16:26:21.588   Training iter 600, batch loss 1.5600, batch acc 0.9210
16:26:21.590 Training @ 484 epoch...
16:26:22.145   Training iter 50, batch loss 1.5578, batch acc 0.9262
16:26:22.682   Training iter 100, batch loss 1.5584, batch acc 0.9260
16:26:23.249   Training iter 150, batch loss 1.5604, batch acc 0.9232
16:26:23.822   Training iter 200, batch loss 1.5685, batch acc 0.9122
16:26:24.396   Training iter 250, batch loss 1.5591, batch acc 0.9232
16:26:24.951   Training iter 300, batch loss 1.5633, batch acc 0.9208
16:26:25.549   Training iter 350, batch loss 1.5644, batch acc 0.9156
16:26:26.117   Training iter 400, batch loss 1.5642, batch acc 0.9202
16:26:26.710   Training iter 450, batch loss 1.5597, batch acc 0.9240
16:26:27.298   Training iter 500, batch loss 1.5602, batch acc 0.9246
16:26:27.875   Training iter 550, batch loss 1.5609, batch acc 0.9202
16:26:28.437   Training iter 600, batch loss 1.5612, batch acc 0.9210
16:26:28.438 Training @ 485 epoch...
16:26:29.005   Training iter 50, batch loss 1.5612, batch acc 0.9162
16:26:29.560   Training iter 100, batch loss 1.5639, batch acc 0.9142
16:26:30.099   Training iter 150, batch loss 1.5612, batch acc 0.9220
16:26:30.626   Training iter 200, batch loss 1.5629, batch acc 0.9192
16:26:31.166   Training iter 250, batch loss 1.5620, batch acc 0.9238
16:26:31.697   Training iter 300, batch loss 1.5564, batch acc 0.9294
16:26:32.230   Training iter 350, batch loss 1.5617, batch acc 0.9208
16:26:32.749   Training iter 400, batch loss 1.5650, batch acc 0.9184
16:26:33.263   Training iter 450, batch loss 1.5623, batch acc 0.9212
16:26:33.784   Training iter 500, batch loss 1.5599, batch acc 0.9264
16:26:34.368   Training iter 550, batch loss 1.5599, batch acc 0.9224
16:26:34.888   Training iter 600, batch loss 1.5615, batch acc 0.9214
16:26:34.890 Testing @ 485 epoch...
16:26:34.938     Testing, total mean loss 1.56278, total acc 0.91650
16:26:34.938 Training @ 486 epoch...
16:26:35.435   Training iter 50, batch loss 1.5593, batch acc 0.9226
16:26:35.917   Training iter 100, batch loss 1.5610, batch acc 0.9226
16:26:36.431   Training iter 150, batch loss 1.5591, batch acc 0.9258
16:26:37.042   Training iter 200, batch loss 1.5641, batch acc 0.9176
16:26:37.650   Training iter 250, batch loss 1.5630, batch acc 0.9224
16:26:38.182   Training iter 300, batch loss 1.5602, batch acc 0.9216
16:26:38.738   Training iter 350, batch loss 1.5600, batch acc 0.9226
16:26:39.323   Training iter 400, batch loss 1.5636, batch acc 0.9170
16:26:39.865   Training iter 450, batch loss 1.5579, batch acc 0.9274
16:26:40.400   Training iter 500, batch loss 1.5646, batch acc 0.9134
16:26:40.941   Training iter 550, batch loss 1.5635, batch acc 0.9190
16:26:41.518   Training iter 600, batch loss 1.5611, batch acc 0.9238
16:26:41.520 Training @ 487 epoch...
16:26:42.120   Training iter 50, batch loss 1.5648, batch acc 0.9180
16:26:42.698   Training iter 100, batch loss 1.5593, batch acc 0.9232
16:26:43.284   Training iter 150, batch loss 1.5575, batch acc 0.9262
16:26:43.870   Training iter 200, batch loss 1.5638, batch acc 0.9192
16:26:44.465   Training iter 250, batch loss 1.5650, batch acc 0.9186
16:26:45.051   Training iter 300, batch loss 1.5648, batch acc 0.9156
16:26:45.611   Training iter 350, batch loss 1.5625, batch acc 0.9198
16:26:46.162   Training iter 400, batch loss 1.5581, batch acc 0.9250
16:26:46.720   Training iter 450, batch loss 1.5612, batch acc 0.9236
16:26:47.272   Training iter 500, batch loss 1.5624, batch acc 0.9200
16:26:47.802   Training iter 550, batch loss 1.5593, batch acc 0.9234
16:26:48.323   Training iter 600, batch loss 1.5586, batch acc 0.9250
16:26:48.325 Training @ 488 epoch...
16:26:48.883   Training iter 50, batch loss 1.5646, batch acc 0.9200
16:26:49.422   Training iter 100, batch loss 1.5591, batch acc 0.9254
16:26:49.966   Training iter 150, batch loss 1.5688, batch acc 0.9086
16:26:50.516   Training iter 200, batch loss 1.5607, batch acc 0.9190
16:26:51.078   Training iter 250, batch loss 1.5607, batch acc 0.9230
16:26:51.591   Training iter 300, batch loss 1.5595, batch acc 0.9270
16:26:52.115   Training iter 350, batch loss 1.5620, batch acc 0.9198
16:26:52.633   Training iter 400, batch loss 1.5604, batch acc 0.9238
16:26:53.148   Training iter 450, batch loss 1.5641, batch acc 0.9190
16:26:53.668   Training iter 500, batch loss 1.5591, batch acc 0.9260
16:26:54.185   Training iter 550, batch loss 1.5604, batch acc 0.9212
16:26:54.707   Training iter 600, batch loss 1.5578, batch acc 0.9240
16:26:54.709 Training @ 489 epoch...
16:26:55.238   Training iter 50, batch loss 1.5626, batch acc 0.9228
16:26:55.760   Training iter 100, batch loss 1.5631, batch acc 0.9192
16:26:56.283   Training iter 150, batch loss 1.5617, batch acc 0.9206
16:26:56.795   Training iter 200, batch loss 1.5591, batch acc 0.9232
16:26:57.340   Training iter 250, batch loss 1.5603, batch acc 0.9248
16:26:57.856   Training iter 300, batch loss 1.5657, batch acc 0.9172
16:26:58.367   Training iter 350, batch loss 1.5588, batch acc 0.9288
16:26:58.892   Training iter 400, batch loss 1.5588, batch acc 0.9240
16:26:59.421   Training iter 450, batch loss 1.5588, batch acc 0.9224
16:26:59.946   Training iter 500, batch loss 1.5617, batch acc 0.9196
16:27:00.480   Training iter 550, batch loss 1.5652, batch acc 0.9202
16:27:01.034   Training iter 600, batch loss 1.5614, batch acc 0.9168
16:27:01.036 Training @ 490 epoch...
16:27:01.624   Training iter 50, batch loss 1.5609, batch acc 0.9236
16:27:02.194   Training iter 100, batch loss 1.5621, batch acc 0.9194
16:27:02.730   Training iter 150, batch loss 1.5627, batch acc 0.9226
16:27:03.276   Training iter 200, batch loss 1.5626, batch acc 0.9190
16:27:03.829   Training iter 250, batch loss 1.5634, batch acc 0.9212
16:27:04.377   Training iter 300, batch loss 1.5586, batch acc 0.9260
16:27:04.921   Training iter 350, batch loss 1.5611, batch acc 0.9214
16:27:05.461   Training iter 400, batch loss 1.5590, batch acc 0.9208
16:27:05.998   Training iter 450, batch loss 1.5654, batch acc 0.9142
16:27:06.544   Training iter 500, batch loss 1.5611, batch acc 0.9218
16:27:07.076   Training iter 550, batch loss 1.5623, batch acc 0.9210
16:27:07.628   Training iter 600, batch loss 1.5574, batch acc 0.9238
16:27:07.630 Testing @ 490 epoch...
16:27:07.679     Testing, total mean loss 1.56293, total acc 0.91670
16:27:07.679 Training @ 491 epoch...
16:27:08.261   Training iter 50, batch loss 1.5647, batch acc 0.9182
16:27:08.810   Training iter 100, batch loss 1.5608, batch acc 0.9232
16:27:09.355   Training iter 150, batch loss 1.5601, batch acc 0.9240
16:27:09.897   Training iter 200, batch loss 1.5623, batch acc 0.9190
16:27:10.450   Training iter 250, batch loss 1.5632, batch acc 0.9194
16:27:10.987   Training iter 300, batch loss 1.5640, batch acc 0.9192
16:27:11.541   Training iter 350, batch loss 1.5616, batch acc 0.9222
16:27:12.100   Training iter 400, batch loss 1.5620, batch acc 0.9206
16:27:12.620   Training iter 450, batch loss 1.5555, batch acc 0.9252
16:27:13.134   Training iter 500, batch loss 1.5599, batch acc 0.9246
16:27:13.658   Training iter 550, batch loss 1.5633, batch acc 0.9184
16:27:14.167   Training iter 600, batch loss 1.5596, batch acc 0.9212
16:27:14.168 Training @ 492 epoch...
16:27:14.693   Training iter 50, batch loss 1.5603, batch acc 0.9244
16:27:15.212   Training iter 100, batch loss 1.5611, batch acc 0.9212
16:27:15.727   Training iter 150, batch loss 1.5594, batch acc 0.9236
16:27:16.306   Training iter 200, batch loss 1.5624, batch acc 0.9202
16:27:16.887   Training iter 250, batch loss 1.5584, batch acc 0.9266
16:27:17.451   Training iter 300, batch loss 1.5616, batch acc 0.9196
16:27:18.013   Training iter 350, batch loss 1.5637, batch acc 0.9240
16:27:18.581   Training iter 400, batch loss 1.5612, batch acc 0.9192
16:27:19.140   Training iter 450, batch loss 1.5656, batch acc 0.9190
16:27:19.698   Training iter 500, batch loss 1.5609, batch acc 0.9216
16:27:20.238   Training iter 550, batch loss 1.5615, batch acc 0.9194
16:27:20.758   Training iter 600, batch loss 1.5602, batch acc 0.9206
16:27:20.759 Training @ 493 epoch...
16:27:21.295   Training iter 50, batch loss 1.5593, batch acc 0.9216
16:27:21.817   Training iter 100, batch loss 1.5597, batch acc 0.9222
16:27:22.318   Training iter 150, batch loss 1.5609, batch acc 0.9242
16:27:22.828   Training iter 200, batch loss 1.5596, batch acc 0.9252
16:27:23.346   Training iter 250, batch loss 1.5616, batch acc 0.9198
16:27:23.847   Training iter 300, batch loss 1.5612, batch acc 0.9206
16:27:24.373   Training iter 350, batch loss 1.5640, batch acc 0.9150
16:27:24.916   Training iter 400, batch loss 1.5618, batch acc 0.9248
16:27:25.466   Training iter 450, batch loss 1.5590, batch acc 0.9238
16:27:26.012   Training iter 500, batch loss 1.5643, batch acc 0.9184
16:27:26.553   Training iter 550, batch loss 1.5636, batch acc 0.9200
16:27:27.091   Training iter 600, batch loss 1.5605, batch acc 0.9234
16:27:27.093 Training @ 494 epoch...
16:27:27.643   Training iter 50, batch loss 1.5580, batch acc 0.9260
16:27:28.196   Training iter 100, batch loss 1.5645, batch acc 0.9180
16:27:28.747   Training iter 150, batch loss 1.5599, batch acc 0.9258
16:27:29.319   Training iter 200, batch loss 1.5672, batch acc 0.9130
16:27:29.880   Training iter 250, batch loss 1.5640, batch acc 0.9194
16:27:30.442   Training iter 300, batch loss 1.5584, batch acc 0.9248
16:27:31.019   Training iter 350, batch loss 1.5612, batch acc 0.9184
16:27:31.572   Training iter 400, batch loss 1.5580, batch acc 0.9252
16:27:32.126   Training iter 450, batch loss 1.5615, batch acc 0.9242
16:27:32.684   Training iter 500, batch loss 1.5615, batch acc 0.9190
16:27:33.233   Training iter 550, batch loss 1.5635, batch acc 0.9222
16:27:33.771   Training iter 600, batch loss 1.5585, batch acc 0.9246
16:27:33.773 Training @ 495 epoch...
16:27:34.333   Training iter 50, batch loss 1.5666, batch acc 0.9194
16:27:34.895   Training iter 100, batch loss 1.5565, batch acc 0.9256
16:27:35.413   Training iter 150, batch loss 1.5620, batch acc 0.9214
16:27:35.915   Training iter 200, batch loss 1.5616, batch acc 0.9198
16:27:36.424   Training iter 250, batch loss 1.5596, batch acc 0.9234
16:27:36.950   Training iter 300, batch loss 1.5629, batch acc 0.9190
16:27:37.494   Training iter 350, batch loss 1.5601, batch acc 0.9248
16:27:38.023   Training iter 400, batch loss 1.5616, batch acc 0.9204
16:27:38.542   Training iter 450, batch loss 1.5628, batch acc 0.9176
16:27:39.058   Training iter 500, batch loss 1.5529, batch acc 0.9322
16:27:39.584   Training iter 550, batch loss 1.5652, batch acc 0.9156
16:27:40.123   Training iter 600, batch loss 1.5635, batch acc 0.9196
16:27:40.125 Testing @ 495 epoch...
16:27:40.174     Testing, total mean loss 1.56261, total acc 0.91790
16:27:40.174 Training @ 496 epoch...
16:27:40.712   Training iter 50, batch loss 1.5652, batch acc 0.9178
16:27:41.245   Training iter 100, batch loss 1.5609, batch acc 0.9188
16:27:41.766   Training iter 150, batch loss 1.5625, batch acc 0.9186
16:27:42.270   Training iter 200, batch loss 1.5612, batch acc 0.9224
16:27:42.758   Training iter 250, batch loss 1.5590, batch acc 0.9242
16:27:43.227   Training iter 300, batch loss 1.5592, batch acc 0.9258
16:27:43.716   Training iter 350, batch loss 1.5622, batch acc 0.9228
16:27:44.238   Training iter 400, batch loss 1.5623, batch acc 0.9192
16:27:44.738   Training iter 450, batch loss 1.5602, batch acc 0.9248
16:27:45.254   Training iter 500, batch loss 1.5668, batch acc 0.9126
16:27:45.723   Training iter 550, batch loss 1.5580, batch acc 0.9222
16:27:46.198   Training iter 600, batch loss 1.5577, batch acc 0.9284
16:27:46.200 Training @ 497 epoch...
16:27:46.707   Training iter 50, batch loss 1.5579, batch acc 0.9276
16:27:47.225   Training iter 100, batch loss 1.5629, batch acc 0.9212
16:27:47.743   Training iter 150, batch loss 1.5660, batch acc 0.9180
16:27:48.289   Training iter 200, batch loss 1.5593, batch acc 0.9226
16:27:48.858   Training iter 250, batch loss 1.5571, batch acc 0.9298
16:27:49.394   Training iter 300, batch loss 1.5611, batch acc 0.9206
16:27:49.935   Training iter 350, batch loss 1.5584, batch acc 0.9216
16:27:50.461   Training iter 400, batch loss 1.5589, batch acc 0.9240
16:27:51.010   Training iter 450, batch loss 1.5649, batch acc 0.9164
16:27:51.561   Training iter 500, batch loss 1.5613, batch acc 0.9200
16:27:52.126   Training iter 550, batch loss 1.5650, batch acc 0.9182
16:27:52.685   Training iter 600, batch loss 1.5622, batch acc 0.9192
16:27:52.687 Training @ 498 epoch...
16:27:53.198   Training iter 50, batch loss 1.5602, batch acc 0.9198
16:27:53.743   Training iter 100, batch loss 1.5558, batch acc 0.9256
16:27:54.290   Training iter 150, batch loss 1.5624, batch acc 0.9208
16:27:54.819   Training iter 200, batch loss 1.5607, batch acc 0.9218
16:27:55.355   Training iter 250, batch loss 1.5633, batch acc 0.9182
16:27:55.884   Training iter 300, batch loss 1.5599, batch acc 0.9220
16:27:56.412   Training iter 350, batch loss 1.5568, batch acc 0.9246
16:27:56.934   Training iter 400, batch loss 1.5630, batch acc 0.9194
16:27:57.455   Training iter 450, batch loss 1.5658, batch acc 0.9162
16:27:57.983   Training iter 500, batch loss 1.5623, batch acc 0.9212
16:27:58.501   Training iter 550, batch loss 1.5613, batch acc 0.9254
16:27:59.037   Training iter 600, batch loss 1.5627, batch acc 0.9226
16:27:59.039 Training @ 499 epoch...
16:27:59.570   Training iter 50, batch loss 1.5573, batch acc 0.9308
16:28:00.096   Training iter 100, batch loss 1.5614, batch acc 0.9218
16:28:00.629   Training iter 150, batch loss 1.5599, batch acc 0.9216
16:28:01.161   Training iter 200, batch loss 1.5606, batch acc 0.9258
16:28:01.662   Training iter 250, batch loss 1.5589, batch acc 0.9228
16:28:02.189   Training iter 300, batch loss 1.5630, batch acc 0.9198
16:28:02.732   Training iter 350, batch loss 1.5630, batch acc 0.9178
16:28:03.236   Training iter 400, batch loss 1.5581, batch acc 0.9246
16:28:03.756   Training iter 450, batch loss 1.5636, batch acc 0.9198
16:28:04.279   Training iter 500, batch loss 1.5612, batch acc 0.9218
16:28:04.809   Training iter 550, batch loss 1.5632, batch acc 0.9164
16:28:05.343   Training iter 600, batch loss 1.5640, batch acc 0.9154
======================================================
16:28:05.345 Testing @ final epoch...
16:28:05.393     Testing, total mean loss 1.56256, total acc 0.91640
training time: 3208 seconds
