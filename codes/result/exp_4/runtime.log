======================================================
learning_rate: 0.001
weight_decay: 0.01
momentum: 0.01
batch_size: 100
max_epoch: 500
disp_freq: 50
test_epoch: 5
plot_epoch: 100
network: Lin-784-10 Sigm
loss: Softmax
result dir: ./result/exp_4
======================================================
12:06:18.140 Training @ 0 epoch...
12:06:18.682   Training iter 50, batch loss 2.2994, batch acc 0.1046
12:06:19.199   Training iter 100, batch loss 2.2969, batch acc 0.1552
12:06:19.718   Training iter 150, batch loss 2.2924, batch acc 0.1980
12:06:20.263   Training iter 200, batch loss 2.2887, batch acc 0.2156
12:06:20.781   Training iter 250, batch loss 2.2859, batch acc 0.2036
12:06:21.290   Training iter 300, batch loss 2.2812, batch acc 0.2154
12:06:21.816   Training iter 350, batch loss 2.2782, batch acc 0.2180
12:06:22.341   Training iter 400, batch loss 2.2753, batch acc 0.2272
12:06:22.869   Training iter 450, batch loss 2.2713, batch acc 0.2404
12:06:23.408   Training iter 500, batch loss 2.2677, batch acc 0.2576
12:06:23.935   Training iter 550, batch loss 2.2643, batch acc 0.2684
12:06:24.452   Training iter 600, batch loss 2.2611, batch acc 0.2794
12:06:24.454 Testing @ 0 epoch...
12:06:24.496     Testing, total mean loss 2.25892, total acc 0.27980
12:06:24.496 Plot @ 0 epoch...
12:06:24.496 Training @ 1 epoch...
12:06:25.023   Training iter 50, batch loss 2.2587, batch acc 0.2798
12:06:25.534   Training iter 100, batch loss 2.2556, batch acc 0.2860
12:06:26.058   Training iter 150, batch loss 2.2505, batch acc 0.3160
12:06:26.586   Training iter 200, batch loss 2.2501, batch acc 0.3022
12:06:27.145   Training iter 250, batch loss 2.2457, batch acc 0.3272
12:06:27.708   Training iter 300, batch loss 2.2437, batch acc 0.3326
12:06:28.275   Training iter 350, batch loss 2.2400, batch acc 0.3656
12:06:28.805   Training iter 400, batch loss 2.2385, batch acc 0.3746
12:06:29.340   Training iter 450, batch loss 2.2338, batch acc 0.3972
12:06:29.875   Training iter 500, batch loss 2.2303, batch acc 0.4038
12:06:30.425   Training iter 550, batch loss 2.2256, batch acc 0.4332
12:06:30.961   Training iter 600, batch loss 2.2248, batch acc 0.4244
12:06:30.963 Training @ 2 epoch...
12:06:31.496   Training iter 50, batch loss 2.2206, batch acc 0.4462
12:06:32.012   Training iter 100, batch loss 2.2199, batch acc 0.4458
12:06:32.520   Training iter 150, batch loss 2.2141, batch acc 0.4736
12:06:33.026   Training iter 200, batch loss 2.2128, batch acc 0.4652
12:06:33.490   Training iter 250, batch loss 2.2094, batch acc 0.4928
12:06:33.949   Training iter 300, batch loss 2.2066, batch acc 0.5002
12:06:34.425   Training iter 350, batch loss 2.2037, batch acc 0.5016
12:06:34.919   Training iter 400, batch loss 2.2026, batch acc 0.5164
12:06:35.416   Training iter 450, batch loss 2.2004, batch acc 0.5156
12:06:35.887   Training iter 500, batch loss 2.1994, batch acc 0.5286
12:06:36.369   Training iter 550, batch loss 2.1952, batch acc 0.5532
12:06:36.876   Training iter 600, batch loss 2.1914, batch acc 0.5634
12:06:36.878 Training @ 3 epoch...
12:06:37.371   Training iter 50, batch loss 2.1874, batch acc 0.6002
12:06:37.877   Training iter 100, batch loss 2.1862, batch acc 0.5778
12:06:38.354   Training iter 150, batch loss 2.1846, batch acc 0.5784
12:06:38.827   Training iter 200, batch loss 2.1806, batch acc 0.5936
12:06:39.321   Training iter 250, batch loss 2.1769, batch acc 0.6032
12:06:39.792   Training iter 300, batch loss 2.1745, batch acc 0.6228
12:06:40.299   Training iter 350, batch loss 2.1730, batch acc 0.6212
12:06:40.771   Training iter 400, batch loss 2.1710, batch acc 0.6150
12:06:41.233   Training iter 450, batch loss 2.1660, batch acc 0.6410
12:06:41.691   Training iter 500, batch loss 2.1669, batch acc 0.6358
12:06:42.205   Training iter 550, batch loss 2.1622, batch acc 0.6396
12:06:42.695   Training iter 600, batch loss 2.1614, batch acc 0.6480
12:06:42.697 Training @ 4 epoch...
12:06:43.197   Training iter 50, batch loss 2.1570, batch acc 0.6518
12:06:43.677   Training iter 100, batch loss 2.1573, batch acc 0.6616
12:06:44.168   Training iter 150, batch loss 2.1540, batch acc 0.6608
12:06:44.652   Training iter 200, batch loss 2.1501, batch acc 0.6604
12:06:45.186   Training iter 250, batch loss 2.1498, batch acc 0.6700
12:06:45.714   Training iter 300, batch loss 2.1460, batch acc 0.6852
12:06:46.238   Training iter 350, batch loss 2.1421, batch acc 0.6842
12:06:46.792   Training iter 400, batch loss 2.1392, batch acc 0.6862
12:06:47.332   Training iter 450, batch loss 2.1411, batch acc 0.6762
12:06:47.870   Training iter 500, batch loss 2.1357, batch acc 0.6810
12:06:48.434   Training iter 550, batch loss 2.1335, batch acc 0.6922
12:06:48.939   Training iter 600, batch loss 2.1326, batch acc 0.6904
12:06:48.941 Training @ 5 epoch...
12:06:49.472   Training iter 50, batch loss 2.1286, batch acc 0.6946
12:06:50.021   Training iter 100, batch loss 2.1270, batch acc 0.6918
12:06:50.567   Training iter 150, batch loss 2.1279, batch acc 0.6990
12:06:51.093   Training iter 200, batch loss 2.1241, batch acc 0.6936
12:06:51.606   Training iter 250, batch loss 2.1236, batch acc 0.6908
12:06:52.120   Training iter 300, batch loss 2.1202, batch acc 0.7028
12:06:52.648   Training iter 350, batch loss 2.1168, batch acc 0.7060
12:06:53.167   Training iter 400, batch loss 2.1143, batch acc 0.7126
12:06:53.685   Training iter 450, batch loss 2.1110, batch acc 0.7104
12:06:54.205   Training iter 500, batch loss 2.1100, batch acc 0.7174
12:06:54.739   Training iter 550, batch loss 2.1086, batch acc 0.7216
12:06:55.265   Training iter 600, batch loss 2.1055, batch acc 0.7220
12:06:55.267 Testing @ 5 epoch...
12:06:55.313     Testing, total mean loss 2.10122, total acc 0.73390
12:06:55.313 Training @ 6 epoch...
12:06:55.807   Training iter 50, batch loss 2.1042, batch acc 0.7110
12:06:56.303   Training iter 100, batch loss 2.1021, batch acc 0.7164
12:06:56.833   Training iter 150, batch loss 2.1010, batch acc 0.7218
12:06:57.352   Training iter 200, batch loss 2.0963, batch acc 0.7366
12:06:57.890   Training iter 250, batch loss 2.0960, batch acc 0.7304
12:06:58.369   Training iter 300, batch loss 2.0972, batch acc 0.7210
12:06:58.830   Training iter 350, batch loss 2.0931, batch acc 0.7258
12:06:59.285   Training iter 400, batch loss 2.0913, batch acc 0.7238
12:06:59.778   Training iter 450, batch loss 2.0903, batch acc 0.7294
12:07:00.275   Training iter 500, batch loss 2.0878, batch acc 0.7340
12:07:00.747   Training iter 550, batch loss 2.0866, batch acc 0.7356
12:07:01.223   Training iter 600, batch loss 2.0808, batch acc 0.7358
12:07:01.225 Training @ 7 epoch...
12:07:01.780   Training iter 50, batch loss 2.0819, batch acc 0.7448
12:07:02.327   Training iter 100, batch loss 2.0824, batch acc 0.7274
12:07:02.833   Training iter 150, batch loss 2.0773, batch acc 0.7386
12:07:03.357   Training iter 200, batch loss 2.0772, batch acc 0.7414
12:07:03.885   Training iter 250, batch loss 2.0761, batch acc 0.7314
12:07:04.440   Training iter 300, batch loss 2.0735, batch acc 0.7430
12:07:04.983   Training iter 350, batch loss 2.0686, batch acc 0.7470
12:07:05.527   Training iter 400, batch loss 2.0701, batch acc 0.7384
12:07:06.069   Training iter 450, batch loss 2.0675, batch acc 0.7470
12:07:06.631   Training iter 500, batch loss 2.0636, batch acc 0.7544
12:07:07.168   Training iter 550, batch loss 2.0637, batch acc 0.7562
12:07:07.711   Training iter 600, batch loss 2.0612, batch acc 0.7456
12:07:07.713 Training @ 8 epoch...
12:07:08.263   Training iter 50, batch loss 2.0590, batch acc 0.7466
12:07:08.787   Training iter 100, batch loss 2.0585, batch acc 0.7546
12:07:09.305   Training iter 150, batch loss 2.0589, batch acc 0.7480
12:07:09.828   Training iter 200, batch loss 2.0546, batch acc 0.7534
12:07:10.368   Training iter 250, batch loss 2.0556, batch acc 0.7574
12:07:10.896   Training iter 300, batch loss 2.0537, batch acc 0.7494
12:07:11.415   Training iter 350, batch loss 2.0507, batch acc 0.7568
12:07:11.932   Training iter 400, batch loss 2.0497, batch acc 0.7598
12:07:12.468   Training iter 450, batch loss 2.0489, batch acc 0.7606
12:07:13.013   Training iter 500, batch loss 2.0472, batch acc 0.7506
12:07:13.553   Training iter 550, batch loss 2.0448, batch acc 0.7632
12:07:14.087   Training iter 600, batch loss 2.0427, batch acc 0.7596
12:07:14.089 Training @ 9 epoch...
12:07:14.616   Training iter 50, batch loss 2.0423, batch acc 0.7568
12:07:15.162   Training iter 100, batch loss 2.0395, batch acc 0.7670
12:07:15.706   Training iter 150, batch loss 2.0424, batch acc 0.7502
12:07:16.242   Training iter 200, batch loss 2.0373, batch acc 0.7630
12:07:16.780   Training iter 250, batch loss 2.0336, batch acc 0.7690
12:07:17.320   Training iter 300, batch loss 2.0328, batch acc 0.7654
12:07:17.860   Training iter 350, batch loss 2.0356, batch acc 0.7606
12:07:18.405   Training iter 400, batch loss 2.0337, batch acc 0.7580
12:07:18.942   Training iter 450, batch loss 2.0309, batch acc 0.7668
12:07:19.475   Training iter 500, batch loss 2.0282, batch acc 0.7652
12:07:20.009   Training iter 550, batch loss 2.0245, batch acc 0.7800
12:07:20.546   Training iter 600, batch loss 2.0271, batch acc 0.7700
12:07:20.547 Training @ 10 epoch...
12:07:21.068   Training iter 50, batch loss 2.0224, batch acc 0.7784
12:07:21.576   Training iter 100, batch loss 2.0232, batch acc 0.7686
12:07:22.077   Training iter 150, batch loss 2.0210, batch acc 0.7744
12:07:22.587   Training iter 200, batch loss 2.0189, batch acc 0.7716
12:07:23.101   Training iter 250, batch loss 2.0219, batch acc 0.7686
12:07:23.613   Training iter 300, batch loss 2.0177, batch acc 0.7716
12:07:24.125   Training iter 350, batch loss 2.0158, batch acc 0.7746
12:07:24.649   Training iter 400, batch loss 2.0150, batch acc 0.7654
12:07:25.174   Training iter 450, batch loss 2.0160, batch acc 0.7658
12:07:25.687   Training iter 500, batch loss 2.0164, batch acc 0.7660
12:07:26.198   Training iter 550, batch loss 2.0140, batch acc 0.7670
12:07:26.709   Training iter 600, batch loss 2.0088, batch acc 0.7762
12:07:26.710 Testing @ 10 epoch...
12:07:26.753     Testing, total mean loss 2.00398, total acc 0.78850
12:07:26.753 Training @ 11 epoch...
12:07:27.286   Training iter 50, batch loss 2.0105, batch acc 0.7690
12:07:27.805   Training iter 100, batch loss 2.0084, batch acc 0.7704
12:07:28.336   Training iter 150, batch loss 2.0065, batch acc 0.7792
12:07:28.856   Training iter 200, batch loss 2.0061, batch acc 0.7686
12:07:29.415   Training iter 250, batch loss 2.0051, batch acc 0.7736
12:07:29.977   Training iter 300, batch loss 2.0023, batch acc 0.7832
12:07:30.541   Training iter 350, batch loss 2.0034, batch acc 0.7766
12:07:31.066   Training iter 400, batch loss 2.0001, batch acc 0.7708
12:07:31.610   Training iter 450, batch loss 1.9958, batch acc 0.7906
12:07:32.172   Training iter 500, batch loss 1.9976, batch acc 0.7898
12:07:32.727   Training iter 550, batch loss 1.9976, batch acc 0.7732
12:07:33.278   Training iter 600, batch loss 1.9984, batch acc 0.7782
12:07:33.280 Training @ 12 epoch...
12:07:33.796   Training iter 50, batch loss 1.9957, batch acc 0.7778
12:07:34.323   Training iter 100, batch loss 1.9958, batch acc 0.7746
12:07:34.833   Training iter 150, batch loss 1.9930, batch acc 0.7798
12:07:35.348   Training iter 200, batch loss 1.9937, batch acc 0.7762
12:07:35.852   Training iter 250, batch loss 1.9891, batch acc 0.7814
12:07:36.376   Training iter 300, batch loss 1.9890, batch acc 0.7860
12:07:36.885   Training iter 350, batch loss 1.9891, batch acc 0.7744
12:07:37.400   Training iter 400, batch loss 1.9871, batch acc 0.7842
12:07:37.893   Training iter 450, batch loss 1.9851, batch acc 0.7882
12:07:38.397   Training iter 500, batch loss 1.9839, batch acc 0.7862
12:07:38.886   Training iter 550, batch loss 1.9851, batch acc 0.7808
12:07:39.376   Training iter 600, batch loss 1.9815, batch acc 0.7920
12:07:39.378 Training @ 13 epoch...
12:07:39.868   Training iter 50, batch loss 1.9804, batch acc 0.7918
12:07:40.352   Training iter 100, batch loss 1.9825, batch acc 0.7816
12:07:40.836   Training iter 150, batch loss 1.9769, batch acc 0.7974
12:07:41.318   Training iter 200, batch loss 1.9782, batch acc 0.7784
12:07:41.796   Training iter 250, batch loss 1.9784, batch acc 0.7828
12:07:42.282   Training iter 300, batch loss 1.9754, batch acc 0.7880
12:07:42.761   Training iter 350, batch loss 1.9765, batch acc 0.7890
12:07:43.261   Training iter 400, batch loss 1.9743, batch acc 0.7850
12:07:43.765   Training iter 450, batch loss 1.9734, batch acc 0.7826
12:07:44.252   Training iter 500, batch loss 1.9762, batch acc 0.7784
12:07:44.780   Training iter 550, batch loss 1.9739, batch acc 0.7934
12:07:45.304   Training iter 600, batch loss 1.9719, batch acc 0.7784
12:07:45.306 Training @ 14 epoch...
12:07:45.802   Training iter 50, batch loss 1.9678, batch acc 0.7982
12:07:46.298   Training iter 100, batch loss 1.9704, batch acc 0.7892
12:07:46.780   Training iter 150, batch loss 1.9686, batch acc 0.7884
12:07:47.258   Training iter 200, batch loss 1.9675, batch acc 0.7946
12:07:47.748   Training iter 250, batch loss 1.9695, batch acc 0.7822
12:07:48.257   Training iter 300, batch loss 1.9639, batch acc 0.7964
12:07:48.749   Training iter 350, batch loss 1.9646, batch acc 0.7906
12:07:49.237   Training iter 400, batch loss 1.9636, batch acc 0.7866
12:07:49.750   Training iter 450, batch loss 1.9621, batch acc 0.7920
12:07:50.264   Training iter 500, batch loss 1.9633, batch acc 0.7838
12:07:50.791   Training iter 550, batch loss 1.9601, batch acc 0.7930
12:07:51.310   Training iter 600, batch loss 1.9590, batch acc 0.7992
12:07:51.312 Training @ 15 epoch...
12:07:51.864   Training iter 50, batch loss 1.9610, batch acc 0.7830
12:07:52.504   Training iter 100, batch loss 1.9579, batch acc 0.7930
12:07:53.094   Training iter 150, batch loss 1.9537, batch acc 0.8034
12:07:53.676   Training iter 200, batch loss 1.9568, batch acc 0.7940
12:07:54.227   Training iter 250, batch loss 1.9550, batch acc 0.7914
12:07:54.816   Training iter 300, batch loss 1.9551, batch acc 0.7930
12:07:55.410   Training iter 350, batch loss 1.9558, batch acc 0.7836
12:07:55.938   Training iter 400, batch loss 1.9534, batch acc 0.7870
12:07:56.520   Training iter 450, batch loss 1.9522, batch acc 0.7942
12:07:57.041   Training iter 500, batch loss 1.9491, batch acc 0.8032
12:07:57.582   Training iter 550, batch loss 1.9530, batch acc 0.7928
12:07:58.117   Training iter 600, batch loss 1.9503, batch acc 0.8006
12:07:58.119 Testing @ 15 epoch...
12:07:58.165     Testing, total mean loss 1.94278, total acc 0.80770
12:07:58.165 Training @ 16 epoch...
12:07:58.694   Training iter 50, batch loss 1.9523, batch acc 0.7894
12:07:59.212   Training iter 100, batch loss 1.9451, batch acc 0.8018
12:07:59.724   Training iter 150, batch loss 1.9464, batch acc 0.7954
12:08:00.259   Training iter 200, batch loss 1.9453, batch acc 0.8012
12:08:00.763   Training iter 250, batch loss 1.9471, batch acc 0.7930
12:08:01.267   Training iter 300, batch loss 1.9473, batch acc 0.7944
12:08:01.813   Training iter 350, batch loss 1.9447, batch acc 0.7910
12:08:02.338   Training iter 400, batch loss 1.9411, batch acc 0.7992
12:08:02.851   Training iter 450, batch loss 1.9438, batch acc 0.7968
12:08:03.396   Training iter 500, batch loss 1.9385, batch acc 0.8062
12:08:03.933   Training iter 550, batch loss 1.9426, batch acc 0.7920
12:08:04.465   Training iter 600, batch loss 1.9419, batch acc 0.7912
12:08:04.467 Training @ 17 epoch...
12:08:04.999   Training iter 50, batch loss 1.9389, batch acc 0.8020
12:08:05.552   Training iter 100, batch loss 1.9395, batch acc 0.7894
12:08:06.063   Training iter 150, batch loss 1.9365, batch acc 0.8006
12:08:06.608   Training iter 200, batch loss 1.9406, batch acc 0.7956
12:08:07.103   Training iter 250, batch loss 1.9397, batch acc 0.7964
12:08:07.604   Training iter 300, batch loss 1.9318, batch acc 0.8070
12:08:08.106   Training iter 350, batch loss 1.9326, batch acc 0.8024
12:08:08.599   Training iter 400, batch loss 1.9362, batch acc 0.7976
12:08:09.075   Training iter 450, batch loss 1.9318, batch acc 0.8014
12:08:09.570   Training iter 500, batch loss 1.9318, batch acc 0.8020
12:08:10.073   Training iter 550, batch loss 1.9372, batch acc 0.7964
12:08:10.598   Training iter 600, batch loss 1.9312, batch acc 0.7986
12:08:10.600 Training @ 18 epoch...
12:08:11.098   Training iter 50, batch loss 1.9313, batch acc 0.7962
12:08:11.595   Training iter 100, batch loss 1.9278, batch acc 0.8096
12:08:12.097   Training iter 150, batch loss 1.9304, batch acc 0.8026
12:08:12.603   Training iter 200, batch loss 1.9292, batch acc 0.8002
12:08:13.137   Training iter 250, batch loss 1.9350, batch acc 0.7856
12:08:13.665   Training iter 300, batch loss 1.9281, batch acc 0.7974
12:08:14.434   Training iter 350, batch loss 1.9257, batch acc 0.8068
12:08:15.309   Training iter 400, batch loss 1.9254, batch acc 0.8070
12:08:15.828   Training iter 450, batch loss 1.9267, batch acc 0.7958
12:08:16.368   Training iter 500, batch loss 1.9221, batch acc 0.8096
12:08:16.895   Training iter 550, batch loss 1.9194, batch acc 0.8092
12:08:17.425   Training iter 600, batch loss 1.9261, batch acc 0.7994
12:08:17.427 Training @ 19 epoch...
12:08:17.962   Training iter 50, batch loss 1.9217, batch acc 0.8124
12:08:18.493   Training iter 100, batch loss 1.9256, batch acc 0.7966
12:08:19.044   Training iter 150, batch loss 1.9229, batch acc 0.8004
12:08:19.597   Training iter 200, batch loss 1.9203, batch acc 0.8034
12:08:20.157   Training iter 250, batch loss 1.9217, batch acc 0.8034
12:08:20.706   Training iter 300, batch loss 1.9233, batch acc 0.7992
12:08:21.264   Training iter 350, batch loss 1.9137, batch acc 0.8114
12:08:21.824   Training iter 400, batch loss 1.9163, batch acc 0.8096
12:08:22.384   Training iter 450, batch loss 1.9173, batch acc 0.8020
12:08:22.939   Training iter 500, batch loss 1.9151, batch acc 0.8060
12:08:23.499   Training iter 550, batch loss 1.9157, batch acc 0.8006
12:08:24.057   Training iter 600, batch loss 1.9196, batch acc 0.7966
12:08:24.059 Training @ 20 epoch...
12:08:24.614   Training iter 50, batch loss 1.9137, batch acc 0.8026
12:08:25.158   Training iter 100, batch loss 1.9153, batch acc 0.8034
12:08:25.691   Training iter 150, batch loss 1.9136, batch acc 0.8094
12:08:26.224   Training iter 200, batch loss 1.9150, batch acc 0.7970
12:08:26.748   Training iter 250, batch loss 1.9152, batch acc 0.7978
12:08:27.303   Training iter 300, batch loss 1.9107, batch acc 0.8102
12:08:27.891   Training iter 350, batch loss 1.9109, batch acc 0.8072
12:08:28.477   Training iter 400, batch loss 1.9139, batch acc 0.8074
12:08:29.058   Training iter 450, batch loss 1.9089, batch acc 0.8088
12:08:29.586   Training iter 500, batch loss 1.9138, batch acc 0.7996
12:08:30.107   Training iter 550, batch loss 1.9088, batch acc 0.8120
12:08:30.644   Training iter 600, batch loss 1.9060, batch acc 0.8140
12:08:30.646 Testing @ 20 epoch...
12:08:30.692     Testing, total mean loss 1.90167, total acc 0.81690
12:08:30.692 Training @ 21 epoch...
12:08:31.237   Training iter 50, batch loss 1.9067, batch acc 0.8084
12:08:31.756   Training iter 100, batch loss 1.9112, batch acc 0.8040
12:08:32.266   Training iter 150, batch loss 1.9083, batch acc 0.8022
12:08:32.768   Training iter 200, batch loss 1.9095, batch acc 0.8034
12:08:33.271   Training iter 250, batch loss 1.9046, batch acc 0.8150
12:08:33.774   Training iter 300, batch loss 1.9069, batch acc 0.8098
12:08:34.274   Training iter 350, batch loss 1.9010, batch acc 0.8196
12:08:34.812   Training iter 400, batch loss 1.9052, batch acc 0.8028
12:08:35.340   Training iter 450, batch loss 1.9046, batch acc 0.8074
12:08:35.871   Training iter 500, batch loss 1.9024, batch acc 0.8030
12:08:36.406   Training iter 550, batch loss 1.9030, batch acc 0.8060
12:08:36.928   Training iter 600, batch loss 1.9008, batch acc 0.8044
12:08:36.930 Training @ 22 epoch...
12:08:37.473   Training iter 50, batch loss 1.9044, batch acc 0.8088
12:08:37.996   Training iter 100, batch loss 1.9002, batch acc 0.8110
12:08:38.529   Training iter 150, batch loss 1.8980, batch acc 0.8162
12:08:39.090   Training iter 200, batch loss 1.9012, batch acc 0.8096
12:08:39.657   Training iter 250, batch loss 1.8986, batch acc 0.8038
12:08:40.215   Training iter 300, batch loss 1.9004, batch acc 0.8100
12:08:40.767   Training iter 350, batch loss 1.9021, batch acc 0.8004
12:08:41.307   Training iter 400, batch loss 1.8998, batch acc 0.8094
12:08:41.840   Training iter 450, batch loss 1.8954, batch acc 0.8110
12:08:42.380   Training iter 500, batch loss 1.8932, batch acc 0.8106
12:08:42.920   Training iter 550, batch loss 1.8961, batch acc 0.8090
12:08:43.450   Training iter 600, batch loss 1.8986, batch acc 0.8022
12:08:43.452 Training @ 23 epoch...
12:08:43.982   Training iter 50, batch loss 1.8979, batch acc 0.8050
12:08:44.523   Training iter 100, batch loss 1.8935, batch acc 0.8120
12:08:45.047   Training iter 150, batch loss 1.8964, batch acc 0.8066
12:08:45.563   Training iter 200, batch loss 1.8948, batch acc 0.8134
12:08:46.075   Training iter 250, batch loss 1.8944, batch acc 0.8082
12:08:46.594   Training iter 300, batch loss 1.8921, batch acc 0.8164
12:08:47.115   Training iter 350, batch loss 1.8934, batch acc 0.8054
12:08:47.635   Training iter 400, batch loss 1.8914, batch acc 0.8128
12:08:48.142   Training iter 450, batch loss 1.8890, batch acc 0.8148
12:08:48.668   Training iter 500, batch loss 1.8917, batch acc 0.8054
12:08:49.204   Training iter 550, batch loss 1.8877, batch acc 0.8192
12:08:49.738   Training iter 600, batch loss 1.8937, batch acc 0.8008
12:08:49.740 Training @ 24 epoch...
12:08:50.258   Training iter 50, batch loss 1.8916, batch acc 0.8050
12:08:50.751   Training iter 100, batch loss 1.8897, batch acc 0.8122
12:08:51.250   Training iter 150, batch loss 1.8897, batch acc 0.8054
12:08:51.758   Training iter 200, batch loss 1.8892, batch acc 0.8086
12:08:52.264   Training iter 250, batch loss 1.8874, batch acc 0.8174
12:08:52.764   Training iter 300, batch loss 1.8899, batch acc 0.8010
12:08:53.268   Training iter 350, batch loss 1.8878, batch acc 0.8186
12:08:53.766   Training iter 400, batch loss 1.8862, batch acc 0.8092
12:08:54.266   Training iter 450, batch loss 1.8834, batch acc 0.8212
12:08:54.787   Training iter 500, batch loss 1.8824, batch acc 0.8194
12:08:55.303   Training iter 550, batch loss 1.8853, batch acc 0.8088
12:08:55.805   Training iter 600, batch loss 1.8858, batch acc 0.8132
12:08:55.807 Training @ 25 epoch...
12:08:56.322   Training iter 50, batch loss 1.8824, batch acc 0.8176
12:08:56.830   Training iter 100, batch loss 1.8824, batch acc 0.8140
12:08:57.360   Training iter 150, batch loss 1.8866, batch acc 0.8060
12:08:57.883   Training iter 200, batch loss 1.8820, batch acc 0.8084
12:08:58.415   Training iter 250, batch loss 1.8869, batch acc 0.8076
12:08:58.924   Training iter 300, batch loss 1.8851, batch acc 0.8038
12:08:59.435   Training iter 350, batch loss 1.8816, batch acc 0.8148
12:08:59.942   Training iter 400, batch loss 1.8771, batch acc 0.8206
12:09:00.454   Training iter 450, batch loss 1.8841, batch acc 0.8110
12:09:00.928   Training iter 500, batch loss 1.8779, batch acc 0.8206
12:09:01.400   Training iter 550, batch loss 1.8811, batch acc 0.8070
12:09:01.924   Training iter 600, batch loss 1.8778, batch acc 0.8190
12:09:01.926 Testing @ 25 epoch...
12:09:01.972     Testing, total mean loss 1.87231, total acc 0.82370
12:09:01.972 Training @ 26 epoch...
12:09:02.519   Training iter 50, batch loss 1.8798, batch acc 0.8140
12:09:03.043   Training iter 100, batch loss 1.8789, batch acc 0.8122
12:09:03.570   Training iter 150, batch loss 1.8775, batch acc 0.8178
12:09:04.098   Training iter 200, batch loss 1.8795, batch acc 0.8140
12:09:04.625   Training iter 250, batch loss 1.8772, batch acc 0.8190
12:09:05.141   Training iter 300, batch loss 1.8759, batch acc 0.8124
12:09:05.660   Training iter 350, batch loss 1.8761, batch acc 0.8212
12:09:06.162   Training iter 400, batch loss 1.8759, batch acc 0.8104
12:09:06.704   Training iter 450, batch loss 1.8821, batch acc 0.8016
12:09:07.247   Training iter 500, batch loss 1.8767, batch acc 0.8088
12:09:07.793   Training iter 550, batch loss 1.8714, batch acc 0.8176
12:09:08.344   Training iter 600, batch loss 1.8741, batch acc 0.8220
12:09:08.346 Training @ 27 epoch...
12:09:08.901   Training iter 50, batch loss 1.8750, batch acc 0.8106
12:09:09.450   Training iter 100, batch loss 1.8747, batch acc 0.8132
12:09:09.993   Training iter 150, batch loss 1.8702, batch acc 0.8220
12:09:10.555   Training iter 200, batch loss 1.8691, batch acc 0.8278
12:09:11.113   Training iter 250, batch loss 1.8734, batch acc 0.8120
12:09:11.637   Training iter 300, batch loss 1.8729, batch acc 0.8094
12:09:12.163   Training iter 350, batch loss 1.8745, batch acc 0.8066
12:09:12.689   Training iter 400, batch loss 1.8729, batch acc 0.8160
12:09:13.206   Training iter 450, batch loss 1.8688, batch acc 0.8162
12:09:13.709   Training iter 500, batch loss 1.8734, batch acc 0.8122
12:09:14.214   Training iter 550, batch loss 1.8702, batch acc 0.8192
12:09:14.733   Training iter 600, batch loss 1.8732, batch acc 0.8172
12:09:14.735 Training @ 28 epoch...
12:09:15.271   Training iter 50, batch loss 1.8697, batch acc 0.8122
12:09:15.778   Training iter 100, batch loss 1.8702, batch acc 0.8128
12:09:16.278   Training iter 150, batch loss 1.8670, batch acc 0.8160
12:09:16.786   Training iter 200, batch loss 1.8684, batch acc 0.8150
12:09:17.304   Training iter 250, batch loss 1.8677, batch acc 0.8164
12:09:17.830   Training iter 300, batch loss 1.8665, batch acc 0.8144
12:09:18.353   Training iter 350, batch loss 1.8707, batch acc 0.8134
12:09:18.869   Training iter 400, batch loss 1.8668, batch acc 0.8184
12:09:19.386   Training iter 450, batch loss 1.8703, batch acc 0.8136
12:09:19.920   Training iter 500, batch loss 1.8645, batch acc 0.8246
12:09:20.447   Training iter 550, batch loss 1.8676, batch acc 0.8200
12:09:20.968   Training iter 600, batch loss 1.8653, batch acc 0.8158
12:09:20.970 Training @ 29 epoch...
12:09:21.496   Training iter 50, batch loss 1.8650, batch acc 0.8166
12:09:22.033   Training iter 100, batch loss 1.8653, batch acc 0.8120
12:09:22.581   Training iter 150, batch loss 1.8692, batch acc 0.8030
12:09:23.139   Training iter 200, batch loss 1.8650, batch acc 0.8152
12:09:23.687   Training iter 250, batch loss 1.8657, batch acc 0.8156
12:09:24.250   Training iter 300, batch loss 1.8634, batch acc 0.8210
12:09:24.799   Training iter 350, batch loss 1.8595, batch acc 0.8190
12:09:25.371   Training iter 400, batch loss 1.8691, batch acc 0.8076
12:09:25.953   Training iter 450, batch loss 1.8621, batch acc 0.8236
12:09:26.529   Training iter 500, batch loss 1.8598, batch acc 0.8254
12:09:27.116   Training iter 550, batch loss 1.8630, batch acc 0.8156
12:09:27.677   Training iter 600, batch loss 1.8571, batch acc 0.8316
12:09:27.679 Training @ 30 epoch...
12:09:28.297   Training iter 50, batch loss 1.8584, batch acc 0.8272
12:09:28.873   Training iter 100, batch loss 1.8636, batch acc 0.8144
12:09:29.426   Training iter 150, batch loss 1.8580, batch acc 0.8330
12:09:29.972   Training iter 200, batch loss 1.8629, batch acc 0.8130
12:09:30.501   Training iter 250, batch loss 1.8588, batch acc 0.8198
12:09:31.009   Training iter 300, batch loss 1.8615, batch acc 0.8084
12:09:31.518   Training iter 350, batch loss 1.8600, batch acc 0.8152
12:09:32.021   Training iter 400, batch loss 1.8585, batch acc 0.8204
12:09:32.533   Training iter 450, batch loss 1.8597, batch acc 0.8156
12:09:33.063   Training iter 500, batch loss 1.8560, batch acc 0.8216
12:09:33.588   Training iter 550, batch loss 1.8596, batch acc 0.8088
12:09:34.089   Training iter 600, batch loss 1.8587, batch acc 0.8190
12:09:34.090 Testing @ 30 epoch...
12:09:34.137     Testing, total mean loss 1.85038, total acc 0.83030
12:09:34.137 Training @ 31 epoch...
12:09:34.664   Training iter 50, batch loss 1.8579, batch acc 0.8194
12:09:35.183   Training iter 100, batch loss 1.8547, batch acc 0.8260
12:09:35.688   Training iter 150, batch loss 1.8565, batch acc 0.8238
12:09:36.205   Training iter 200, batch loss 1.8553, batch acc 0.8248
12:09:36.711   Training iter 250, batch loss 1.8540, batch acc 0.8224
12:09:37.231   Training iter 300, batch loss 1.8556, batch acc 0.8214
12:09:37.720   Training iter 350, batch loss 1.8601, batch acc 0.8122
12:09:38.213   Training iter 400, batch loss 1.8594, batch acc 0.8030
12:09:38.704   Training iter 450, batch loss 1.8545, batch acc 0.8228
12:09:39.212   Training iter 500, batch loss 1.8566, batch acc 0.8156
12:09:39.748   Training iter 550, batch loss 1.8539, batch acc 0.8136
12:09:40.291   Training iter 600, batch loss 1.8514, batch acc 0.8264
12:09:40.292 Training @ 32 epoch...
12:09:40.827   Training iter 50, batch loss 1.8535, batch acc 0.8204
12:09:41.365   Training iter 100, batch loss 1.8523, batch acc 0.8254
12:09:41.903   Training iter 150, batch loss 1.8534, batch acc 0.8180
12:09:42.444   Training iter 200, batch loss 1.8527, batch acc 0.8160
12:09:42.983   Training iter 250, batch loss 1.8539, batch acc 0.8192
12:09:43.527   Training iter 300, batch loss 1.8547, batch acc 0.8222
12:09:44.044   Training iter 350, batch loss 1.8529, batch acc 0.8158
12:09:44.546   Training iter 400, batch loss 1.8516, batch acc 0.8184
12:09:45.049   Training iter 450, batch loss 1.8525, batch acc 0.8206
12:09:45.533   Training iter 500, batch loss 1.8514, batch acc 0.8266
12:09:46.011   Training iter 550, batch loss 1.8507, batch acc 0.8170
12:09:46.523   Training iter 600, batch loss 1.8469, batch acc 0.8276
12:09:46.525 Training @ 33 epoch...
12:09:47.024   Training iter 50, batch loss 1.8509, batch acc 0.8228
12:09:47.527   Training iter 100, batch loss 1.8550, batch acc 0.8088
12:09:48.039   Training iter 150, batch loss 1.8521, batch acc 0.8184
12:09:48.544   Training iter 200, batch loss 1.8491, batch acc 0.8258
12:09:49.064   Training iter 250, batch loss 1.8488, batch acc 0.8184
12:09:49.583   Training iter 300, batch loss 1.8482, batch acc 0.8294
12:09:50.106   Training iter 350, batch loss 1.8470, batch acc 0.8256
12:09:50.622   Training iter 400, batch loss 1.8487, batch acc 0.8238
12:09:51.131   Training iter 450, batch loss 1.8509, batch acc 0.8122
12:09:51.638   Training iter 500, batch loss 1.8467, batch acc 0.8200
12:09:52.138   Training iter 550, batch loss 1.8444, batch acc 0.8210
12:09:52.613   Training iter 600, batch loss 1.8429, batch acc 0.8296
12:09:52.614 Training @ 34 epoch...
12:09:53.081   Training iter 50, batch loss 1.8488, batch acc 0.8178
12:09:53.548   Training iter 100, batch loss 1.8460, batch acc 0.8240
12:09:54.008   Training iter 150, batch loss 1.8466, batch acc 0.8268
12:09:54.487   Training iter 200, batch loss 1.8450, batch acc 0.8218
12:09:54.995   Training iter 250, batch loss 1.8491, batch acc 0.8104
12:09:55.512   Training iter 300, batch loss 1.8452, batch acc 0.8198
12:09:56.022   Training iter 350, batch loss 1.8472, batch acc 0.8158
12:09:56.506   Training iter 400, batch loss 1.8429, batch acc 0.8232
12:09:56.993   Training iter 450, batch loss 1.8393, batch acc 0.8320
12:09:57.506   Training iter 500, batch loss 1.8455, batch acc 0.8232
12:09:58.038   Training iter 550, batch loss 1.8439, batch acc 0.8224
12:09:58.555   Training iter 600, batch loss 1.8457, batch acc 0.8250
12:09:58.557 Training @ 35 epoch...
12:09:59.084   Training iter 50, batch loss 1.8431, batch acc 0.8210
12:09:59.614   Training iter 100, batch loss 1.8450, batch acc 0.8200
12:10:00.168   Training iter 150, batch loss 1.8427, batch acc 0.8220
12:10:00.733   Training iter 200, batch loss 1.8426, batch acc 0.8278
12:10:01.277   Training iter 250, batch loss 1.8417, batch acc 0.8258
12:10:01.847   Training iter 300, batch loss 1.8413, batch acc 0.8226
12:10:02.421   Training iter 350, batch loss 1.8430, batch acc 0.8220
12:10:02.940   Training iter 400, batch loss 1.8427, batch acc 0.8244
12:10:03.447   Training iter 450, batch loss 1.8412, batch acc 0.8250
12:10:03.969   Training iter 500, batch loss 1.8411, batch acc 0.8226
12:10:04.494   Training iter 550, batch loss 1.8423, batch acc 0.8188
12:10:05.032   Training iter 600, batch loss 1.8407, batch acc 0.8252
12:10:05.034 Testing @ 35 epoch...
12:10:05.080     Testing, total mean loss 1.83337, total acc 0.83400
12:10:05.081 Training @ 36 epoch...
12:10:05.622   Training iter 50, batch loss 1.8411, batch acc 0.8174
12:10:06.155   Training iter 100, batch loss 1.8390, batch acc 0.8246
12:10:06.652   Training iter 150, batch loss 1.8415, batch acc 0.8238
12:10:07.161   Training iter 200, batch loss 1.8385, batch acc 0.8246
12:10:07.644   Training iter 250, batch loss 1.8417, batch acc 0.8186
12:10:08.178   Training iter 300, batch loss 1.8378, batch acc 0.8248
12:10:08.722   Training iter 350, batch loss 1.8433, batch acc 0.8114
12:10:09.233   Training iter 400, batch loss 1.8357, batch acc 0.8292
12:10:09.717   Training iter 450, batch loss 1.8356, batch acc 0.8238
12:10:10.203   Training iter 500, batch loss 1.8373, batch acc 0.8312
12:10:10.678   Training iter 550, batch loss 1.8394, batch acc 0.8308
12:10:11.165   Training iter 600, batch loss 1.8404, batch acc 0.8212
12:10:11.167 Training @ 37 epoch...
12:10:11.656   Training iter 50, batch loss 1.8361, batch acc 0.8256
12:10:12.157   Training iter 100, batch loss 1.8405, batch acc 0.8222
12:10:12.644   Training iter 150, batch loss 1.8377, batch acc 0.8232
12:10:13.139   Training iter 200, batch loss 1.8348, batch acc 0.8366
12:10:13.636   Training iter 250, batch loss 1.8402, batch acc 0.8170
12:10:14.142   Training iter 300, batch loss 1.8347, batch acc 0.8176
12:10:14.629   Training iter 350, batch loss 1.8339, batch acc 0.8250
12:10:15.103   Training iter 400, batch loss 1.8378, batch acc 0.8226
12:10:15.577   Training iter 450, batch loss 1.8355, batch acc 0.8310
12:10:16.059   Training iter 500, batch loss 1.8350, batch acc 0.8246
12:10:16.556   Training iter 550, batch loss 1.8371, batch acc 0.8216
12:10:17.052   Training iter 600, batch loss 1.8335, batch acc 0.8252
12:10:17.054 Training @ 38 epoch...
12:10:17.530   Training iter 50, batch loss 1.8374, batch acc 0.8158
12:10:18.007   Training iter 100, batch loss 1.8366, batch acc 0.8222
12:10:18.499   Training iter 150, batch loss 1.8373, batch acc 0.8216
12:10:18.963   Training iter 200, batch loss 1.8358, batch acc 0.8266
12:10:19.427   Training iter 250, batch loss 1.8315, batch acc 0.8296
12:10:19.937   Training iter 300, batch loss 1.8380, batch acc 0.8188
12:10:20.488   Training iter 350, batch loss 1.8300, batch acc 0.8278
12:10:21.019   Training iter 400, batch loss 1.8342, batch acc 0.8216
12:10:21.554   Training iter 450, batch loss 1.8295, batch acc 0.8352
12:10:22.064   Training iter 500, batch loss 1.8294, batch acc 0.8286
12:10:22.577   Training iter 550, batch loss 1.8334, batch acc 0.8252
12:10:23.090   Training iter 600, batch loss 1.8307, batch acc 0.8240
12:10:23.092 Training @ 39 epoch...
12:10:23.614   Training iter 50, batch loss 1.8326, batch acc 0.8264
12:10:24.132   Training iter 100, batch loss 1.8370, batch acc 0.8172
12:10:24.647   Training iter 150, batch loss 1.8292, batch acc 0.8282
12:10:25.155   Training iter 200, batch loss 1.8314, batch acc 0.8240
12:10:25.680   Training iter 250, batch loss 1.8331, batch acc 0.8242
12:10:26.212   Training iter 300, batch loss 1.8265, batch acc 0.8340
12:10:26.725   Training iter 350, batch loss 1.8347, batch acc 0.8244
12:10:27.220   Training iter 400, batch loss 1.8317, batch acc 0.8212
12:10:27.747   Training iter 450, batch loss 1.8270, batch acc 0.8336
12:10:28.265   Training iter 500, batch loss 1.8311, batch acc 0.8184
12:10:28.790   Training iter 550, batch loss 1.8271, batch acc 0.8256
12:10:29.298   Training iter 600, batch loss 1.8304, batch acc 0.8298
12:10:29.300 Training @ 40 epoch...
12:10:29.821   Training iter 50, batch loss 1.8369, batch acc 0.8160
12:10:30.339   Training iter 100, batch loss 1.8278, batch acc 0.8312
12:10:30.859   Training iter 150, batch loss 1.8274, batch acc 0.8344
12:10:31.371   Training iter 200, batch loss 1.8288, batch acc 0.8274
12:10:31.877   Training iter 250, batch loss 1.8290, batch acc 0.8262
12:10:32.386   Training iter 300, batch loss 1.8273, batch acc 0.8250
12:10:32.909   Training iter 350, batch loss 1.8265, batch acc 0.8270
12:10:33.453   Training iter 400, batch loss 1.8280, batch acc 0.8234
12:10:33.994   Training iter 450, batch loss 1.8288, batch acc 0.8272
12:10:34.522   Training iter 500, batch loss 1.8326, batch acc 0.8138
12:10:35.063   Training iter 550, batch loss 1.8240, batch acc 0.8330
12:10:35.586   Training iter 600, batch loss 1.8243, batch acc 0.8310
12:10:35.587 Testing @ 40 epoch...
12:10:35.633     Testing, total mean loss 1.81979, total acc 0.83760
12:10:35.633 Training @ 41 epoch...
12:10:36.166   Training iter 50, batch loss 1.8272, batch acc 0.8282
12:10:36.681   Training iter 100, batch loss 1.8272, batch acc 0.8282
12:10:37.188   Training iter 150, batch loss 1.8258, batch acc 0.8302
12:10:37.705   Training iter 200, batch loss 1.8260, batch acc 0.8262
12:10:38.221   Training iter 250, batch loss 1.8260, batch acc 0.8224
12:10:38.737   Training iter 300, batch loss 1.8270, batch acc 0.8254
12:10:39.242   Training iter 350, batch loss 1.8253, batch acc 0.8358
12:10:39.740   Training iter 400, batch loss 1.8265, batch acc 0.8210
12:10:40.240   Training iter 450, batch loss 1.8263, batch acc 0.8264
12:10:40.744   Training iter 500, batch loss 1.8292, batch acc 0.8220
12:10:41.245   Training iter 550, batch loss 1.8229, batch acc 0.8316
12:10:41.752   Training iter 600, batch loss 1.8228, batch acc 0.8270
12:10:41.754 Training @ 42 epoch...
12:10:42.284   Training iter 50, batch loss 1.8225, batch acc 0.8310
12:10:42.805   Training iter 100, batch loss 1.8253, batch acc 0.8308
12:10:43.329   Training iter 150, batch loss 1.8223, batch acc 0.8280
12:10:43.816   Training iter 200, batch loss 1.8236, batch acc 0.8240
12:10:44.308   Training iter 250, batch loss 1.8225, batch acc 0.8316
12:10:44.821   Training iter 300, batch loss 1.8276, batch acc 0.8232
12:10:45.353   Training iter 350, batch loss 1.8244, batch acc 0.8262
12:10:45.873   Training iter 400, batch loss 1.8252, batch acc 0.8276
12:10:46.363   Training iter 450, batch loss 1.8253, batch acc 0.8150
12:10:46.843   Training iter 500, batch loss 1.8221, batch acc 0.8312
12:10:47.326   Training iter 550, batch loss 1.8178, batch acc 0.8370
12:10:47.810   Training iter 600, batch loss 1.8254, batch acc 0.8252
12:10:47.812 Training @ 43 epoch...
12:10:48.298   Training iter 50, batch loss 1.8272, batch acc 0.8186
12:10:48.779   Training iter 100, batch loss 1.8234, batch acc 0.8228
12:10:49.256   Training iter 150, batch loss 1.8249, batch acc 0.8224
12:10:49.741   Training iter 200, batch loss 1.8206, batch acc 0.8332
12:10:50.208   Training iter 250, batch loss 1.8244, batch acc 0.8282
12:10:50.670   Training iter 300, batch loss 1.8208, batch acc 0.8286
12:10:51.129   Training iter 350, batch loss 1.8195, batch acc 0.8292
12:10:51.589   Training iter 400, batch loss 1.8208, batch acc 0.8310
12:10:52.075   Training iter 450, batch loss 1.8208, batch acc 0.8304
12:10:52.580   Training iter 500, batch loss 1.8193, batch acc 0.8324
12:10:53.100   Training iter 550, batch loss 1.8218, batch acc 0.8252
12:10:53.613   Training iter 600, batch loss 1.8135, batch acc 0.8388
12:10:53.615 Training @ 44 epoch...
12:10:54.135   Training iter 50, batch loss 1.8186, batch acc 0.8302
12:10:54.660   Training iter 100, batch loss 1.8215, batch acc 0.8218
12:10:55.118   Training iter 150, batch loss 1.8228, batch acc 0.8256
12:10:55.580   Training iter 200, batch loss 1.8191, batch acc 0.8260
12:10:56.036   Training iter 250, batch loss 1.8130, batch acc 0.8384
12:10:56.500   Training iter 300, batch loss 1.8206, batch acc 0.8286
12:10:56.990   Training iter 350, batch loss 1.8252, batch acc 0.8228
12:10:57.465   Training iter 400, batch loss 1.8210, batch acc 0.8294
12:10:57.941   Training iter 450, batch loss 1.8185, batch acc 0.8262
12:10:58.407   Training iter 500, batch loss 1.8202, batch acc 0.8276
12:10:58.908   Training iter 550, batch loss 1.8145, batch acc 0.8324
12:10:59.420   Training iter 600, batch loss 1.8162, batch acc 0.8340
12:10:59.422 Training @ 45 epoch...
12:10:59.941   Training iter 50, batch loss 1.8123, batch acc 0.8364
12:11:00.455   Training iter 100, batch loss 1.8185, batch acc 0.8320
12:11:00.959   Training iter 150, batch loss 1.8157, batch acc 0.8240
12:11:01.473   Training iter 200, batch loss 1.8214, batch acc 0.8232
12:11:02.014   Training iter 250, batch loss 1.8163, batch acc 0.8286
12:11:02.549   Training iter 300, batch loss 1.8200, batch acc 0.8212
12:11:03.085   Training iter 350, batch loss 1.8178, batch acc 0.8324
12:11:03.615   Training iter 400, batch loss 1.8200, batch acc 0.8264
12:11:04.149   Training iter 450, batch loss 1.8184, batch acc 0.8238
12:11:04.676   Training iter 500, batch loss 1.8147, batch acc 0.8344
12:11:05.191   Training iter 550, batch loss 1.8154, batch acc 0.8312
12:11:05.709   Training iter 600, batch loss 1.8155, batch acc 0.8340
12:11:05.711 Testing @ 45 epoch...
12:11:05.756     Testing, total mean loss 1.80872, total acc 0.83970
12:11:05.756 Training @ 46 epoch...
12:11:06.276   Training iter 50, batch loss 1.8174, batch acc 0.8256
12:11:06.782   Training iter 100, batch loss 1.8136, batch acc 0.8284
12:11:07.309   Training iter 150, batch loss 1.8185, batch acc 0.8228
12:11:07.825   Training iter 200, batch loss 1.8189, batch acc 0.8214
12:11:08.351   Training iter 250, batch loss 1.8151, batch acc 0.8320
12:11:08.877   Training iter 300, batch loss 1.8173, batch acc 0.8298
12:11:09.403   Training iter 350, batch loss 1.8133, batch acc 0.8360
12:11:09.914   Training iter 400, batch loss 1.8136, batch acc 0.8328
12:11:10.429   Training iter 450, batch loss 1.8144, batch acc 0.8266
12:11:10.941   Training iter 500, batch loss 1.8120, batch acc 0.8372
12:11:11.467   Training iter 550, batch loss 1.8118, batch acc 0.8406
12:11:11.984   Training iter 600, batch loss 1.8159, batch acc 0.8268
12:11:11.986 Training @ 47 epoch...
12:11:12.506   Training iter 50, batch loss 1.8128, batch acc 0.8344
12:11:13.019   Training iter 100, batch loss 1.8125, batch acc 0.8302
12:11:13.530   Training iter 150, batch loss 1.8143, batch acc 0.8318
12:11:14.045   Training iter 200, batch loss 1.8116, batch acc 0.8374
12:11:14.553   Training iter 250, batch loss 1.8163, batch acc 0.8262
12:11:15.067   Training iter 300, batch loss 1.8158, batch acc 0.8294
12:11:15.586   Training iter 350, batch loss 1.8113, batch acc 0.8346
12:11:16.109   Training iter 400, batch loss 1.8122, batch acc 0.8288
12:11:16.648   Training iter 450, batch loss 1.8133, batch acc 0.8288
12:11:17.202   Training iter 500, batch loss 1.8145, batch acc 0.8248
12:11:17.756   Training iter 550, batch loss 1.8137, batch acc 0.8234
12:11:18.283   Training iter 600, batch loss 1.8103, batch acc 0.8322
12:11:18.285 Training @ 48 epoch...
12:11:18.827   Training iter 50, batch loss 1.8110, batch acc 0.8328
12:11:19.372   Training iter 100, batch loss 1.8157, batch acc 0.8198
12:11:19.914   Training iter 150, batch loss 1.8069, batch acc 0.8396
12:11:20.506   Training iter 200, batch loss 1.8115, batch acc 0.8360
12:11:21.041   Training iter 250, batch loss 1.8089, batch acc 0.8362
12:11:21.569   Training iter 300, batch loss 1.8144, batch acc 0.8234
12:11:22.103   Training iter 350, batch loss 1.8136, batch acc 0.8224
12:11:22.629   Training iter 400, batch loss 1.8101, batch acc 0.8364
12:11:23.137   Training iter 450, batch loss 1.8109, batch acc 0.8306
12:11:23.650   Training iter 500, batch loss 1.8115, batch acc 0.8310
12:11:24.140   Training iter 550, batch loss 1.8102, batch acc 0.8278
12:11:24.660   Training iter 600, batch loss 1.8116, batch acc 0.8370
12:11:24.662 Training @ 49 epoch...
12:11:25.197   Training iter 50, batch loss 1.8065, batch acc 0.8402
12:11:25.697   Training iter 100, batch loss 1.8123, batch acc 0.8282
12:11:26.184   Training iter 150, batch loss 1.8069, batch acc 0.8320
12:11:26.663   Training iter 200, batch loss 1.8119, batch acc 0.8240
12:11:27.146   Training iter 250, batch loss 1.8123, batch acc 0.8262
12:11:27.630   Training iter 300, batch loss 1.8036, batch acc 0.8438
12:11:28.114   Training iter 350, batch loss 1.8100, batch acc 0.8318
12:11:28.602   Training iter 400, batch loss 1.8118, batch acc 0.8206
12:11:29.099   Training iter 450, batch loss 1.8097, batch acc 0.8334
12:11:29.596   Training iter 500, batch loss 1.8091, batch acc 0.8282
12:11:30.097   Training iter 550, batch loss 1.8125, batch acc 0.8290
12:11:30.621   Training iter 600, batch loss 1.8080, batch acc 0.8336
12:11:30.623 Training @ 50 epoch...
12:11:31.186   Training iter 50, batch loss 1.8090, batch acc 0.8340
12:11:31.744   Training iter 100, batch loss 1.8065, batch acc 0.8358
12:11:32.305   Training iter 150, batch loss 1.8118, batch acc 0.8292
12:11:32.867   Training iter 200, batch loss 1.8037, batch acc 0.8390
12:11:33.441   Training iter 250, batch loss 1.8084, batch acc 0.8332
12:11:34.002   Training iter 300, batch loss 1.8067, batch acc 0.8268
12:11:34.554   Training iter 350, batch loss 1.8050, batch acc 0.8372
12:11:35.111   Training iter 400, batch loss 1.8086, batch acc 0.8322
12:11:35.667   Training iter 450, batch loss 1.8061, batch acc 0.8328
12:11:36.208   Training iter 500, batch loss 1.8105, batch acc 0.8284
12:11:36.755   Training iter 550, batch loss 1.8075, batch acc 0.8290
12:11:37.268   Training iter 600, batch loss 1.8099, batch acc 0.8264
12:11:37.270 Testing @ 50 epoch...
12:11:37.316     Testing, total mean loss 1.79950, total acc 0.84140
12:11:37.316 Training @ 51 epoch...
12:11:37.829   Training iter 50, batch loss 1.8077, batch acc 0.8330
12:11:38.350   Training iter 100, batch loss 1.8114, batch acc 0.8190
12:11:38.833   Training iter 150, batch loss 1.8101, batch acc 0.8318
12:11:39.314   Training iter 200, batch loss 1.8062, batch acc 0.8316
12:11:39.796   Training iter 250, batch loss 1.8058, batch acc 0.8298
12:11:40.287   Training iter 300, batch loss 1.8052, batch acc 0.8300
12:11:40.764   Training iter 350, batch loss 1.8001, batch acc 0.8444
12:11:41.241   Training iter 400, batch loss 1.8050, batch acc 0.8344
12:11:41.718   Training iter 450, batch loss 1.8041, batch acc 0.8374
12:11:42.198   Training iter 500, batch loss 1.8055, batch acc 0.8324
12:11:42.697   Training iter 550, batch loss 1.8045, batch acc 0.8342
12:11:43.213   Training iter 600, batch loss 1.8077, batch acc 0.8314
12:11:43.215 Training @ 52 epoch...
12:11:43.723   Training iter 50, batch loss 1.8048, batch acc 0.8314
12:11:44.212   Training iter 100, batch loss 1.8033, batch acc 0.8350
12:11:44.738   Training iter 150, batch loss 1.8069, batch acc 0.8222
12:11:45.265   Training iter 200, batch loss 1.8055, batch acc 0.8326
12:11:45.782   Training iter 250, batch loss 1.8042, batch acc 0.8286
12:11:46.318   Training iter 300, batch loss 1.8067, batch acc 0.8284
12:11:46.859   Training iter 350, batch loss 1.8027, batch acc 0.8342
12:11:47.422   Training iter 400, batch loss 1.8048, batch acc 0.8312
12:11:47.993   Training iter 450, batch loss 1.8056, batch acc 0.8388
12:11:48.565   Training iter 500, batch loss 1.8053, batch acc 0.8402
12:11:49.114   Training iter 550, batch loss 1.8014, batch acc 0.8400
12:11:49.679   Training iter 600, batch loss 1.8026, batch acc 0.8332
12:11:49.681 Training @ 53 epoch...
12:11:50.268   Training iter 50, batch loss 1.8062, batch acc 0.8306
12:11:50.829   Training iter 100, batch loss 1.7996, batch acc 0.8424
12:11:51.374   Training iter 150, batch loss 1.8065, batch acc 0.8244
12:11:51.914   Training iter 200, batch loss 1.8002, batch acc 0.8390
12:11:52.447   Training iter 250, batch loss 1.8027, batch acc 0.8330
12:11:52.971   Training iter 300, batch loss 1.8066, batch acc 0.8264
12:11:53.482   Training iter 350, batch loss 1.8014, batch acc 0.8376
12:11:53.969   Training iter 400, batch loss 1.8026, batch acc 0.8314
12:11:54.463   Training iter 450, batch loss 1.7992, batch acc 0.8370
12:11:54.969   Training iter 500, batch loss 1.8032, batch acc 0.8304
12:11:55.499   Training iter 550, batch loss 1.8052, batch acc 0.8264
12:11:55.959   Training iter 600, batch loss 1.8014, batch acc 0.8382
12:11:55.961 Training @ 54 epoch...
12:11:56.441   Training iter 50, batch loss 1.8011, batch acc 0.8352
12:11:56.946   Training iter 100, batch loss 1.8040, batch acc 0.8266
12:11:57.450   Training iter 150, batch loss 1.8030, batch acc 0.8234
12:11:57.964   Training iter 200, batch loss 1.8033, batch acc 0.8264
12:11:58.481   Training iter 250, batch loss 1.8005, batch acc 0.8444
12:11:58.995   Training iter 300, batch loss 1.7985, batch acc 0.8420
12:11:59.505   Training iter 350, batch loss 1.7996, batch acc 0.8438
12:12:00.026   Training iter 400, batch loss 1.8063, batch acc 0.8248
12:12:00.559   Training iter 450, batch loss 1.8008, batch acc 0.8336
12:12:01.055   Training iter 500, batch loss 1.8014, batch acc 0.8328
12:12:01.602   Training iter 550, batch loss 1.7990, batch acc 0.8356
12:12:02.186   Training iter 600, batch loss 1.7991, batch acc 0.8340
12:12:02.188 Training @ 55 epoch...
12:12:02.736   Training iter 50, batch loss 1.8010, batch acc 0.8364
12:12:03.305   Training iter 100, batch loss 1.7996, batch acc 0.8350
12:12:03.878   Training iter 150, batch loss 1.8031, batch acc 0.8256
12:12:04.451   Training iter 200, batch loss 1.7969, batch acc 0.8380
12:12:05.014   Training iter 250, batch loss 1.7990, batch acc 0.8320
12:12:05.566   Training iter 300, batch loss 1.8024, batch acc 0.8244
12:12:06.121   Training iter 350, batch loss 1.8001, batch acc 0.8324
12:12:06.702   Training iter 400, batch loss 1.7967, batch acc 0.8386
12:12:07.289   Training iter 450, batch loss 1.8015, batch acc 0.8336
12:12:07.854   Training iter 500, batch loss 1.8026, batch acc 0.8300
12:12:08.404   Training iter 550, batch loss 1.7927, batch acc 0.8500
12:12:08.961   Training iter 600, batch loss 1.8031, batch acc 0.8340
12:12:08.962 Testing @ 55 epoch...
12:12:09.010     Testing, total mean loss 1.79173, total acc 0.84370
12:12:09.010 Training @ 56 epoch...
12:12:09.548   Training iter 50, batch loss 1.8011, batch acc 0.8286
12:12:10.074   Training iter 100, batch loss 1.7946, batch acc 0.8444
12:12:10.602   Training iter 150, batch loss 1.7982, batch acc 0.8358
12:12:11.131   Training iter 200, batch loss 1.8029, batch acc 0.8314
12:12:11.627   Training iter 250, batch loss 1.7965, batch acc 0.8370
12:12:12.118   Training iter 300, batch loss 1.7968, batch acc 0.8336
12:12:12.613   Training iter 350, batch loss 1.7974, batch acc 0.8322
12:12:13.109   Training iter 400, batch loss 1.7975, batch acc 0.8388
12:12:13.597   Training iter 450, batch loss 1.8036, batch acc 0.8258
12:12:14.103   Training iter 500, batch loss 1.7942, batch acc 0.8458
12:12:14.615   Training iter 550, batch loss 1.7971, batch acc 0.8298
12:12:15.118   Training iter 600, batch loss 1.8015, batch acc 0.8314
12:12:15.120 Training @ 57 epoch...
12:12:15.649   Training iter 50, batch loss 1.7951, batch acc 0.8426
12:12:16.162   Training iter 100, batch loss 1.7989, batch acc 0.8386
12:12:16.678   Training iter 150, batch loss 1.7958, batch acc 0.8388
12:12:17.177   Training iter 200, batch loss 1.7968, batch acc 0.8378
12:12:17.655   Training iter 250, batch loss 1.7984, batch acc 0.8360
12:12:18.131   Training iter 300, batch loss 1.8010, batch acc 0.8264
12:12:18.605   Training iter 350, batch loss 1.7961, batch acc 0.8290
12:12:19.088   Training iter 400, batch loss 1.7953, batch acc 0.8400
12:12:19.604   Training iter 450, batch loss 1.7974, batch acc 0.8334
12:12:20.119   Training iter 500, batch loss 1.7946, batch acc 0.8330
12:12:20.631   Training iter 550, batch loss 1.7990, batch acc 0.8320
12:12:21.134   Training iter 600, batch loss 1.7965, batch acc 0.8304
12:12:21.136 Training @ 58 epoch...
12:12:21.647   Training iter 50, batch loss 1.7931, batch acc 0.8382
12:12:22.143   Training iter 100, batch loss 1.7967, batch acc 0.8320
12:12:22.648   Training iter 150, batch loss 1.7965, batch acc 0.8372
12:12:23.148   Training iter 200, batch loss 1.7929, batch acc 0.8402
12:12:23.665   Training iter 250, batch loss 1.7998, batch acc 0.8262
12:12:24.218   Training iter 300, batch loss 1.7951, batch acc 0.8392
12:12:24.760   Training iter 350, batch loss 1.7960, batch acc 0.8342
12:12:25.317   Training iter 400, batch loss 1.7932, batch acc 0.8430
12:12:25.831   Training iter 450, batch loss 1.7925, batch acc 0.8414
12:12:26.333   Training iter 500, batch loss 1.8005, batch acc 0.8282
12:12:26.853   Training iter 550, batch loss 1.7952, batch acc 0.8342
12:12:27.380   Training iter 600, batch loss 1.7973, batch acc 0.8308
12:12:27.381 Training @ 59 epoch...
12:12:27.924   Training iter 50, batch loss 1.7928, batch acc 0.8396
12:12:28.458   Training iter 100, batch loss 1.7957, batch acc 0.8360
12:12:28.985   Training iter 150, batch loss 1.7964, batch acc 0.8322
12:12:29.512   Training iter 200, batch loss 1.7941, batch acc 0.8364
12:12:30.033   Training iter 250, batch loss 1.7932, batch acc 0.8358
12:12:30.542   Training iter 300, batch loss 1.7951, batch acc 0.8328
12:12:31.069   Training iter 350, batch loss 1.7914, batch acc 0.8348
12:12:31.610   Training iter 400, batch loss 1.7930, batch acc 0.8394
12:12:32.130   Training iter 450, batch loss 1.7964, batch acc 0.8350
12:12:32.655   Training iter 500, batch loss 1.7935, batch acc 0.8362
12:12:33.193   Training iter 550, batch loss 1.7987, batch acc 0.8342
12:12:33.722   Training iter 600, batch loss 1.7929, batch acc 0.8340
12:12:33.724 Training @ 60 epoch...
12:12:34.262   Training iter 50, batch loss 1.7946, batch acc 0.8344
12:12:34.799   Training iter 100, batch loss 1.7967, batch acc 0.8252
12:12:35.334   Training iter 150, batch loss 1.7933, batch acc 0.8344
12:12:35.886   Training iter 200, batch loss 1.7925, batch acc 0.8416
12:12:36.456   Training iter 250, batch loss 1.7906, batch acc 0.8428
12:12:37.020   Training iter 300, batch loss 1.7938, batch acc 0.8360
12:12:37.580   Training iter 350, batch loss 1.7950, batch acc 0.8328
12:12:38.136   Training iter 400, batch loss 1.7960, batch acc 0.8328
12:12:38.690   Training iter 450, batch loss 1.7931, batch acc 0.8362
12:12:39.245   Training iter 500, batch loss 1.7927, batch acc 0.8388
12:12:39.799   Training iter 550, batch loss 1.7912, batch acc 0.8326
12:12:40.352   Training iter 600, batch loss 1.7883, batch acc 0.8456
12:12:40.354 Testing @ 60 epoch...
12:12:40.400     Testing, total mean loss 1.78509, total acc 0.84570
12:12:40.400 Training @ 61 epoch...
12:12:40.963   Training iter 50, batch loss 1.7946, batch acc 0.8300
12:12:41.526   Training iter 100, batch loss 1.7949, batch acc 0.8322
12:12:42.066   Training iter 150, batch loss 1.7919, batch acc 0.8416
12:12:42.590   Training iter 200, batch loss 1.7895, batch acc 0.8384
12:12:43.101   Training iter 250, batch loss 1.7929, batch acc 0.8292
12:12:43.624   Training iter 300, batch loss 1.7911, batch acc 0.8374
12:12:44.153   Training iter 350, batch loss 1.7922, batch acc 0.8414
12:12:44.694   Training iter 400, batch loss 1.7892, batch acc 0.8412
12:12:45.228   Training iter 450, batch loss 1.7909, batch acc 0.8344
12:12:45.731   Training iter 500, batch loss 1.7910, batch acc 0.8408
12:12:46.241   Training iter 550, batch loss 1.7917, batch acc 0.8368
12:12:46.782   Training iter 600, batch loss 1.7931, batch acc 0.8344
12:12:46.783 Training @ 62 epoch...
12:12:47.360   Training iter 50, batch loss 1.7912, batch acc 0.8394
12:12:47.925   Training iter 100, batch loss 1.7982, batch acc 0.8254
12:12:48.450   Training iter 150, batch loss 1.7919, batch acc 0.8352
12:12:48.933   Training iter 200, batch loss 1.7911, batch acc 0.8428
12:12:49.422   Training iter 250, batch loss 1.7936, batch acc 0.8306
12:12:49.927   Training iter 300, batch loss 1.7919, batch acc 0.8360
12:12:50.428   Training iter 350, batch loss 1.7884, batch acc 0.8374
12:12:50.923   Training iter 400, batch loss 1.7898, batch acc 0.8444
12:12:51.415   Training iter 450, batch loss 1.7912, batch acc 0.8354
12:12:51.911   Training iter 500, batch loss 1.7847, batch acc 0.8426
12:12:52.417   Training iter 550, batch loss 1.7886, batch acc 0.8384
12:12:52.928   Training iter 600, batch loss 1.7882, batch acc 0.8340
12:12:52.930 Training @ 63 epoch...
12:12:53.451   Training iter 50, batch loss 1.7883, batch acc 0.8384
12:12:53.951   Training iter 100, batch loss 1.7885, batch acc 0.8414
12:12:54.469   Training iter 150, batch loss 1.7900, batch acc 0.8332
12:12:55.022   Training iter 200, batch loss 1.7914, batch acc 0.8326
12:12:55.569   Training iter 250, batch loss 1.7896, batch acc 0.8342
12:12:56.116   Training iter 300, batch loss 1.7906, batch acc 0.8426
12:12:56.663   Training iter 350, batch loss 1.7906, batch acc 0.8360
12:12:57.211   Training iter 400, batch loss 1.7864, batch acc 0.8410
12:12:57.766   Training iter 450, batch loss 1.7896, batch acc 0.8350
12:12:58.286   Training iter 500, batch loss 1.7910, batch acc 0.8294
12:12:58.805   Training iter 550, batch loss 1.7875, batch acc 0.8438
12:12:59.324   Training iter 600, batch loss 1.7912, batch acc 0.8366
12:12:59.325 Training @ 64 epoch...
12:12:59.850   Training iter 50, batch loss 1.7881, batch acc 0.8396
12:13:00.378   Training iter 100, batch loss 1.7902, batch acc 0.8332
12:13:00.895   Training iter 150, batch loss 1.7845, batch acc 0.8406
12:13:01.413   Training iter 200, batch loss 1.7852, batch acc 0.8460
12:13:01.953   Training iter 250, batch loss 1.7918, batch acc 0.8354
12:13:02.523   Training iter 300, batch loss 1.7905, batch acc 0.8328
12:13:03.077   Training iter 350, batch loss 1.7898, batch acc 0.8298
12:13:03.638   Training iter 400, batch loss 1.7925, batch acc 0.8334
12:13:04.190   Training iter 450, batch loss 1.7872, batch acc 0.8396
12:13:04.750   Training iter 500, batch loss 1.7891, batch acc 0.8312
12:13:05.320   Training iter 550, batch loss 1.7844, batch acc 0.8440
12:13:05.862   Training iter 600, batch loss 1.7879, batch acc 0.8408
12:13:05.864 Training @ 65 epoch...
12:13:06.437   Training iter 50, batch loss 1.7863, batch acc 0.8418
12:13:06.997   Training iter 100, batch loss 1.7844, batch acc 0.8396
12:13:07.554   Training iter 150, batch loss 1.7891, batch acc 0.8336
12:13:08.099   Training iter 200, batch loss 1.7876, batch acc 0.8348
12:13:08.650   Training iter 250, batch loss 1.7883, batch acc 0.8366
12:13:09.186   Training iter 300, batch loss 1.7899, batch acc 0.8318
12:13:09.737   Training iter 350, batch loss 1.7894, batch acc 0.8318
12:13:10.277   Training iter 400, batch loss 1.7881, batch acc 0.8354
12:13:10.819   Training iter 450, batch loss 1.7858, batch acc 0.8458
12:13:11.359   Training iter 500, batch loss 1.7853, batch acc 0.8430
12:13:11.909   Training iter 550, batch loss 1.7868, batch acc 0.8384
12:13:12.461   Training iter 600, batch loss 1.7869, batch acc 0.8412
12:13:12.463 Testing @ 65 epoch...
12:13:12.508     Testing, total mean loss 1.77935, total acc 0.84830
12:13:12.508 Training @ 66 epoch...
12:13:13.072   Training iter 50, batch loss 1.7878, batch acc 0.8388
12:13:13.631   Training iter 100, batch loss 1.7855, batch acc 0.8360
12:13:14.175   Training iter 150, batch loss 1.7856, batch acc 0.8346
12:13:14.711   Training iter 200, batch loss 1.7851, batch acc 0.8342
12:13:15.239   Training iter 250, batch loss 1.7889, batch acc 0.8428
12:13:15.754   Training iter 300, batch loss 1.7849, batch acc 0.8406
12:13:16.268   Training iter 350, batch loss 1.7886, batch acc 0.8276
12:13:16.786   Training iter 400, batch loss 1.7853, batch acc 0.8376
12:13:17.304   Training iter 450, batch loss 1.7890, batch acc 0.8396
12:13:17.816   Training iter 500, batch loss 1.7853, batch acc 0.8464
12:13:18.342   Training iter 550, batch loss 1.7858, batch acc 0.8394
12:13:18.867   Training iter 600, batch loss 1.7834, batch acc 0.8398
12:13:18.868 Training @ 67 epoch...
12:13:19.407   Training iter 50, batch loss 1.7810, batch acc 0.8464
12:13:19.947   Training iter 100, batch loss 1.7865, batch acc 0.8328
12:13:20.494   Training iter 150, batch loss 1.7862, batch acc 0.8398
12:13:21.029   Training iter 200, batch loss 1.7906, batch acc 0.8372
12:13:21.556   Training iter 250, batch loss 1.7840, batch acc 0.8414
12:13:22.084   Training iter 300, batch loss 1.7863, batch acc 0.8374
12:13:22.616   Training iter 350, batch loss 1.7845, batch acc 0.8400
12:13:23.156   Training iter 400, batch loss 1.7865, batch acc 0.8360
12:13:23.701   Training iter 450, batch loss 1.7865, batch acc 0.8302
12:13:24.251   Training iter 500, batch loss 1.7882, batch acc 0.8352
12:13:24.811   Training iter 550, batch loss 1.7810, batch acc 0.8464
12:13:25.372   Training iter 600, batch loss 1.7815, batch acc 0.8406
12:13:25.374 Training @ 68 epoch...
12:13:25.929   Training iter 50, batch loss 1.7840, batch acc 0.8396
12:13:26.495   Training iter 100, batch loss 1.7832, batch acc 0.8378
12:13:27.055   Training iter 150, batch loss 1.7828, batch acc 0.8462
12:13:27.592   Training iter 200, batch loss 1.7872, batch acc 0.8386
12:13:28.131   Training iter 250, batch loss 1.7806, batch acc 0.8410
12:13:28.668   Training iter 300, batch loss 1.7817, batch acc 0.8450
12:13:29.210   Training iter 350, batch loss 1.7866, batch acc 0.8294
12:13:29.751   Training iter 400, batch loss 1.7874, batch acc 0.8288
12:13:30.276   Training iter 450, batch loss 1.7867, batch acc 0.8364
12:13:30.799   Training iter 500, batch loss 1.7836, batch acc 0.8438
12:13:31.335   Training iter 550, batch loss 1.7818, batch acc 0.8402
12:13:31.861   Training iter 600, batch loss 1.7849, batch acc 0.8390
12:13:31.863 Training @ 69 epoch...
12:13:32.389   Training iter 50, batch loss 1.7840, batch acc 0.8340
12:13:32.905   Training iter 100, batch loss 1.7788, batch acc 0.8444
12:13:33.444   Training iter 150, batch loss 1.7818, batch acc 0.8444
12:13:33.972   Training iter 200, batch loss 1.7801, batch acc 0.8398
12:13:34.498   Training iter 250, batch loss 1.7853, batch acc 0.8378
12:13:35.020   Training iter 300, batch loss 1.7857, batch acc 0.8314
12:13:35.557   Training iter 350, batch loss 1.7814, batch acc 0.8434
12:13:36.085   Training iter 400, batch loss 1.7809, batch acc 0.8420
12:13:36.618   Training iter 450, batch loss 1.7856, batch acc 0.8362
12:13:37.136   Training iter 500, batch loss 1.7878, batch acc 0.8384
12:13:37.653   Training iter 550, batch loss 1.7838, batch acc 0.8400
12:13:38.180   Training iter 600, batch loss 1.7834, batch acc 0.8372
12:13:38.182 Training @ 70 epoch...
12:13:38.714   Training iter 50, batch loss 1.7807, batch acc 0.8374
12:13:39.244   Training iter 100, batch loss 1.7852, batch acc 0.8296
12:13:39.776   Training iter 150, batch loss 1.7793, batch acc 0.8488
12:13:40.329   Training iter 200, batch loss 1.7790, batch acc 0.8484
12:13:40.886   Training iter 250, batch loss 1.7866, batch acc 0.8336
12:13:41.431   Training iter 300, batch loss 1.7834, batch acc 0.8382
12:13:41.957   Training iter 350, batch loss 1.7758, batch acc 0.8496
12:13:42.497   Training iter 400, batch loss 1.7833, batch acc 0.8380
12:13:43.039   Training iter 450, batch loss 1.7823, batch acc 0.8402
12:13:43.618   Training iter 500, batch loss 1.7826, batch acc 0.8416
12:13:44.193   Training iter 550, batch loss 1.7817, batch acc 0.8396
12:13:44.778   Training iter 600, batch loss 1.7873, batch acc 0.8278
12:13:44.780 Testing @ 70 epoch...
12:13:44.826     Testing, total mean loss 1.77437, total acc 0.84980
12:13:44.826 Training @ 71 epoch...
12:13:45.389   Training iter 50, batch loss 1.7821, batch acc 0.8336
12:13:45.903   Training iter 100, batch loss 1.7813, batch acc 0.8408
12:13:46.417   Training iter 150, batch loss 1.7809, batch acc 0.8412
12:13:46.909   Training iter 200, batch loss 1.7840, batch acc 0.8342
12:13:47.407   Training iter 250, batch loss 1.7814, batch acc 0.8398
12:13:47.899   Training iter 300, batch loss 1.7822, batch acc 0.8442
12:13:48.413   Training iter 350, batch loss 1.7799, batch acc 0.8422
12:13:48.896   Training iter 400, batch loss 1.7836, batch acc 0.8330
12:13:49.374   Training iter 450, batch loss 1.7781, batch acc 0.8452
12:13:49.859   Training iter 500, batch loss 1.7814, batch acc 0.8384
12:13:50.356   Training iter 550, batch loss 1.7794, batch acc 0.8468
12:13:50.848   Training iter 600, batch loss 1.7816, batch acc 0.8378
12:13:50.850 Training @ 72 epoch...
12:13:51.362   Training iter 50, batch loss 1.7854, batch acc 0.8352
12:13:51.856   Training iter 100, batch loss 1.7812, batch acc 0.8432
12:13:52.351   Training iter 150, batch loss 1.7810, batch acc 0.8422
12:13:52.842   Training iter 200, batch loss 1.7791, batch acc 0.8434
12:13:53.335   Training iter 250, batch loss 1.7799, batch acc 0.8374
12:13:53.814   Training iter 300, batch loss 1.7818, batch acc 0.8362
12:13:54.295   Training iter 350, batch loss 1.7761, batch acc 0.8454
12:13:54.771   Training iter 400, batch loss 1.7802, batch acc 0.8380
12:13:55.272   Training iter 450, batch loss 1.7842, batch acc 0.8382
12:13:55.770   Training iter 500, batch loss 1.7755, batch acc 0.8410
12:13:56.265   Training iter 550, batch loss 1.7794, batch acc 0.8376
12:13:56.784   Training iter 600, batch loss 1.7812, batch acc 0.8404
12:13:56.786 Training @ 73 epoch...
12:13:57.309   Training iter 50, batch loss 1.7862, batch acc 0.8254
12:13:57.822   Training iter 100, batch loss 1.7805, batch acc 0.8440
12:13:58.328   Training iter 150, batch loss 1.7827, batch acc 0.8390
12:13:58.820   Training iter 200, batch loss 1.7810, batch acc 0.8326
12:13:59.323   Training iter 250, batch loss 1.7841, batch acc 0.8330
12:13:59.820   Training iter 300, batch loss 1.7793, batch acc 0.8428
12:14:00.347   Training iter 350, batch loss 1.7774, batch acc 0.8426
12:14:00.883   Training iter 400, batch loss 1.7756, batch acc 0.8420
12:14:01.417   Training iter 450, batch loss 1.7768, batch acc 0.8446
12:14:01.998   Training iter 500, batch loss 1.7752, batch acc 0.8456
12:14:02.569   Training iter 550, batch loss 1.7769, batch acc 0.8480
12:14:03.078   Training iter 600, batch loss 1.7789, batch acc 0.8416
12:14:03.080 Training @ 74 epoch...
12:14:03.596   Training iter 50, batch loss 1.7800, batch acc 0.8382
12:14:04.121   Training iter 100, batch loss 1.7782, batch acc 0.8444
12:14:04.631   Training iter 150, batch loss 1.7771, batch acc 0.8438
12:14:05.142   Training iter 200, batch loss 1.7758, batch acc 0.8470
12:14:05.647   Training iter 250, batch loss 1.7779, batch acc 0.8364
12:14:06.192   Training iter 300, batch loss 1.7788, batch acc 0.8450
12:14:06.732   Training iter 350, batch loss 1.7813, batch acc 0.8346
12:14:07.272   Training iter 400, batch loss 1.7793, batch acc 0.8378
12:14:07.793   Training iter 450, batch loss 1.7811, batch acc 0.8316
12:14:08.303   Training iter 500, batch loss 1.7810, batch acc 0.8404
12:14:08.808   Training iter 550, batch loss 1.7761, batch acc 0.8460
12:14:09.293   Training iter 600, batch loss 1.7773, batch acc 0.8418
12:14:09.294 Training @ 75 epoch...
12:14:09.794   Training iter 50, batch loss 1.7772, batch acc 0.8472
12:14:10.298   Training iter 100, batch loss 1.7772, batch acc 0.8428
12:14:10.795   Training iter 150, batch loss 1.7779, batch acc 0.8366
12:14:11.291   Training iter 200, batch loss 1.7781, batch acc 0.8360
12:14:11.789   Training iter 250, batch loss 1.7786, batch acc 0.8434
12:14:12.283   Training iter 300, batch loss 1.7785, batch acc 0.8416
12:14:12.783   Training iter 350, batch loss 1.7799, batch acc 0.8350
12:14:13.287   Training iter 400, batch loss 1.7757, batch acc 0.8486
12:14:13.785   Training iter 450, batch loss 1.7760, batch acc 0.8452
12:14:14.283   Training iter 500, batch loss 1.7783, batch acc 0.8356
12:14:14.790   Training iter 550, batch loss 1.7803, batch acc 0.8330
12:14:15.308   Training iter 600, batch loss 1.7760, batch acc 0.8426
12:14:15.310 Testing @ 75 epoch...
12:14:15.355     Testing, total mean loss 1.76998, total acc 0.85150
12:14:15.355 Training @ 76 epoch...
12:14:15.876   Training iter 50, batch loss 1.7836, batch acc 0.8306
12:14:16.382   Training iter 100, batch loss 1.7759, batch acc 0.8446
12:14:16.906   Training iter 150, batch loss 1.7740, batch acc 0.8478
12:14:17.419   Training iter 200, batch loss 1.7767, batch acc 0.8404
12:14:17.927   Training iter 250, batch loss 1.7741, batch acc 0.8398
12:14:18.435   Training iter 300, batch loss 1.7788, batch acc 0.8396
12:14:18.939   Training iter 350, batch loss 1.7788, batch acc 0.8420
12:14:19.437   Training iter 400, batch loss 1.7791, batch acc 0.8446
12:14:19.931   Training iter 450, batch loss 1.7759, batch acc 0.8430
12:14:20.433   Training iter 500, batch loss 1.7743, batch acc 0.8448
12:14:20.924   Training iter 550, batch loss 1.7763, batch acc 0.8420
12:14:21.413   Training iter 600, batch loss 1.7766, batch acc 0.8366
12:14:21.415 Training @ 77 epoch...
12:14:21.901   Training iter 50, batch loss 1.7792, batch acc 0.8358
12:14:22.403   Training iter 100, batch loss 1.7783, batch acc 0.8348
12:14:22.902   Training iter 150, batch loss 1.7800, batch acc 0.8422
12:14:23.401   Training iter 200, batch loss 1.7757, batch acc 0.8434
12:14:23.906   Training iter 250, batch loss 1.7789, batch acc 0.8346
12:14:24.414   Training iter 300, batch loss 1.7750, batch acc 0.8414
12:14:24.885   Training iter 350, batch loss 1.7812, batch acc 0.8344
12:14:25.355   Training iter 400, batch loss 1.7718, batch acc 0.8488
12:14:25.819   Training iter 450, batch loss 1.7704, batch acc 0.8556
12:14:26.273   Training iter 500, batch loss 1.7751, batch acc 0.8426
12:14:26.720   Training iter 550, batch loss 1.7761, batch acc 0.8368
12:14:27.168   Training iter 600, batch loss 1.7727, batch acc 0.8446
12:14:27.170 Training @ 78 epoch...
12:14:27.634   Training iter 50, batch loss 1.7751, batch acc 0.8460
12:14:28.108   Training iter 100, batch loss 1.7805, batch acc 0.8344
12:14:28.576   Training iter 150, batch loss 1.7750, batch acc 0.8462
12:14:29.055   Training iter 200, batch loss 1.7755, batch acc 0.8428
12:14:29.555   Training iter 250, batch loss 1.7799, batch acc 0.8348
12:14:30.075   Training iter 300, batch loss 1.7729, batch acc 0.8452
12:14:30.599   Training iter 350, batch loss 1.7773, batch acc 0.8368
12:14:31.105   Training iter 400, batch loss 1.7685, batch acc 0.8542
12:14:31.613   Training iter 450, batch loss 1.7747, batch acc 0.8364
12:14:32.119   Training iter 500, batch loss 1.7750, batch acc 0.8452
12:14:32.617   Training iter 550, batch loss 1.7754, batch acc 0.8386
12:14:33.113   Training iter 600, batch loss 1.7752, batch acc 0.8408
12:14:33.115 Training @ 79 epoch...
12:14:33.618   Training iter 50, batch loss 1.7740, batch acc 0.8392
12:14:34.121   Training iter 100, batch loss 1.7729, batch acc 0.8446
12:14:34.634   Training iter 150, batch loss 1.7729, batch acc 0.8440
12:14:35.136   Training iter 200, batch loss 1.7737, batch acc 0.8430
12:14:35.633   Training iter 250, batch loss 1.7779, batch acc 0.8400
12:14:36.134   Training iter 300, batch loss 1.7754, batch acc 0.8468
12:14:36.644   Training iter 350, batch loss 1.7731, batch acc 0.8402
12:14:37.156   Training iter 400, batch loss 1.7756, batch acc 0.8422
12:14:37.681   Training iter 450, batch loss 1.7729, batch acc 0.8402
12:14:38.196   Training iter 500, batch loss 1.7739, batch acc 0.8428
12:14:38.727   Training iter 550, batch loss 1.7787, batch acc 0.8386
12:14:39.258   Training iter 600, batch loss 1.7749, batch acc 0.8440
12:14:39.260 Training @ 80 epoch...
12:14:39.770   Training iter 50, batch loss 1.7737, batch acc 0.8438
12:14:40.292   Training iter 100, batch loss 1.7750, batch acc 0.8340
12:14:40.794   Training iter 150, batch loss 1.7771, batch acc 0.8466
12:14:41.304   Training iter 200, batch loss 1.7777, batch acc 0.8364
12:14:41.816   Training iter 250, batch loss 1.7757, batch acc 0.8372
12:14:42.340   Training iter 300, batch loss 1.7721, batch acc 0.8422
12:14:42.910   Training iter 350, batch loss 1.7712, batch acc 0.8456
12:14:43.494   Training iter 400, batch loss 1.7753, batch acc 0.8396
12:14:44.064   Training iter 450, batch loss 1.7747, batch acc 0.8388
12:14:44.608   Training iter 500, batch loss 1.7722, batch acc 0.8496
12:14:45.132   Training iter 550, batch loss 1.7728, batch acc 0.8408
12:14:45.660   Training iter 600, batch loss 1.7694, batch acc 0.8524
12:14:45.662 Testing @ 80 epoch...
12:14:45.708     Testing, total mean loss 1.76611, total acc 0.85260
12:14:45.708 Training @ 81 epoch...
12:14:46.242   Training iter 50, batch loss 1.7707, batch acc 0.8482
12:14:46.779   Training iter 100, batch loss 1.7724, batch acc 0.8468
12:14:47.315   Training iter 150, batch loss 1.7717, batch acc 0.8436
12:14:47.811   Training iter 200, batch loss 1.7704, batch acc 0.8454
12:14:48.298   Training iter 250, batch loss 1.7737, batch acc 0.8392
12:14:48.801   Training iter 300, batch loss 1.7750, batch acc 0.8394
12:14:49.330   Training iter 350, batch loss 1.7753, batch acc 0.8370
12:14:49.861   Training iter 400, batch loss 1.7722, batch acc 0.8404
12:14:50.393   Training iter 450, batch loss 1.7766, batch acc 0.8328
12:14:50.926   Training iter 500, batch loss 1.7761, batch acc 0.8404
12:14:51.434   Training iter 550, batch loss 1.7729, batch acc 0.8446
12:14:51.938   Training iter 600, batch loss 1.7710, batch acc 0.8486
12:14:51.940 Training @ 82 epoch...
12:14:52.426   Training iter 50, batch loss 1.7697, batch acc 0.8430
12:14:52.935   Training iter 100, batch loss 1.7750, batch acc 0.8396
12:14:53.464   Training iter 150, batch loss 1.7696, batch acc 0.8544
12:14:53.974   Training iter 200, batch loss 1.7709, batch acc 0.8428
12:14:54.486   Training iter 250, batch loss 1.7762, batch acc 0.8372
12:14:55.006   Training iter 300, batch loss 1.7704, batch acc 0.8464
12:14:55.522   Training iter 350, batch loss 1.7761, batch acc 0.8370
12:14:56.024   Training iter 400, batch loss 1.7745, batch acc 0.8362
12:14:56.525   Training iter 450, batch loss 1.7752, batch acc 0.8398
12:14:57.023   Training iter 500, batch loss 1.7706, batch acc 0.8430
12:14:57.513   Training iter 550, batch loss 1.7713, batch acc 0.8416
12:14:57.975   Training iter 600, batch loss 1.7701, batch acc 0.8490
12:14:57.976 Training @ 83 epoch...
12:14:58.479   Training iter 50, batch loss 1.7707, batch acc 0.8436
12:14:58.983   Training iter 100, batch loss 1.7725, batch acc 0.8468
12:14:59.477   Training iter 150, batch loss 1.7734, batch acc 0.8450
12:14:59.970   Training iter 200, batch loss 1.7762, batch acc 0.8372
12:15:00.469   Training iter 250, batch loss 1.7727, batch acc 0.8398
12:15:00.978   Training iter 300, batch loss 1.7724, batch acc 0.8424
12:15:01.532   Training iter 350, batch loss 1.7704, batch acc 0.8440
12:15:02.083   Training iter 400, batch loss 1.7709, batch acc 0.8426
12:15:02.658   Training iter 450, batch loss 1.7706, batch acc 0.8392
12:15:03.212   Training iter 500, batch loss 1.7712, batch acc 0.8440
12:15:03.748   Training iter 550, batch loss 1.7714, batch acc 0.8428
12:15:04.282   Training iter 600, batch loss 1.7687, batch acc 0.8450
12:15:04.283 Training @ 84 epoch...
12:15:04.831   Training iter 50, batch loss 1.7682, batch acc 0.8504
12:15:05.370   Training iter 100, batch loss 1.7719, batch acc 0.8420
12:15:05.901   Training iter 150, batch loss 1.7749, batch acc 0.8356
12:15:06.431   Training iter 200, batch loss 1.7767, batch acc 0.8330
12:15:06.969   Training iter 250, batch loss 1.7667, batch acc 0.8518
12:15:07.530   Training iter 300, batch loss 1.7729, batch acc 0.8370
12:15:08.050   Training iter 350, batch loss 1.7702, batch acc 0.8480
12:15:08.561   Training iter 400, batch loss 1.7685, batch acc 0.8462
12:15:09.087   Training iter 450, batch loss 1.7710, batch acc 0.8440
12:15:09.592   Training iter 500, batch loss 1.7733, batch acc 0.8422
12:15:10.083   Training iter 550, batch loss 1.7712, batch acc 0.8362
12:15:10.578   Training iter 600, batch loss 1.7676, batch acc 0.8488
12:15:10.580 Training @ 85 epoch...
12:15:11.054   Training iter 50, batch loss 1.7692, batch acc 0.8466
12:15:11.537   Training iter 100, batch loss 1.7714, batch acc 0.8396
12:15:12.018   Training iter 150, batch loss 1.7708, batch acc 0.8440
12:15:12.520   Training iter 200, batch loss 1.7692, batch acc 0.8450
12:15:13.010   Training iter 250, batch loss 1.7695, batch acc 0.8482
12:15:13.505   Training iter 300, batch loss 1.7694, batch acc 0.8404
12:15:13.989   Training iter 350, batch loss 1.7748, batch acc 0.8380
12:15:14.474   Training iter 400, batch loss 1.7744, batch acc 0.8388
12:15:14.960   Training iter 450, batch loss 1.7722, batch acc 0.8382
12:15:15.463   Training iter 500, batch loss 1.7756, batch acc 0.8392
12:15:15.945   Training iter 550, batch loss 1.7627, batch acc 0.8542
12:15:16.459   Training iter 600, batch loss 1.7661, batch acc 0.8478
12:15:16.461 Testing @ 85 epoch...
12:15:16.506     Testing, total mean loss 1.76268, total acc 0.85350
12:15:16.506 Training @ 86 epoch...
12:15:17.019   Training iter 50, batch loss 1.7724, batch acc 0.8406
12:15:17.515   Training iter 100, batch loss 1.7666, batch acc 0.8460
12:15:18.015   Training iter 150, batch loss 1.7644, batch acc 0.8476
12:15:18.517   Training iter 200, batch loss 1.7731, batch acc 0.8404
12:15:18.998   Training iter 250, batch loss 1.7748, batch acc 0.8374
12:15:19.531   Training iter 300, batch loss 1.7731, batch acc 0.8442
12:15:20.070   Training iter 350, batch loss 1.7688, batch acc 0.8474
12:15:20.598   Training iter 400, batch loss 1.7658, batch acc 0.8472
12:15:21.112   Training iter 450, batch loss 1.7684, batch acc 0.8428
12:15:21.625   Training iter 500, batch loss 1.7715, batch acc 0.8412
12:15:22.149   Training iter 550, batch loss 1.7715, batch acc 0.8380
12:15:22.640   Training iter 600, batch loss 1.7668, batch acc 0.8464
12:15:22.642 Training @ 87 epoch...
12:15:23.123   Training iter 50, batch loss 1.7686, batch acc 0.8442
12:15:23.614   Training iter 100, batch loss 1.7736, batch acc 0.8420
12:15:24.115   Training iter 150, batch loss 1.7644, batch acc 0.8510
12:15:24.638   Training iter 200, batch loss 1.7728, batch acc 0.8386
12:15:25.154   Training iter 250, batch loss 1.7630, batch acc 0.8544
12:15:25.648   Training iter 300, batch loss 1.7677, batch acc 0.8432
12:15:26.144   Training iter 350, batch loss 1.7685, batch acc 0.8454
12:15:26.644   Training iter 400, batch loss 1.7726, batch acc 0.8428
12:15:27.161   Training iter 450, batch loss 1.7717, batch acc 0.8426
12:15:27.630   Training iter 500, batch loss 1.7674, batch acc 0.8430
12:15:28.088   Training iter 550, batch loss 1.7703, batch acc 0.8354
12:15:28.584   Training iter 600, batch loss 1.7690, batch acc 0.8408
12:15:28.585 Training @ 88 epoch...
12:15:29.074   Training iter 50, batch loss 1.7703, batch acc 0.8434
12:15:29.594   Training iter 100, batch loss 1.7703, batch acc 0.8412
12:15:30.119   Training iter 150, batch loss 1.7639, batch acc 0.8564
12:15:30.644   Training iter 200, batch loss 1.7673, batch acc 0.8462
12:15:31.157   Training iter 250, batch loss 1.7729, batch acc 0.8332
12:15:31.673   Training iter 300, batch loss 1.7667, batch acc 0.8440
12:15:32.187   Training iter 350, batch loss 1.7651, batch acc 0.8424
12:15:32.719   Training iter 400, batch loss 1.7678, batch acc 0.8450
12:15:33.260   Training iter 450, batch loss 1.7685, batch acc 0.8448
12:15:33.777   Training iter 500, batch loss 1.7722, batch acc 0.8420
12:15:34.296   Training iter 550, batch loss 1.7685, batch acc 0.8438
12:15:34.828   Training iter 600, batch loss 1.7686, batch acc 0.8416
12:15:34.829 Training @ 89 epoch...
12:15:35.351   Training iter 50, batch loss 1.7655, batch acc 0.8476
12:15:35.872   Training iter 100, batch loss 1.7712, batch acc 0.8354
12:15:36.402   Training iter 150, batch loss 1.7689, batch acc 0.8416
12:15:36.931   Training iter 200, batch loss 1.7631, batch acc 0.8544
12:15:37.467   Training iter 250, batch loss 1.7710, batch acc 0.8398
12:15:37.996   Training iter 300, batch loss 1.7696, batch acc 0.8494
12:15:38.539   Training iter 350, batch loss 1.7659, batch acc 0.8464
12:15:39.067   Training iter 400, batch loss 1.7670, batch acc 0.8440
12:15:39.637   Training iter 450, batch loss 1.7641, batch acc 0.8480
12:15:40.222   Training iter 500, batch loss 1.7724, batch acc 0.8356
12:15:40.809   Training iter 550, batch loss 1.7695, batch acc 0.8364
12:15:41.360   Training iter 600, batch loss 1.7666, batch acc 0.8528
12:15:41.362 Training @ 90 epoch...
12:15:41.868   Training iter 50, batch loss 1.7672, batch acc 0.8386
12:15:42.388   Training iter 100, batch loss 1.7682, batch acc 0.8414
12:15:42.928   Training iter 150, batch loss 1.7708, batch acc 0.8372
12:15:43.441   Training iter 200, batch loss 1.7666, batch acc 0.8466
12:15:43.950   Training iter 250, batch loss 1.7618, batch acc 0.8508
12:15:44.443   Training iter 300, batch loss 1.7657, batch acc 0.8448
12:15:44.931   Training iter 350, batch loss 1.7670, batch acc 0.8472
12:15:45.413   Training iter 400, batch loss 1.7674, batch acc 0.8428
12:15:45.859   Training iter 450, batch loss 1.7676, batch acc 0.8448
12:15:46.310   Training iter 500, batch loss 1.7733, batch acc 0.8380
12:15:46.798   Training iter 550, batch loss 1.7673, batch acc 0.8474
12:15:47.285   Training iter 600, batch loss 1.7649, batch acc 0.8490
12:15:47.286 Testing @ 90 epoch...
12:15:47.333     Testing, total mean loss 1.75961, total acc 0.85420
12:15:47.333 Training @ 91 epoch...
12:15:47.835   Training iter 50, batch loss 1.7671, batch acc 0.8448
12:15:48.330   Training iter 100, batch loss 1.7706, batch acc 0.8362
12:15:48.828   Training iter 150, batch loss 1.7673, batch acc 0.8382
12:15:49.364   Training iter 200, batch loss 1.7671, batch acc 0.8410
12:15:49.923   Training iter 250, batch loss 1.7654, batch acc 0.8502
12:15:50.486   Training iter 300, batch loss 1.7648, batch acc 0.8444
12:15:51.045   Training iter 350, batch loss 1.7658, batch acc 0.8490
12:15:51.606   Training iter 400, batch loss 1.7644, batch acc 0.8472
12:15:52.166   Training iter 450, batch loss 1.7678, batch acc 0.8436
12:15:52.720   Training iter 500, batch loss 1.7724, batch acc 0.8428
12:15:53.265   Training iter 550, batch loss 1.7629, batch acc 0.8490
12:15:53.803   Training iter 600, batch loss 1.7652, batch acc 0.8450
12:15:53.804 Training @ 92 epoch...
12:15:54.353   Training iter 50, batch loss 1.7651, batch acc 0.8470
12:15:54.909   Training iter 100, batch loss 1.7617, batch acc 0.8480
12:15:55.422   Training iter 150, batch loss 1.7691, batch acc 0.8376
12:15:55.911   Training iter 200, batch loss 1.7659, batch acc 0.8400
12:15:56.420   Training iter 250, batch loss 1.7712, batch acc 0.8398
12:15:56.926   Training iter 300, batch loss 1.7629, batch acc 0.8552
12:15:57.432   Training iter 350, batch loss 1.7683, batch acc 0.8438
12:15:57.941   Training iter 400, batch loss 1.7648, batch acc 0.8406
12:15:58.460   Training iter 450, batch loss 1.7649, batch acc 0.8478
12:15:58.971   Training iter 500, batch loss 1.7678, batch acc 0.8406
12:15:59.497   Training iter 550, batch loss 1.7634, batch acc 0.8476
12:16:00.031   Training iter 600, batch loss 1.7689, batch acc 0.8460
12:16:00.033 Training @ 93 epoch...
12:16:00.581   Training iter 50, batch loss 1.7688, batch acc 0.8390
12:16:01.097   Training iter 100, batch loss 1.7651, batch acc 0.8476
12:16:01.652   Training iter 150, batch loss 1.7688, batch acc 0.8406
12:16:02.210   Training iter 200, batch loss 1.7662, batch acc 0.8480
12:16:02.770   Training iter 250, batch loss 1.7645, batch acc 0.8446
12:16:03.308   Training iter 300, batch loss 1.7672, batch acc 0.8408
12:16:03.844   Training iter 350, batch loss 1.7682, batch acc 0.8408
12:16:04.379   Training iter 400, batch loss 1.7644, batch acc 0.8450
12:16:04.932   Training iter 450, batch loss 1.7661, batch acc 0.8446
12:16:05.527   Training iter 500, batch loss 1.7604, batch acc 0.8540
12:16:06.102   Training iter 550, batch loss 1.7682, batch acc 0.8446
12:16:06.649   Training iter 600, batch loss 1.7593, batch acc 0.8484
12:16:06.651 Training @ 94 epoch...
12:16:07.201   Training iter 50, batch loss 1.7638, batch acc 0.8498
12:16:07.739   Training iter 100, batch loss 1.7655, batch acc 0.8476
12:16:08.281   Training iter 150, batch loss 1.7594, batch acc 0.8596
12:16:08.843   Training iter 200, batch loss 1.7658, batch acc 0.8394
12:16:09.400   Training iter 250, batch loss 1.7678, batch acc 0.8360
12:16:09.932   Training iter 300, batch loss 1.7659, batch acc 0.8414
12:16:10.498   Training iter 350, batch loss 1.7678, batch acc 0.8442
12:16:11.033   Training iter 400, batch loss 1.7687, batch acc 0.8426
12:16:11.566   Training iter 450, batch loss 1.7611, batch acc 0.8578
12:16:12.068   Training iter 500, batch loss 1.7623, batch acc 0.8380
12:16:12.572   Training iter 550, batch loss 1.7679, batch acc 0.8400
12:16:13.070   Training iter 600, batch loss 1.7649, batch acc 0.8460
12:16:13.071 Training @ 95 epoch...
12:16:13.587   Training iter 50, batch loss 1.7631, batch acc 0.8452
12:16:14.095   Training iter 100, batch loss 1.7632, batch acc 0.8470
12:16:14.596   Training iter 150, batch loss 1.7608, batch acc 0.8540
12:16:15.107   Training iter 200, batch loss 1.7662, batch acc 0.8484
12:16:15.622   Training iter 250, batch loss 1.7603, batch acc 0.8520
12:16:16.134   Training iter 300, batch loss 1.7667, batch acc 0.8444
12:16:16.660   Training iter 350, batch loss 1.7649, batch acc 0.8518
12:16:17.188   Training iter 400, batch loss 1.7627, batch acc 0.8436
12:16:17.705   Training iter 450, batch loss 1.7674, batch acc 0.8374
12:16:18.220   Training iter 500, batch loss 1.7644, batch acc 0.8434
12:16:18.730   Training iter 550, batch loss 1.7697, batch acc 0.8348
12:16:19.243   Training iter 600, batch loss 1.7649, batch acc 0.8434
12:16:19.245 Testing @ 95 epoch...
12:16:19.291     Testing, total mean loss 1.75686, total acc 0.85440
12:16:19.291 Training @ 96 epoch...
12:16:19.818   Training iter 50, batch loss 1.7675, batch acc 0.8434
12:16:20.347   Training iter 100, batch loss 1.7614, batch acc 0.8508
12:16:20.881   Training iter 150, batch loss 1.7635, batch acc 0.8446
12:16:21.411   Training iter 200, batch loss 1.7630, batch acc 0.8508
12:16:21.970   Training iter 250, batch loss 1.7596, batch acc 0.8486
12:16:22.530   Training iter 300, batch loss 1.7636, batch acc 0.8446
12:16:23.084   Training iter 350, batch loss 1.7637, batch acc 0.8450
12:16:23.635   Training iter 400, batch loss 1.7657, batch acc 0.8438
12:16:24.193   Training iter 450, batch loss 1.7650, batch acc 0.8438
12:16:24.753   Training iter 500, batch loss 1.7640, batch acc 0.8438
12:16:25.326   Training iter 550, batch loss 1.7636, batch acc 0.8462
12:16:25.882   Training iter 600, batch loss 1.7676, batch acc 0.8412
12:16:25.884 Training @ 97 epoch...
12:16:26.447   Training iter 50, batch loss 1.7650, batch acc 0.8390
12:16:27.001   Training iter 100, batch loss 1.7627, batch acc 0.8466
12:16:27.561   Training iter 150, batch loss 1.7621, batch acc 0.8488
12:16:28.088   Training iter 200, batch loss 1.7652, batch acc 0.8412
12:16:28.615   Training iter 250, batch loss 1.7595, batch acc 0.8546
12:16:29.143   Training iter 300, batch loss 1.7633, batch acc 0.8450
12:16:29.672   Training iter 350, batch loss 1.7622, batch acc 0.8486
12:16:30.191   Training iter 400, batch loss 1.7636, batch acc 0.8492
12:16:30.722   Training iter 450, batch loss 1.7644, batch acc 0.8486
12:16:31.239   Training iter 500, batch loss 1.7671, batch acc 0.8400
12:16:31.750   Training iter 550, batch loss 1.7665, batch acc 0.8426
12:16:32.269   Training iter 600, batch loss 1.7603, batch acc 0.8444
12:16:32.271 Training @ 98 epoch...
12:16:32.794   Training iter 50, batch loss 1.7648, batch acc 0.8510
12:16:33.318   Training iter 100, batch loss 1.7567, batch acc 0.8500
12:16:33.852   Training iter 150, batch loss 1.7696, batch acc 0.8340
12:16:34.388   Training iter 200, batch loss 1.7661, batch acc 0.8408
12:16:34.926   Training iter 250, batch loss 1.7644, batch acc 0.8454
12:16:35.458   Training iter 300, batch loss 1.7609, batch acc 0.8470
12:16:35.976   Training iter 350, batch loss 1.7632, batch acc 0.8498
12:16:36.493   Training iter 400, batch loss 1.7626, batch acc 0.8376
12:16:36.984   Training iter 450, batch loss 1.7598, batch acc 0.8530
12:16:37.476   Training iter 500, batch loss 1.7614, batch acc 0.8496
12:16:37.979   Training iter 550, batch loss 1.7630, batch acc 0.8412
12:16:38.513   Training iter 600, batch loss 1.7634, batch acc 0.8508
12:16:38.515 Training @ 99 epoch...
12:16:39.059   Training iter 50, batch loss 1.7626, batch acc 0.8452
12:16:39.585   Training iter 100, batch loss 1.7592, batch acc 0.8508
12:16:40.082   Training iter 150, batch loss 1.7579, batch acc 0.8554
12:16:40.575   Training iter 200, batch loss 1.7630, batch acc 0.8470
12:16:41.066   Training iter 250, batch loss 1.7618, batch acc 0.8448
12:16:41.575   Training iter 300, batch loss 1.7668, batch acc 0.8468
12:16:42.120   Training iter 350, batch loss 1.7655, batch acc 0.8386
12:16:42.657   Training iter 400, batch loss 1.7669, batch acc 0.8378
12:16:43.176   Training iter 450, batch loss 1.7584, batch acc 0.8508
12:16:43.683   Training iter 500, batch loss 1.7642, batch acc 0.8456
12:16:44.167   Training iter 550, batch loss 1.7658, batch acc 0.8416
12:16:44.650   Training iter 600, batch loss 1.7580, batch acc 0.8514
12:16:44.651 Training @ 100 epoch...
12:16:45.153   Training iter 50, batch loss 1.7605, batch acc 0.8484
12:16:45.647   Training iter 100, batch loss 1.7601, batch acc 0.8478
12:16:46.136   Training iter 150, batch loss 1.7671, batch acc 0.8384
12:16:46.631   Training iter 200, batch loss 1.7587, batch acc 0.8556
12:16:47.102   Training iter 250, batch loss 1.7577, batch acc 0.8530
12:16:47.575   Training iter 300, batch loss 1.7641, batch acc 0.8442
12:16:48.031   Training iter 350, batch loss 1.7633, batch acc 0.8430
12:16:48.507   Training iter 400, batch loss 1.7570, batch acc 0.8540
12:16:49.008   Training iter 450, batch loss 1.7625, batch acc 0.8470
12:16:49.505   Training iter 500, batch loss 1.7650, batch acc 0.8436
12:16:50.010   Training iter 550, batch loss 1.7631, batch acc 0.8402
12:16:50.514   Training iter 600, batch loss 1.7652, batch acc 0.8402
12:16:50.515 Testing @ 100 epoch...
12:16:50.561     Testing, total mean loss 1.75439, total acc 0.85530
12:16:50.561 Plot @ 100 epoch...
12:16:50.561 Training @ 101 epoch...
12:16:51.062   Training iter 50, batch loss 1.7581, batch acc 0.8498
12:16:51.560   Training iter 100, batch loss 1.7637, batch acc 0.8432
12:16:52.052   Training iter 150, batch loss 1.7643, batch acc 0.8458
12:16:52.543   Training iter 200, batch loss 1.7645, batch acc 0.8448
12:16:53.039   Training iter 250, batch loss 1.7585, batch acc 0.8500
12:16:53.527   Training iter 300, batch loss 1.7639, batch acc 0.8474
12:16:54.004   Training iter 350, batch loss 1.7592, batch acc 0.8444
12:16:54.489   Training iter 400, batch loss 1.7591, batch acc 0.8514
12:16:54.996   Training iter 450, batch loss 1.7637, batch acc 0.8458
12:16:55.493   Training iter 500, batch loss 1.7607, batch acc 0.8458
12:16:55.992   Training iter 550, batch loss 1.7612, batch acc 0.8446
12:16:56.506   Training iter 600, batch loss 1.7616, batch acc 0.8420
12:16:56.508 Training @ 102 epoch...
12:16:57.026   Training iter 50, batch loss 1.7590, batch acc 0.8526
12:16:57.502   Training iter 100, batch loss 1.7595, batch acc 0.8482
12:16:57.989   Training iter 150, batch loss 1.7624, batch acc 0.8424
12:16:58.457   Training iter 200, batch loss 1.7660, batch acc 0.8434
12:16:58.919   Training iter 250, batch loss 1.7620, batch acc 0.8484
12:16:59.399   Training iter 300, batch loss 1.7602, batch acc 0.8452
12:16:59.905   Training iter 350, batch loss 1.7575, batch acc 0.8512
12:17:00.414   Training iter 400, batch loss 1.7608, batch acc 0.8396
12:17:00.886   Training iter 450, batch loss 1.7651, batch acc 0.8446
12:17:01.365   Training iter 500, batch loss 1.7600, batch acc 0.8520
12:17:01.914   Training iter 550, batch loss 1.7561, batch acc 0.8542
12:17:02.428   Training iter 600, batch loss 1.7644, batch acc 0.8394
12:17:02.430 Training @ 103 epoch...
12:17:02.967   Training iter 50, batch loss 1.7601, batch acc 0.8508
12:17:03.473   Training iter 100, batch loss 1.7586, batch acc 0.8480
12:17:03.989   Training iter 150, batch loss 1.7645, batch acc 0.8418
12:17:04.506   Training iter 200, batch loss 1.7589, batch acc 0.8522
12:17:05.007   Training iter 250, batch loss 1.7621, batch acc 0.8402
12:17:05.553   Training iter 300, batch loss 1.7581, batch acc 0.8532
12:17:06.102   Training iter 350, batch loss 1.7657, batch acc 0.8364
12:17:06.652   Training iter 400, batch loss 1.7613, batch acc 0.8498
12:17:07.167   Training iter 450, batch loss 1.7597, batch acc 0.8500
12:17:07.675   Training iter 500, batch loss 1.7587, batch acc 0.8464
12:17:08.176   Training iter 550, batch loss 1.7608, batch acc 0.8464
12:17:08.694   Training iter 600, batch loss 1.7592, batch acc 0.8478
12:17:08.695 Training @ 104 epoch...
12:17:09.246   Training iter 50, batch loss 1.7584, batch acc 0.8524
12:17:09.772   Training iter 100, batch loss 1.7600, batch acc 0.8508
12:17:10.290   Training iter 150, batch loss 1.7617, batch acc 0.8486
12:17:10.795   Training iter 200, batch loss 1.7586, batch acc 0.8462
12:17:11.315   Training iter 250, batch loss 1.7586, batch acc 0.8500
12:17:11.824   Training iter 300, batch loss 1.7618, batch acc 0.8518
12:17:12.342   Training iter 350, batch loss 1.7616, batch acc 0.8398
12:17:12.886   Training iter 400, batch loss 1.7596, batch acc 0.8408
12:17:13.441   Training iter 450, batch loss 1.7621, batch acc 0.8410
12:17:13.996   Training iter 500, batch loss 1.7607, batch acc 0.8396
12:17:14.558   Training iter 550, batch loss 1.7566, batch acc 0.8560
12:17:15.105   Training iter 600, batch loss 1.7628, batch acc 0.8466
12:17:15.106 Training @ 105 epoch...
12:17:15.644   Training iter 50, batch loss 1.7556, batch acc 0.8546
12:17:16.188   Training iter 100, batch loss 1.7601, batch acc 0.8482
12:17:16.721   Training iter 150, batch loss 1.7614, batch acc 0.8426
12:17:17.251   Training iter 200, batch loss 1.7616, batch acc 0.8440
12:17:17.773   Training iter 250, batch loss 1.7606, batch acc 0.8436
12:17:18.306   Training iter 300, batch loss 1.7619, batch acc 0.8446
12:17:18.833   Training iter 350, batch loss 1.7654, batch acc 0.8400
12:17:19.360   Training iter 400, batch loss 1.7574, batch acc 0.8556
12:17:19.890   Training iter 450, batch loss 1.7592, batch acc 0.8438
12:17:20.436   Training iter 500, batch loss 1.7600, batch acc 0.8500
12:17:20.964   Training iter 550, batch loss 1.7559, batch acc 0.8518
12:17:21.490   Training iter 600, batch loss 1.7581, batch acc 0.8484
12:17:21.492 Testing @ 105 epoch...
12:17:21.539     Testing, total mean loss 1.75214, total acc 0.85570
12:17:21.539 Training @ 106 epoch...
12:17:22.101   Training iter 50, batch loss 1.7617, batch acc 0.8416
12:17:22.679   Training iter 100, batch loss 1.7611, batch acc 0.8412
12:17:23.265   Training iter 150, batch loss 1.7618, batch acc 0.8438
12:17:23.850   Training iter 200, batch loss 1.7617, batch acc 0.8430
12:17:24.445   Training iter 250, batch loss 1.7541, batch acc 0.8582
12:17:25.025   Training iter 300, batch loss 1.7619, batch acc 0.8466
12:17:25.633   Training iter 350, batch loss 1.7574, batch acc 0.8484
12:17:26.249   Training iter 400, batch loss 1.7534, batch acc 0.8578
12:17:26.869   Training iter 450, batch loss 1.7616, batch acc 0.8450
12:17:27.497   Training iter 500, batch loss 1.7583, batch acc 0.8500
12:17:28.129   Training iter 550, batch loss 1.7574, batch acc 0.8500
12:17:28.751   Training iter 600, batch loss 1.7617, batch acc 0.8428
12:17:28.753 Training @ 107 epoch...
12:17:29.378   Training iter 50, batch loss 1.7585, batch acc 0.8488
12:17:30.016   Training iter 100, batch loss 1.7566, batch acc 0.8492
12:17:30.633   Training iter 150, batch loss 1.7552, batch acc 0.8486
12:17:31.279   Training iter 200, batch loss 1.7593, batch acc 0.8438
12:17:31.914   Training iter 250, batch loss 1.7575, batch acc 0.8536
12:17:32.532   Training iter 300, batch loss 1.7620, batch acc 0.8368
12:17:33.096   Training iter 350, batch loss 1.7584, batch acc 0.8502
12:17:33.669   Training iter 400, batch loss 1.7589, batch acc 0.8458
12:17:34.228   Training iter 450, batch loss 1.7566, batch acc 0.8502
12:17:34.789   Training iter 500, batch loss 1.7599, batch acc 0.8498
12:17:35.358   Training iter 550, batch loss 1.7583, batch acc 0.8514
12:17:35.902   Training iter 600, batch loss 1.7659, batch acc 0.8386
12:17:35.904 Training @ 108 epoch...
12:17:36.478   Training iter 50, batch loss 1.7611, batch acc 0.8452
12:17:37.014   Training iter 100, batch loss 1.7619, batch acc 0.8428
12:17:37.501   Training iter 150, batch loss 1.7586, batch acc 0.8472
12:17:37.992   Training iter 200, batch loss 1.7591, batch acc 0.8470
12:17:38.502   Training iter 250, batch loss 1.7593, batch acc 0.8476
12:17:38.998   Training iter 300, batch loss 1.7584, batch acc 0.8492
12:17:39.494   Training iter 350, batch loss 1.7619, batch acc 0.8432
12:17:39.993   Training iter 400, batch loss 1.7552, batch acc 0.8540
12:17:40.499   Training iter 450, batch loss 1.7545, batch acc 0.8528
12:17:40.985   Training iter 500, batch loss 1.7552, batch acc 0.8508
12:17:41.491   Training iter 550, batch loss 1.7591, batch acc 0.8430
12:17:41.989   Training iter 600, batch loss 1.7577, batch acc 0.8512
12:17:41.991 Training @ 109 epoch...
12:17:42.500   Training iter 50, batch loss 1.7606, batch acc 0.8420
12:17:43.005   Training iter 100, batch loss 1.7545, batch acc 0.8528
12:17:43.500   Training iter 150, batch loss 1.7553, batch acc 0.8522
12:17:43.995   Training iter 200, batch loss 1.7633, batch acc 0.8432
12:17:44.503   Training iter 250, batch loss 1.7575, batch acc 0.8514
12:17:44.980   Training iter 300, batch loss 1.7567, batch acc 0.8504
12:17:45.458   Training iter 350, batch loss 1.7591, batch acc 0.8474
12:17:45.904   Training iter 400, batch loss 1.7625, batch acc 0.8388
12:17:46.381   Training iter 450, batch loss 1.7574, batch acc 0.8484
12:17:46.846   Training iter 500, batch loss 1.7582, batch acc 0.8520
12:17:47.319   Training iter 550, batch loss 1.7565, batch acc 0.8546
12:17:47.794   Training iter 600, batch loss 1.7557, batch acc 0.8432
12:17:47.796 Training @ 110 epoch...
12:17:48.255   Training iter 50, batch loss 1.7606, batch acc 0.8418
12:17:48.755   Training iter 100, batch loss 1.7580, batch acc 0.8488
12:17:49.249   Training iter 150, batch loss 1.7571, batch acc 0.8458
12:17:49.758   Training iter 200, batch loss 1.7589, batch acc 0.8458
12:17:50.271   Training iter 250, batch loss 1.7591, batch acc 0.8524
12:17:50.776   Training iter 300, batch loss 1.7589, batch acc 0.8458
12:17:51.267   Training iter 350, batch loss 1.7529, batch acc 0.8518
12:17:51.746   Training iter 400, batch loss 1.7585, batch acc 0.8412
12:17:52.249   Training iter 450, batch loss 1.7600, batch acc 0.8474
12:17:52.757   Training iter 500, batch loss 1.7587, batch acc 0.8492
12:17:53.253   Training iter 550, batch loss 1.7530, batch acc 0.8568
12:17:53.745   Training iter 600, batch loss 1.7570, batch acc 0.8504
12:17:53.747 Testing @ 110 epoch...
12:17:53.793     Testing, total mean loss 1.75012, total acc 0.85650
12:17:53.793 Training @ 111 epoch...
12:17:54.292   Training iter 50, batch loss 1.7631, batch acc 0.8348
12:17:54.783   Training iter 100, batch loss 1.7595, batch acc 0.8456
12:17:55.257   Training iter 150, batch loss 1.7594, batch acc 0.8544
12:17:55.715   Training iter 200, batch loss 1.7539, batch acc 0.8560
12:17:56.169   Training iter 250, batch loss 1.7562, batch acc 0.8514
12:17:56.659   Training iter 300, batch loss 1.7547, batch acc 0.8456
12:17:57.161   Training iter 350, batch loss 1.7564, batch acc 0.8470
12:17:57.655   Training iter 400, batch loss 1.7588, batch acc 0.8450
12:17:58.154   Training iter 450, batch loss 1.7585, batch acc 0.8438
12:17:58.675   Training iter 500, batch loss 1.7577, batch acc 0.8532
12:17:59.179   Training iter 550, batch loss 1.7610, batch acc 0.8422
12:17:59.677   Training iter 600, batch loss 1.7489, batch acc 0.8596
12:17:59.679 Training @ 112 epoch...
12:18:00.200   Training iter 50, batch loss 1.7553, batch acc 0.8508
12:18:00.741   Training iter 100, batch loss 1.7568, batch acc 0.8448
12:18:01.268   Training iter 150, batch loss 1.7579, batch acc 0.8494
12:18:01.845   Training iter 200, batch loss 1.7531, batch acc 0.8564
12:18:02.421   Training iter 250, batch loss 1.7575, batch acc 0.8476
12:18:02.973   Training iter 300, batch loss 1.7584, batch acc 0.8456
12:18:03.520   Training iter 350, batch loss 1.7576, batch acc 0.8422
12:18:04.074   Training iter 400, batch loss 1.7598, batch acc 0.8432
12:18:04.661   Training iter 450, batch loss 1.7596, batch acc 0.8474
12:18:05.252   Training iter 500, batch loss 1.7574, batch acc 0.8484
12:18:05.834   Training iter 550, batch loss 1.7545, batch acc 0.8512
12:18:06.361   Training iter 600, batch loss 1.7554, batch acc 0.8512
12:18:06.363 Training @ 113 epoch...
12:18:06.912   Training iter 50, batch loss 1.7561, batch acc 0.8472
12:18:07.469   Training iter 100, batch loss 1.7520, batch acc 0.8556
12:18:08.015   Training iter 150, batch loss 1.7593, batch acc 0.8496
12:18:08.510   Training iter 200, batch loss 1.7547, batch acc 0.8508
12:18:08.980   Training iter 250, batch loss 1.7543, batch acc 0.8532
12:18:09.461   Training iter 300, batch loss 1.7536, batch acc 0.8536
12:18:09.946   Training iter 350, batch loss 1.7612, batch acc 0.8412
12:18:10.455   Training iter 400, batch loss 1.7583, batch acc 0.8444
12:18:10.977   Training iter 450, batch loss 1.7610, batch acc 0.8438
12:18:11.477   Training iter 500, batch loss 1.7541, batch acc 0.8460
12:18:11.932   Training iter 550, batch loss 1.7584, batch acc 0.8462
12:18:12.393   Training iter 600, batch loss 1.7559, batch acc 0.8500
12:18:12.394 Training @ 114 epoch...
12:18:12.862   Training iter 50, batch loss 1.7531, batch acc 0.8490
12:18:13.333   Training iter 100, batch loss 1.7552, batch acc 0.8556
12:18:13.815   Training iter 150, batch loss 1.7570, batch acc 0.8508
12:18:14.291   Training iter 200, batch loss 1.7566, batch acc 0.8502
12:18:14.769   Training iter 250, batch loss 1.7552, batch acc 0.8508
12:18:15.243   Training iter 300, batch loss 1.7612, batch acc 0.8364
12:18:15.716   Training iter 350, batch loss 1.7536, batch acc 0.8516
12:18:16.197   Training iter 400, batch loss 1.7581, batch acc 0.8480
12:18:16.693   Training iter 450, batch loss 1.7524, batch acc 0.8516
12:18:17.185   Training iter 500, batch loss 1.7558, batch acc 0.8486
12:18:17.656   Training iter 550, batch loss 1.7567, batch acc 0.8538
12:18:18.144   Training iter 600, batch loss 1.7599, batch acc 0.8418
12:18:18.145 Training @ 115 epoch...
12:18:18.653   Training iter 50, batch loss 1.7568, batch acc 0.8410
12:18:19.154   Training iter 100, batch loss 1.7537, batch acc 0.8572
12:18:19.689   Training iter 150, batch loss 1.7589, batch acc 0.8468
12:18:20.235   Training iter 200, batch loss 1.7564, batch acc 0.8486
12:18:20.763   Training iter 250, batch loss 1.7568, batch acc 0.8460
12:18:21.267   Training iter 300, batch loss 1.7560, batch acc 0.8506
12:18:21.765   Training iter 350, batch loss 1.7549, batch acc 0.8498
12:18:22.265   Training iter 400, batch loss 1.7562, batch acc 0.8500
12:18:22.776   Training iter 450, batch loss 1.7553, batch acc 0.8462
12:18:23.294   Training iter 500, batch loss 1.7558, batch acc 0.8528
12:18:23.803   Training iter 550, batch loss 1.7563, batch acc 0.8428
12:18:24.329   Training iter 600, batch loss 1.7533, batch acc 0.8546
12:18:24.330 Testing @ 115 epoch...
12:18:24.377     Testing, total mean loss 1.74828, total acc 0.85780
12:18:24.377 Training @ 116 epoch...
12:18:24.904   Training iter 50, batch loss 1.7586, batch acc 0.8490
12:18:25.445   Training iter 100, batch loss 1.7514, batch acc 0.8564
12:18:25.967   Training iter 150, batch loss 1.7549, batch acc 0.8498
12:18:26.500   Training iter 200, batch loss 1.7558, batch acc 0.8520
12:18:27.039   Training iter 250, batch loss 1.7600, batch acc 0.8376
12:18:27.591   Training iter 300, batch loss 1.7516, batch acc 0.8506
12:18:28.121   Training iter 350, batch loss 1.7573, batch acc 0.8396
12:18:28.651   Training iter 400, batch loss 1.7551, batch acc 0.8488
12:18:29.165   Training iter 450, batch loss 1.7518, batch acc 0.8548
12:18:29.663   Training iter 500, batch loss 1.7548, batch acc 0.8502
12:18:30.181   Training iter 550, batch loss 1.7604, batch acc 0.8436
12:18:30.721   Training iter 600, batch loss 1.7544, batch acc 0.8536
12:18:30.723 Training @ 117 epoch...
12:18:31.258   Training iter 50, batch loss 1.7539, batch acc 0.8524
12:18:31.788   Training iter 100, batch loss 1.7582, batch acc 0.8422
12:18:32.313   Training iter 150, batch loss 1.7542, batch acc 0.8456
12:18:32.846   Training iter 200, batch loss 1.7552, batch acc 0.8452
12:18:33.385   Training iter 250, batch loss 1.7553, batch acc 0.8510
12:18:33.924   Training iter 300, batch loss 1.7518, batch acc 0.8552
12:18:34.465   Training iter 350, batch loss 1.7508, batch acc 0.8534
12:18:35.004   Training iter 400, batch loss 1.7580, batch acc 0.8490
12:18:35.543   Training iter 450, batch loss 1.7565, batch acc 0.8476
12:18:36.066   Training iter 500, batch loss 1.7571, batch acc 0.8508
12:18:36.595   Training iter 550, batch loss 1.7541, batch acc 0.8524
12:18:37.130   Training iter 600, batch loss 1.7567, batch acc 0.8470
12:18:37.132 Training @ 118 epoch...
12:18:37.667   Training iter 50, batch loss 1.7569, batch acc 0.8492
12:18:38.202   Training iter 100, batch loss 1.7533, batch acc 0.8456
12:18:38.737   Training iter 150, batch loss 1.7551, batch acc 0.8498
12:18:39.261   Training iter 200, batch loss 1.7551, batch acc 0.8434
12:18:39.797   Training iter 250, batch loss 1.7482, batch acc 0.8546
12:18:40.329   Training iter 300, batch loss 1.7541, batch acc 0.8512
12:18:40.876   Training iter 350, batch loss 1.7572, batch acc 0.8512
12:18:41.407   Training iter 400, batch loss 1.7579, batch acc 0.8466
12:18:41.936   Training iter 450, batch loss 1.7545, batch acc 0.8498
12:18:42.462   Training iter 500, batch loss 1.7562, batch acc 0.8482
12:18:42.995   Training iter 550, batch loss 1.7569, batch acc 0.8518
12:18:43.527   Training iter 600, batch loss 1.7525, batch acc 0.8526
12:18:43.529 Training @ 119 epoch...
12:18:44.058   Training iter 50, batch loss 1.7529, batch acc 0.8490
12:18:44.575   Training iter 100, batch loss 1.7557, batch acc 0.8434
12:18:45.115   Training iter 150, batch loss 1.7564, batch acc 0.8478
12:18:45.651   Training iter 200, batch loss 1.7587, batch acc 0.8430
12:18:46.188   Training iter 250, batch loss 1.7602, batch acc 0.8460
12:18:46.721   Training iter 300, batch loss 1.7496, batch acc 0.8584
12:18:47.257   Training iter 350, batch loss 1.7559, batch acc 0.8470
12:18:47.800   Training iter 400, batch loss 1.7548, batch acc 0.8506
12:18:48.351   Training iter 450, batch loss 1.7502, batch acc 0.8556
12:18:48.887   Training iter 500, batch loss 1.7553, batch acc 0.8446
12:18:49.433   Training iter 550, batch loss 1.7505, batch acc 0.8588
12:18:49.988   Training iter 600, batch loss 1.7536, batch acc 0.8488
12:18:49.990 Training @ 120 epoch...
12:18:50.540   Training iter 50, batch loss 1.7555, batch acc 0.8492
12:18:51.079   Training iter 100, batch loss 1.7534, batch acc 0.8580
12:18:51.598   Training iter 150, batch loss 1.7543, batch acc 0.8514
12:18:52.115   Training iter 200, batch loss 1.7517, batch acc 0.8460
12:18:52.647   Training iter 250, batch loss 1.7557, batch acc 0.8496
12:18:53.194   Training iter 300, batch loss 1.7558, batch acc 0.8452
12:18:53.732   Training iter 350, batch loss 1.7575, batch acc 0.8406
12:18:54.249   Training iter 400, batch loss 1.7568, batch acc 0.8410
12:18:54.764   Training iter 450, batch loss 1.7483, batch acc 0.8558
12:18:55.294   Training iter 500, batch loss 1.7525, batch acc 0.8576
12:18:55.817   Training iter 550, batch loss 1.7508, batch acc 0.8542
12:18:56.335   Training iter 600, batch loss 1.7576, batch acc 0.8456
12:18:56.337 Testing @ 120 epoch...
12:18:56.383     Testing, total mean loss 1.74661, total acc 0.85820
12:18:56.383 Training @ 121 epoch...
12:18:56.911   Training iter 50, batch loss 1.7531, batch acc 0.8504
12:18:57.438   Training iter 100, batch loss 1.7579, batch acc 0.8456
12:18:57.967   Training iter 150, batch loss 1.7508, batch acc 0.8530
12:18:58.503   Training iter 200, batch loss 1.7579, batch acc 0.8460
12:18:59.035   Training iter 250, batch loss 1.7535, batch acc 0.8488
12:18:59.588   Training iter 300, batch loss 1.7559, batch acc 0.8410
12:19:00.169   Training iter 350, batch loss 1.7524, batch acc 0.8534
12:19:00.739   Training iter 400, batch loss 1.7557, batch acc 0.8536
12:19:01.322   Training iter 450, batch loss 1.7522, batch acc 0.8488
12:19:01.896   Training iter 500, batch loss 1.7518, batch acc 0.8524
12:19:02.474   Training iter 550, batch loss 1.7466, batch acc 0.8584
12:19:03.033   Training iter 600, batch loss 1.7582, batch acc 0.8460
12:19:03.035 Training @ 122 epoch...
12:19:03.569   Training iter 50, batch loss 1.7532, batch acc 0.8452
12:19:04.120   Training iter 100, batch loss 1.7528, batch acc 0.8510
12:19:04.661   Training iter 150, batch loss 1.7508, batch acc 0.8524
12:19:05.192   Training iter 200, batch loss 1.7546, batch acc 0.8464
12:19:05.726   Training iter 250, batch loss 1.7572, batch acc 0.8428
12:19:06.283   Training iter 300, batch loss 1.7538, batch acc 0.8518
12:19:06.841   Training iter 350, batch loss 1.7555, batch acc 0.8502
12:19:07.363   Training iter 400, batch loss 1.7538, batch acc 0.8466
12:19:07.885   Training iter 450, batch loss 1.7599, batch acc 0.8434
12:19:08.412   Training iter 500, batch loss 1.7488, batch acc 0.8602
12:19:08.943   Training iter 550, batch loss 1.7507, batch acc 0.8534
12:19:09.462   Training iter 600, batch loss 1.7513, batch acc 0.8544
12:19:09.463 Training @ 123 epoch...
12:19:09.998   Training iter 50, batch loss 1.7546, batch acc 0.8456
12:19:10.535   Training iter 100, batch loss 1.7521, batch acc 0.8502
12:19:11.073   Training iter 150, batch loss 1.7577, batch acc 0.8478
12:19:11.605   Training iter 200, batch loss 1.7541, batch acc 0.8490
12:19:12.142   Training iter 250, batch loss 1.7540, batch acc 0.8498
12:19:12.673   Training iter 300, batch loss 1.7530, batch acc 0.8498
12:19:13.210   Training iter 350, batch loss 1.7540, batch acc 0.8496
12:19:13.743   Training iter 400, batch loss 1.7526, batch acc 0.8480
12:19:14.250   Training iter 450, batch loss 1.7480, batch acc 0.8572
12:19:14.770   Training iter 500, batch loss 1.7529, batch acc 0.8558
12:19:15.297   Training iter 550, batch loss 1.7507, batch acc 0.8542
12:19:15.818   Training iter 600, batch loss 1.7552, batch acc 0.8426
12:19:15.820 Training @ 124 epoch...
12:19:16.360   Training iter 50, batch loss 1.7503, batch acc 0.8570
12:19:16.898   Training iter 100, batch loss 1.7472, batch acc 0.8580
12:19:17.437   Training iter 150, batch loss 1.7537, batch acc 0.8492
12:19:17.994   Training iter 200, batch loss 1.7558, batch acc 0.8418
12:19:18.546   Training iter 250, batch loss 1.7519, batch acc 0.8498
12:19:19.098   Training iter 300, batch loss 1.7542, batch acc 0.8434
12:19:19.652   Training iter 350, batch loss 1.7535, batch acc 0.8482
12:19:20.207   Training iter 400, batch loss 1.7533, batch acc 0.8560
12:19:20.765   Training iter 450, batch loss 1.7539, batch acc 0.8516
12:19:21.314   Training iter 500, batch loss 1.7520, batch acc 0.8522
12:19:21.852   Training iter 550, batch loss 1.7548, batch acc 0.8470
12:19:22.408   Training iter 600, batch loss 1.7544, batch acc 0.8476
12:19:22.410 Training @ 125 epoch...
12:19:22.958   Training iter 50, batch loss 1.7522, batch acc 0.8462
12:19:23.501   Training iter 100, batch loss 1.7510, batch acc 0.8502
12:19:24.009   Training iter 150, batch loss 1.7565, batch acc 0.8466
12:19:24.516   Training iter 200, batch loss 1.7529, batch acc 0.8544
12:19:25.048   Training iter 250, batch loss 1.7562, batch acc 0.8466
12:19:25.580   Training iter 300, batch loss 1.7508, batch acc 0.8506
12:19:26.104   Training iter 350, batch loss 1.7512, batch acc 0.8506
12:19:26.633   Training iter 400, batch loss 1.7534, batch acc 0.8478
12:19:27.164   Training iter 450, batch loss 1.7516, batch acc 0.8488
12:19:27.691   Training iter 500, batch loss 1.7479, batch acc 0.8618
12:19:28.215   Training iter 550, batch loss 1.7566, batch acc 0.8444
12:19:28.742   Training iter 600, batch loss 1.7510, batch acc 0.8518
12:19:28.744 Testing @ 125 epoch...
12:19:28.789     Testing, total mean loss 1.74508, total acc 0.85950
12:19:28.789 Training @ 126 epoch...
12:19:29.345   Training iter 50, batch loss 1.7536, batch acc 0.8490
12:19:29.887   Training iter 100, batch loss 1.7495, batch acc 0.8480
12:19:30.431   Training iter 150, batch loss 1.7490, batch acc 0.8546
12:19:30.944   Training iter 200, batch loss 1.7560, batch acc 0.8468
12:19:31.457   Training iter 250, batch loss 1.7539, batch acc 0.8494
12:19:31.982   Training iter 300, batch loss 1.7551, batch acc 0.8494
12:19:32.514   Training iter 350, batch loss 1.7540, batch acc 0.8438
12:19:33.057   Training iter 400, batch loss 1.7512, batch acc 0.8474
12:19:33.620   Training iter 450, batch loss 1.7524, batch acc 0.8568
12:19:34.157   Training iter 500, batch loss 1.7501, batch acc 0.8538
12:19:34.719   Training iter 550, batch loss 1.7493, batch acc 0.8564
12:19:35.275   Training iter 600, batch loss 1.7538, batch acc 0.8486
12:19:35.277 Training @ 127 epoch...
12:19:35.817   Training iter 50, batch loss 1.7558, batch acc 0.8418
12:19:36.354   Training iter 100, batch loss 1.7516, batch acc 0.8510
12:19:36.877   Training iter 150, batch loss 1.7496, batch acc 0.8514
12:19:37.387   Training iter 200, batch loss 1.7585, batch acc 0.8408
12:19:37.895   Training iter 250, batch loss 1.7520, batch acc 0.8488
12:19:38.407   Training iter 300, batch loss 1.7461, batch acc 0.8620
12:19:38.916   Training iter 350, batch loss 1.7520, batch acc 0.8530
12:19:39.429   Training iter 400, batch loss 1.7519, batch acc 0.8524
12:19:39.908   Training iter 450, batch loss 1.7536, batch acc 0.8514
12:19:40.384   Training iter 500, batch loss 1.7552, batch acc 0.8470
12:19:40.854   Training iter 550, batch loss 1.7476, batch acc 0.8540
12:19:41.334   Training iter 600, batch loss 1.7507, batch acc 0.8516
12:19:41.336 Training @ 128 epoch...
12:19:41.810   Training iter 50, batch loss 1.7490, batch acc 0.8548
12:19:42.282   Training iter 100, batch loss 1.7484, batch acc 0.8590
12:19:42.754   Training iter 150, batch loss 1.7548, batch acc 0.8490
12:19:43.233   Training iter 200, batch loss 1.7550, batch acc 0.8436
12:19:43.702   Training iter 250, batch loss 1.7488, batch acc 0.8524
12:19:44.171   Training iter 300, batch loss 1.7557, batch acc 0.8424
12:19:44.641   Training iter 350, batch loss 1.7470, batch acc 0.8510
12:19:45.108   Training iter 400, batch loss 1.7538, batch acc 0.8472
12:19:45.576   Training iter 450, batch loss 1.7559, batch acc 0.8482
12:19:46.033   Training iter 500, batch loss 1.7492, batch acc 0.8558
12:19:46.482   Training iter 550, batch loss 1.7540, batch acc 0.8494
12:19:46.920   Training iter 600, batch loss 1.7497, batch acc 0.8508
12:19:46.922 Training @ 129 epoch...
12:19:47.374   Training iter 50, batch loss 1.7520, batch acc 0.8414
12:19:47.813   Training iter 100, batch loss 1.7517, batch acc 0.8490
12:19:48.269   Training iter 150, batch loss 1.7534, batch acc 0.8492
12:19:48.723   Training iter 200, batch loss 1.7476, batch acc 0.8598
12:19:49.179   Training iter 250, batch loss 1.7552, batch acc 0.8390
12:19:49.678   Training iter 300, batch loss 1.7571, batch acc 0.8410
12:19:50.172   Training iter 350, batch loss 1.7467, batch acc 0.8608
12:19:50.666   Training iter 400, batch loss 1.7535, batch acc 0.8522
12:19:51.209   Training iter 450, batch loss 1.7461, batch acc 0.8652
12:19:51.747   Training iter 500, batch loss 1.7516, batch acc 0.8470
12:19:52.310   Training iter 550, batch loss 1.7513, batch acc 0.8480
12:19:52.865   Training iter 600, batch loss 1.7516, batch acc 0.8558
12:19:52.866 Training @ 130 epoch...
12:19:53.427   Training iter 50, batch loss 1.7485, batch acc 0.8508
12:19:53.962   Training iter 100, batch loss 1.7524, batch acc 0.8516
12:19:54.530   Training iter 150, batch loss 1.7496, batch acc 0.8536
12:19:55.120   Training iter 200, batch loss 1.7536, batch acc 0.8482
12:19:55.683   Training iter 250, batch loss 1.7484, batch acc 0.8472
12:19:56.217   Training iter 300, batch loss 1.7541, batch acc 0.8502
12:19:56.777   Training iter 350, batch loss 1.7506, batch acc 0.8518
12:19:57.344   Training iter 400, batch loss 1.7509, batch acc 0.8490
12:19:57.967   Training iter 450, batch loss 1.7530, batch acc 0.8546
12:19:58.606   Training iter 500, batch loss 1.7494, batch acc 0.8530
12:19:59.236   Training iter 550, batch loss 1.7492, batch acc 0.8562
12:19:59.800   Training iter 600, batch loss 1.7547, batch acc 0.8426
12:19:59.802 Testing @ 130 epoch...
12:19:59.855     Testing, total mean loss 1.74368, total acc 0.86010
12:19:59.855 Training @ 131 epoch...
12:20:00.440   Training iter 50, batch loss 1.7519, batch acc 0.8510
12:20:01.004   Training iter 100, batch loss 1.7518, batch acc 0.8490
12:20:01.619   Training iter 150, batch loss 1.7519, batch acc 0.8526
12:20:02.260   Training iter 200, batch loss 1.7522, batch acc 0.8494
12:20:02.863   Training iter 250, batch loss 1.7566, batch acc 0.8410
12:20:03.413   Training iter 300, batch loss 1.7533, batch acc 0.8482
12:20:03.959   Training iter 350, batch loss 1.7474, batch acc 0.8548
12:20:04.507   Training iter 400, batch loss 1.7460, batch acc 0.8590
12:20:05.063   Training iter 450, batch loss 1.7479, batch acc 0.8536
12:20:05.619   Training iter 500, batch loss 1.7478, batch acc 0.8524
12:20:06.180   Training iter 550, batch loss 1.7515, batch acc 0.8484
12:20:06.738   Training iter 600, batch loss 1.7530, batch acc 0.8520
12:20:06.740 Training @ 132 epoch...
12:20:07.282   Training iter 50, batch loss 1.7529, batch acc 0.8520
12:20:07.799   Training iter 100, batch loss 1.7487, batch acc 0.8510
12:20:08.346   Training iter 150, batch loss 1.7490, batch acc 0.8530
12:20:08.911   Training iter 200, batch loss 1.7472, batch acc 0.8544
12:20:09.473   Training iter 250, batch loss 1.7482, batch acc 0.8522
12:20:10.017   Training iter 300, batch loss 1.7521, batch acc 0.8424
12:20:10.549   Training iter 350, batch loss 1.7521, batch acc 0.8512
12:20:11.060   Training iter 400, batch loss 1.7498, batch acc 0.8504
12:20:11.523   Training iter 450, batch loss 1.7511, batch acc 0.8566
12:20:11.998   Training iter 500, batch loss 1.7516, batch acc 0.8486
12:20:12.459   Training iter 550, batch loss 1.7501, batch acc 0.8530
12:20:12.924   Training iter 600, batch loss 1.7555, batch acc 0.8460
12:20:12.926 Training @ 133 epoch...
12:20:13.410   Training iter 50, batch loss 1.7516, batch acc 0.8522
12:20:13.894   Training iter 100, batch loss 1.7511, batch acc 0.8538
12:20:14.400   Training iter 150, batch loss 1.7534, batch acc 0.8442
12:20:14.879   Training iter 200, batch loss 1.7519, batch acc 0.8486
12:20:15.385   Training iter 250, batch loss 1.7482, batch acc 0.8564
12:20:15.885   Training iter 300, batch loss 1.7484, batch acc 0.8574
12:20:16.381   Training iter 350, batch loss 1.7506, batch acc 0.8488
12:20:16.924   Training iter 400, batch loss 1.7538, batch acc 0.8442
12:20:17.433   Training iter 450, batch loss 1.7461, batch acc 0.8498
12:20:17.932   Training iter 500, batch loss 1.7481, batch acc 0.8526
12:20:18.413   Training iter 550, batch loss 1.7504, batch acc 0.8536
12:20:18.908   Training iter 600, batch loss 1.7515, batch acc 0.8504
12:20:18.910 Training @ 134 epoch...
12:20:19.425   Training iter 50, batch loss 1.7471, batch acc 0.8618
12:20:19.929   Training iter 100, batch loss 1.7513, batch acc 0.8408
12:20:20.438   Training iter 150, batch loss 1.7519, batch acc 0.8474
12:20:20.917   Training iter 200, batch loss 1.7499, batch acc 0.8510
12:20:21.406   Training iter 250, batch loss 1.7526, batch acc 0.8506
12:20:21.894   Training iter 300, batch loss 1.7563, batch acc 0.8412
12:20:22.396   Training iter 350, batch loss 1.7486, batch acc 0.8564
12:20:22.901   Training iter 400, batch loss 1.7456, batch acc 0.8554
12:20:23.390   Training iter 450, batch loss 1.7524, batch acc 0.8478
12:20:23.876   Training iter 500, batch loss 1.7507, batch acc 0.8500
12:20:24.371   Training iter 550, batch loss 1.7489, batch acc 0.8536
12:20:24.946   Training iter 600, batch loss 1.7466, batch acc 0.8592
12:20:24.948 Training @ 135 epoch...
12:20:25.495   Training iter 50, batch loss 1.7471, batch acc 0.8600
12:20:25.987   Training iter 100, batch loss 1.7494, batch acc 0.8546
12:20:26.484   Training iter 150, batch loss 1.7473, batch acc 0.8510
12:20:27.016   Training iter 200, batch loss 1.7502, batch acc 0.8518
12:20:27.616   Training iter 250, batch loss 1.7500, batch acc 0.8532
12:20:28.158   Training iter 300, batch loss 1.7514, batch acc 0.8514
12:20:28.661   Training iter 350, batch loss 1.7501, batch acc 0.8540
12:20:29.150   Training iter 400, batch loss 1.7490, batch acc 0.8546
12:20:29.711   Training iter 450, batch loss 1.7569, batch acc 0.8362
12:20:30.264   Training iter 500, batch loss 1.7503, batch acc 0.8546
12:20:30.800   Training iter 550, batch loss 1.7504, batch acc 0.8458
12:20:31.325   Training iter 600, batch loss 1.7469, batch acc 0.8508
12:20:31.327 Testing @ 135 epoch...
12:20:31.373     Testing, total mean loss 1.74240, total acc 0.86100
12:20:31.373 Training @ 136 epoch...
12:20:31.923   Training iter 50, batch loss 1.7474, batch acc 0.8548
12:20:32.479   Training iter 100, batch loss 1.7495, batch acc 0.8482
12:20:33.044   Training iter 150, batch loss 1.7514, batch acc 0.8546
12:20:33.645   Training iter 200, batch loss 1.7548, batch acc 0.8406
12:20:34.158   Training iter 250, batch loss 1.7486, batch acc 0.8510
12:20:34.682   Training iter 300, batch loss 1.7478, batch acc 0.8562
12:20:35.197   Training iter 350, batch loss 1.7519, batch acc 0.8518
12:20:35.714   Training iter 400, batch loss 1.7529, batch acc 0.8492
12:20:36.248   Training iter 450, batch loss 1.7466, batch acc 0.8556
12:20:36.799   Training iter 500, batch loss 1.7513, batch acc 0.8504
12:20:37.341   Training iter 550, batch loss 1.7477, batch acc 0.8528
12:20:37.878   Training iter 600, batch loss 1.7461, batch acc 0.8540
12:20:37.880 Training @ 137 epoch...
12:20:38.418   Training iter 50, batch loss 1.7479, batch acc 0.8538
12:20:38.968   Training iter 100, batch loss 1.7480, batch acc 0.8548
12:20:39.522   Training iter 150, batch loss 1.7488, batch acc 0.8510
12:20:40.098   Training iter 200, batch loss 1.7504, batch acc 0.8498
12:20:40.660   Training iter 250, batch loss 1.7518, batch acc 0.8480
12:20:41.184   Training iter 300, batch loss 1.7516, batch acc 0.8482
12:20:41.731   Training iter 350, batch loss 1.7459, batch acc 0.8502
12:20:42.293   Training iter 400, batch loss 1.7475, batch acc 0.8608
12:20:42.860   Training iter 450, batch loss 1.7490, batch acc 0.8486
12:20:43.392   Training iter 500, batch loss 1.7500, batch acc 0.8516
12:20:43.895   Training iter 550, batch loss 1.7473, batch acc 0.8528
12:20:44.387   Training iter 600, batch loss 1.7552, batch acc 0.8472
12:20:44.388 Training @ 138 epoch...
12:20:44.894   Training iter 50, batch loss 1.7497, batch acc 0.8496
12:20:45.399   Training iter 100, batch loss 1.7496, batch acc 0.8512
12:20:45.915   Training iter 150, batch loss 1.7448, batch acc 0.8592
12:20:46.443   Training iter 200, batch loss 1.7477, batch acc 0.8536
12:20:46.969   Training iter 250, batch loss 1.7483, batch acc 0.8568
12:20:47.463   Training iter 300, batch loss 1.7492, batch acc 0.8512
12:20:47.943   Training iter 350, batch loss 1.7508, batch acc 0.8480
12:20:48.436   Training iter 400, batch loss 1.7482, batch acc 0.8518
12:20:48.931   Training iter 450, batch loss 1.7516, batch acc 0.8448
12:20:49.424   Training iter 500, batch loss 1.7487, batch acc 0.8550
12:20:49.915   Training iter 550, batch loss 1.7523, batch acc 0.8470
12:20:50.403   Training iter 600, batch loss 1.7492, batch acc 0.8528
12:20:50.405 Training @ 139 epoch...
12:20:50.904   Training iter 50, batch loss 1.7503, batch acc 0.8544
12:20:51.390   Training iter 100, batch loss 1.7505, batch acc 0.8492
12:20:51.872   Training iter 150, batch loss 1.7481, batch acc 0.8540
12:20:52.368   Training iter 200, batch loss 1.7485, batch acc 0.8558
12:20:52.864   Training iter 250, batch loss 1.7457, batch acc 0.8534
12:20:53.337   Training iter 300, batch loss 1.7520, batch acc 0.8464
12:20:53.809   Training iter 350, batch loss 1.7488, batch acc 0.8522
12:20:54.289   Training iter 400, batch loss 1.7485, batch acc 0.8504
12:20:54.771   Training iter 450, batch loss 1.7473, batch acc 0.8560
12:20:55.256   Training iter 500, batch loss 1.7467, batch acc 0.8502
12:20:55.764   Training iter 550, batch loss 1.7491, batch acc 0.8478
12:20:56.308   Training iter 600, batch loss 1.7517, batch acc 0.8506
12:20:56.309 Training @ 140 epoch...
12:20:56.851   Training iter 50, batch loss 1.7465, batch acc 0.8580
12:20:57.451   Training iter 100, batch loss 1.7457, batch acc 0.8552
12:20:57.986   Training iter 150, batch loss 1.7522, batch acc 0.8420
12:20:58.504   Training iter 200, batch loss 1.7460, batch acc 0.8554
12:20:59.029   Training iter 250, batch loss 1.7489, batch acc 0.8498
12:20:59.527   Training iter 300, batch loss 1.7462, batch acc 0.8544
12:21:00.031   Training iter 350, batch loss 1.7521, batch acc 0.8464
12:21:00.601   Training iter 400, batch loss 1.7544, batch acc 0.8426
12:21:01.257   Training iter 450, batch loss 1.7484, batch acc 0.8534
12:21:01.841   Training iter 500, batch loss 1.7501, batch acc 0.8478
12:21:02.395   Training iter 550, batch loss 1.7493, batch acc 0.8532
12:21:02.942   Training iter 600, batch loss 1.7448, batch acc 0.8638
12:21:02.944 Testing @ 140 epoch...
12:21:02.989     Testing, total mean loss 1.74123, total acc 0.86140
12:21:02.989 Training @ 141 epoch...
12:21:03.545   Training iter 50, batch loss 1.7475, batch acc 0.8544
12:21:04.131   Training iter 100, batch loss 1.7511, batch acc 0.8468
12:21:04.714   Training iter 150, batch loss 1.7488, batch acc 0.8524
12:21:05.296   Training iter 200, batch loss 1.7452, batch acc 0.8574
12:21:05.882   Training iter 250, batch loss 1.7464, batch acc 0.8512
12:21:06.452   Training iter 300, batch loss 1.7501, batch acc 0.8512
12:21:06.990   Training iter 350, batch loss 1.7487, batch acc 0.8522
12:21:07.527   Training iter 400, batch loss 1.7450, batch acc 0.8562
12:21:08.049   Training iter 450, batch loss 1.7493, batch acc 0.8524
12:21:08.563   Training iter 500, batch loss 1.7478, batch acc 0.8552
12:21:09.056   Training iter 550, batch loss 1.7480, batch acc 0.8526
12:21:09.535   Training iter 600, batch loss 1.7541, batch acc 0.8420
12:21:09.536 Training @ 142 epoch...
12:21:10.022   Training iter 50, batch loss 1.7481, batch acc 0.8574
12:21:10.544   Training iter 100, batch loss 1.7500, batch acc 0.8472
12:21:11.095   Training iter 150, batch loss 1.7504, batch acc 0.8528
12:21:11.624   Training iter 200, batch loss 1.7494, batch acc 0.8500
12:21:12.160   Training iter 250, batch loss 1.7452, batch acc 0.8622
12:21:12.707   Training iter 300, batch loss 1.7458, batch acc 0.8570
12:21:13.261   Training iter 350, batch loss 1.7530, batch acc 0.8462
12:21:13.803   Training iter 400, batch loss 1.7530, batch acc 0.8412
12:21:14.357   Training iter 450, batch loss 1.7431, batch acc 0.8562
12:21:14.906   Training iter 500, batch loss 1.7495, batch acc 0.8468
12:21:15.450   Training iter 550, batch loss 1.7464, batch acc 0.8548
12:21:15.987   Training iter 600, batch loss 1.7454, batch acc 0.8514
12:21:15.989 Training @ 143 epoch...
12:21:16.533   Training iter 50, batch loss 1.7478, batch acc 0.8548
12:21:17.056   Training iter 100, batch loss 1.7494, batch acc 0.8538
12:21:17.541   Training iter 150, batch loss 1.7505, batch acc 0.8472
12:21:18.018   Training iter 200, batch loss 1.7455, batch acc 0.8532
12:21:18.498   Training iter 250, batch loss 1.7491, batch acc 0.8456
12:21:18.976   Training iter 300, batch loss 1.7529, batch acc 0.8480
12:21:19.455   Training iter 350, batch loss 1.7435, batch acc 0.8578
12:21:19.922   Training iter 400, batch loss 1.7449, batch acc 0.8622
12:21:20.403   Training iter 450, batch loss 1.7447, batch acc 0.8540
12:21:20.879   Training iter 500, batch loss 1.7453, batch acc 0.8568
12:21:21.388   Training iter 550, batch loss 1.7518, batch acc 0.8454
12:21:21.909   Training iter 600, batch loss 1.7514, batch acc 0.8466
12:21:21.910 Training @ 144 epoch...
12:21:22.424   Training iter 50, batch loss 1.7494, batch acc 0.8482
12:21:22.935   Training iter 100, batch loss 1.7487, batch acc 0.8526
12:21:23.455   Training iter 150, batch loss 1.7516, batch acc 0.8510
12:21:23.954   Training iter 200, batch loss 1.7478, batch acc 0.8510
12:21:24.455   Training iter 250, batch loss 1.7488, batch acc 0.8500
12:21:24.964   Training iter 300, batch loss 1.7439, batch acc 0.8552
12:21:25.489   Training iter 350, batch loss 1.7502, batch acc 0.8496
12:21:25.998   Training iter 400, batch loss 1.7466, batch acc 0.8536
12:21:26.512   Training iter 450, batch loss 1.7502, batch acc 0.8428
12:21:27.026   Training iter 500, batch loss 1.7459, batch acc 0.8550
12:21:27.565   Training iter 550, batch loss 1.7429, batch acc 0.8666
12:21:28.087   Training iter 600, batch loss 1.7481, batch acc 0.8516
12:21:28.088 Training @ 145 epoch...
12:21:28.643   Training iter 50, batch loss 1.7493, batch acc 0.8514
12:21:29.173   Training iter 100, batch loss 1.7444, batch acc 0.8576
12:21:29.686   Training iter 150, batch loss 1.7442, batch acc 0.8520
12:21:30.190   Training iter 200, batch loss 1.7504, batch acc 0.8528
12:21:30.742   Training iter 250, batch loss 1.7518, batch acc 0.8456
12:21:31.279   Training iter 300, batch loss 1.7460, batch acc 0.8520
12:21:31.821   Training iter 350, batch loss 1.7460, batch acc 0.8590
12:21:32.354   Training iter 400, batch loss 1.7459, batch acc 0.8546
12:21:32.879   Training iter 450, batch loss 1.7467, batch acc 0.8546
12:21:33.460   Training iter 500, batch loss 1.7528, batch acc 0.8450
12:21:33.957   Training iter 550, batch loss 1.7427, batch acc 0.8600
12:21:34.450   Training iter 600, batch loss 1.7513, batch acc 0.8420
12:21:34.452 Testing @ 145 epoch...
12:21:34.496     Testing, total mean loss 1.74013, total acc 0.86160
12:21:34.496 Training @ 146 epoch...
12:21:34.976   Training iter 50, batch loss 1.7487, batch acc 0.8522
12:21:35.425   Training iter 100, batch loss 1.7499, batch acc 0.8496
12:21:35.878   Training iter 150, batch loss 1.7468, batch acc 0.8512
12:21:36.349   Training iter 200, batch loss 1.7451, batch acc 0.8590
12:21:36.827   Training iter 250, batch loss 1.7439, batch acc 0.8560
12:21:37.299   Training iter 300, batch loss 1.7495, batch acc 0.8516
12:21:37.772   Training iter 350, batch loss 1.7448, batch acc 0.8544
12:21:38.249   Training iter 400, batch loss 1.7536, batch acc 0.8458
12:21:38.731   Training iter 450, batch loss 1.7526, batch acc 0.8446
12:21:39.225   Training iter 500, batch loss 1.7433, batch acc 0.8560
12:21:39.717   Training iter 550, batch loss 1.7417, batch acc 0.8648
12:21:40.238   Training iter 600, batch loss 1.7491, batch acc 0.8444
12:21:40.240 Training @ 147 epoch...
12:21:40.748   Training iter 50, batch loss 1.7473, batch acc 0.8528
12:21:41.259   Training iter 100, batch loss 1.7457, batch acc 0.8532
12:21:41.763   Training iter 150, batch loss 1.7518, batch acc 0.8430
12:21:42.303   Training iter 200, batch loss 1.7520, batch acc 0.8494
12:21:42.837   Training iter 250, batch loss 1.7486, batch acc 0.8512
12:21:43.389   Training iter 300, batch loss 1.7469, batch acc 0.8484
12:21:43.947   Training iter 350, batch loss 1.7481, batch acc 0.8524
12:21:44.534   Training iter 400, batch loss 1.7440, batch acc 0.8568
12:21:45.100   Training iter 450, batch loss 1.7459, batch acc 0.8540
12:21:45.680   Training iter 500, batch loss 1.7421, batch acc 0.8570
12:21:46.428   Training iter 550, batch loss 1.7474, batch acc 0.8554
12:21:47.188   Training iter 600, batch loss 1.7469, batch acc 0.8544
12:21:47.191 Training @ 148 epoch...
12:21:47.937   Training iter 50, batch loss 1.7474, batch acc 0.8530
12:21:48.706   Training iter 100, batch loss 1.7476, batch acc 0.8490
12:21:49.389   Training iter 150, batch loss 1.7466, batch acc 0.8552
12:21:49.964   Training iter 200, batch loss 1.7496, batch acc 0.8484
12:21:50.535   Training iter 250, batch loss 1.7471, batch acc 0.8492
12:21:51.061   Training iter 300, batch loss 1.7437, batch acc 0.8598
12:21:51.580   Training iter 350, batch loss 1.7513, batch acc 0.8440
12:21:52.124   Training iter 400, batch loss 1.7498, batch acc 0.8520
12:21:52.636   Training iter 450, batch loss 1.7419, batch acc 0.8554
12:21:53.165   Training iter 500, batch loss 1.7469, batch acc 0.8554
12:21:53.754   Training iter 550, batch loss 1.7452, batch acc 0.8554
12:21:54.346   Training iter 600, batch loss 1.7471, batch acc 0.8538
12:21:54.347 Training @ 149 epoch...
12:21:54.927   Training iter 50, batch loss 1.7472, batch acc 0.8496
12:21:55.446   Training iter 100, batch loss 1.7442, batch acc 0.8608
12:21:55.933   Training iter 150, batch loss 1.7513, batch acc 0.8464
12:21:56.420   Training iter 200, batch loss 1.7446, batch acc 0.8602
12:21:56.917   Training iter 250, batch loss 1.7518, batch acc 0.8408
12:21:57.424   Training iter 300, batch loss 1.7466, batch acc 0.8574
12:21:57.926   Training iter 350, batch loss 1.7462, batch acc 0.8514
12:21:58.403   Training iter 400, batch loss 1.7451, batch acc 0.8558
12:21:58.884   Training iter 450, batch loss 1.7460, batch acc 0.8490
12:21:59.367   Training iter 500, batch loss 1.7493, batch acc 0.8524
12:21:59.847   Training iter 550, batch loss 1.7447, batch acc 0.8538
12:22:00.345   Training iter 600, batch loss 1.7448, batch acc 0.8532
12:22:00.347 Training @ 150 epoch...
12:22:00.838   Training iter 50, batch loss 1.7463, batch acc 0.8488
12:22:01.338   Training iter 100, batch loss 1.7514, batch acc 0.8526
12:22:01.895   Training iter 150, batch loss 1.7501, batch acc 0.8500
12:22:02.446   Training iter 200, batch loss 1.7458, batch acc 0.8570
12:22:02.987   Training iter 250, batch loss 1.7492, batch acc 0.8450
12:22:03.547   Training iter 300, batch loss 1.7443, batch acc 0.8530
12:22:04.099   Training iter 350, batch loss 1.7472, batch acc 0.8512
12:22:04.654   Training iter 400, batch loss 1.7456, batch acc 0.8554
12:22:05.185   Training iter 450, batch loss 1.7456, batch acc 0.8502
12:22:05.710   Training iter 500, batch loss 1.7424, batch acc 0.8622
12:22:06.229   Training iter 550, batch loss 1.7476, batch acc 0.8516
12:22:06.755   Training iter 600, batch loss 1.7441, batch acc 0.8552
12:22:06.757 Testing @ 150 epoch...
12:22:06.800     Testing, total mean loss 1.73914, total acc 0.86140
12:22:06.800 Training @ 151 epoch...
12:22:07.303   Training iter 50, batch loss 1.7474, batch acc 0.8542
12:22:07.779   Training iter 100, batch loss 1.7457, batch acc 0.8558
12:22:08.253   Training iter 150, batch loss 1.7455, batch acc 0.8500
12:22:08.721   Training iter 200, batch loss 1.7441, batch acc 0.8514
12:22:09.239   Training iter 250, batch loss 1.7512, batch acc 0.8544
12:22:09.766   Training iter 300, batch loss 1.7432, batch acc 0.8564
12:22:10.305   Training iter 350, batch loss 1.7442, batch acc 0.8544
12:22:10.817   Training iter 400, batch loss 1.7445, batch acc 0.8592
12:22:11.319   Training iter 450, batch loss 1.7473, batch acc 0.8568
12:22:11.852   Training iter 500, batch loss 1.7486, batch acc 0.8456
12:22:12.357   Training iter 550, batch loss 1.7494, batch acc 0.8478
12:22:12.847   Training iter 600, batch loss 1.7461, batch acc 0.8490
12:22:12.848 Training @ 152 epoch...
12:22:13.341   Training iter 50, batch loss 1.7467, batch acc 0.8500
12:22:13.814   Training iter 100, batch loss 1.7464, batch acc 0.8484
12:22:14.328   Training iter 150, batch loss 1.7486, batch acc 0.8400
12:22:14.860   Training iter 200, batch loss 1.7443, batch acc 0.8592
12:22:15.396   Training iter 250, batch loss 1.7442, batch acc 0.8588
12:22:15.928   Training iter 300, batch loss 1.7460, batch acc 0.8600
12:22:16.457   Training iter 350, batch loss 1.7485, batch acc 0.8500
12:22:16.984   Training iter 400, batch loss 1.7466, batch acc 0.8516
12:22:17.531   Training iter 450, batch loss 1.7505, batch acc 0.8444
12:22:18.037   Training iter 500, batch loss 1.7433, batch acc 0.8598
12:22:18.547   Training iter 550, batch loss 1.7446, batch acc 0.8544
12:22:19.062   Training iter 600, batch loss 1.7452, batch acc 0.8582
12:22:19.064 Training @ 153 epoch...
12:22:19.587   Training iter 50, batch loss 1.7478, batch acc 0.8500
12:22:20.109   Training iter 100, batch loss 1.7434, batch acc 0.8546
12:22:20.605   Training iter 150, batch loss 1.7489, batch acc 0.8516
12:22:21.095   Training iter 200, batch loss 1.7472, batch acc 0.8464
12:22:21.588   Training iter 250, batch loss 1.7464, batch acc 0.8546
12:22:22.087   Training iter 300, batch loss 1.7457, batch acc 0.8526
12:22:22.592   Training iter 350, batch loss 1.7453, batch acc 0.8516
12:22:23.089   Training iter 400, batch loss 1.7496, batch acc 0.8462
12:22:23.609   Training iter 450, batch loss 1.7477, batch acc 0.8522
12:22:24.116   Training iter 500, batch loss 1.7445, batch acc 0.8556
12:22:24.631   Training iter 550, batch loss 1.7435, batch acc 0.8576
12:22:25.154   Training iter 600, batch loss 1.7427, batch acc 0.8618
12:22:25.156 Training @ 154 epoch...
12:22:25.685   Training iter 50, batch loss 1.7442, batch acc 0.8546
12:22:26.211   Training iter 100, batch loss 1.7478, batch acc 0.8468
12:22:26.730   Training iter 150, batch loss 1.7473, batch acc 0.8544
12:22:27.264   Training iter 200, batch loss 1.7446, batch acc 0.8556
12:22:27.816   Training iter 250, batch loss 1.7495, batch acc 0.8452
12:22:28.370   Training iter 300, batch loss 1.7485, batch acc 0.8540
12:22:28.935   Training iter 350, batch loss 1.7455, batch acc 0.8488
12:22:29.483   Training iter 400, batch loss 1.7417, batch acc 0.8564
12:22:30.024   Training iter 450, batch loss 1.7470, batch acc 0.8568
12:22:30.587   Training iter 500, batch loss 1.7438, batch acc 0.8532
12:22:31.151   Training iter 550, batch loss 1.7481, batch acc 0.8442
12:22:31.726   Training iter 600, batch loss 1.7424, batch acc 0.8664
12:22:31.727 Training @ 155 epoch...
12:22:32.324   Training iter 50, batch loss 1.7469, batch acc 0.8502
12:22:32.916   Training iter 100, batch loss 1.7489, batch acc 0.8458
12:22:33.531   Training iter 150, batch loss 1.7455, batch acc 0.8526
12:22:34.203   Training iter 200, batch loss 1.7485, batch acc 0.8534
12:22:34.781   Training iter 250, batch loss 1.7424, batch acc 0.8602
12:22:35.350   Training iter 300, batch loss 1.7446, batch acc 0.8570
12:22:35.898   Training iter 350, batch loss 1.7469, batch acc 0.8512
12:22:36.463   Training iter 400, batch loss 1.7474, batch acc 0.8496
12:22:36.997   Training iter 450, batch loss 1.7440, batch acc 0.8562
12:22:37.525   Training iter 500, batch loss 1.7419, batch acc 0.8584
12:22:38.033   Training iter 550, batch loss 1.7432, batch acc 0.8506
12:22:38.546   Training iter 600, batch loss 1.7481, batch acc 0.8516
12:22:38.548 Testing @ 155 epoch...
12:22:38.594     Testing, total mean loss 1.73822, total acc 0.86220
12:22:38.594 Training @ 156 epoch...
12:22:39.125   Training iter 50, batch loss 1.7517, batch acc 0.8436
12:22:39.660   Training iter 100, batch loss 1.7449, batch acc 0.8622
12:22:40.174   Training iter 150, batch loss 1.7440, batch acc 0.8524
12:22:40.697   Training iter 200, batch loss 1.7466, batch acc 0.8540
12:22:41.211   Training iter 250, batch loss 1.7435, batch acc 0.8522
12:22:41.755   Training iter 300, batch loss 1.7460, batch acc 0.8580
12:22:42.288   Training iter 350, batch loss 1.7480, batch acc 0.8510
12:22:42.809   Training iter 400, batch loss 1.7447, batch acc 0.8492
12:22:43.337   Training iter 450, batch loss 1.7470, batch acc 0.8498
12:22:43.851   Training iter 500, batch loss 1.7465, batch acc 0.8514
12:22:44.364   Training iter 550, batch loss 1.7426, batch acc 0.8568
12:22:44.874   Training iter 600, batch loss 1.7407, batch acc 0.8594
12:22:44.876 Training @ 157 epoch...
12:22:45.405   Training iter 50, batch loss 1.7421, batch acc 0.8586
12:22:45.972   Training iter 100, batch loss 1.7483, batch acc 0.8524
12:22:46.471   Training iter 150, batch loss 1.7434, batch acc 0.8588
12:22:46.966   Training iter 200, batch loss 1.7495, batch acc 0.8488
12:22:47.451   Training iter 250, batch loss 1.7457, batch acc 0.8466
12:22:47.926   Training iter 300, batch loss 1.7456, batch acc 0.8552
12:22:48.464   Training iter 350, batch loss 1.7466, batch acc 0.8460
12:22:49.035   Training iter 400, batch loss 1.7424, batch acc 0.8528
12:22:49.606   Training iter 450, batch loss 1.7467, batch acc 0.8516
12:22:50.118   Training iter 500, batch loss 1.7465, batch acc 0.8524
12:22:50.605   Training iter 550, batch loss 1.7422, batch acc 0.8586
12:22:51.088   Training iter 600, batch loss 1.7452, batch acc 0.8580
12:22:51.090 Training @ 158 epoch...
12:22:51.590   Training iter 50, batch loss 1.7481, batch acc 0.8416
12:22:52.099   Training iter 100, batch loss 1.7469, batch acc 0.8528
12:22:52.606   Training iter 150, batch loss 1.7434, batch acc 0.8498
12:22:53.114   Training iter 200, batch loss 1.7443, batch acc 0.8572
12:22:53.596   Training iter 250, batch loss 1.7486, batch acc 0.8468
12:22:54.081   Training iter 300, batch loss 1.7436, batch acc 0.8540
12:22:54.565   Training iter 350, batch loss 1.7435, batch acc 0.8606
12:22:55.064   Training iter 400, batch loss 1.7458, batch acc 0.8530
12:22:55.552   Training iter 450, batch loss 1.7421, batch acc 0.8564
12:22:56.030   Training iter 500, batch loss 1.7415, batch acc 0.8634
12:22:56.516   Training iter 550, batch loss 1.7507, batch acc 0.8470
12:22:57.018   Training iter 600, batch loss 1.7436, batch acc 0.8564
12:22:57.019 Training @ 159 epoch...
12:22:57.550   Training iter 50, batch loss 1.7449, batch acc 0.8554
12:22:58.073   Training iter 100, batch loss 1.7489, batch acc 0.8472
12:22:58.586   Training iter 150, batch loss 1.7478, batch acc 0.8492
12:22:59.151   Training iter 200, batch loss 1.7421, batch acc 0.8568
12:22:59.713   Training iter 250, batch loss 1.7450, batch acc 0.8552
12:23:00.252   Training iter 300, batch loss 1.7452, batch acc 0.8588
12:23:00.751   Training iter 350, batch loss 1.7480, batch acc 0.8458
12:23:01.246   Training iter 400, batch loss 1.7429, batch acc 0.8564
12:23:01.783   Training iter 450, batch loss 1.7398, batch acc 0.8644
12:23:02.320   Training iter 500, batch loss 1.7429, batch acc 0.8564
12:23:02.844   Training iter 550, batch loss 1.7481, batch acc 0.8418
12:23:03.394   Training iter 600, batch loss 1.7445, batch acc 0.8506
12:23:03.396 Training @ 160 epoch...
12:23:03.923   Training iter 50, batch loss 1.7476, batch acc 0.8488
12:23:04.458   Training iter 100, batch loss 1.7470, batch acc 0.8442
12:23:05.037   Training iter 150, batch loss 1.7413, batch acc 0.8598
12:23:05.613   Training iter 200, batch loss 1.7452, batch acc 0.8570
12:23:06.157   Training iter 250, batch loss 1.7389, batch acc 0.8570
12:23:06.719   Training iter 300, batch loss 1.7473, batch acc 0.8540
12:23:07.260   Training iter 350, batch loss 1.7444, batch acc 0.8542
12:23:07.779   Training iter 400, batch loss 1.7453, batch acc 0.8546
12:23:08.304   Training iter 450, batch loss 1.7410, batch acc 0.8574
12:23:08.825   Training iter 500, batch loss 1.7493, batch acc 0.8450
12:23:09.347   Training iter 550, batch loss 1.7425, batch acc 0.8560
12:23:09.861   Training iter 600, batch loss 1.7480, batch acc 0.8536
12:23:09.862 Testing @ 160 epoch...
12:23:09.907     Testing, total mean loss 1.73737, total acc 0.86210
12:23:09.907 Training @ 161 epoch...
12:23:10.452   Training iter 50, batch loss 1.7444, batch acc 0.8440
12:23:10.976   Training iter 100, batch loss 1.7459, batch acc 0.8542
12:23:11.503   Training iter 150, batch loss 1.7451, batch acc 0.8546
12:23:12.037   Training iter 200, batch loss 1.7454, batch acc 0.8546
12:23:12.583   Training iter 250, batch loss 1.7443, batch acc 0.8496
12:23:13.129   Training iter 300, batch loss 1.7375, batch acc 0.8622
12:23:13.674   Training iter 350, batch loss 1.7510, batch acc 0.8444
12:23:14.235   Training iter 400, batch loss 1.7453, batch acc 0.8524
12:23:14.777   Training iter 450, batch loss 1.7478, batch acc 0.8496
12:23:15.330   Training iter 500, batch loss 1.7444, batch acc 0.8618
12:23:15.872   Training iter 550, batch loss 1.7429, batch acc 0.8594
12:23:16.427   Training iter 600, batch loss 1.7420, batch acc 0.8546
12:23:16.428 Training @ 162 epoch...
12:23:16.980   Training iter 50, batch loss 1.7436, batch acc 0.8538
12:23:17.546   Training iter 100, batch loss 1.7429, batch acc 0.8544
12:23:18.094   Training iter 150, batch loss 1.7462, batch acc 0.8484
12:23:18.672   Training iter 200, batch loss 1.7476, batch acc 0.8518
12:23:19.250   Training iter 250, batch loss 1.7418, batch acc 0.8600
12:23:19.811   Training iter 300, batch loss 1.7456, batch acc 0.8494
12:23:20.361   Training iter 350, batch loss 1.7431, batch acc 0.8564
12:23:20.913   Training iter 400, batch loss 1.7473, batch acc 0.8492
12:23:21.485   Training iter 450, batch loss 1.7468, batch acc 0.8506
12:23:22.044   Training iter 500, batch loss 1.7440, batch acc 0.8550
12:23:22.599   Training iter 550, batch loss 1.7436, batch acc 0.8532
12:23:23.154   Training iter 600, batch loss 1.7417, batch acc 0.8612
12:23:23.157 Training @ 163 epoch...
12:23:23.715   Training iter 50, batch loss 1.7440, batch acc 0.8560
12:23:24.256   Training iter 100, batch loss 1.7469, batch acc 0.8524
12:23:24.755   Training iter 150, batch loss 1.7431, batch acc 0.8552
12:23:25.271   Training iter 200, batch loss 1.7454, batch acc 0.8524
12:23:25.794   Training iter 250, batch loss 1.7437, batch acc 0.8482
12:23:26.352   Training iter 300, batch loss 1.7442, batch acc 0.8526
12:23:26.906   Training iter 350, batch loss 1.7505, batch acc 0.8504
12:23:27.457   Training iter 400, batch loss 1.7426, batch acc 0.8590
12:23:28.006   Training iter 450, batch loss 1.7444, batch acc 0.8558
12:23:28.586   Training iter 500, batch loss 1.7427, batch acc 0.8538
12:23:29.146   Training iter 550, batch loss 1.7410, batch acc 0.8566
12:23:29.703   Training iter 600, batch loss 1.7439, batch acc 0.8528
12:23:29.705 Training @ 164 epoch...
12:23:30.268   Training iter 50, batch loss 1.7460, batch acc 0.8572
12:23:30.798   Training iter 100, batch loss 1.7421, batch acc 0.8548
12:23:31.330   Training iter 150, batch loss 1.7482, batch acc 0.8480
12:23:31.869   Training iter 200, batch loss 1.7387, batch acc 0.8658
12:23:32.394   Training iter 250, batch loss 1.7455, batch acc 0.8488
12:23:32.910   Training iter 300, batch loss 1.7435, batch acc 0.8556
12:23:33.435   Training iter 350, batch loss 1.7394, batch acc 0.8570
12:23:33.914   Training iter 400, batch loss 1.7468, batch acc 0.8470
12:23:34.418   Training iter 450, batch loss 1.7476, batch acc 0.8486
12:23:34.953   Training iter 500, batch loss 1.7455, batch acc 0.8546
12:23:35.527   Training iter 550, batch loss 1.7425, batch acc 0.8538
12:23:36.068   Training iter 600, batch loss 1.7445, batch acc 0.8540
12:23:36.069 Training @ 165 epoch...
12:23:36.608   Training iter 50, batch loss 1.7400, batch acc 0.8642
12:23:37.130   Training iter 100, batch loss 1.7387, batch acc 0.8592
12:23:37.709   Training iter 150, batch loss 1.7456, batch acc 0.8440
12:23:38.272   Training iter 200, batch loss 1.7445, batch acc 0.8552
12:23:38.845   Training iter 250, batch loss 1.7430, batch acc 0.8550
12:23:39.419   Training iter 300, batch loss 1.7505, batch acc 0.8466
12:23:39.981   Training iter 350, batch loss 1.7469, batch acc 0.8528
12:23:40.521   Training iter 400, batch loss 1.7435, batch acc 0.8544
12:23:41.043   Training iter 450, batch loss 1.7375, batch acc 0.8642
12:23:41.576   Training iter 500, batch loss 1.7407, batch acc 0.8540
12:23:42.110   Training iter 550, batch loss 1.7461, batch acc 0.8522
12:23:42.653   Training iter 600, batch loss 1.7514, batch acc 0.8456
12:23:42.654 Testing @ 165 epoch...
12:23:42.699     Testing, total mean loss 1.73658, total acc 0.86280
12:23:42.699 Training @ 166 epoch...
12:23:43.245   Training iter 50, batch loss 1.7431, batch acc 0.8586
12:23:43.769   Training iter 100, batch loss 1.7437, batch acc 0.8502
12:23:44.287   Training iter 150, batch loss 1.7485, batch acc 0.8504
12:23:44.817   Training iter 200, batch loss 1.7438, batch acc 0.8546
12:23:45.350   Training iter 250, batch loss 1.7398, batch acc 0.8570
12:23:45.871   Training iter 300, batch loss 1.7452, batch acc 0.8532
12:23:46.392   Training iter 350, batch loss 1.7415, batch acc 0.8556
12:23:46.920   Training iter 400, batch loss 1.7475, batch acc 0.8448
12:23:47.445   Training iter 450, batch loss 1.7393, batch acc 0.8606
12:23:47.957   Training iter 500, batch loss 1.7481, batch acc 0.8488
12:23:48.471   Training iter 550, batch loss 1.7391, batch acc 0.8568
12:23:48.967   Training iter 600, batch loss 1.7470, batch acc 0.8558
12:23:48.969 Training @ 167 epoch...
12:23:49.487   Training iter 50, batch loss 1.7417, batch acc 0.8636
12:23:50.009   Training iter 100, batch loss 1.7434, batch acc 0.8520
12:23:50.560   Training iter 150, batch loss 1.7450, batch acc 0.8500
12:23:51.132   Training iter 200, batch loss 1.7446, batch acc 0.8504
12:23:51.684   Training iter 250, batch loss 1.7497, batch acc 0.8470
12:23:52.227   Training iter 300, batch loss 1.7441, batch acc 0.8506
12:23:52.782   Training iter 350, batch loss 1.7420, batch acc 0.8596
12:23:53.358   Training iter 400, batch loss 1.7392, batch acc 0.8596
12:23:53.909   Training iter 450, batch loss 1.7461, batch acc 0.8558
12:23:54.467   Training iter 500, batch loss 1.7466, batch acc 0.8510
12:23:55.010   Training iter 550, batch loss 1.7412, batch acc 0.8548
12:23:55.573   Training iter 600, batch loss 1.7413, batch acc 0.8548
12:23:55.575 Training @ 168 epoch...
12:23:56.131   Training iter 50, batch loss 1.7379, batch acc 0.8626
12:23:56.667   Training iter 100, batch loss 1.7422, batch acc 0.8558
12:23:57.185   Training iter 150, batch loss 1.7473, batch acc 0.8534
12:23:57.733   Training iter 200, batch loss 1.7453, batch acc 0.8566
12:23:58.282   Training iter 250, batch loss 1.7484, batch acc 0.8452
12:23:58.840   Training iter 300, batch loss 1.7485, batch acc 0.8508
12:23:59.370   Training iter 350, batch loss 1.7402, batch acc 0.8582
12:23:59.873   Training iter 400, batch loss 1.7437, batch acc 0.8532
12:24:00.395   Training iter 450, batch loss 1.7393, batch acc 0.8560
12:24:00.913   Training iter 500, batch loss 1.7417, batch acc 0.8590
12:24:01.449   Training iter 550, batch loss 1.7428, batch acc 0.8504
12:24:02.009   Training iter 600, batch loss 1.7459, batch acc 0.8504
12:24:02.011 Training @ 169 epoch...
12:24:02.577   Training iter 50, batch loss 1.7390, batch acc 0.8642
12:24:03.118   Training iter 100, batch loss 1.7448, batch acc 0.8470
12:24:03.695   Training iter 150, batch loss 1.7432, batch acc 0.8630
12:24:04.267   Training iter 200, batch loss 1.7435, batch acc 0.8496
12:24:04.822   Training iter 250, batch loss 1.7420, batch acc 0.8554
12:24:05.343   Training iter 300, batch loss 1.7469, batch acc 0.8506
12:24:05.873   Training iter 350, batch loss 1.7430, batch acc 0.8536
12:24:06.417   Training iter 400, batch loss 1.7439, batch acc 0.8502
12:24:06.954   Training iter 450, batch loss 1.7443, batch acc 0.8512
12:24:07.470   Training iter 500, batch loss 1.7417, batch acc 0.8646
12:24:07.982   Training iter 550, batch loss 1.7424, batch acc 0.8542
12:24:08.526   Training iter 600, batch loss 1.7466, batch acc 0.8492
12:24:08.528 Training @ 170 epoch...
12:24:09.040   Training iter 50, batch loss 1.7473, batch acc 0.8502
12:24:09.545   Training iter 100, batch loss 1.7450, batch acc 0.8542
12:24:10.059   Training iter 150, batch loss 1.7395, batch acc 0.8608
12:24:10.587   Training iter 200, batch loss 1.7407, batch acc 0.8576
12:24:11.095   Training iter 250, batch loss 1.7372, batch acc 0.8608
12:24:11.602   Training iter 300, batch loss 1.7452, batch acc 0.8516
12:24:12.107   Training iter 350, batch loss 1.7460, batch acc 0.8494
12:24:12.639   Training iter 400, batch loss 1.7452, batch acc 0.8574
12:24:13.166   Training iter 450, batch loss 1.7449, batch acc 0.8478
12:24:13.691   Training iter 500, batch loss 1.7434, batch acc 0.8540
12:24:14.206   Training iter 550, batch loss 1.7425, batch acc 0.8506
12:24:14.722   Training iter 600, batch loss 1.7428, batch acc 0.8556
12:24:14.724 Testing @ 170 epoch...
12:24:14.768     Testing, total mean loss 1.73586, total acc 0.86250
12:24:14.768 Training @ 171 epoch...
12:24:15.284   Training iter 50, batch loss 1.7471, batch acc 0.8504
12:24:15.791   Training iter 100, batch loss 1.7398, batch acc 0.8600
12:24:16.313   Training iter 150, batch loss 1.7467, batch acc 0.8512
12:24:16.829   Training iter 200, batch loss 1.7484, batch acc 0.8462
12:24:17.330   Training iter 250, batch loss 1.7359, batch acc 0.8662
12:24:17.823   Training iter 300, batch loss 1.7407, batch acc 0.8600
12:24:18.335   Training iter 350, batch loss 1.7404, batch acc 0.8576
12:24:18.855   Training iter 400, batch loss 1.7433, batch acc 0.8472
12:24:19.373   Training iter 450, batch loss 1.7441, batch acc 0.8556
12:24:19.938   Training iter 500, batch loss 1.7430, batch acc 0.8496
12:24:20.517   Training iter 550, batch loss 1.7447, batch acc 0.8538
12:24:21.097   Training iter 600, batch loss 1.7439, batch acc 0.8544
12:24:21.099 Training @ 172 epoch...
12:24:21.612   Training iter 50, batch loss 1.7414, batch acc 0.8554
12:24:22.105   Training iter 100, batch loss 1.7447, batch acc 0.8498
12:24:22.615   Training iter 150, batch loss 1.7482, batch acc 0.8502
12:24:23.185   Training iter 200, batch loss 1.7458, batch acc 0.8500
12:24:23.741   Training iter 250, batch loss 1.7395, batch acc 0.8638
12:24:24.266   Training iter 300, batch loss 1.7457, batch acc 0.8518
12:24:24.822   Training iter 350, batch loss 1.7381, batch acc 0.8590
12:24:25.378   Training iter 400, batch loss 1.7400, batch acc 0.8586
12:24:25.934   Training iter 450, batch loss 1.7479, batch acc 0.8512
12:24:26.472   Training iter 500, batch loss 1.7416, batch acc 0.8570
12:24:27.012   Training iter 550, batch loss 1.7422, batch acc 0.8488
12:24:27.559   Training iter 600, batch loss 1.7412, batch acc 0.8570
12:24:27.561 Training @ 173 epoch...
12:24:28.122   Training iter 50, batch loss 1.7453, batch acc 0.8534
12:24:28.669   Training iter 100, batch loss 1.7387, batch acc 0.8624
12:24:29.219   Training iter 150, batch loss 1.7438, batch acc 0.8502
12:24:29.747   Training iter 200, batch loss 1.7393, batch acc 0.8550
12:24:30.286   Training iter 250, batch loss 1.7437, batch acc 0.8548
12:24:30.811   Training iter 300, batch loss 1.7376, batch acc 0.8594
12:24:31.325   Training iter 350, batch loss 1.7415, batch acc 0.8534
12:24:31.830   Training iter 400, batch loss 1.7431, batch acc 0.8550
12:24:32.328   Training iter 450, batch loss 1.7414, batch acc 0.8588
12:24:32.839   Training iter 500, batch loss 1.7479, batch acc 0.8486
12:24:33.369   Training iter 550, batch loss 1.7497, batch acc 0.8446
12:24:33.881   Training iter 600, batch loss 1.7428, batch acc 0.8590
12:24:33.882 Training @ 174 epoch...
12:24:34.395   Training iter 50, batch loss 1.7453, batch acc 0.8580
12:24:34.911   Training iter 100, batch loss 1.7449, batch acc 0.8494
12:24:35.422   Training iter 150, batch loss 1.7424, batch acc 0.8584
12:24:35.929   Training iter 200, batch loss 1.7434, batch acc 0.8486
12:24:36.435   Training iter 250, batch loss 1.7407, batch acc 0.8606
12:24:36.920   Training iter 300, batch loss 1.7452, batch acc 0.8464
12:24:37.423   Training iter 350, batch loss 1.7410, batch acc 0.8630
12:24:37.925   Training iter 400, batch loss 1.7420, batch acc 0.8554
12:24:38.451   Training iter 450, batch loss 1.7414, batch acc 0.8552
12:24:38.964   Training iter 500, batch loss 1.7438, batch acc 0.8482
12:24:39.491   Training iter 550, batch loss 1.7448, batch acc 0.8534
12:24:40.060   Training iter 600, batch loss 1.7382, batch acc 0.8602
12:24:40.062 Training @ 175 epoch...
12:24:40.625   Training iter 50, batch loss 1.7402, batch acc 0.8638
12:24:41.169   Training iter 100, batch loss 1.7439, batch acc 0.8520
12:24:41.714   Training iter 150, batch loss 1.7409, batch acc 0.8560
12:24:42.260   Training iter 200, batch loss 1.7448, batch acc 0.8520
12:24:42.804   Training iter 250, batch loss 1.7427, batch acc 0.8518
12:24:43.371   Training iter 300, batch loss 1.7464, batch acc 0.8456
12:24:43.923   Training iter 350, batch loss 1.7377, batch acc 0.8634
12:24:44.474   Training iter 400, batch loss 1.7396, batch acc 0.8582
12:24:45.028   Training iter 450, batch loss 1.7430, batch acc 0.8532
12:24:45.556   Training iter 500, batch loss 1.7453, batch acc 0.8492
12:24:46.058   Training iter 550, batch loss 1.7453, batch acc 0.8538
12:24:46.563   Training iter 600, batch loss 1.7416, batch acc 0.8548
12:24:46.565 Testing @ 175 epoch...
12:24:46.609     Testing, total mean loss 1.73518, total acc 0.86350
12:24:46.609 Training @ 176 epoch...
12:24:47.139   Training iter 50, batch loss 1.7446, batch acc 0.8582
12:24:47.657   Training iter 100, batch loss 1.7363, batch acc 0.8626
12:24:48.191   Training iter 150, batch loss 1.7452, batch acc 0.8542
12:24:48.732   Training iter 200, batch loss 1.7456, batch acc 0.8510
12:24:49.256   Training iter 250, batch loss 1.7402, batch acc 0.8568
12:24:49.793   Training iter 300, batch loss 1.7410, batch acc 0.8496
12:24:50.344   Training iter 350, batch loss 1.7412, batch acc 0.8586
12:24:50.886   Training iter 400, batch loss 1.7445, batch acc 0.8492
12:24:51.417   Training iter 450, batch loss 1.7399, batch acc 0.8584
12:24:51.951   Training iter 500, batch loss 1.7418, batch acc 0.8534
12:24:52.487   Training iter 550, batch loss 1.7452, batch acc 0.8538
12:24:53.043   Training iter 600, batch loss 1.7444, batch acc 0.8516
12:24:53.045 Training @ 177 epoch...
12:24:53.584   Training iter 50, batch loss 1.7429, batch acc 0.8532
12:24:54.067   Training iter 100, batch loss 1.7396, batch acc 0.8568
12:24:54.563   Training iter 150, batch loss 1.7423, batch acc 0.8632
12:24:55.065   Training iter 200, batch loss 1.7407, batch acc 0.8546
12:24:55.603   Training iter 250, batch loss 1.7471, batch acc 0.8452
12:24:56.152   Training iter 300, batch loss 1.7446, batch acc 0.8526
12:24:56.700   Training iter 350, batch loss 1.7427, batch acc 0.8588
12:24:57.240   Training iter 400, batch loss 1.7440, batch acc 0.8512
12:24:57.788   Training iter 450, batch loss 1.7402, batch acc 0.8560
12:24:58.338   Training iter 500, batch loss 1.7402, batch acc 0.8570
12:24:58.891   Training iter 550, batch loss 1.7418, batch acc 0.8506
12:24:59.421   Training iter 600, batch loss 1.7423, batch acc 0.8580
12:24:59.423 Training @ 178 epoch...
12:24:59.974   Training iter 50, batch loss 1.7410, batch acc 0.8572
12:25:00.540   Training iter 100, batch loss 1.7430, batch acc 0.8540
12:25:01.087   Training iter 150, batch loss 1.7425, batch acc 0.8526
12:25:01.629   Training iter 200, batch loss 1.7446, batch acc 0.8540
12:25:02.190   Training iter 250, batch loss 1.7420, batch acc 0.8494
12:25:02.749   Training iter 300, batch loss 1.7413, batch acc 0.8574
12:25:03.310   Training iter 350, batch loss 1.7426, batch acc 0.8528
12:25:03.871   Training iter 400, batch loss 1.7435, batch acc 0.8522
12:25:04.450   Training iter 450, batch loss 1.7392, batch acc 0.8576
12:25:05.015   Training iter 500, batch loss 1.7422, batch acc 0.8570
12:25:05.567   Training iter 550, batch loss 1.7412, batch acc 0.8610
12:25:06.095   Training iter 600, batch loss 1.7439, batch acc 0.8520
12:25:06.097 Training @ 179 epoch...
12:25:06.652   Training iter 50, batch loss 1.7417, batch acc 0.8552
12:25:07.212   Training iter 100, batch loss 1.7421, batch acc 0.8590
12:25:07.761   Training iter 150, batch loss 1.7432, batch acc 0.8460
12:25:08.294   Training iter 200, batch loss 1.7409, batch acc 0.8592
12:25:08.814   Training iter 250, batch loss 1.7425, batch acc 0.8516
12:25:09.336   Training iter 300, batch loss 1.7406, batch acc 0.8558
12:25:09.855   Training iter 350, batch loss 1.7467, batch acc 0.8498
12:25:10.392   Training iter 400, batch loss 1.7422, batch acc 0.8576
12:25:10.923   Training iter 450, batch loss 1.7433, batch acc 0.8514
12:25:11.465   Training iter 500, batch loss 1.7409, batch acc 0.8578
12:25:11.987   Training iter 550, batch loss 1.7423, batch acc 0.8558
12:25:12.523   Training iter 600, batch loss 1.7391, batch acc 0.8612
12:25:12.524 Training @ 180 epoch...
12:25:13.054   Training iter 50, batch loss 1.7398, batch acc 0.8608
12:25:13.569   Training iter 100, batch loss 1.7448, batch acc 0.8466
12:25:14.093   Training iter 150, batch loss 1.7477, batch acc 0.8446
12:25:14.616   Training iter 200, batch loss 1.7426, batch acc 0.8526
12:25:15.158   Training iter 250, batch loss 1.7416, batch acc 0.8510
12:25:15.679   Training iter 300, batch loss 1.7410, batch acc 0.8546
12:25:16.272   Training iter 350, batch loss 1.7440, batch acc 0.8530
12:25:16.864   Training iter 400, batch loss 1.7412, batch acc 0.8560
12:25:17.448   Training iter 450, batch loss 1.7389, batch acc 0.8660
12:25:17.952   Training iter 500, batch loss 1.7399, batch acc 0.8640
12:25:18.432   Training iter 550, batch loss 1.7410, batch acc 0.8552
12:25:18.901   Training iter 600, batch loss 1.7415, batch acc 0.8540
12:25:18.902 Testing @ 180 epoch...
12:25:18.946     Testing, total mean loss 1.73455, total acc 0.86400
12:25:18.946 Training @ 181 epoch...
12:25:19.431   Training iter 50, batch loss 1.7401, batch acc 0.8548
12:25:19.926   Training iter 100, batch loss 1.7430, batch acc 0.8540
12:25:20.423   Training iter 150, batch loss 1.7430, batch acc 0.8558
12:25:20.923   Training iter 200, batch loss 1.7416, batch acc 0.8556
12:25:21.438   Training iter 250, batch loss 1.7429, batch acc 0.8564
12:25:21.954   Training iter 300, batch loss 1.7426, batch acc 0.8506
12:25:22.478   Training iter 350, batch loss 1.7376, batch acc 0.8670
12:25:23.007   Training iter 400, batch loss 1.7405, batch acc 0.8568
12:25:23.552   Training iter 450, batch loss 1.7431, batch acc 0.8498
12:25:24.076   Training iter 500, batch loss 1.7378, batch acc 0.8612
12:25:24.589   Training iter 550, batch loss 1.7455, batch acc 0.8520
12:25:25.110   Training iter 600, batch loss 1.7448, batch acc 0.8488
12:25:25.112 Training @ 182 epoch...
12:25:25.641   Training iter 50, batch loss 1.7429, batch acc 0.8502
12:25:26.170   Training iter 100, batch loss 1.7439, batch acc 0.8538
12:25:26.679   Training iter 150, batch loss 1.7438, batch acc 0.8500
12:25:27.208   Training iter 200, batch loss 1.7444, batch acc 0.8510
12:25:27.766   Training iter 250, batch loss 1.7460, batch acc 0.8514
12:25:28.320   Training iter 300, batch loss 1.7375, batch acc 0.8642
12:25:28.903   Training iter 350, batch loss 1.7392, batch acc 0.8528
12:25:29.481   Training iter 400, batch loss 1.7411, batch acc 0.8624
12:25:30.029   Training iter 450, batch loss 1.7392, batch acc 0.8570
12:25:30.552   Training iter 500, batch loss 1.7424, batch acc 0.8498
12:25:31.063   Training iter 550, batch loss 1.7403, batch acc 0.8624
12:25:31.566   Training iter 600, batch loss 1.7404, batch acc 0.8560
12:25:31.568 Training @ 183 epoch...
12:25:32.075   Training iter 50, batch loss 1.7466, batch acc 0.8540
12:25:32.604   Training iter 100, batch loss 1.7424, batch acc 0.8530
12:25:33.133   Training iter 150, batch loss 1.7376, batch acc 0.8560
12:25:33.614   Training iter 200, batch loss 1.7419, batch acc 0.8554
12:25:34.114   Training iter 250, batch loss 1.7445, batch acc 0.8498
12:25:34.618   Training iter 300, batch loss 1.7364, batch acc 0.8680
12:25:35.130   Training iter 350, batch loss 1.7418, batch acc 0.8512
12:25:35.636   Training iter 400, batch loss 1.7402, batch acc 0.8612
12:25:36.145   Training iter 450, batch loss 1.7416, batch acc 0.8506
12:25:36.623   Training iter 500, batch loss 1.7398, batch acc 0.8588
12:25:37.099   Training iter 550, batch loss 1.7398, batch acc 0.8564
12:25:37.585   Training iter 600, batch loss 1.7472, batch acc 0.8470
12:25:37.587 Training @ 184 epoch...
12:25:38.059   Training iter 50, batch loss 1.7377, batch acc 0.8624
12:25:38.535   Training iter 100, batch loss 1.7440, batch acc 0.8544
12:25:39.002   Training iter 150, batch loss 1.7417, batch acc 0.8566
12:25:39.476   Training iter 200, batch loss 1.7393, batch acc 0.8598
12:25:39.981   Training iter 250, batch loss 1.7432, batch acc 0.8568
12:25:40.474   Training iter 300, batch loss 1.7437, batch acc 0.8500
12:25:40.943   Training iter 350, batch loss 1.7441, batch acc 0.8502
12:25:41.413   Training iter 400, batch loss 1.7397, batch acc 0.8622
12:25:41.893   Training iter 450, batch loss 1.7386, batch acc 0.8580
12:25:42.376   Training iter 500, batch loss 1.7460, batch acc 0.8492
12:25:42.881   Training iter 550, batch loss 1.7395, batch acc 0.8528
12:25:43.363   Training iter 600, batch loss 1.7407, batch acc 0.8506
12:25:43.364 Training @ 185 epoch...
12:25:43.847   Training iter 50, batch loss 1.7411, batch acc 0.8550
12:25:44.335   Training iter 100, batch loss 1.7424, batch acc 0.8494
12:25:44.826   Training iter 150, batch loss 1.7373, batch acc 0.8606
12:25:45.307   Training iter 200, batch loss 1.7447, batch acc 0.8476
12:25:45.777   Training iter 250, batch loss 1.7411, batch acc 0.8570
12:25:46.259   Training iter 300, batch loss 1.7433, batch acc 0.8558
12:25:46.734   Training iter 350, batch loss 1.7434, batch acc 0.8550
12:25:47.193   Training iter 400, batch loss 1.7418, batch acc 0.8556
12:25:47.663   Training iter 450, batch loss 1.7381, batch acc 0.8610
12:25:48.132   Training iter 500, batch loss 1.7388, batch acc 0.8592
12:25:48.612   Training iter 550, batch loss 1.7429, batch acc 0.8562
12:25:49.088   Training iter 600, batch loss 1.7420, batch acc 0.8510
12:25:49.090 Testing @ 185 epoch...
12:25:49.134     Testing, total mean loss 1.73397, total acc 0.86470
12:25:49.134 Training @ 186 epoch...
12:25:49.616   Training iter 50, batch loss 1.7439, batch acc 0.8468
12:25:50.111   Training iter 100, batch loss 1.7366, batch acc 0.8636
12:25:50.581   Training iter 150, batch loss 1.7427, batch acc 0.8552
12:25:51.067   Training iter 200, batch loss 1.7425, batch acc 0.8524
12:25:51.587   Training iter 250, batch loss 1.7388, batch acc 0.8596
12:25:52.084   Training iter 300, batch loss 1.7426, batch acc 0.8528
12:25:52.587   Training iter 350, batch loss 1.7414, batch acc 0.8546
12:25:53.092   Training iter 400, batch loss 1.7375, batch acc 0.8632
12:25:53.602   Training iter 450, batch loss 1.7430, batch acc 0.8584
12:25:54.095   Training iter 500, batch loss 1.7439, batch acc 0.8510
12:25:54.590   Training iter 550, batch loss 1.7429, batch acc 0.8508
12:25:55.129   Training iter 600, batch loss 1.7397, batch acc 0.8560
12:25:55.131 Training @ 187 epoch...
12:25:55.656   Training iter 50, batch loss 1.7448, batch acc 0.8508
12:25:56.166   Training iter 100, batch loss 1.7379, batch acc 0.8586
12:25:56.666   Training iter 150, batch loss 1.7386, batch acc 0.8594
12:25:57.204   Training iter 200, batch loss 1.7469, batch acc 0.8448
12:25:57.751   Training iter 250, batch loss 1.7445, batch acc 0.8574
12:25:58.296   Training iter 300, batch loss 1.7372, batch acc 0.8518
12:25:58.830   Training iter 350, batch loss 1.7423, batch acc 0.8614
12:25:59.345   Training iter 400, batch loss 1.7395, batch acc 0.8526
12:25:59.911   Training iter 450, batch loss 1.7407, batch acc 0.8598
12:26:00.506   Training iter 500, batch loss 1.7409, batch acc 0.8520
12:26:01.072   Training iter 550, batch loss 1.7412, batch acc 0.8552
12:26:01.655   Training iter 600, batch loss 1.7399, batch acc 0.8620
12:26:01.656 Training @ 188 epoch...
12:26:02.281   Training iter 50, batch loss 1.7390, batch acc 0.8508
12:26:02.876   Training iter 100, batch loss 1.7387, batch acc 0.8588
12:26:03.462   Training iter 150, batch loss 1.7463, batch acc 0.8472
12:26:04.045   Training iter 200, batch loss 1.7407, batch acc 0.8544
12:26:04.624   Training iter 250, batch loss 1.7397, batch acc 0.8584
12:26:05.198   Training iter 300, batch loss 1.7379, batch acc 0.8630
12:26:05.795   Training iter 350, batch loss 1.7432, batch acc 0.8550
12:26:06.360   Training iter 400, batch loss 1.7426, batch acc 0.8548
12:26:06.911   Training iter 450, batch loss 1.7405, batch acc 0.8612
12:26:07.452   Training iter 500, batch loss 1.7400, batch acc 0.8530
12:26:07.982   Training iter 550, batch loss 1.7433, batch acc 0.8550
12:26:08.522   Training iter 600, batch loss 1.7412, batch acc 0.8548
12:26:08.523 Training @ 189 epoch...
12:26:09.050   Training iter 50, batch loss 1.7378, batch acc 0.8632
12:26:09.542   Training iter 100, batch loss 1.7419, batch acc 0.8506
12:26:10.080   Training iter 150, batch loss 1.7395, batch acc 0.8546
12:26:10.624   Training iter 200, batch loss 1.7458, batch acc 0.8520
12:26:11.143   Training iter 250, batch loss 1.7393, batch acc 0.8580
12:26:11.654   Training iter 300, batch loss 1.7385, batch acc 0.8590
12:26:12.178   Training iter 350, batch loss 1.7356, batch acc 0.8632
12:26:12.743   Training iter 400, batch loss 1.7403, batch acc 0.8570
12:26:13.323   Training iter 450, batch loss 1.7415, batch acc 0.8534
12:26:13.899   Training iter 500, batch loss 1.7396, batch acc 0.8586
12:26:14.508   Training iter 550, batch loss 1.7504, batch acc 0.8454
12:26:15.129   Training iter 600, batch loss 1.7413, batch acc 0.8534
12:26:15.131 Training @ 190 epoch...
12:26:15.746   Training iter 50, batch loss 1.7428, batch acc 0.8508
12:26:16.291   Training iter 100, batch loss 1.7402, batch acc 0.8548
12:26:16.865   Training iter 150, batch loss 1.7378, batch acc 0.8622
12:26:17.412   Training iter 200, batch loss 1.7380, batch acc 0.8588
12:26:17.958   Training iter 250, batch loss 1.7406, batch acc 0.8554
12:26:18.516   Training iter 300, batch loss 1.7420, batch acc 0.8528
12:26:19.030   Training iter 350, batch loss 1.7418, batch acc 0.8528
12:26:19.580   Training iter 400, batch loss 1.7435, batch acc 0.8536
12:26:20.127   Training iter 450, batch loss 1.7408, batch acc 0.8560
12:26:20.660   Training iter 500, batch loss 1.7402, batch acc 0.8620
12:26:21.186   Training iter 550, batch loss 1.7423, batch acc 0.8522
12:26:21.711   Training iter 600, batch loss 1.7404, batch acc 0.8552
12:26:21.713 Testing @ 190 epoch...
12:26:21.758     Testing, total mean loss 1.73343, total acc 0.86530
12:26:21.758 Training @ 191 epoch...
12:26:22.333   Training iter 50, batch loss 1.7377, batch acc 0.8562
12:26:22.879   Training iter 100, batch loss 1.7402, batch acc 0.8618
12:26:23.415   Training iter 150, batch loss 1.7429, batch acc 0.8510
12:26:23.941   Training iter 200, batch loss 1.7369, batch acc 0.8604
12:26:24.460   Training iter 250, batch loss 1.7442, batch acc 0.8556
12:26:24.998   Training iter 300, batch loss 1.7440, batch acc 0.8532
12:26:25.547   Training iter 350, batch loss 1.7376, batch acc 0.8606
12:26:26.075   Training iter 400, batch loss 1.7437, batch acc 0.8476
12:26:26.606   Training iter 450, batch loss 1.7404, batch acc 0.8558
12:26:27.146   Training iter 500, batch loss 1.7412, batch acc 0.8508
12:26:27.703   Training iter 550, batch loss 1.7423, batch acc 0.8550
12:26:28.248   Training iter 600, batch loss 1.7380, batch acc 0.8612
12:26:28.249 Training @ 192 epoch...
12:26:28.783   Training iter 50, batch loss 1.7440, batch acc 0.8552
12:26:29.327   Training iter 100, batch loss 1.7389, batch acc 0.8536
12:26:29.850   Training iter 150, batch loss 1.7388, batch acc 0.8556
12:26:30.409   Training iter 200, batch loss 1.7433, batch acc 0.8548
12:26:30.935   Training iter 250, batch loss 1.7413, batch acc 0.8496
12:26:31.470   Training iter 300, batch loss 1.7371, batch acc 0.8622
12:26:31.990   Training iter 350, batch loss 1.7378, batch acc 0.8584
12:26:32.521   Training iter 400, batch loss 1.7434, batch acc 0.8526
12:26:33.051   Training iter 450, batch loss 1.7420, batch acc 0.8592
12:26:33.593   Training iter 500, batch loss 1.7397, batch acc 0.8524
12:26:34.112   Training iter 550, batch loss 1.7401, batch acc 0.8612
12:26:34.609   Training iter 600, batch loss 1.7415, batch acc 0.8554
12:26:34.611 Training @ 193 epoch...
12:26:35.131   Training iter 50, batch loss 1.7407, batch acc 0.8516
12:26:35.660   Training iter 100, batch loss 1.7382, batch acc 0.8612
12:26:36.187   Training iter 150, batch loss 1.7429, batch acc 0.8482
12:26:36.738   Training iter 200, batch loss 1.7409, batch acc 0.8486
12:26:37.314   Training iter 250, batch loss 1.7416, batch acc 0.8578
12:26:37.869   Training iter 300, batch loss 1.7407, batch acc 0.8630
12:26:38.429   Training iter 350, batch loss 1.7401, batch acc 0.8582
12:26:38.989   Training iter 400, batch loss 1.7380, batch acc 0.8610
12:26:39.519   Training iter 450, batch loss 1.7438, batch acc 0.8522
12:26:40.068   Training iter 500, batch loss 1.7409, batch acc 0.8546
12:26:40.633   Training iter 550, batch loss 1.7393, batch acc 0.8544
12:26:41.181   Training iter 600, batch loss 1.7396, batch acc 0.8578
12:26:41.183 Training @ 194 epoch...
12:26:41.748   Training iter 50, batch loss 1.7414, batch acc 0.8588
12:26:42.306   Training iter 100, batch loss 1.7405, batch acc 0.8570
12:26:42.873   Training iter 150, batch loss 1.7422, batch acc 0.8546
12:26:43.442   Training iter 200, batch loss 1.7372, batch acc 0.8616
12:26:44.014   Training iter 250, batch loss 1.7403, batch acc 0.8536
12:26:44.568   Training iter 300, batch loss 1.7392, batch acc 0.8620
12:26:45.117   Training iter 350, batch loss 1.7477, batch acc 0.8426
12:26:45.654   Training iter 400, batch loss 1.7347, batch acc 0.8600
12:26:46.163   Training iter 450, batch loss 1.7418, batch acc 0.8508
12:26:46.701   Training iter 500, batch loss 1.7428, batch acc 0.8516
12:26:47.221   Training iter 550, batch loss 1.7414, batch acc 0.8538
12:26:47.755   Training iter 600, batch loss 1.7359, batch acc 0.8648
12:26:47.757 Training @ 195 epoch...
12:26:48.291   Training iter 50, batch loss 1.7412, batch acc 0.8582
12:26:48.826   Training iter 100, batch loss 1.7343, batch acc 0.8638
12:26:49.344   Training iter 150, batch loss 1.7428, batch acc 0.8480
12:26:49.918   Training iter 200, batch loss 1.7448, batch acc 0.8530
12:26:50.461   Training iter 250, batch loss 1.7386, batch acc 0.8566
12:26:51.017   Training iter 300, batch loss 1.7387, batch acc 0.8544
12:26:51.566   Training iter 350, batch loss 1.7428, batch acc 0.8500
12:26:52.123   Training iter 400, batch loss 1.7378, batch acc 0.8576
12:26:52.697   Training iter 450, batch loss 1.7445, batch acc 0.8504
12:26:53.273   Training iter 500, batch loss 1.7446, batch acc 0.8550
12:26:53.829   Training iter 550, batch loss 1.7384, batch acc 0.8604
12:26:54.349   Training iter 600, batch loss 1.7357, batch acc 0.8628
12:26:54.351 Testing @ 195 epoch...
12:26:54.395     Testing, total mean loss 1.73293, total acc 0.86540
12:26:54.395 Training @ 196 epoch...
12:26:54.921   Training iter 50, batch loss 1.7411, batch acc 0.8542
12:26:55.435   Training iter 100, batch loss 1.7365, batch acc 0.8600
12:26:55.957   Training iter 150, batch loss 1.7374, batch acc 0.8594
12:26:56.484   Training iter 200, batch loss 1.7426, batch acc 0.8566
12:26:57.002   Training iter 250, batch loss 1.7388, batch acc 0.8572
12:26:57.509   Training iter 300, batch loss 1.7418, batch acc 0.8532
12:26:58.015   Training iter 350, batch loss 1.7423, batch acc 0.8556
12:26:58.523   Training iter 400, batch loss 1.7415, batch acc 0.8564
12:26:59.039   Training iter 450, batch loss 1.7415, batch acc 0.8540
12:26:59.550   Training iter 500, batch loss 1.7435, batch acc 0.8484
12:27:00.074   Training iter 550, batch loss 1.7347, batch acc 0.8634
12:27:00.627   Training iter 600, batch loss 1.7414, batch acc 0.8516
12:27:00.629 Training @ 197 epoch...
12:27:01.162   Training iter 50, batch loss 1.7435, batch acc 0.8538
12:27:01.723   Training iter 100, batch loss 1.7424, batch acc 0.8508
12:27:02.277   Training iter 150, batch loss 1.7379, batch acc 0.8570
12:27:02.812   Training iter 200, batch loss 1.7404, batch acc 0.8560
12:27:03.350   Training iter 250, batch loss 1.7375, batch acc 0.8578
12:27:03.907   Training iter 300, batch loss 1.7381, batch acc 0.8630
12:27:04.466   Training iter 350, batch loss 1.7399, batch acc 0.8548
12:27:05.023   Training iter 400, batch loss 1.7384, batch acc 0.8578
12:27:05.570   Training iter 450, batch loss 1.7353, batch acc 0.8672
12:27:06.121   Training iter 500, batch loss 1.7470, batch acc 0.8452
12:27:06.679   Training iter 550, batch loss 1.7403, batch acc 0.8594
12:27:07.232   Training iter 600, batch loss 1.7410, batch acc 0.8490
12:27:07.234 Training @ 198 epoch...
12:27:07.789   Training iter 50, batch loss 1.7396, batch acc 0.8616
12:27:08.328   Training iter 100, batch loss 1.7423, batch acc 0.8580
12:27:08.858   Training iter 150, batch loss 1.7345, batch acc 0.8638
12:27:09.372   Training iter 200, batch loss 1.7384, batch acc 0.8540
12:27:09.868   Training iter 250, batch loss 1.7432, batch acc 0.8500
12:27:10.379   Training iter 300, batch loss 1.7452, batch acc 0.8508
12:27:10.880   Training iter 350, batch loss 1.7395, batch acc 0.8560
12:27:11.431   Training iter 400, batch loss 1.7404, batch acc 0.8546
12:27:12.015   Training iter 450, batch loss 1.7376, batch acc 0.8588
12:27:12.583   Training iter 500, batch loss 1.7380, batch acc 0.8584
12:27:13.139   Training iter 550, batch loss 1.7398, batch acc 0.8584
12:27:13.643   Training iter 600, batch loss 1.7423, batch acc 0.8494
12:27:13.644 Training @ 199 epoch...
12:27:14.131   Training iter 50, batch loss 1.7408, batch acc 0.8490
12:27:14.629   Training iter 100, batch loss 1.7373, batch acc 0.8558
12:27:15.139   Training iter 150, batch loss 1.7426, batch acc 0.8554
12:27:15.680   Training iter 200, batch loss 1.7371, batch acc 0.8540
12:27:16.236   Training iter 250, batch loss 1.7432, batch acc 0.8572
12:27:16.752   Training iter 300, batch loss 1.7404, batch acc 0.8556
12:27:17.264   Training iter 350, batch loss 1.7368, batch acc 0.8656
12:27:17.782   Training iter 400, batch loss 1.7355, batch acc 0.8624
12:27:18.294   Training iter 450, batch loss 1.7369, batch acc 0.8562
12:27:18.798   Training iter 500, batch loss 1.7429, batch acc 0.8548
12:27:19.313   Training iter 550, batch loss 1.7400, batch acc 0.8564
12:27:19.872   Training iter 600, batch loss 1.7461, batch acc 0.8504
12:27:19.874 Training @ 200 epoch...
12:27:20.455   Training iter 50, batch loss 1.7410, batch acc 0.8516
12:27:21.015   Training iter 100, batch loss 1.7402, batch acc 0.8532
12:27:21.571   Training iter 150, batch loss 1.7384, batch acc 0.8620
12:27:22.128   Training iter 200, batch loss 1.7432, batch acc 0.8522
12:27:22.722   Training iter 250, batch loss 1.7440, batch acc 0.8504
12:27:23.313   Training iter 300, batch loss 1.7381, batch acc 0.8618
12:27:23.877   Training iter 350, batch loss 1.7397, batch acc 0.8534
12:27:24.437   Training iter 400, batch loss 1.7391, batch acc 0.8582
12:27:24.994   Training iter 450, batch loss 1.7360, batch acc 0.8618
12:27:25.565   Training iter 500, batch loss 1.7384, batch acc 0.8526
12:27:26.133   Training iter 550, batch loss 1.7416, batch acc 0.8522
12:27:26.693   Training iter 600, batch loss 1.7388, batch acc 0.8644
12:27:26.695 Testing @ 200 epoch...
12:27:26.739     Testing, total mean loss 1.73246, total acc 0.86570
12:27:26.739 Plot @ 200 epoch...
12:27:26.739 Training @ 201 epoch...
12:27:27.303   Training iter 50, batch loss 1.7423, batch acc 0.8544
12:27:27.867   Training iter 100, batch loss 1.7395, batch acc 0.8592
12:27:28.437   Training iter 150, batch loss 1.7416, batch acc 0.8548
12:27:29.004   Training iter 200, batch loss 1.7422, batch acc 0.8516
12:27:29.565   Training iter 250, batch loss 1.7380, batch acc 0.8536
12:27:30.116   Training iter 300, batch loss 1.7425, batch acc 0.8512
12:27:30.667   Training iter 350, batch loss 1.7396, batch acc 0.8578
12:27:31.221   Training iter 400, batch loss 1.7372, batch acc 0.8586
12:27:31.781   Training iter 450, batch loss 1.7388, batch acc 0.8598
12:27:32.344   Training iter 500, batch loss 1.7365, batch acc 0.8608
12:27:32.890   Training iter 550, batch loss 1.7419, batch acc 0.8534
12:27:33.452   Training iter 600, batch loss 1.7374, batch acc 0.8598
12:27:33.453 Training @ 202 epoch...
12:27:33.981   Training iter 50, batch loss 1.7337, batch acc 0.8662
12:27:34.506   Training iter 100, batch loss 1.7426, batch acc 0.8534
12:27:35.043   Training iter 150, batch loss 1.7395, batch acc 0.8536
12:27:35.589   Training iter 200, batch loss 1.7412, batch acc 0.8618
12:27:36.141   Training iter 250, batch loss 1.7369, batch acc 0.8594
12:27:36.645   Training iter 300, batch loss 1.7430, batch acc 0.8492
12:27:37.164   Training iter 350, batch loss 1.7375, batch acc 0.8616
12:27:37.714   Training iter 400, batch loss 1.7397, batch acc 0.8552
12:27:38.257   Training iter 450, batch loss 1.7419, batch acc 0.8466
12:27:38.777   Training iter 500, batch loss 1.7402, batch acc 0.8550
12:27:39.300   Training iter 550, batch loss 1.7380, batch acc 0.8580
12:27:39.826   Training iter 600, batch loss 1.7422, batch acc 0.8538
12:27:39.828 Training @ 203 epoch...
12:27:40.385   Training iter 50, batch loss 1.7367, batch acc 0.8518
12:27:40.903   Training iter 100, batch loss 1.7392, batch acc 0.8546
12:27:41.438   Training iter 150, batch loss 1.7404, batch acc 0.8604
12:27:41.981   Training iter 200, batch loss 1.7375, batch acc 0.8606
12:27:42.535   Training iter 250, batch loss 1.7412, batch acc 0.8578
12:27:43.066   Training iter 300, batch loss 1.7379, batch acc 0.8542
12:27:43.604   Training iter 350, batch loss 1.7423, batch acc 0.8496
12:27:44.129   Training iter 400, batch loss 1.7385, batch acc 0.8554
12:27:44.674   Training iter 450, batch loss 1.7421, batch acc 0.8586
12:27:45.240   Training iter 500, batch loss 1.7404, batch acc 0.8582
12:27:45.792   Training iter 550, batch loss 1.7367, batch acc 0.8622
12:27:46.316   Training iter 600, batch loss 1.7426, batch acc 0.8530
12:27:46.318 Training @ 204 epoch...
12:27:46.861   Training iter 50, batch loss 1.7387, batch acc 0.8570
12:27:47.394   Training iter 100, batch loss 1.7348, batch acc 0.8584
12:27:47.951   Training iter 150, batch loss 1.7417, batch acc 0.8592
12:27:48.535   Training iter 200, batch loss 1.7401, batch acc 0.8572
12:27:49.104   Training iter 250, batch loss 1.7361, batch acc 0.8604
12:27:49.676   Training iter 300, batch loss 1.7390, batch acc 0.8588
12:27:50.260   Training iter 350, batch loss 1.7377, batch acc 0.8592
12:27:50.860   Training iter 400, batch loss 1.7416, batch acc 0.8564
12:27:51.428   Training iter 450, batch loss 1.7421, batch acc 0.8564
12:27:52.003   Training iter 500, batch loss 1.7408, batch acc 0.8512
12:27:52.574   Training iter 550, batch loss 1.7391, batch acc 0.8526
12:27:53.146   Training iter 600, batch loss 1.7427, batch acc 0.8504
12:27:53.148 Training @ 205 epoch...
12:27:53.743   Training iter 50, batch loss 1.7355, batch acc 0.8624
12:27:54.328   Training iter 100, batch loss 1.7414, batch acc 0.8576
12:27:54.895   Training iter 150, batch loss 1.7422, batch acc 0.8548
12:27:55.475   Training iter 200, batch loss 1.7417, batch acc 0.8572
12:27:56.054   Training iter 250, batch loss 1.7376, batch acc 0.8556
12:27:56.611   Training iter 300, batch loss 1.7418, batch acc 0.8502
12:27:57.165   Training iter 350, batch loss 1.7366, batch acc 0.8606
12:27:57.725   Training iter 400, batch loss 1.7375, batch acc 0.8618
12:27:58.282   Training iter 450, batch loss 1.7366, batch acc 0.8564
12:27:58.857   Training iter 500, batch loss 1.7411, batch acc 0.8540
12:27:59.423   Training iter 550, batch loss 1.7436, batch acc 0.8474
12:27:59.965   Training iter 600, batch loss 1.7377, batch acc 0.8614
12:27:59.967 Testing @ 205 epoch...
12:28:00.015     Testing, total mean loss 1.73202, total acc 0.86640
12:28:00.015 Training @ 206 epoch...
12:28:00.572   Training iter 50, batch loss 1.7366, batch acc 0.8632
12:28:01.121   Training iter 100, batch loss 1.7384, batch acc 0.8626
12:28:01.736   Training iter 150, batch loss 1.7382, batch acc 0.8622
12:28:02.383   Training iter 200, batch loss 1.7408, batch acc 0.8506
12:28:02.922   Training iter 250, batch loss 1.7386, batch acc 0.8568
12:28:03.455   Training iter 300, batch loss 1.7406, batch acc 0.8538
12:28:03.975   Training iter 350, batch loss 1.7398, batch acc 0.8532
12:28:04.495   Training iter 400, batch loss 1.7376, batch acc 0.8536
12:28:05.027   Training iter 450, batch loss 1.7390, batch acc 0.8556
12:28:05.553   Training iter 500, batch loss 1.7384, batch acc 0.8592
12:28:06.084   Training iter 550, batch loss 1.7417, batch acc 0.8532
12:28:06.628   Training iter 600, batch loss 1.7425, batch acc 0.8538
12:28:06.630 Training @ 207 epoch...
12:28:07.172   Training iter 50, batch loss 1.7345, batch acc 0.8694
12:28:07.727   Training iter 100, batch loss 1.7362, batch acc 0.8572
12:28:08.280   Training iter 150, batch loss 1.7370, batch acc 0.8574
12:28:08.829   Training iter 200, batch loss 1.7370, batch acc 0.8588
12:28:09.370   Training iter 250, batch loss 1.7434, batch acc 0.8496
12:28:09.895   Training iter 300, batch loss 1.7405, batch acc 0.8522
12:28:10.471   Training iter 350, batch loss 1.7425, batch acc 0.8510
12:28:11.063   Training iter 400, batch loss 1.7416, batch acc 0.8534
12:28:11.650   Training iter 450, batch loss 1.7389, batch acc 0.8578
12:28:12.214   Training iter 500, batch loss 1.7396, batch acc 0.8570
12:28:12.760   Training iter 550, batch loss 1.7412, batch acc 0.8586
12:28:13.307   Training iter 600, batch loss 1.7388, batch acc 0.8556
12:28:13.309 Training @ 208 epoch...
12:28:13.851   Training iter 50, batch loss 1.7363, batch acc 0.8570
12:28:14.388   Training iter 100, batch loss 1.7389, batch acc 0.8572
12:28:14.908   Training iter 150, batch loss 1.7416, batch acc 0.8488
12:28:15.419   Training iter 200, batch loss 1.7366, batch acc 0.8624
12:28:15.923   Training iter 250, batch loss 1.7365, batch acc 0.8598
12:28:16.438   Training iter 300, batch loss 1.7402, batch acc 0.8578
12:28:16.908   Training iter 350, batch loss 1.7396, batch acc 0.8580
12:28:17.355   Training iter 400, batch loss 1.7406, batch acc 0.8528
12:28:17.844   Training iter 450, batch loss 1.7411, batch acc 0.8572
12:28:18.369   Training iter 500, batch loss 1.7406, batch acc 0.8542
12:28:18.870   Training iter 550, batch loss 1.7368, batch acc 0.8622
12:28:19.374   Training iter 600, batch loss 1.7415, batch acc 0.8526
12:28:19.376 Training @ 209 epoch...
12:28:19.880   Training iter 50, batch loss 1.7386, batch acc 0.8532
12:28:20.388   Training iter 100, batch loss 1.7405, batch acc 0.8576
12:28:20.875   Training iter 150, batch loss 1.7364, batch acc 0.8602
12:28:21.372   Training iter 200, batch loss 1.7372, batch acc 0.8608
12:28:21.865   Training iter 250, batch loss 1.7451, batch acc 0.8506
12:28:22.363   Training iter 300, batch loss 1.7387, batch acc 0.8586
12:28:22.863   Training iter 350, batch loss 1.7374, batch acc 0.8566
12:28:23.401   Training iter 400, batch loss 1.7401, batch acc 0.8550
12:28:23.932   Training iter 450, batch loss 1.7405, batch acc 0.8550
12:28:24.462   Training iter 500, batch loss 1.7415, batch acc 0.8536
12:28:25.049   Training iter 550, batch loss 1.7395, batch acc 0.8566
12:28:25.631   Training iter 600, batch loss 1.7338, batch acc 0.8638
12:28:25.633 Training @ 210 epoch...
12:28:26.233   Training iter 50, batch loss 1.7406, batch acc 0.8594
12:28:26.834   Training iter 100, batch loss 1.7380, batch acc 0.8546
12:28:27.428   Training iter 150, batch loss 1.7403, batch acc 0.8582
12:28:28.019   Training iter 200, batch loss 1.7380, batch acc 0.8554
12:28:28.628   Training iter 250, batch loss 1.7360, batch acc 0.8654
12:28:29.227   Training iter 300, batch loss 1.7408, batch acc 0.8524
12:28:29.790   Training iter 350, batch loss 1.7446, batch acc 0.8514
12:28:30.355   Training iter 400, batch loss 1.7410, batch acc 0.8468
12:28:30.862   Training iter 450, batch loss 1.7355, batch acc 0.8598
12:28:31.325   Training iter 500, batch loss 1.7422, batch acc 0.8512
12:28:31.796   Training iter 550, batch loss 1.7361, batch acc 0.8638
12:28:32.274   Training iter 600, batch loss 1.7352, batch acc 0.8630
12:28:32.275 Testing @ 210 epoch...
12:28:32.320     Testing, total mean loss 1.73161, total acc 0.86660
12:28:32.320 Training @ 211 epoch...
12:28:32.813   Training iter 50, batch loss 1.7348, batch acc 0.8618
12:28:33.295   Training iter 100, batch loss 1.7372, batch acc 0.8604
12:28:33.765   Training iter 150, batch loss 1.7378, batch acc 0.8642
12:28:34.239   Training iter 200, batch loss 1.7401, batch acc 0.8514
12:28:34.720   Training iter 250, batch loss 1.7388, batch acc 0.8582
12:28:35.210   Training iter 300, batch loss 1.7401, batch acc 0.8550
12:28:35.678   Training iter 350, batch loss 1.7378, batch acc 0.8580
12:28:36.150   Training iter 400, batch loss 1.7488, batch acc 0.8416
12:28:36.620   Training iter 450, batch loss 1.7373, batch acc 0.8618
12:28:37.092   Training iter 500, batch loss 1.7369, batch acc 0.8554
12:28:37.609   Training iter 550, batch loss 1.7411, batch acc 0.8580
12:28:38.110   Training iter 600, batch loss 1.7368, batch acc 0.8568
12:28:38.111 Training @ 212 epoch...
12:28:38.622   Training iter 50, batch loss 1.7404, batch acc 0.8582
12:28:39.143   Training iter 100, batch loss 1.7392, batch acc 0.8556
12:28:39.705   Training iter 150, batch loss 1.7376, batch acc 0.8590
12:28:40.268   Training iter 200, batch loss 1.7374, batch acc 0.8556
12:28:40.815   Training iter 250, batch loss 1.7384, batch acc 0.8552
12:28:41.365   Training iter 300, batch loss 1.7362, batch acc 0.8598
12:28:41.930   Training iter 350, batch loss 1.7412, batch acc 0.8502
12:28:42.498   Training iter 400, batch loss 1.7426, batch acc 0.8574
12:28:43.060   Training iter 450, batch loss 1.7392, batch acc 0.8598
12:28:43.623   Training iter 500, batch loss 1.7390, batch acc 0.8558
12:28:44.175   Training iter 550, batch loss 1.7368, batch acc 0.8602
12:28:44.729   Training iter 600, batch loss 1.7385, batch acc 0.8558
12:28:44.730 Training @ 213 epoch...
12:28:45.290   Training iter 50, batch loss 1.7413, batch acc 0.8548
12:28:45.828   Training iter 100, batch loss 1.7404, batch acc 0.8578
12:28:46.408   Training iter 150, batch loss 1.7347, batch acc 0.8572
12:28:46.977   Training iter 200, batch loss 1.7393, batch acc 0.8578
12:28:47.534   Training iter 250, batch loss 1.7392, batch acc 0.8542
12:28:48.025   Training iter 300, batch loss 1.7404, batch acc 0.8520
12:28:48.513   Training iter 350, batch loss 1.7362, batch acc 0.8650
12:28:48.984   Training iter 400, batch loss 1.7389, batch acc 0.8510
12:28:49.457   Training iter 450, batch loss 1.7382, batch acc 0.8578
12:28:49.946   Training iter 500, batch loss 1.7388, batch acc 0.8552
12:28:50.415   Training iter 550, batch loss 1.7384, batch acc 0.8608
12:28:50.889   Training iter 600, batch loss 1.7398, batch acc 0.8578
12:28:50.891 Training @ 214 epoch...
12:28:51.350   Training iter 50, batch loss 1.7393, batch acc 0.8590
12:28:51.802   Training iter 100, batch loss 1.7430, batch acc 0.8506
12:28:52.264   Training iter 150, batch loss 1.7403, batch acc 0.8550
12:28:52.737   Training iter 200, batch loss 1.7304, batch acc 0.8640
12:28:53.219   Training iter 250, batch loss 1.7416, batch acc 0.8508
12:28:53.699   Training iter 300, batch loss 1.7358, batch acc 0.8600
12:28:54.164   Training iter 350, batch loss 1.7377, batch acc 0.8548
12:28:54.643   Training iter 400, batch loss 1.7410, batch acc 0.8524
12:28:55.140   Training iter 450, batch loss 1.7371, batch acc 0.8660
12:28:55.643   Training iter 500, batch loss 1.7438, batch acc 0.8504
12:28:56.148   Training iter 550, batch loss 1.7369, batch acc 0.8590
12:28:56.659   Training iter 600, batch loss 1.7376, batch acc 0.8606
12:28:56.661 Training @ 215 epoch...
12:28:57.176   Training iter 50, batch loss 1.7392, batch acc 0.8618
12:28:57.693   Training iter 100, batch loss 1.7401, batch acc 0.8534
12:28:58.199   Training iter 150, batch loss 1.7409, batch acc 0.8544
12:28:58.696   Training iter 200, batch loss 1.7398, batch acc 0.8560
12:28:59.198   Training iter 250, batch loss 1.7377, batch acc 0.8572
12:28:59.705   Training iter 300, batch loss 1.7407, batch acc 0.8540
12:29:00.209   Training iter 350, batch loss 1.7343, batch acc 0.8652
12:29:00.714   Training iter 400, batch loss 1.7398, batch acc 0.8596
12:29:01.232   Training iter 450, batch loss 1.7372, batch acc 0.8550
12:29:01.810   Training iter 500, batch loss 1.7369, batch acc 0.8530
12:29:02.362   Training iter 550, batch loss 1.7372, batch acc 0.8582
12:29:02.864   Training iter 600, batch loss 1.7398, batch acc 0.8552
12:29:02.866 Testing @ 215 epoch...
12:29:02.911     Testing, total mean loss 1.73123, total acc 0.86680
12:29:02.911 Training @ 216 epoch...
12:29:03.443   Training iter 50, batch loss 1.7442, batch acc 0.8452
12:29:03.966   Training iter 100, batch loss 1.7406, batch acc 0.8510
12:29:04.510   Training iter 150, batch loss 1.7421, batch acc 0.8538
12:29:05.058   Training iter 200, batch loss 1.7401, batch acc 0.8514
12:29:05.570   Training iter 250, batch loss 1.7355, batch acc 0.8628
12:29:06.081   Training iter 300, batch loss 1.7375, batch acc 0.8636
12:29:06.615   Training iter 350, batch loss 1.7376, batch acc 0.8596
12:29:07.161   Training iter 400, batch loss 1.7401, batch acc 0.8560
12:29:07.683   Training iter 450, batch loss 1.7366, batch acc 0.8622
12:29:08.193   Training iter 500, batch loss 1.7360, batch acc 0.8600
12:29:08.692   Training iter 550, batch loss 1.7352, batch acc 0.8552
12:29:09.184   Training iter 600, batch loss 1.7374, batch acc 0.8636
12:29:09.185 Training @ 217 epoch...
12:29:09.704   Training iter 50, batch loss 1.7335, batch acc 0.8674
12:29:10.211   Training iter 100, batch loss 1.7419, batch acc 0.8498
12:29:10.730   Training iter 150, batch loss 1.7382, batch acc 0.8520
12:29:11.215   Training iter 200, batch loss 1.7420, batch acc 0.8604
12:29:11.726   Training iter 250, batch loss 1.7386, batch acc 0.8530
12:29:12.255   Training iter 300, batch loss 1.7356, batch acc 0.8564
12:29:12.806   Training iter 350, batch loss 1.7385, batch acc 0.8592
12:29:13.354   Training iter 400, batch loss 1.7408, batch acc 0.8554
12:29:13.891   Training iter 450, batch loss 1.7364, batch acc 0.8572
12:29:14.423   Training iter 500, batch loss 1.7379, batch acc 0.8618
12:29:14.961   Training iter 550, batch loss 1.7401, batch acc 0.8562
12:29:15.481   Training iter 600, batch loss 1.7384, batch acc 0.8556
12:29:15.483 Training @ 218 epoch...
12:29:16.013   Training iter 50, batch loss 1.7388, batch acc 0.8588
12:29:16.555   Training iter 100, batch loss 1.7343, batch acc 0.8580
12:29:17.104   Training iter 150, batch loss 1.7393, batch acc 0.8578
12:29:17.598   Training iter 200, batch loss 1.7357, batch acc 0.8658
12:29:18.081   Training iter 250, batch loss 1.7410, batch acc 0.8478
12:29:18.581   Training iter 300, batch loss 1.7384, batch acc 0.8594
12:29:19.049   Training iter 350, batch loss 1.7397, batch acc 0.8536
12:29:19.513   Training iter 400, batch loss 1.7404, batch acc 0.8564
12:29:19.975   Training iter 450, batch loss 1.7424, batch acc 0.8522
12:29:20.424   Training iter 500, batch loss 1.7404, batch acc 0.8532
12:29:20.892   Training iter 550, batch loss 1.7330, batch acc 0.8620
12:29:21.361   Training iter 600, batch loss 1.7376, batch acc 0.8580
12:29:21.362 Training @ 219 epoch...
12:29:21.824   Training iter 50, batch loss 1.7350, batch acc 0.8632
12:29:22.288   Training iter 100, batch loss 1.7403, batch acc 0.8562
12:29:22.749   Training iter 150, batch loss 1.7390, batch acc 0.8602
12:29:23.198   Training iter 200, batch loss 1.7395, batch acc 0.8604
12:29:23.650   Training iter 250, batch loss 1.7382, batch acc 0.8514
12:29:24.102   Training iter 300, batch loss 1.7385, batch acc 0.8530
12:29:24.560   Training iter 350, batch loss 1.7385, batch acc 0.8560
12:29:25.026   Training iter 400, batch loss 1.7430, batch acc 0.8514
12:29:25.502   Training iter 450, batch loss 1.7336, batch acc 0.8638
12:29:25.968   Training iter 500, batch loss 1.7402, batch acc 0.8554
12:29:26.438   Training iter 550, batch loss 1.7341, batch acc 0.8584
12:29:26.917   Training iter 600, batch loss 1.7403, batch acc 0.8528
12:29:26.919 Training @ 220 epoch...
12:29:27.419   Training iter 50, batch loss 1.7390, batch acc 0.8588
12:29:27.957   Training iter 100, batch loss 1.7443, batch acc 0.8478
12:29:28.481   Training iter 150, batch loss 1.7389, batch acc 0.8572
12:29:28.999   Training iter 200, batch loss 1.7368, batch acc 0.8646
12:29:29.519   Training iter 250, batch loss 1.7356, batch acc 0.8596
12:29:30.045   Training iter 300, batch loss 1.7401, batch acc 0.8548
12:29:30.578   Training iter 350, batch loss 1.7394, batch acc 0.8544
12:29:31.112   Training iter 400, batch loss 1.7386, batch acc 0.8546
12:29:31.640   Training iter 450, batch loss 1.7393, batch acc 0.8506
12:29:32.169   Training iter 500, batch loss 1.7356, batch acc 0.8572
12:29:32.712   Training iter 550, batch loss 1.7376, batch acc 0.8588
12:29:33.243   Training iter 600, batch loss 1.7343, batch acc 0.8664
12:29:33.245 Testing @ 220 epoch...
12:29:33.289     Testing, total mean loss 1.73087, total acc 0.86660
12:29:33.289 Training @ 221 epoch...
12:29:33.815   Training iter 50, batch loss 1.7380, batch acc 0.8550
12:29:34.343   Training iter 100, batch loss 1.7370, batch acc 0.8524
12:29:34.898   Training iter 150, batch loss 1.7423, batch acc 0.8528
12:29:35.453   Training iter 200, batch loss 1.7353, batch acc 0.8648
12:29:35.972   Training iter 250, batch loss 1.7414, batch acc 0.8550
12:29:36.492   Training iter 300, batch loss 1.7394, batch acc 0.8518
12:29:37.014   Training iter 350, batch loss 1.7383, batch acc 0.8550
12:29:37.545   Training iter 400, batch loss 1.7349, batch acc 0.8638
12:29:38.080   Training iter 450, batch loss 1.7375, batch acc 0.8576
12:29:38.621   Training iter 500, batch loss 1.7339, batch acc 0.8650
12:29:39.151   Training iter 550, batch loss 1.7390, batch acc 0.8560
12:29:39.690   Training iter 600, batch loss 1.7416, batch acc 0.8562
12:29:39.692 Training @ 222 epoch...
12:29:40.232   Training iter 50, batch loss 1.7409, batch acc 0.8520
12:29:40.815   Training iter 100, batch loss 1.7402, batch acc 0.8486
12:29:41.392   Training iter 150, batch loss 1.7425, batch acc 0.8524
12:29:41.970   Training iter 200, batch loss 1.7347, batch acc 0.8686
12:29:42.483   Training iter 250, batch loss 1.7371, batch acc 0.8554
12:29:42.995   Training iter 300, batch loss 1.7373, batch acc 0.8594
12:29:43.498   Training iter 350, batch loss 1.7345, batch acc 0.8612
12:29:44.009   Training iter 400, batch loss 1.7383, batch acc 0.8590
12:29:44.512   Training iter 450, batch loss 1.7381, batch acc 0.8572
12:29:45.015   Training iter 500, batch loss 1.7346, batch acc 0.8598
12:29:45.516   Training iter 550, batch loss 1.7397, batch acc 0.8534
12:29:46.040   Training iter 600, batch loss 1.7399, batch acc 0.8586
12:29:46.042 Training @ 223 epoch...
12:29:46.568   Training iter 50, batch loss 1.7403, batch acc 0.8550
12:29:47.108   Training iter 100, batch loss 1.7381, batch acc 0.8568
12:29:47.648   Training iter 150, batch loss 1.7377, batch acc 0.8634
12:29:48.212   Training iter 200, batch loss 1.7370, batch acc 0.8646
12:29:48.773   Training iter 250, batch loss 1.7365, batch acc 0.8538
12:29:49.342   Training iter 300, batch loss 1.7360, batch acc 0.8566
12:29:49.892   Training iter 350, batch loss 1.7356, batch acc 0.8562
12:29:50.428   Training iter 400, batch loss 1.7432, batch acc 0.8502
12:29:50.950   Training iter 450, batch loss 1.7373, batch acc 0.8622
12:29:51.479   Training iter 500, batch loss 1.7378, batch acc 0.8612
12:29:51.962   Training iter 550, batch loss 1.7383, batch acc 0.8572
12:29:52.452   Training iter 600, batch loss 1.7392, batch acc 0.8492
12:29:52.454 Training @ 224 epoch...
12:29:52.951   Training iter 50, batch loss 1.7377, batch acc 0.8572
12:29:53.449   Training iter 100, batch loss 1.7399, batch acc 0.8568
12:29:53.937   Training iter 150, batch loss 1.7363, batch acc 0.8598
12:29:54.425   Training iter 200, batch loss 1.7418, batch acc 0.8440
12:29:54.910   Training iter 250, batch loss 1.7346, batch acc 0.8604
12:29:55.402   Training iter 300, batch loss 1.7363, batch acc 0.8608
12:29:55.878   Training iter 350, batch loss 1.7365, batch acc 0.8590
12:29:56.385   Training iter 400, batch loss 1.7354, batch acc 0.8634
12:29:56.899   Training iter 450, batch loss 1.7370, batch acc 0.8600
12:29:57.419   Training iter 500, batch loss 1.7410, batch acc 0.8536
12:29:57.938   Training iter 550, batch loss 1.7391, batch acc 0.8590
12:29:58.451   Training iter 600, batch loss 1.7406, batch acc 0.8524
12:29:58.453 Training @ 225 epoch...
12:29:58.972   Training iter 50, batch loss 1.7433, batch acc 0.8430
12:29:59.491   Training iter 100, batch loss 1.7340, batch acc 0.8620
12:30:00.010   Training iter 150, batch loss 1.7403, batch acc 0.8508
12:30:00.544   Training iter 200, batch loss 1.7345, batch acc 0.8608
12:30:01.079   Training iter 250, batch loss 1.7379, batch acc 0.8548
12:30:01.632   Training iter 300, batch loss 1.7402, batch acc 0.8550
12:30:02.224   Training iter 350, batch loss 1.7367, batch acc 0.8638
12:30:02.774   Training iter 400, batch loss 1.7369, batch acc 0.8558
12:30:03.322   Training iter 450, batch loss 1.7322, batch acc 0.8682
12:30:03.882   Training iter 500, batch loss 1.7416, batch acc 0.8568
12:30:04.390   Training iter 550, batch loss 1.7390, batch acc 0.8624
12:30:04.942   Training iter 600, batch loss 1.7388, batch acc 0.8550
12:30:04.944 Testing @ 225 epoch...
12:30:04.990     Testing, total mean loss 1.73054, total acc 0.86680
12:30:04.990 Training @ 226 epoch...
12:30:05.585   Training iter 50, batch loss 1.7380, batch acc 0.8632
12:30:06.168   Training iter 100, batch loss 1.7418, batch acc 0.8562
12:30:06.761   Training iter 150, batch loss 1.7345, batch acc 0.8594
12:30:07.352   Training iter 200, batch loss 1.7363, batch acc 0.8604
12:30:07.937   Training iter 250, batch loss 1.7407, batch acc 0.8504
12:30:08.516   Training iter 300, batch loss 1.7413, batch acc 0.8540
12:30:09.088   Training iter 350, batch loss 1.7330, batch acc 0.8610
12:30:09.655   Training iter 400, batch loss 1.7387, batch acc 0.8544
12:30:10.222   Training iter 450, batch loss 1.7352, batch acc 0.8620
12:30:10.774   Training iter 500, batch loss 1.7380, batch acc 0.8576
12:30:11.337   Training iter 550, batch loss 1.7374, batch acc 0.8556
12:30:11.897   Training iter 600, batch loss 1.7398, batch acc 0.8544
12:30:11.898 Training @ 227 epoch...
12:30:12.470   Training iter 50, batch loss 1.7391, batch acc 0.8574
12:30:13.033   Training iter 100, batch loss 1.7380, batch acc 0.8538
12:30:13.571   Training iter 150, batch loss 1.7370, batch acc 0.8548
12:30:14.117   Training iter 200, batch loss 1.7382, batch acc 0.8578
12:30:14.649   Training iter 250, batch loss 1.7387, batch acc 0.8582
12:30:15.182   Training iter 300, batch loss 1.7365, batch acc 0.8554
12:30:15.703   Training iter 350, batch loss 1.7400, batch acc 0.8552
12:30:16.230   Training iter 400, batch loss 1.7355, batch acc 0.8614
12:30:16.756   Training iter 450, batch loss 1.7441, batch acc 0.8480
12:30:17.283   Training iter 500, batch loss 1.7355, batch acc 0.8594
12:30:17.829   Training iter 550, batch loss 1.7374, batch acc 0.8628
12:30:18.387   Training iter 600, batch loss 1.7338, batch acc 0.8656
12:30:18.388 Training @ 228 epoch...
12:30:18.934   Training iter 50, batch loss 1.7398, batch acc 0.8554
12:30:19.476   Training iter 100, batch loss 1.7333, batch acc 0.8622
12:30:20.032   Training iter 150, batch loss 1.7384, batch acc 0.8588
12:30:20.591   Training iter 200, batch loss 1.7378, batch acc 0.8548
12:30:21.139   Training iter 250, batch loss 1.7395, batch acc 0.8490
12:30:21.682   Training iter 300, batch loss 1.7392, batch acc 0.8584
12:30:22.240   Training iter 350, batch loss 1.7412, batch acc 0.8530
12:30:22.793   Training iter 400, batch loss 1.7332, batch acc 0.8622
12:30:23.342   Training iter 450, batch loss 1.7370, batch acc 0.8580
12:30:23.875   Training iter 500, batch loss 1.7384, batch acc 0.8592
12:30:24.395   Training iter 550, batch loss 1.7373, batch acc 0.8602
12:30:24.913   Training iter 600, batch loss 1.7380, batch acc 0.8564
12:30:24.915 Training @ 229 epoch...
12:30:25.449   Training iter 50, batch loss 1.7386, batch acc 0.8574
12:30:25.974   Training iter 100, batch loss 1.7394, batch acc 0.8554
12:30:26.513   Training iter 150, batch loss 1.7350, batch acc 0.8592
12:30:27.044   Training iter 200, batch loss 1.7392, batch acc 0.8584
12:30:27.580   Training iter 250, batch loss 1.7357, batch acc 0.8640
12:30:28.115   Training iter 300, batch loss 1.7364, batch acc 0.8562
12:30:28.629   Training iter 350, batch loss 1.7377, batch acc 0.8596
12:30:29.131   Training iter 400, batch loss 1.7355, batch acc 0.8620
12:30:29.660   Training iter 450, batch loss 1.7400, batch acc 0.8528
12:30:30.211   Training iter 500, batch loss 1.7391, batch acc 0.8532
12:30:30.760   Training iter 550, batch loss 1.7366, batch acc 0.8584
12:30:31.305   Training iter 600, batch loss 1.7391, batch acc 0.8526
12:30:31.306 Training @ 230 epoch...
12:30:31.854   Training iter 50, batch loss 1.7359, batch acc 0.8568
12:30:32.395   Training iter 100, batch loss 1.7395, batch acc 0.8572
12:30:32.933   Training iter 150, batch loss 1.7399, batch acc 0.8482
12:30:33.474   Training iter 200, batch loss 1.7415, batch acc 0.8520
12:30:34.030   Training iter 250, batch loss 1.7354, batch acc 0.8592
12:30:34.601   Training iter 300, batch loss 1.7397, batch acc 0.8558
12:30:35.153   Training iter 350, batch loss 1.7392, batch acc 0.8600
12:30:35.690   Training iter 400, batch loss 1.7345, batch acc 0.8648
12:30:36.268   Training iter 450, batch loss 1.7373, batch acc 0.8586
12:30:36.800   Training iter 500, batch loss 1.7369, batch acc 0.8576
12:30:37.412   Training iter 550, batch loss 1.7365, batch acc 0.8616
12:30:38.030   Training iter 600, batch loss 1.7354, batch acc 0.8594
12:30:38.032 Testing @ 230 epoch...
12:30:38.082     Testing, total mean loss 1.73022, total acc 0.86720
12:30:38.082 Training @ 231 epoch...
12:30:38.697   Training iter 50, batch loss 1.7386, batch acc 0.8552
12:30:39.256   Training iter 100, batch loss 1.7313, batch acc 0.8648
12:30:39.816   Training iter 150, batch loss 1.7372, batch acc 0.8604
12:30:40.359   Training iter 200, batch loss 1.7365, batch acc 0.8566
12:30:40.907   Training iter 250, batch loss 1.7386, batch acc 0.8522
12:30:41.458   Training iter 300, batch loss 1.7354, batch acc 0.8570
12:30:42.004   Training iter 350, batch loss 1.7374, batch acc 0.8600
12:30:42.539   Training iter 400, batch loss 1.7405, batch acc 0.8524
12:30:43.066   Training iter 450, batch loss 1.7383, batch acc 0.8570
12:30:43.597   Training iter 500, batch loss 1.7383, batch acc 0.8568
12:30:44.118   Training iter 550, batch loss 1.7388, batch acc 0.8570
12:30:44.628   Training iter 600, batch loss 1.7401, batch acc 0.8614
12:30:44.629 Training @ 232 epoch...
12:30:45.151   Training iter 50, batch loss 1.7358, batch acc 0.8572
12:30:45.664   Training iter 100, batch loss 1.7319, batch acc 0.8672
12:30:46.199   Training iter 150, batch loss 1.7383, batch acc 0.8598
12:30:46.726   Training iter 200, batch loss 1.7397, batch acc 0.8542
12:30:47.236   Training iter 250, batch loss 1.7397, batch acc 0.8558
12:30:47.745   Training iter 300, batch loss 1.7419, batch acc 0.8542
12:30:48.261   Training iter 350, batch loss 1.7369, batch acc 0.8602
12:30:48.798   Training iter 400, batch loss 1.7374, batch acc 0.8556
12:30:49.319   Training iter 450, batch loss 1.7396, batch acc 0.8568
12:30:49.846   Training iter 500, batch loss 1.7376, batch acc 0.8544
12:30:50.420   Training iter 550, batch loss 1.7376, batch acc 0.8558
12:30:50.995   Training iter 600, batch loss 1.7337, batch acc 0.8600
12:30:50.997 Training @ 233 epoch...
12:30:51.600   Training iter 50, batch loss 1.7373, batch acc 0.8626
12:30:52.210   Training iter 100, batch loss 1.7372, batch acc 0.8586
12:30:52.829   Training iter 150, batch loss 1.7431, batch acc 0.8498
12:30:53.435   Training iter 200, batch loss 1.7400, batch acc 0.8544
12:30:53.994   Training iter 250, batch loss 1.7362, batch acc 0.8546
12:30:54.568   Training iter 300, batch loss 1.7338, batch acc 0.8608
12:30:55.145   Training iter 350, batch loss 1.7338, batch acc 0.8622
12:30:55.733   Training iter 400, batch loss 1.7385, batch acc 0.8580
12:30:56.264   Training iter 450, batch loss 1.7367, batch acc 0.8576
12:30:56.766   Training iter 500, batch loss 1.7365, batch acc 0.8562
12:30:57.243   Training iter 550, batch loss 1.7337, batch acc 0.8688
12:30:57.727   Training iter 600, batch loss 1.7427, batch acc 0.8484
12:30:57.729 Training @ 234 epoch...
12:30:58.241   Training iter 50, batch loss 1.7391, batch acc 0.8522
12:30:58.764   Training iter 100, batch loss 1.7327, batch acc 0.8624
12:30:59.329   Training iter 150, batch loss 1.7404, batch acc 0.8526
12:30:59.889   Training iter 200, batch loss 1.7372, batch acc 0.8570
12:31:00.452   Training iter 250, batch loss 1.7405, batch acc 0.8540
12:31:01.030   Training iter 300, batch loss 1.7324, batch acc 0.8682
12:31:01.566   Training iter 350, batch loss 1.7366, batch acc 0.8596
12:31:02.099   Training iter 400, batch loss 1.7397, batch acc 0.8538
12:31:02.622   Training iter 450, batch loss 1.7370, batch acc 0.8594
12:31:03.331   Training iter 500, batch loss 1.7424, batch acc 0.8484
12:31:03.875   Training iter 550, batch loss 1.7329, batch acc 0.8656
12:31:04.373   Training iter 600, batch loss 1.7379, batch acc 0.8584
12:31:04.375 Training @ 235 epoch...
12:31:04.882   Training iter 50, batch loss 1.7379, batch acc 0.8628
12:31:05.381   Training iter 100, batch loss 1.7371, batch acc 0.8554
12:31:05.886   Training iter 150, batch loss 1.7375, batch acc 0.8504
12:31:06.421   Training iter 200, batch loss 1.7379, batch acc 0.8544
12:31:06.961   Training iter 250, batch loss 1.7357, batch acc 0.8616
12:31:07.507   Training iter 300, batch loss 1.7373, batch acc 0.8604
12:31:08.041   Training iter 350, batch loss 1.7393, batch acc 0.8576
12:31:08.593   Training iter 400, batch loss 1.7380, batch acc 0.8576
12:31:09.133   Training iter 450, batch loss 1.7327, batch acc 0.8618
12:31:09.687   Training iter 500, batch loss 1.7390, batch acc 0.8538
12:31:10.196   Training iter 550, batch loss 1.7356, batch acc 0.8608
12:31:10.698   Training iter 600, batch loss 1.7402, batch acc 0.8526
12:31:10.700 Testing @ 235 epoch...
12:31:10.744     Testing, total mean loss 1.72993, total acc 0.86750
12:31:10.744 Training @ 236 epoch...
12:31:11.257   Training iter 50, batch loss 1.7410, batch acc 0.8506
12:31:11.754   Training iter 100, batch loss 1.7342, batch acc 0.8618
12:31:12.234   Training iter 150, batch loss 1.7377, batch acc 0.8520
12:31:12.712   Training iter 200, batch loss 1.7361, batch acc 0.8568
12:31:13.195   Training iter 250, batch loss 1.7390, batch acc 0.8574
12:31:13.695   Training iter 300, batch loss 1.7351, batch acc 0.8594
12:31:14.179   Training iter 350, batch loss 1.7413, batch acc 0.8552
12:31:14.678   Training iter 400, batch loss 1.7381, batch acc 0.8540
12:31:15.168   Training iter 450, batch loss 1.7356, batch acc 0.8610
12:31:15.672   Training iter 500, batch loss 1.7359, batch acc 0.8580
12:31:16.147   Training iter 550, batch loss 1.7361, batch acc 0.8654
12:31:16.610   Training iter 600, batch loss 1.7372, batch acc 0.8618
12:31:16.612 Training @ 237 epoch...
12:31:17.111   Training iter 50, batch loss 1.7368, batch acc 0.8624
12:31:17.575   Training iter 100, batch loss 1.7355, batch acc 0.8602
12:31:18.040   Training iter 150, batch loss 1.7398, batch acc 0.8564
12:31:18.507   Training iter 200, batch loss 1.7395, batch acc 0.8540
12:31:18.967   Training iter 250, batch loss 1.7400, batch acc 0.8562
12:31:19.442   Training iter 300, batch loss 1.7350, batch acc 0.8556
12:31:19.916   Training iter 350, batch loss 1.7380, batch acc 0.8538
12:31:20.382   Training iter 400, batch loss 1.7365, batch acc 0.8588
12:31:20.844   Training iter 450, batch loss 1.7386, batch acc 0.8508
12:31:21.303   Training iter 500, batch loss 1.7358, batch acc 0.8614
12:31:21.787   Training iter 550, batch loss 1.7322, batch acc 0.8650
12:31:22.310   Training iter 600, batch loss 1.7391, batch acc 0.8564
12:31:22.312 Training @ 238 epoch...
12:31:22.846   Training iter 50, batch loss 1.7358, batch acc 0.8630
12:31:23.369   Training iter 100, batch loss 1.7340, batch acc 0.8614
12:31:23.897   Training iter 150, batch loss 1.7365, batch acc 0.8572
12:31:24.412   Training iter 200, batch loss 1.7398, batch acc 0.8520
12:31:24.961   Training iter 250, batch loss 1.7395, batch acc 0.8538
12:31:25.521   Training iter 300, batch loss 1.7386, batch acc 0.8584
12:31:26.059   Training iter 350, batch loss 1.7375, batch acc 0.8574
12:31:26.594   Training iter 400, batch loss 1.7384, batch acc 0.8540
12:31:27.153   Training iter 450, batch loss 1.7365, batch acc 0.8570
12:31:27.727   Training iter 500, batch loss 1.7391, batch acc 0.8542
12:31:28.300   Training iter 550, batch loss 1.7348, batch acc 0.8632
12:31:28.806   Training iter 600, batch loss 1.7356, batch acc 0.8620
12:31:28.808 Training @ 239 epoch...
12:31:29.349   Training iter 50, batch loss 1.7354, batch acc 0.8606
12:31:29.862   Training iter 100, batch loss 1.7392, batch acc 0.8506
12:31:30.354   Training iter 150, batch loss 1.7351, batch acc 0.8616
12:31:30.890   Training iter 200, batch loss 1.7401, batch acc 0.8562
12:31:31.410   Training iter 250, batch loss 1.7330, batch acc 0.8636
12:31:31.940   Training iter 300, batch loss 1.7387, batch acc 0.8540
12:31:32.474   Training iter 350, batch loss 1.7405, batch acc 0.8566
12:31:33.014   Training iter 400, batch loss 1.7314, batch acc 0.8704
12:31:33.532   Training iter 450, batch loss 1.7438, batch acc 0.8478
12:31:34.055   Training iter 500, batch loss 1.7352, batch acc 0.8590
12:31:34.573   Training iter 550, batch loss 1.7392, batch acc 0.8526
12:31:35.078   Training iter 600, batch loss 1.7337, batch acc 0.8616
12:31:35.080 Training @ 240 epoch...
12:31:35.586   Training iter 50, batch loss 1.7404, batch acc 0.8538
12:31:36.084   Training iter 100, batch loss 1.7372, batch acc 0.8552
12:31:36.573   Training iter 150, batch loss 1.7319, batch acc 0.8678
12:31:37.082   Training iter 200, batch loss 1.7341, batch acc 0.8616
12:31:37.607   Training iter 250, batch loss 1.7432, batch acc 0.8438
12:31:38.143   Training iter 300, batch loss 1.7395, batch acc 0.8560
12:31:38.666   Training iter 350, batch loss 1.7383, batch acc 0.8544
12:31:39.192   Training iter 400, batch loss 1.7359, batch acc 0.8550
12:31:39.712   Training iter 450, batch loss 1.7362, batch acc 0.8644
12:31:40.247   Training iter 500, batch loss 1.7377, batch acc 0.8580
12:31:40.771   Training iter 550, batch loss 1.7338, batch acc 0.8650
12:31:41.296   Training iter 600, batch loss 1.7366, batch acc 0.8592
12:31:41.298 Testing @ 240 epoch...
12:31:41.344     Testing, total mean loss 1.72966, total acc 0.86770
12:31:41.344 Training @ 241 epoch...
12:31:41.868   Training iter 50, batch loss 1.7385, batch acc 0.8546
12:31:42.394   Training iter 100, batch loss 1.7352, batch acc 0.8614
12:31:42.918   Training iter 150, batch loss 1.7383, batch acc 0.8546
12:31:43.428   Training iter 200, batch loss 1.7388, batch acc 0.8560
12:31:43.942   Training iter 250, batch loss 1.7373, batch acc 0.8632
12:31:44.433   Training iter 300, batch loss 1.7347, batch acc 0.8598
12:31:44.939   Training iter 350, batch loss 1.7323, batch acc 0.8646
12:31:45.435   Training iter 400, batch loss 1.7409, batch acc 0.8544
12:31:45.941   Training iter 450, batch loss 1.7395, batch acc 0.8552
12:31:46.440   Training iter 500, batch loss 1.7331, batch acc 0.8596
12:31:46.933   Training iter 550, batch loss 1.7398, batch acc 0.8502
12:31:47.431   Training iter 600, batch loss 1.7358, batch acc 0.8612
12:31:47.433 Training @ 242 epoch...
12:31:47.941   Training iter 50, batch loss 1.7393, batch acc 0.8534
12:31:48.465   Training iter 100, batch loss 1.7345, batch acc 0.8560
12:31:48.952   Training iter 150, batch loss 1.7374, batch acc 0.8582
12:31:49.435   Training iter 200, batch loss 1.7372, batch acc 0.8578
12:31:49.935   Training iter 250, batch loss 1.7349, batch acc 0.8578
12:31:50.431   Training iter 300, batch loss 1.7364, batch acc 0.8578
12:31:50.913   Training iter 350, batch loss 1.7387, batch acc 0.8558
12:31:51.388   Training iter 400, batch loss 1.7400, batch acc 0.8528
12:31:51.894   Training iter 450, batch loss 1.7389, batch acc 0.8606
12:31:52.390   Training iter 500, batch loss 1.7338, batch acc 0.8638
12:31:52.899   Training iter 550, batch loss 1.7330, batch acc 0.8664
12:31:53.416   Training iter 600, batch loss 1.7395, batch acc 0.8524
12:31:53.418 Training @ 243 epoch...
12:31:53.906   Training iter 50, batch loss 1.7335, batch acc 0.8646
12:31:54.417   Training iter 100, batch loss 1.7401, batch acc 0.8554
12:31:54.942   Training iter 150, batch loss 1.7387, batch acc 0.8548
12:31:55.487   Training iter 200, batch loss 1.7402, batch acc 0.8498
12:31:55.997   Training iter 250, batch loss 1.7355, batch acc 0.8580
12:31:56.519   Training iter 300, batch loss 1.7375, batch acc 0.8610
12:31:57.045   Training iter 350, batch loss 1.7339, batch acc 0.8588
12:31:57.562   Training iter 400, batch loss 1.7338, batch acc 0.8604
12:31:58.074   Training iter 450, batch loss 1.7388, batch acc 0.8556
12:31:58.578   Training iter 500, batch loss 1.7387, batch acc 0.8538
12:31:59.086   Training iter 550, batch loss 1.7375, batch acc 0.8590
12:31:59.614   Training iter 600, batch loss 1.7346, batch acc 0.8650
12:31:59.616 Training @ 244 epoch...
12:32:00.162   Training iter 50, batch loss 1.7353, batch acc 0.8618
12:32:00.672   Training iter 100, batch loss 1.7336, batch acc 0.8636
12:32:01.210   Training iter 150, batch loss 1.7359, batch acc 0.8616
12:32:01.726   Training iter 200, batch loss 1.7401, batch acc 0.8502
12:32:02.298   Training iter 250, batch loss 1.7343, batch acc 0.8658
12:32:02.859   Training iter 300, batch loss 1.7392, batch acc 0.8520
12:32:03.418   Training iter 350, batch loss 1.7343, batch acc 0.8608
12:32:03.966   Training iter 400, batch loss 1.7349, batch acc 0.8644
12:32:04.517   Training iter 450, batch loss 1.7374, batch acc 0.8522
12:32:05.083   Training iter 500, batch loss 1.7388, batch acc 0.8572
12:32:05.636   Training iter 550, batch loss 1.7434, batch acc 0.8478
12:32:06.194   Training iter 600, batch loss 1.7352, batch acc 0.8580
12:32:06.196 Training @ 245 epoch...
12:32:06.760   Training iter 50, batch loss 1.7398, batch acc 0.8582
12:32:07.305   Training iter 100, batch loss 1.7391, batch acc 0.8588
12:32:07.820   Training iter 150, batch loss 1.7368, batch acc 0.8526
12:32:08.323   Training iter 200, batch loss 1.7360, batch acc 0.8638
12:32:08.821   Training iter 250, batch loss 1.7374, batch acc 0.8584
12:32:09.317   Training iter 300, batch loss 1.7309, batch acc 0.8656
12:32:09.821   Training iter 350, batch loss 1.7365, batch acc 0.8620
12:32:10.346   Training iter 400, batch loss 1.7359, batch acc 0.8578
12:32:10.894   Training iter 450, batch loss 1.7346, batch acc 0.8564
12:32:11.432   Training iter 500, batch loss 1.7378, batch acc 0.8546
12:32:11.972   Training iter 550, batch loss 1.7411, batch acc 0.8508
12:32:12.523   Training iter 600, batch loss 1.7357, batch acc 0.8560
12:32:12.525 Testing @ 245 epoch...
12:32:12.569     Testing, total mean loss 1.72941, total acc 0.86800
12:32:12.569 Training @ 246 epoch...
12:32:13.159   Training iter 50, batch loss 1.7357, batch acc 0.8628
12:32:13.702   Training iter 100, batch loss 1.7375, batch acc 0.8546
12:32:14.239   Training iter 150, batch loss 1.7352, batch acc 0.8598
12:32:14.788   Training iter 200, batch loss 1.7338, batch acc 0.8588
12:32:15.351   Training iter 250, batch loss 1.7353, batch acc 0.8608
12:32:15.918   Training iter 300, batch loss 1.7377, batch acc 0.8636
12:32:16.490   Training iter 350, batch loss 1.7351, batch acc 0.8582
12:32:16.997   Training iter 400, batch loss 1.7349, batch acc 0.8608
12:32:17.498   Training iter 450, batch loss 1.7398, batch acc 0.8574
12:32:18.012   Training iter 500, batch loss 1.7364, batch acc 0.8540
12:32:18.523   Training iter 550, batch loss 1.7368, batch acc 0.8572
12:32:19.027   Training iter 600, batch loss 1.7427, batch acc 0.8492
12:32:19.029 Training @ 247 epoch...
12:32:19.538   Training iter 50, batch loss 1.7341, batch acc 0.8558
12:32:20.061   Training iter 100, batch loss 1.7394, batch acc 0.8586
12:32:20.588   Training iter 150, batch loss 1.7339, batch acc 0.8610
12:32:21.126   Training iter 200, batch loss 1.7406, batch acc 0.8530
12:32:21.658   Training iter 250, batch loss 1.7374, batch acc 0.8562
12:32:22.238   Training iter 300, batch loss 1.7354, batch acc 0.8506
12:32:22.777   Training iter 350, batch loss 1.7384, batch acc 0.8518
12:32:23.297   Training iter 400, batch loss 1.7406, batch acc 0.8556
12:32:23.829   Training iter 450, batch loss 1.7317, batch acc 0.8688
12:32:24.355   Training iter 500, batch loss 1.7346, batch acc 0.8640
12:32:24.951   Training iter 550, batch loss 1.7383, batch acc 0.8596
12:32:25.518   Training iter 600, batch loss 1.7360, batch acc 0.8598
12:32:25.520 Training @ 248 epoch...
12:32:26.140   Training iter 50, batch loss 1.7346, batch acc 0.8610
12:32:26.769   Training iter 100, batch loss 1.7366, batch acc 0.8598
12:32:27.284   Training iter 150, batch loss 1.7348, batch acc 0.8576
12:32:27.839   Training iter 200, batch loss 1.7354, batch acc 0.8614
12:32:28.388   Training iter 250, batch loss 1.7372, batch acc 0.8550
12:32:28.928   Training iter 300, batch loss 1.7370, batch acc 0.8588
12:32:29.464   Training iter 350, batch loss 1.7387, batch acc 0.8516
12:32:29.984   Training iter 400, batch loss 1.7396, batch acc 0.8526
12:32:30.506   Training iter 450, batch loss 1.7367, batch acc 0.8586
12:32:31.020   Training iter 500, batch loss 1.7381, batch acc 0.8580
12:32:31.533   Training iter 550, batch loss 1.7361, batch acc 0.8592
12:32:32.025   Training iter 600, batch loss 1.7350, batch acc 0.8642
12:32:32.027 Training @ 249 epoch...
12:32:32.503   Training iter 50, batch loss 1.7388, batch acc 0.8524
12:32:32.983   Training iter 100, batch loss 1.7381, batch acc 0.8588
12:32:33.462   Training iter 150, batch loss 1.7368, batch acc 0.8578
12:32:33.937   Training iter 200, batch loss 1.7360, batch acc 0.8602
12:32:34.402   Training iter 250, batch loss 1.7367, batch acc 0.8560
12:32:34.876   Training iter 300, batch loss 1.7363, batch acc 0.8580
12:32:35.367   Training iter 350, batch loss 1.7410, batch acc 0.8576
12:32:35.832   Training iter 400, batch loss 1.7361, batch acc 0.8602
12:32:36.300   Training iter 450, batch loss 1.7355, batch acc 0.8566
12:32:36.765   Training iter 500, batch loss 1.7352, batch acc 0.8578
12:32:37.232   Training iter 550, batch loss 1.7358, batch acc 0.8568
12:32:37.704   Training iter 600, batch loss 1.7329, batch acc 0.8634
12:32:37.706 Training @ 250 epoch...
12:32:38.184   Training iter 50, batch loss 1.7371, batch acc 0.8570
12:32:38.651   Training iter 100, batch loss 1.7336, batch acc 0.8620
12:32:39.111   Training iter 150, batch loss 1.7365, batch acc 0.8632
12:32:39.563   Training iter 200, batch loss 1.7369, batch acc 0.8616
12:32:40.023   Training iter 250, batch loss 1.7335, batch acc 0.8636
12:32:40.484   Training iter 300, batch loss 1.7377, batch acc 0.8508
12:32:40.951   Training iter 350, batch loss 1.7373, batch acc 0.8566
12:32:41.421   Training iter 400, batch loss 1.7400, batch acc 0.8542
12:32:41.907   Training iter 450, batch loss 1.7388, batch acc 0.8566
12:32:42.391   Training iter 500, batch loss 1.7360, batch acc 0.8592
12:32:42.876   Training iter 550, batch loss 1.7323, batch acc 0.8594
12:32:43.380   Training iter 600, batch loss 1.7391, batch acc 0.8536
12:32:43.382 Testing @ 250 epoch...
12:32:43.424     Testing, total mean loss 1.72917, total acc 0.86830
12:32:43.424 Training @ 251 epoch...
12:32:43.931   Training iter 50, batch loss 1.7406, batch acc 0.8518
12:32:44.428   Training iter 100, batch loss 1.7365, batch acc 0.8566
12:32:44.911   Training iter 150, batch loss 1.7329, batch acc 0.8644
12:32:45.372   Training iter 200, batch loss 1.7367, batch acc 0.8558
12:32:45.831   Training iter 250, batch loss 1.7392, batch acc 0.8518
12:32:46.293   Training iter 300, batch loss 1.7329, batch acc 0.8650
12:32:46.760   Training iter 350, batch loss 1.7335, batch acc 0.8612
12:32:47.214   Training iter 400, batch loss 1.7372, batch acc 0.8560
12:32:47.668   Training iter 450, batch loss 1.7363, batch acc 0.8596
12:32:48.142   Training iter 500, batch loss 1.7443, batch acc 0.8462
12:32:48.631   Training iter 550, batch loss 1.7330, batch acc 0.8646
12:32:49.084   Training iter 600, batch loss 1.7351, batch acc 0.8644
12:32:49.086 Training @ 252 epoch...
12:32:49.547   Training iter 50, batch loss 1.7397, batch acc 0.8498
12:32:50.016   Training iter 100, batch loss 1.7409, batch acc 0.8564
12:32:50.493   Training iter 150, batch loss 1.7308, batch acc 0.8680
12:32:50.957   Training iter 200, batch loss 1.7352, batch acc 0.8590
12:32:51.423   Training iter 250, batch loss 1.7333, batch acc 0.8634
12:32:51.916   Training iter 300, batch loss 1.7363, batch acc 0.8578
12:32:52.403   Training iter 350, batch loss 1.7356, batch acc 0.8600
12:32:52.890   Training iter 400, batch loss 1.7346, batch acc 0.8592
12:32:53.379   Training iter 450, batch loss 1.7367, batch acc 0.8552
12:32:53.846   Training iter 500, batch loss 1.7382, batch acc 0.8596
12:32:54.328   Training iter 550, batch loss 1.7390, batch acc 0.8520
12:32:54.790   Training iter 600, batch loss 1.7372, batch acc 0.8610
12:32:54.791 Training @ 253 epoch...
12:32:55.280   Training iter 50, batch loss 1.7415, batch acc 0.8498
12:32:55.789   Training iter 100, batch loss 1.7335, batch acc 0.8656
12:32:56.278   Training iter 150, batch loss 1.7307, batch acc 0.8700
12:32:56.767   Training iter 200, batch loss 1.7359, batch acc 0.8562
12:32:57.252   Training iter 250, batch loss 1.7374, batch acc 0.8598
12:32:57.737   Training iter 300, batch loss 1.7313, batch acc 0.8690
12:32:58.222   Training iter 350, batch loss 1.7370, batch acc 0.8592
12:32:58.696   Training iter 400, batch loss 1.7402, batch acc 0.8510
12:32:59.171   Training iter 450, batch loss 1.7402, batch acc 0.8506
12:32:59.636   Training iter 500, batch loss 1.7348, batch acc 0.8574
12:33:00.118   Training iter 550, batch loss 1.7369, batch acc 0.8590
12:33:00.601   Training iter 600, batch loss 1.7377, batch acc 0.8522
12:33:00.602 Training @ 254 epoch...
12:33:01.105   Training iter 50, batch loss 1.7388, batch acc 0.8544
12:33:01.634   Training iter 100, batch loss 1.7384, batch acc 0.8596
12:33:02.169   Training iter 150, batch loss 1.7348, batch acc 0.8622
12:33:02.671   Training iter 200, batch loss 1.7371, batch acc 0.8570
12:33:03.181   Training iter 250, batch loss 1.7348, batch acc 0.8540
12:33:03.684   Training iter 300, batch loss 1.7354, batch acc 0.8586
12:33:04.198   Training iter 350, batch loss 1.7338, batch acc 0.8564
12:33:04.719   Training iter 400, batch loss 1.7397, batch acc 0.8572
12:33:05.237   Training iter 450, batch loss 1.7356, batch acc 0.8610
12:33:05.743   Training iter 500, batch loss 1.7365, batch acc 0.8618
12:33:06.229   Training iter 550, batch loss 1.7313, batch acc 0.8652
12:33:06.723   Training iter 600, batch loss 1.7402, batch acc 0.8518
12:33:06.725 Training @ 255 epoch...
12:33:07.231   Training iter 50, batch loss 1.7418, batch acc 0.8476
12:33:07.747   Training iter 100, batch loss 1.7309, batch acc 0.8682
12:33:08.254   Training iter 150, batch loss 1.7358, batch acc 0.8598
12:33:08.747   Training iter 200, batch loss 1.7344, batch acc 0.8608
12:33:09.223   Training iter 250, batch loss 1.7394, batch acc 0.8570
12:33:09.702   Training iter 300, batch loss 1.7366, batch acc 0.8602
12:33:10.192   Training iter 350, batch loss 1.7372, batch acc 0.8532
12:33:10.668   Training iter 400, batch loss 1.7334, batch acc 0.8598
12:33:11.145   Training iter 450, batch loss 1.7368, batch acc 0.8556
12:33:11.620   Training iter 500, batch loss 1.7366, batch acc 0.8576
12:33:12.097   Training iter 550, batch loss 1.7358, batch acc 0.8628
12:33:12.611   Training iter 600, batch loss 1.7373, batch acc 0.8582
12:33:12.613 Testing @ 255 epoch...
12:33:12.655     Testing, total mean loss 1.72894, total acc 0.86830
12:33:12.655 Training @ 256 epoch...
12:33:13.181   Training iter 50, batch loss 1.7377, batch acc 0.8554
12:33:13.681   Training iter 100, batch loss 1.7356, batch acc 0.8594
12:33:14.190   Training iter 150, batch loss 1.7409, batch acc 0.8538
12:33:14.704   Training iter 200, batch loss 1.7350, batch acc 0.8632
12:33:15.220   Training iter 250, batch loss 1.7391, batch acc 0.8578
12:33:15.730   Training iter 300, batch loss 1.7382, batch acc 0.8606
12:33:16.239   Training iter 350, batch loss 1.7403, batch acc 0.8512
12:33:16.758   Training iter 400, batch loss 1.7324, batch acc 0.8608
12:33:17.264   Training iter 450, batch loss 1.7365, batch acc 0.8576
12:33:17.766   Training iter 500, batch loss 1.7296, batch acc 0.8640
12:33:18.275   Training iter 550, batch loss 1.7359, batch acc 0.8590
12:33:18.778   Training iter 600, batch loss 1.7343, batch acc 0.8570
12:33:18.780 Training @ 257 epoch...
12:33:19.282   Training iter 50, batch loss 1.7370, batch acc 0.8568
12:33:19.785   Training iter 100, batch loss 1.7347, batch acc 0.8638
12:33:20.282   Training iter 150, batch loss 1.7362, batch acc 0.8574
12:33:20.787   Training iter 200, batch loss 1.7413, batch acc 0.8494
12:33:21.279   Training iter 250, batch loss 1.7367, batch acc 0.8506
12:33:21.773   Training iter 300, batch loss 1.7317, batch acc 0.8626
12:33:22.271   Training iter 350, batch loss 1.7375, batch acc 0.8560
12:33:22.775   Training iter 400, batch loss 1.7314, batch acc 0.8640
12:33:23.279   Training iter 450, batch loss 1.7342, batch acc 0.8612
12:33:23.781   Training iter 500, batch loss 1.7409, batch acc 0.8570
12:33:24.285   Training iter 550, batch loss 1.7384, batch acc 0.8630
12:33:24.811   Training iter 600, batch loss 1.7349, batch acc 0.8592
12:33:24.813 Training @ 258 epoch...
12:33:25.359   Training iter 50, batch loss 1.7381, batch acc 0.8544
12:33:25.908   Training iter 100, batch loss 1.7339, batch acc 0.8676
12:33:26.420   Training iter 150, batch loss 1.7358, batch acc 0.8584
12:33:26.936   Training iter 200, batch loss 1.7359, batch acc 0.8612
12:33:27.460   Training iter 250, batch loss 1.7330, batch acc 0.8568
12:33:28.005   Training iter 300, batch loss 1.7365, batch acc 0.8560
12:33:28.576   Training iter 350, batch loss 1.7364, batch acc 0.8544
12:33:29.129   Training iter 400, batch loss 1.7383, batch acc 0.8590
12:33:29.665   Training iter 450, batch loss 1.7355, batch acc 0.8638
12:33:30.135   Training iter 500, batch loss 1.7384, batch acc 0.8586
12:33:30.620   Training iter 550, batch loss 1.7348, batch acc 0.8570
12:33:31.103   Training iter 600, batch loss 1.7377, batch acc 0.8536
12:33:31.105 Training @ 259 epoch...
12:33:31.605   Training iter 50, batch loss 1.7338, batch acc 0.8626
12:33:32.102   Training iter 100, batch loss 1.7362, batch acc 0.8624
12:33:32.613   Training iter 150, batch loss 1.7370, batch acc 0.8566
12:33:33.137   Training iter 200, batch loss 1.7354, batch acc 0.8530
12:33:33.676   Training iter 250, batch loss 1.7298, batch acc 0.8694
12:33:34.204   Training iter 300, batch loss 1.7363, batch acc 0.8560
12:33:34.719   Training iter 350, batch loss 1.7416, batch acc 0.8478
12:33:35.225   Training iter 400, batch loss 1.7394, batch acc 0.8594
12:33:35.722   Training iter 450, batch loss 1.7365, batch acc 0.8602
12:33:36.267   Training iter 500, batch loss 1.7357, batch acc 0.8638
12:33:36.850   Training iter 550, batch loss 1.7360, batch acc 0.8548
12:33:37.354   Training iter 600, batch loss 1.7362, batch acc 0.8560
12:33:37.356 Training @ 260 epoch...
12:33:37.852   Training iter 50, batch loss 1.7317, batch acc 0.8654
12:33:38.349   Training iter 100, batch loss 1.7426, batch acc 0.8504
12:33:38.868   Training iter 150, batch loss 1.7379, batch acc 0.8590
12:33:39.406   Training iter 200, batch loss 1.7318, batch acc 0.8594
12:33:39.928   Training iter 250, batch loss 1.7336, batch acc 0.8632
12:33:40.479   Training iter 300, batch loss 1.7351, batch acc 0.8650
12:33:41.009   Training iter 350, batch loss 1.7354, batch acc 0.8580
12:33:41.512   Training iter 400, batch loss 1.7374, batch acc 0.8526
12:33:42.009   Training iter 450, batch loss 1.7362, batch acc 0.8570
12:33:42.518   Training iter 500, batch loss 1.7380, batch acc 0.8578
12:33:43.041   Training iter 550, batch loss 1.7387, batch acc 0.8550
12:33:43.549   Training iter 600, batch loss 1.7350, batch acc 0.8578
12:33:43.550 Testing @ 260 epoch...
12:33:43.593     Testing, total mean loss 1.72872, total acc 0.86860
12:33:43.593 Training @ 261 epoch...
12:33:44.110   Training iter 50, batch loss 1.7348, batch acc 0.8634
12:33:44.628   Training iter 100, batch loss 1.7382, batch acc 0.8570
12:33:45.143   Training iter 150, batch loss 1.7350, batch acc 0.8610
12:33:45.651   Training iter 200, batch loss 1.7339, batch acc 0.8602
12:33:46.160   Training iter 250, batch loss 1.7352, batch acc 0.8570
12:33:46.683   Training iter 300, batch loss 1.7378, batch acc 0.8596
12:33:47.204   Training iter 350, batch loss 1.7361, batch acc 0.8582
12:33:47.716   Training iter 400, batch loss 1.7357, batch acc 0.8584
12:33:48.247   Training iter 450, batch loss 1.7372, batch acc 0.8574
12:33:48.777   Training iter 500, batch loss 1.7366, batch acc 0.8580
12:33:49.306   Training iter 550, batch loss 1.7335, batch acc 0.8618
12:33:49.797   Training iter 600, batch loss 1.7391, batch acc 0.8504
12:33:49.799 Training @ 262 epoch...
12:33:50.306   Training iter 50, batch loss 1.7342, batch acc 0.8614
12:33:50.781   Training iter 100, batch loss 1.7344, batch acc 0.8590
12:33:51.279   Training iter 150, batch loss 1.7368, batch acc 0.8572
12:33:51.765   Training iter 200, batch loss 1.7366, batch acc 0.8556
12:33:52.256   Training iter 250, batch loss 1.7375, batch acc 0.8582
12:33:52.737   Training iter 300, batch loss 1.7414, batch acc 0.8534
12:33:53.231   Training iter 350, batch loss 1.7363, batch acc 0.8592
12:33:53.738   Training iter 400, batch loss 1.7342, batch acc 0.8606
12:33:54.242   Training iter 450, batch loss 1.7372, batch acc 0.8566
12:33:54.734   Training iter 500, batch loss 1.7384, batch acc 0.8556
12:33:55.275   Training iter 550, batch loss 1.7332, batch acc 0.8652
12:33:55.825   Training iter 600, batch loss 1.7322, batch acc 0.8608
12:33:55.827 Training @ 263 epoch...
12:33:56.372   Training iter 50, batch loss 1.7362, batch acc 0.8590
12:33:56.895   Training iter 100, batch loss 1.7386, batch acc 0.8586
12:33:57.430   Training iter 150, batch loss 1.7308, batch acc 0.8622
12:33:57.952   Training iter 200, batch loss 1.7359, batch acc 0.8604
12:33:58.451   Training iter 250, batch loss 1.7401, batch acc 0.8504
12:33:58.943   Training iter 300, batch loss 1.7328, batch acc 0.8612
12:33:59.410   Training iter 350, batch loss 1.7350, batch acc 0.8620
12:33:59.886   Training iter 400, batch loss 1.7366, batch acc 0.8614
12:34:00.395   Training iter 450, batch loss 1.7377, batch acc 0.8572
12:34:00.881   Training iter 500, batch loss 1.7323, batch acc 0.8602
12:34:01.389   Training iter 550, batch loss 1.7385, batch acc 0.8536
12:34:01.932   Training iter 600, batch loss 1.7376, batch acc 0.8566
12:34:01.934 Training @ 264 epoch...
12:34:02.496   Training iter 50, batch loss 1.7362, batch acc 0.8560
12:34:03.030   Training iter 100, batch loss 1.7318, batch acc 0.8680
12:34:03.560   Training iter 150, batch loss 1.7396, batch acc 0.8536
12:34:04.120   Training iter 200, batch loss 1.7351, batch acc 0.8600
12:34:04.668   Training iter 250, batch loss 1.7358, batch acc 0.8568
12:34:05.211   Training iter 300, batch loss 1.7354, batch acc 0.8590
12:34:05.740   Training iter 350, batch loss 1.7364, batch acc 0.8548
12:34:06.267   Training iter 400, batch loss 1.7369, batch acc 0.8572
12:34:06.814   Training iter 450, batch loss 1.7379, batch acc 0.8486
12:34:07.387   Training iter 500, batch loss 1.7317, batch acc 0.8680
12:34:07.955   Training iter 550, batch loss 1.7400, batch acc 0.8556
12:34:08.488   Training iter 600, batch loss 1.7346, batch acc 0.8646
12:34:08.489 Training @ 265 epoch...
12:34:09.020   Training iter 50, batch loss 1.7320, batch acc 0.8660
12:34:09.494   Training iter 100, batch loss 1.7354, batch acc 0.8594
12:34:09.983   Training iter 150, batch loss 1.7393, batch acc 0.8550
12:34:10.497   Training iter 200, batch loss 1.7358, batch acc 0.8572
12:34:10.976   Training iter 250, batch loss 1.7341, batch acc 0.8592
12:34:11.438   Training iter 300, batch loss 1.7359, batch acc 0.8610
12:34:11.897   Training iter 350, batch loss 1.7417, batch acc 0.8496
12:34:12.400   Training iter 400, batch loss 1.7345, batch acc 0.8580
12:34:12.930   Training iter 450, batch loss 1.7354, batch acc 0.8632
12:34:13.453   Training iter 500, batch loss 1.7380, batch acc 0.8584
12:34:13.968   Training iter 550, batch loss 1.7334, batch acc 0.8588
12:34:14.487   Training iter 600, batch loss 1.7354, batch acc 0.8580
12:34:14.489 Testing @ 265 epoch...
12:34:14.531     Testing, total mean loss 1.72853, total acc 0.86870
12:34:14.531 Training @ 266 epoch...
12:34:15.053   Training iter 50, batch loss 1.7372, batch acc 0.8574
12:34:15.562   Training iter 100, batch loss 1.7341, batch acc 0.8598
12:34:16.071   Training iter 150, batch loss 1.7389, batch acc 0.8516
12:34:16.570   Training iter 200, batch loss 1.7341, batch acc 0.8580
12:34:17.081   Training iter 250, batch loss 1.7344, batch acc 0.8574
12:34:17.583   Training iter 300, batch loss 1.7324, batch acc 0.8658
12:34:18.116   Training iter 350, batch loss 1.7355, batch acc 0.8568
12:34:18.659   Training iter 400, batch loss 1.7405, batch acc 0.8578
12:34:19.198   Training iter 450, batch loss 1.7370, batch acc 0.8620
12:34:19.734   Training iter 500, batch loss 1.7389, batch acc 0.8520
12:34:20.281   Training iter 550, batch loss 1.7325, batch acc 0.8626
12:34:20.818   Training iter 600, batch loss 1.7351, batch acc 0.8630
12:34:20.820 Training @ 267 epoch...
12:34:21.363   Training iter 50, batch loss 1.7361, batch acc 0.8548
12:34:21.904   Training iter 100, batch loss 1.7356, batch acc 0.8562
12:34:22.449   Training iter 150, batch loss 1.7338, batch acc 0.8640
12:34:22.990   Training iter 200, batch loss 1.7362, batch acc 0.8592
12:34:23.538   Training iter 250, batch loss 1.7380, batch acc 0.8560
12:34:24.075   Training iter 300, batch loss 1.7395, batch acc 0.8504
12:34:24.590   Training iter 350, batch loss 1.7314, batch acc 0.8686
12:34:25.094   Training iter 400, batch loss 1.7372, batch acc 0.8570
12:34:25.595   Training iter 450, batch loss 1.7301, batch acc 0.8680
12:34:26.097   Training iter 500, batch loss 1.7365, batch acc 0.8612
12:34:26.598   Training iter 550, batch loss 1.7395, batch acc 0.8564
12:34:27.105   Training iter 600, batch loss 1.7361, batch acc 0.8538
12:34:27.106 Training @ 268 epoch...
12:34:27.622   Training iter 50, batch loss 1.7339, batch acc 0.8612
12:34:28.138   Training iter 100, batch loss 1.7362, batch acc 0.8584
12:34:28.662   Training iter 150, batch loss 1.7386, batch acc 0.8540
12:34:29.193   Training iter 200, batch loss 1.7321, batch acc 0.8610
12:34:29.698   Training iter 250, batch loss 1.7359, batch acc 0.8568
12:34:30.191   Training iter 300, batch loss 1.7354, batch acc 0.8534
12:34:30.707   Training iter 350, batch loss 1.7373, batch acc 0.8612
12:34:31.177   Training iter 400, batch loss 1.7329, batch acc 0.8636
12:34:31.646   Training iter 450, batch loss 1.7371, batch acc 0.8564
12:34:32.111   Training iter 500, batch loss 1.7355, batch acc 0.8590
12:34:32.594   Training iter 550, batch loss 1.7378, batch acc 0.8610
12:34:33.072   Training iter 600, batch loss 1.7371, batch acc 0.8578
12:34:33.074 Training @ 269 epoch...
12:34:33.611   Training iter 50, batch loss 1.7311, batch acc 0.8704
12:34:34.096   Training iter 100, batch loss 1.7380, batch acc 0.8588
12:34:34.594   Training iter 150, batch loss 1.7349, batch acc 0.8628
12:34:35.116   Training iter 200, batch loss 1.7386, batch acc 0.8514
12:34:35.628   Training iter 250, batch loss 1.7366, batch acc 0.8538
12:34:36.132   Training iter 300, batch loss 1.7393, batch acc 0.8548
12:34:36.630   Training iter 350, batch loss 1.7360, batch acc 0.8492
12:34:37.135   Training iter 400, batch loss 1.7374, batch acc 0.8608
12:34:37.636   Training iter 450, batch loss 1.7333, batch acc 0.8604
12:34:38.138   Training iter 500, batch loss 1.7337, batch acc 0.8628
12:34:38.660   Training iter 550, batch loss 1.7393, batch acc 0.8524
12:34:39.181   Training iter 600, batch loss 1.7309, batch acc 0.8674
12:34:39.182 Training @ 270 epoch...
12:34:39.710   Training iter 50, batch loss 1.7348, batch acc 0.8580
12:34:40.221   Training iter 100, batch loss 1.7372, batch acc 0.8606
12:34:40.698   Training iter 150, batch loss 1.7370, batch acc 0.8558
12:34:41.159   Training iter 200, batch loss 1.7345, batch acc 0.8570
12:34:41.622   Training iter 250, batch loss 1.7363, batch acc 0.8572
12:34:42.084   Training iter 300, batch loss 1.7320, batch acc 0.8618
12:34:42.537   Training iter 350, batch loss 1.7355, batch acc 0.8660
12:34:43.008   Training iter 400, batch loss 1.7351, batch acc 0.8578
12:34:43.484   Training iter 450, batch loss 1.7411, batch acc 0.8510
12:34:43.949   Training iter 500, batch loss 1.7320, batch acc 0.8666
12:34:44.423   Training iter 550, batch loss 1.7380, batch acc 0.8528
12:34:44.907   Training iter 600, batch loss 1.7352, batch acc 0.8626
12:34:44.909 Testing @ 270 epoch...
12:34:44.952     Testing, total mean loss 1.72834, total acc 0.86860
12:34:44.952 Training @ 271 epoch...
12:34:45.449   Training iter 50, batch loss 1.7350, batch acc 0.8586
12:34:45.939   Training iter 100, batch loss 1.7381, batch acc 0.8564
12:34:46.446   Training iter 150, batch loss 1.7375, batch acc 0.8548
12:34:46.942   Training iter 200, batch loss 1.7347, batch acc 0.8612
12:34:47.425   Training iter 250, batch loss 1.7369, batch acc 0.8550
12:34:47.895   Training iter 300, batch loss 1.7378, batch acc 0.8584
12:34:48.390   Training iter 350, batch loss 1.7380, batch acc 0.8558
12:34:48.910   Training iter 400, batch loss 1.7328, batch acc 0.8622
12:34:49.422   Training iter 450, batch loss 1.7323, batch acc 0.8614
12:34:49.937   Training iter 500, batch loss 1.7373, batch acc 0.8562
12:34:50.466   Training iter 550, batch loss 1.7328, batch acc 0.8648
12:34:51.003   Training iter 600, batch loss 1.7350, batch acc 0.8610
12:34:51.005 Training @ 272 epoch...
12:34:51.552   Training iter 50, batch loss 1.7395, batch acc 0.8554
12:34:52.088   Training iter 100, batch loss 1.7362, batch acc 0.8564
12:34:52.630   Training iter 150, batch loss 1.7330, batch acc 0.8630
12:34:53.176   Training iter 200, batch loss 1.7368, batch acc 0.8528
12:34:53.695   Training iter 250, batch loss 1.7336, batch acc 0.8628
12:34:54.195   Training iter 300, batch loss 1.7372, batch acc 0.8590
12:34:54.736   Training iter 350, batch loss 1.7347, batch acc 0.8576
12:34:55.280   Training iter 400, batch loss 1.7350, batch acc 0.8588
12:34:55.812   Training iter 450, batch loss 1.7363, batch acc 0.8620
12:34:56.353   Training iter 500, batch loss 1.7380, batch acc 0.8584
12:34:56.941   Training iter 550, batch loss 1.7347, batch acc 0.8610
12:34:57.557   Training iter 600, batch loss 1.7328, batch acc 0.8610
12:34:57.559 Training @ 273 epoch...
12:34:58.178   Training iter 50, batch loss 1.7361, batch acc 0.8584
12:34:58.698   Training iter 100, batch loss 1.7339, batch acc 0.8626
12:34:59.218   Training iter 150, batch loss 1.7391, batch acc 0.8520
12:34:59.730   Training iter 200, batch loss 1.7399, batch acc 0.8534
12:35:00.244   Training iter 250, batch loss 1.7358, batch acc 0.8572
12:35:00.792   Training iter 300, batch loss 1.7327, batch acc 0.8616
12:35:01.344   Training iter 350, batch loss 1.7337, batch acc 0.8592
12:35:01.892   Training iter 400, batch loss 1.7298, batch acc 0.8674
12:35:02.436   Training iter 450, batch loss 1.7424, batch acc 0.8534
12:35:02.984   Training iter 500, batch loss 1.7366, batch acc 0.8586
12:35:03.522   Training iter 550, batch loss 1.7365, batch acc 0.8548
12:35:04.044   Training iter 600, batch loss 1.7310, batch acc 0.8658
12:35:04.046 Training @ 274 epoch...
12:35:04.594   Training iter 50, batch loss 1.7364, batch acc 0.8558
12:35:05.141   Training iter 100, batch loss 1.7356, batch acc 0.8578
12:35:05.690   Training iter 150, batch loss 1.7365, batch acc 0.8562
12:35:06.233   Training iter 200, batch loss 1.7336, batch acc 0.8678
12:35:06.785   Training iter 250, batch loss 1.7338, batch acc 0.8636
12:35:07.348   Training iter 300, batch loss 1.7349, batch acc 0.8582
12:35:07.874   Training iter 350, batch loss 1.7373, batch acc 0.8574
12:35:08.408   Training iter 400, batch loss 1.7359, batch acc 0.8620
12:35:08.931   Training iter 450, batch loss 1.7359, batch acc 0.8550
12:35:09.444   Training iter 500, batch loss 1.7375, batch acc 0.8584
12:35:09.935   Training iter 550, batch loss 1.7315, batch acc 0.8662
12:35:10.407   Training iter 600, batch loss 1.7382, batch acc 0.8496
12:35:10.409 Training @ 275 epoch...
12:35:10.891   Training iter 50, batch loss 1.7376, batch acc 0.8570
12:35:11.397   Training iter 100, batch loss 1.7346, batch acc 0.8600
12:35:11.900   Training iter 150, batch loss 1.7390, batch acc 0.8560
12:35:12.433   Training iter 200, batch loss 1.7393, batch acc 0.8546
12:35:12.981   Training iter 250, batch loss 1.7351, batch acc 0.8626
12:35:13.484   Training iter 300, batch loss 1.7334, batch acc 0.8578
12:35:13.974   Training iter 350, batch loss 1.7343, batch acc 0.8560
12:35:14.469   Training iter 400, batch loss 1.7347, batch acc 0.8678
12:35:14.966   Training iter 450, batch loss 1.7374, batch acc 0.8578
12:35:15.486   Training iter 500, batch loss 1.7352, batch acc 0.8528
12:35:15.992   Training iter 550, batch loss 1.7257, batch acc 0.8718
12:35:16.494   Training iter 600, batch loss 1.7403, batch acc 0.8524
12:35:16.496 Testing @ 275 epoch...
12:35:16.542     Testing, total mean loss 1.72816, total acc 0.86880
12:35:16.542 Training @ 276 epoch...
12:35:17.073   Training iter 50, batch loss 1.7356, batch acc 0.8590
12:35:17.578   Training iter 100, batch loss 1.7352, batch acc 0.8692
12:35:18.073   Training iter 150, batch loss 1.7383, batch acc 0.8512
12:35:18.572   Training iter 200, batch loss 1.7345, batch acc 0.8574
12:35:19.068   Training iter 250, batch loss 1.7353, batch acc 0.8604
12:35:19.585   Training iter 300, batch loss 1.7332, batch acc 0.8632
12:35:20.078   Training iter 350, batch loss 1.7361, batch acc 0.8596
12:35:20.584   Training iter 400, batch loss 1.7335, batch acc 0.8588
12:35:21.063   Training iter 450, batch loss 1.7351, batch acc 0.8584
12:35:21.542   Training iter 500, batch loss 1.7374, batch acc 0.8574
12:35:22.021   Training iter 550, batch loss 1.7364, batch acc 0.8566
12:35:22.514   Training iter 600, batch loss 1.7355, batch acc 0.8570
12:35:22.515 Training @ 277 epoch...
12:35:23.048   Training iter 50, batch loss 1.7362, batch acc 0.8562
12:35:23.576   Training iter 100, batch loss 1.7357, batch acc 0.8604
12:35:24.100   Training iter 150, batch loss 1.7325, batch acc 0.8624
12:35:24.633   Training iter 200, batch loss 1.7339, batch acc 0.8584
12:35:25.174   Training iter 250, batch loss 1.7369, batch acc 0.8562
12:35:25.716   Training iter 300, batch loss 1.7353, batch acc 0.8564
12:35:26.261   Training iter 350, batch loss 1.7367, batch acc 0.8570
12:35:26.813   Training iter 400, batch loss 1.7362, batch acc 0.8620
12:35:27.355   Training iter 450, batch loss 1.7378, batch acc 0.8550
12:35:27.901   Training iter 500, batch loss 1.7361, batch acc 0.8570
12:35:28.437   Training iter 550, batch loss 1.7329, batch acc 0.8678
12:35:28.942   Training iter 600, batch loss 1.7356, batch acc 0.8580
12:35:28.943 Training @ 278 epoch...
12:35:29.464   Training iter 50, batch loss 1.7328, batch acc 0.8658
12:35:29.938   Training iter 100, batch loss 1.7368, batch acc 0.8566
12:35:30.410   Training iter 150, batch loss 1.7383, batch acc 0.8516
12:35:30.888   Training iter 200, batch loss 1.7354, batch acc 0.8594
12:35:31.368   Training iter 250, batch loss 1.7375, batch acc 0.8542
12:35:31.847   Training iter 300, batch loss 1.7388, batch acc 0.8576
12:35:32.325   Training iter 350, batch loss 1.7335, batch acc 0.8580
12:35:32.793   Training iter 400, batch loss 1.7331, batch acc 0.8644
12:35:33.305   Training iter 450, batch loss 1.7369, batch acc 0.8538
12:35:33.788   Training iter 500, batch loss 1.7333, batch acc 0.8626
12:35:34.258   Training iter 550, batch loss 1.7357, batch acc 0.8578
12:35:34.757   Training iter 600, batch loss 1.7335, batch acc 0.8638
12:35:34.758 Training @ 279 epoch...
12:35:35.272   Training iter 50, batch loss 1.7361, batch acc 0.8608
12:35:35.753   Training iter 100, batch loss 1.7341, batch acc 0.8646
12:35:36.227   Training iter 150, batch loss 1.7393, batch acc 0.8520
12:35:36.723   Training iter 200, batch loss 1.7359, batch acc 0.8526
12:35:37.249   Training iter 250, batch loss 1.7369, batch acc 0.8602
12:35:37.754   Training iter 300, batch loss 1.7339, batch acc 0.8592
12:35:38.273   Training iter 350, batch loss 1.7336, batch acc 0.8582
12:35:38.790   Training iter 400, batch loss 1.7347, batch acc 0.8602
12:35:39.345   Training iter 450, batch loss 1.7301, batch acc 0.8690
12:35:39.864   Training iter 500, batch loss 1.7376, batch acc 0.8564
12:35:40.373   Training iter 550, batch loss 1.7360, batch acc 0.8516
12:35:40.872   Training iter 600, batch loss 1.7368, batch acc 0.8640
12:35:40.874 Training @ 280 epoch...
12:35:41.376   Training iter 50, batch loss 1.7337, batch acc 0.8586
12:35:41.896   Training iter 100, batch loss 1.7317, batch acc 0.8666
12:35:42.415   Training iter 150, batch loss 1.7348, batch acc 0.8644
12:35:42.925   Training iter 200, batch loss 1.7350, batch acc 0.8548
12:35:43.444   Training iter 250, batch loss 1.7378, batch acc 0.8612
12:35:43.944   Training iter 300, batch loss 1.7377, batch acc 0.8542
12:35:44.436   Training iter 350, batch loss 1.7329, batch acc 0.8590
12:35:44.942   Training iter 400, batch loss 1.7335, batch acc 0.8620
12:35:45.426   Training iter 450, batch loss 1.7339, batch acc 0.8606
12:35:45.900   Training iter 500, batch loss 1.7398, batch acc 0.8552
12:35:46.376   Training iter 550, batch loss 1.7386, batch acc 0.8532
12:35:46.856   Training iter 600, batch loss 1.7352, batch acc 0.8598
12:35:46.857 Testing @ 280 epoch...
12:35:46.901     Testing, total mean loss 1.72799, total acc 0.86870
12:35:46.901 Training @ 281 epoch...
12:35:47.403   Training iter 50, batch loss 1.7383, batch acc 0.8584
12:35:47.898   Training iter 100, batch loss 1.7344, batch acc 0.8550
12:35:48.392   Training iter 150, batch loss 1.7297, batch acc 0.8712
12:35:48.881   Training iter 200, batch loss 1.7391, batch acc 0.8548
12:35:49.332   Training iter 250, batch loss 1.7354, batch acc 0.8524
12:35:49.779   Training iter 300, batch loss 1.7359, batch acc 0.8592
12:35:50.220   Training iter 350, batch loss 1.7339, batch acc 0.8636
12:35:50.661   Training iter 400, batch loss 1.7369, batch acc 0.8548
12:35:51.120   Training iter 450, batch loss 1.7341, batch acc 0.8610
12:35:51.573   Training iter 500, batch loss 1.7355, batch acc 0.8566
12:35:52.043   Training iter 550, batch loss 1.7371, batch acc 0.8622
12:35:52.523   Training iter 600, batch loss 1.7339, batch acc 0.8596
12:35:52.525 Training @ 282 epoch...
12:35:53.044   Training iter 50, batch loss 1.7355, batch acc 0.8606
12:35:53.544   Training iter 100, batch loss 1.7357, batch acc 0.8648
12:35:54.039   Training iter 150, batch loss 1.7312, batch acc 0.8624
12:35:54.581   Training iter 200, batch loss 1.7393, batch acc 0.8528
12:35:55.159   Training iter 250, batch loss 1.7391, batch acc 0.8504
12:35:55.729   Training iter 300, batch loss 1.7328, batch acc 0.8666
12:35:56.272   Training iter 350, batch loss 1.7367, batch acc 0.8572
12:35:56.754   Training iter 400, batch loss 1.7336, batch acc 0.8584
12:35:57.259   Training iter 450, batch loss 1.7361, batch acc 0.8576
12:35:57.770   Training iter 500, batch loss 1.7313, batch acc 0.8666
12:35:58.252   Training iter 550, batch loss 1.7361, batch acc 0.8550
12:35:58.731   Training iter 600, batch loss 1.7364, batch acc 0.8578
12:35:58.733 Training @ 283 epoch...
12:35:59.222   Training iter 50, batch loss 1.7358, batch acc 0.8616
12:35:59.688   Training iter 100, batch loss 1.7320, batch acc 0.8660
12:36:00.158   Training iter 150, batch loss 1.7347, batch acc 0.8570
12:36:00.648   Training iter 200, batch loss 1.7352, batch acc 0.8600
12:36:01.126   Training iter 250, batch loss 1.7347, batch acc 0.8626
12:36:01.644   Training iter 300, batch loss 1.7384, batch acc 0.8524
12:36:02.195   Training iter 350, batch loss 1.7391, batch acc 0.8502
12:36:02.751   Training iter 400, batch loss 1.7353, batch acc 0.8566
12:36:03.297   Training iter 450, batch loss 1.7349, batch acc 0.8622
12:36:03.828   Training iter 500, batch loss 1.7318, batch acc 0.8648
12:36:04.368   Training iter 550, batch loss 1.7354, batch acc 0.8580
12:36:04.911   Training iter 600, batch loss 1.7360, batch acc 0.8576
12:36:04.913 Training @ 284 epoch...
12:36:05.461   Training iter 50, batch loss 1.7384, batch acc 0.8518
12:36:05.959   Training iter 100, batch loss 1.7371, batch acc 0.8556
12:36:06.434   Training iter 150, batch loss 1.7294, batch acc 0.8678
12:36:06.961   Training iter 200, batch loss 1.7379, batch acc 0.8562
12:36:07.524   Training iter 250, batch loss 1.7364, batch acc 0.8604
12:36:08.044   Training iter 300, batch loss 1.7365, batch acc 0.8542
12:36:08.545   Training iter 350, batch loss 1.7319, batch acc 0.8592
12:36:09.035   Training iter 400, batch loss 1.7314, batch acc 0.8712
12:36:09.528   Training iter 450, batch loss 1.7348, batch acc 0.8570
12:36:10.036   Training iter 500, batch loss 1.7360, batch acc 0.8634
12:36:10.577   Training iter 550, batch loss 1.7354, batch acc 0.8590
12:36:11.106   Training iter 600, batch loss 1.7377, batch acc 0.8526
12:36:11.107 Training @ 285 epoch...
12:36:11.632   Training iter 50, batch loss 1.7348, batch acc 0.8638
12:36:12.169   Training iter 100, batch loss 1.7349, batch acc 0.8602
12:36:12.708   Training iter 150, batch loss 1.7360, batch acc 0.8550
12:36:13.231   Training iter 200, batch loss 1.7338, batch acc 0.8608
12:36:13.746   Training iter 250, batch loss 1.7402, batch acc 0.8512
12:36:14.267   Training iter 300, batch loss 1.7350, batch acc 0.8548
12:36:14.792   Training iter 350, batch loss 1.7378, batch acc 0.8542
12:36:15.323   Training iter 400, batch loss 1.7367, batch acc 0.8594
12:36:15.845   Training iter 450, batch loss 1.7337, batch acc 0.8624
12:36:16.350   Training iter 500, batch loss 1.7335, batch acc 0.8620
12:36:16.846   Training iter 550, batch loss 1.7326, batch acc 0.8666
12:36:17.344   Training iter 600, batch loss 1.7336, batch acc 0.8618
12:36:17.345 Testing @ 285 epoch...
12:36:17.388     Testing, total mean loss 1.72783, total acc 0.86920
12:36:17.388 Training @ 286 epoch...
12:36:17.891   Training iter 50, batch loss 1.7348, batch acc 0.8550
12:36:18.406   Training iter 100, batch loss 1.7414, batch acc 0.8502
12:36:18.930   Training iter 150, batch loss 1.7346, batch acc 0.8606
12:36:19.437   Training iter 200, batch loss 1.7398, batch acc 0.8554
12:36:19.947   Training iter 250, batch loss 1.7342, batch acc 0.8594
12:36:20.465   Training iter 300, batch loss 1.7349, batch acc 0.8568
12:36:20.969   Training iter 350, batch loss 1.7350, batch acc 0.8602
12:36:21.468   Training iter 400, batch loss 1.7330, batch acc 0.8582
12:36:21.965   Training iter 450, batch loss 1.7332, batch acc 0.8622
12:36:22.461   Training iter 500, batch loss 1.7315, batch acc 0.8660
12:36:22.953   Training iter 550, batch loss 1.7338, batch acc 0.8664
12:36:23.471   Training iter 600, batch loss 1.7363, batch acc 0.8576
12:36:23.473 Training @ 287 epoch...
12:36:23.976   Training iter 50, batch loss 1.7370, batch acc 0.8564
12:36:24.469   Training iter 100, batch loss 1.7330, batch acc 0.8590
12:36:24.963   Training iter 150, batch loss 1.7368, batch acc 0.8574
12:36:25.456   Training iter 200, batch loss 1.7351, batch acc 0.8634
12:36:25.960   Training iter 250, batch loss 1.7357, batch acc 0.8616
12:36:26.496   Training iter 300, batch loss 1.7343, batch acc 0.8632
12:36:27.054   Training iter 350, batch loss 1.7330, batch acc 0.8600
12:36:27.603   Training iter 400, batch loss 1.7362, batch acc 0.8572
12:36:28.145   Training iter 450, batch loss 1.7379, batch acc 0.8520
12:36:28.675   Training iter 500, batch loss 1.7345, batch acc 0.8598
12:36:29.238   Training iter 550, batch loss 1.7312, batch acc 0.8650
12:36:29.792   Training iter 600, batch loss 1.7373, batch acc 0.8552
12:36:29.794 Training @ 288 epoch...
12:36:30.349   Training iter 50, batch loss 1.7372, batch acc 0.8538
12:36:30.907   Training iter 100, batch loss 1.7333, batch acc 0.8588
12:36:31.457   Training iter 150, batch loss 1.7314, batch acc 0.8656
12:36:32.010   Training iter 200, batch loss 1.7323, batch acc 0.8624
12:36:32.560   Training iter 250, batch loss 1.7321, batch acc 0.8640
12:36:33.250   Training iter 300, batch loss 1.7333, batch acc 0.8624
12:36:33.772   Training iter 350, batch loss 1.7350, batch acc 0.8600
12:36:34.295   Training iter 400, batch loss 1.7342, batch acc 0.8576
12:36:34.821   Training iter 450, batch loss 1.7387, batch acc 0.8536
12:36:35.340   Training iter 500, batch loss 1.7432, batch acc 0.8498
12:36:35.858   Training iter 550, batch loss 1.7363, batch acc 0.8596
12:36:36.373   Training iter 600, batch loss 1.7345, batch acc 0.8620
12:36:36.375 Training @ 289 epoch...
12:36:36.901   Training iter 50, batch loss 1.7363, batch acc 0.8574
12:36:37.411   Training iter 100, batch loss 1.7357, batch acc 0.8566
12:36:37.933   Training iter 150, batch loss 1.7322, batch acc 0.8616
12:36:38.456   Training iter 200, batch loss 1.7369, batch acc 0.8622
12:36:38.984   Training iter 250, batch loss 1.7336, batch acc 0.8632
12:36:39.509   Training iter 300, batch loss 1.7370, batch acc 0.8502
12:36:40.047   Training iter 350, batch loss 1.7395, batch acc 0.8506
12:36:40.574   Training iter 400, batch loss 1.7354, batch acc 0.8580
12:36:41.086   Training iter 450, batch loss 1.7301, batch acc 0.8678
12:36:41.601   Training iter 500, batch loss 1.7341, batch acc 0.8648
12:36:42.120   Training iter 550, batch loss 1.7349, batch acc 0.8610
12:36:42.661   Training iter 600, batch loss 1.7355, batch acc 0.8588
12:36:42.663 Training @ 290 epoch...
12:36:43.205   Training iter 50, batch loss 1.7338, batch acc 0.8554
12:36:43.740   Training iter 100, batch loss 1.7371, batch acc 0.8546
12:36:44.280   Training iter 150, batch loss 1.7338, batch acc 0.8584
12:36:44.832   Training iter 200, batch loss 1.7357, batch acc 0.8608
12:36:45.367   Training iter 250, batch loss 1.7333, batch acc 0.8630
12:36:45.907   Training iter 300, batch loss 1.7308, batch acc 0.8680
12:36:46.464   Training iter 350, batch loss 1.7350, batch acc 0.8606
12:36:47.011   Training iter 400, batch loss 1.7360, batch acc 0.8554
12:36:47.545   Training iter 450, batch loss 1.7394, batch acc 0.8596
12:36:48.071   Training iter 500, batch loss 1.7363, batch acc 0.8596
12:36:48.621   Training iter 550, batch loss 1.7341, batch acc 0.8552
12:36:49.164   Training iter 600, batch loss 1.7356, batch acc 0.8608
12:36:49.165 Testing @ 290 epoch...
12:36:49.209     Testing, total mean loss 1.72769, total acc 0.86900
12:36:49.209 Training @ 291 epoch...
12:36:49.756   Training iter 50, batch loss 1.7383, batch acc 0.8582
12:36:50.294   Training iter 100, batch loss 1.7387, batch acc 0.8508
12:36:50.821   Training iter 150, batch loss 1.7347, batch acc 0.8564
12:36:51.355   Training iter 200, batch loss 1.7331, batch acc 0.8604
12:36:51.877   Training iter 250, batch loss 1.7302, batch acc 0.8678
12:36:52.385   Training iter 300, batch loss 1.7313, batch acc 0.8590
12:36:52.898   Training iter 350, batch loss 1.7364, batch acc 0.8560
12:36:53.408   Training iter 400, batch loss 1.7359, batch acc 0.8528
12:36:53.914   Training iter 450, batch loss 1.7361, batch acc 0.8624
12:36:54.447   Training iter 500, batch loss 1.7347, batch acc 0.8644
12:36:54.989   Training iter 550, batch loss 1.7377, batch acc 0.8596
12:36:55.495   Training iter 600, batch loss 1.7333, batch acc 0.8630
12:36:55.497 Training @ 292 epoch...
12:36:56.027   Training iter 50, batch loss 1.7333, batch acc 0.8622
12:36:56.548   Training iter 100, batch loss 1.7356, batch acc 0.8502
12:36:57.065   Training iter 150, batch loss 1.7340, batch acc 0.8566
12:36:57.582   Training iter 200, batch loss 1.7377, batch acc 0.8566
12:36:58.106   Training iter 250, batch loss 1.7388, batch acc 0.8564
12:36:58.617   Training iter 300, batch loss 1.7319, batch acc 0.8640
12:36:59.142   Training iter 350, batch loss 1.7334, batch acc 0.8652
12:36:59.624   Training iter 400, batch loss 1.7338, batch acc 0.8638
12:37:00.114   Training iter 450, batch loss 1.7358, batch acc 0.8548
12:37:00.588   Training iter 500, batch loss 1.7346, batch acc 0.8606
12:37:01.067   Training iter 550, batch loss 1.7357, batch acc 0.8616
12:37:01.576   Training iter 600, batch loss 1.7356, batch acc 0.8572
12:37:01.578 Training @ 293 epoch...
12:37:02.136   Training iter 50, batch loss 1.7367, batch acc 0.8588
12:37:02.678   Training iter 100, batch loss 1.7362, batch acc 0.8608
12:37:03.184   Training iter 150, batch loss 1.7331, batch acc 0.8606
12:37:03.677   Training iter 200, batch loss 1.7378, batch acc 0.8564
12:37:04.168   Training iter 250, batch loss 1.7315, batch acc 0.8630
12:37:04.664   Training iter 300, batch loss 1.7385, batch acc 0.8548
12:37:05.167   Training iter 350, batch loss 1.7330, batch acc 0.8606
12:37:05.699   Training iter 400, batch loss 1.7343, batch acc 0.8606
12:37:06.190   Training iter 450, batch loss 1.7355, batch acc 0.8590
12:37:06.701   Training iter 500, batch loss 1.7349, batch acc 0.8608
12:37:07.245   Training iter 550, batch loss 1.7302, batch acc 0.8628
12:37:07.782   Training iter 600, batch loss 1.7383, batch acc 0.8564
12:37:07.784 Training @ 294 epoch...
12:37:08.304   Training iter 50, batch loss 1.7349, batch acc 0.8578
12:37:08.808   Training iter 100, batch loss 1.7349, batch acc 0.8614
12:37:09.311   Training iter 150, batch loss 1.7332, batch acc 0.8606
12:37:09.806   Training iter 200, batch loss 1.7338, batch acc 0.8592
12:37:10.298   Training iter 250, batch loss 1.7419, batch acc 0.8474
12:37:10.790   Training iter 300, batch loss 1.7355, batch acc 0.8596
12:37:11.254   Training iter 350, batch loss 1.7336, batch acc 0.8624
12:37:11.699   Training iter 400, batch loss 1.7371, batch acc 0.8600
12:37:12.155   Training iter 450, batch loss 1.7312, batch acc 0.8662
12:37:12.588   Training iter 500, batch loss 1.7386, batch acc 0.8568
12:37:13.048   Training iter 550, batch loss 1.7331, batch acc 0.8630
12:37:13.488   Training iter 600, batch loss 1.7317, batch acc 0.8594
12:37:13.489 Training @ 295 epoch...
12:37:13.935   Training iter 50, batch loss 1.7327, batch acc 0.8642
12:37:14.418   Training iter 100, batch loss 1.7341, batch acc 0.8566
12:37:14.896   Training iter 150, batch loss 1.7396, batch acc 0.8546
12:37:15.370   Training iter 200, batch loss 1.7336, batch acc 0.8656
12:37:15.847   Training iter 250, batch loss 1.7364, batch acc 0.8552
12:37:16.349   Training iter 300, batch loss 1.7328, batch acc 0.8604
12:37:16.871   Training iter 350, batch loss 1.7371, batch acc 0.8582
12:37:17.382   Training iter 400, batch loss 1.7375, batch acc 0.8560
12:37:17.893   Training iter 450, batch loss 1.7318, batch acc 0.8644
12:37:18.417   Training iter 500, batch loss 1.7369, batch acc 0.8560
12:37:18.942   Training iter 550, batch loss 1.7326, batch acc 0.8606
12:37:19.462   Training iter 600, batch loss 1.7340, batch acc 0.8598
12:37:19.464 Testing @ 295 epoch...
12:37:19.507     Testing, total mean loss 1.72756, total acc 0.86920
12:37:19.507 Training @ 296 epoch...
12:37:20.047   Training iter 50, batch loss 1.7341, batch acc 0.8584
12:37:20.546   Training iter 100, batch loss 1.7370, batch acc 0.8586
12:37:21.044   Training iter 150, batch loss 1.7390, batch acc 0.8482
12:37:21.543   Training iter 200, batch loss 1.7359, batch acc 0.8610
12:37:22.036   Training iter 250, batch loss 1.7350, batch acc 0.8574
12:37:22.540   Training iter 300, batch loss 1.7379, batch acc 0.8544
12:37:23.051   Training iter 350, batch loss 1.7407, batch acc 0.8546
12:37:23.575   Training iter 400, batch loss 1.7313, batch acc 0.8630
12:37:24.099   Training iter 450, batch loss 1.7338, batch acc 0.8610
12:37:24.624   Training iter 500, batch loss 1.7316, batch acc 0.8640
12:37:25.121   Training iter 550, batch loss 1.7327, batch acc 0.8634
12:37:25.602   Training iter 600, batch loss 1.7298, batch acc 0.8644
12:37:25.604 Training @ 297 epoch...
12:37:26.104   Training iter 50, batch loss 1.7388, batch acc 0.8556
12:37:26.610   Training iter 100, batch loss 1.7348, batch acc 0.8574
12:37:27.125   Training iter 150, batch loss 1.7331, batch acc 0.8616
12:37:27.630   Training iter 200, batch loss 1.7408, batch acc 0.8548
12:37:28.142   Training iter 250, batch loss 1.7364, batch acc 0.8574
12:37:28.648   Training iter 300, batch loss 1.7332, batch acc 0.8642
12:37:29.159   Training iter 350, batch loss 1.7301, batch acc 0.8652
12:37:29.678   Training iter 400, batch loss 1.7375, batch acc 0.8552
12:37:30.192   Training iter 450, batch loss 1.7316, batch acc 0.8602
12:37:30.731   Training iter 500, batch loss 1.7364, batch acc 0.8556
12:37:31.250   Training iter 550, batch loss 1.7303, batch acc 0.8686
12:37:31.774   Training iter 600, batch loss 1.7356, batch acc 0.8560
12:37:31.776 Training @ 298 epoch...
12:37:32.308   Training iter 50, batch loss 1.7339, batch acc 0.8616
12:37:32.842   Training iter 100, batch loss 1.7336, batch acc 0.8642
12:37:33.369   Training iter 150, batch loss 1.7377, batch acc 0.8584
12:37:33.892   Training iter 200, batch loss 1.7342, batch acc 0.8594
12:37:34.412   Training iter 250, batch loss 1.7401, batch acc 0.8490
12:37:34.942   Training iter 300, batch loss 1.7307, batch acc 0.8688
12:37:35.494   Training iter 350, batch loss 1.7343, batch acc 0.8616
12:37:35.986   Training iter 400, batch loss 1.7364, batch acc 0.8558
12:37:36.481   Training iter 450, batch loss 1.7361, batch acc 0.8530
12:37:36.957   Training iter 500, batch loss 1.7345, batch acc 0.8594
12:37:37.434   Training iter 550, batch loss 1.7341, batch acc 0.8576
12:37:37.919   Training iter 600, batch loss 1.7326, batch acc 0.8654
12:37:37.921 Training @ 299 epoch...
12:37:38.415   Training iter 50, batch loss 1.7339, batch acc 0.8550
12:37:38.902   Training iter 100, batch loss 1.7375, batch acc 0.8560
12:37:39.391   Training iter 150, batch loss 1.7323, batch acc 0.8590
12:37:39.874   Training iter 200, batch loss 1.7368, batch acc 0.8576
12:37:40.374   Training iter 250, batch loss 1.7365, batch acc 0.8598
12:37:40.854   Training iter 300, batch loss 1.7371, batch acc 0.8548
12:37:41.344   Training iter 350, batch loss 1.7347, batch acc 0.8612
12:37:41.826   Training iter 400, batch loss 1.7313, batch acc 0.8628
12:37:42.321   Training iter 450, batch loss 1.7307, batch acc 0.8656
12:37:42.820   Training iter 500, batch loss 1.7299, batch acc 0.8668
12:37:43.315   Training iter 550, batch loss 1.7394, batch acc 0.8566
12:37:43.800   Training iter 600, batch loss 1.7377, batch acc 0.8600
12:37:43.802 Training @ 300 epoch...
12:37:44.299   Training iter 50, batch loss 1.7327, batch acc 0.8618
12:37:44.795   Training iter 100, batch loss 1.7352, batch acc 0.8622
12:37:45.283   Training iter 150, batch loss 1.7312, batch acc 0.8610
12:37:45.793   Training iter 200, batch loss 1.7339, batch acc 0.8620
12:37:46.376   Training iter 250, batch loss 1.7417, batch acc 0.8488
12:37:46.960   Training iter 300, batch loss 1.7343, batch acc 0.8614
12:37:47.536   Training iter 350, batch loss 1.7329, batch acc 0.8574
12:37:48.068   Training iter 400, batch loss 1.7350, batch acc 0.8634
12:37:48.593   Training iter 450, batch loss 1.7340, batch acc 0.8600
12:37:49.137   Training iter 500, batch loss 1.7367, batch acc 0.8568
12:37:49.692   Training iter 550, batch loss 1.7324, batch acc 0.8630
12:37:50.293   Training iter 600, batch loss 1.7376, batch acc 0.8562
12:37:50.295 Testing @ 300 epoch...
12:37:50.350     Testing, total mean loss 1.72742, total acc 0.86940
12:37:50.350 Plot @ 300 epoch...
12:37:50.350 Training @ 301 epoch...
12:37:50.975   Training iter 50, batch loss 1.7389, batch acc 0.8520
12:37:51.536   Training iter 100, batch loss 1.7316, batch acc 0.8594
12:37:52.101   Training iter 150, batch loss 1.7356, batch acc 0.8600
12:37:52.643   Training iter 200, batch loss 1.7381, batch acc 0.8554
12:37:53.173   Training iter 250, batch loss 1.7380, batch acc 0.8516
12:37:53.706   Training iter 300, batch loss 1.7342, batch acc 0.8624
12:37:54.211   Training iter 350, batch loss 1.7396, batch acc 0.8544
12:37:54.725   Training iter 400, batch loss 1.7287, batch acc 0.8656
12:37:55.249   Training iter 450, batch loss 1.7331, batch acc 0.8628
12:37:55.763   Training iter 500, batch loss 1.7338, batch acc 0.8628
12:37:56.275   Training iter 550, batch loss 1.7342, batch acc 0.8564
12:37:56.781   Training iter 600, batch loss 1.7315, batch acc 0.8704
12:37:56.783 Training @ 302 epoch...
12:37:57.316   Training iter 50, batch loss 1.7333, batch acc 0.8618
12:37:57.841   Training iter 100, batch loss 1.7358, batch acc 0.8570
12:37:58.363   Training iter 150, batch loss 1.7310, batch acc 0.8632
12:37:58.872   Training iter 200, batch loss 1.7350, batch acc 0.8546
12:37:59.375   Training iter 250, batch loss 1.7370, batch acc 0.8626
12:37:59.857   Training iter 300, batch loss 1.7313, batch acc 0.8624
12:38:00.383   Training iter 350, batch loss 1.7374, batch acc 0.8612
12:38:00.898   Training iter 400, batch loss 1.7333, batch acc 0.8580
12:38:01.415   Training iter 450, batch loss 1.7353, batch acc 0.8580
12:38:01.973   Training iter 500, batch loss 1.7372, batch acc 0.8562
12:38:02.513   Training iter 550, batch loss 1.7343, batch acc 0.8638
12:38:03.017   Training iter 600, batch loss 1.7361, batch acc 0.8574
12:38:03.018 Training @ 303 epoch...
12:38:03.533   Training iter 50, batch loss 1.7365, batch acc 0.8564
12:38:04.024   Training iter 100, batch loss 1.7306, batch acc 0.8680
12:38:04.521   Training iter 150, batch loss 1.7371, batch acc 0.8540
12:38:05.044   Training iter 200, batch loss 1.7341, batch acc 0.8616
12:38:05.589   Training iter 250, batch loss 1.7348, batch acc 0.8524
12:38:06.132   Training iter 300, batch loss 1.7370, batch acc 0.8624
12:38:06.669   Training iter 350, batch loss 1.7378, batch acc 0.8600
12:38:07.211   Training iter 400, batch loss 1.7346, batch acc 0.8614
12:38:07.738   Training iter 450, batch loss 1.7343, batch acc 0.8602
12:38:08.262   Training iter 500, batch loss 1.7313, batch acc 0.8628
12:38:08.731   Training iter 550, batch loss 1.7336, batch acc 0.8614
12:38:09.199   Training iter 600, batch loss 1.7351, batch acc 0.8544
12:38:09.201 Training @ 304 epoch...
12:38:09.669   Training iter 50, batch loss 1.7347, batch acc 0.8618
12:38:10.181   Training iter 100, batch loss 1.7312, batch acc 0.8632
12:38:10.653   Training iter 150, batch loss 1.7390, batch acc 0.8550
12:38:11.130   Training iter 200, batch loss 1.7353, batch acc 0.8552
12:38:11.605   Training iter 250, batch loss 1.7358, batch acc 0.8614
12:38:12.077   Training iter 300, batch loss 1.7332, batch acc 0.8632
12:38:12.565   Training iter 350, batch loss 1.7354, batch acc 0.8564
12:38:13.044   Training iter 400, batch loss 1.7326, batch acc 0.8610
12:38:13.505   Training iter 450, batch loss 1.7327, batch acc 0.8584
12:38:13.971   Training iter 500, batch loss 1.7344, batch acc 0.8614
12:38:14.447   Training iter 550, batch loss 1.7355, batch acc 0.8598
12:38:14.939   Training iter 600, batch loss 1.7364, batch acc 0.8578
12:38:14.940 Training @ 305 epoch...
12:38:15.435   Training iter 50, batch loss 1.7398, batch acc 0.8470
12:38:15.938   Training iter 100, batch loss 1.7344, batch acc 0.8632
12:38:16.443   Training iter 150, batch loss 1.7369, batch acc 0.8526
12:38:16.941   Training iter 200, batch loss 1.7316, batch acc 0.8646
12:38:17.441   Training iter 250, batch loss 1.7334, batch acc 0.8692
12:38:17.937   Training iter 300, batch loss 1.7351, batch acc 0.8604
12:38:18.461   Training iter 350, batch loss 1.7325, batch acc 0.8624
12:38:18.999   Training iter 400, batch loss 1.7360, batch acc 0.8574
12:38:19.541   Training iter 450, batch loss 1.7400, batch acc 0.8532
12:38:20.074   Training iter 500, batch loss 1.7301, batch acc 0.8632
12:38:20.609   Training iter 550, batch loss 1.7350, batch acc 0.8558
12:38:21.152   Training iter 600, batch loss 1.7314, batch acc 0.8662
12:38:21.154 Testing @ 305 epoch...
12:38:21.199     Testing, total mean loss 1.72730, total acc 0.86940
12:38:21.199 Training @ 306 epoch...
12:38:21.736   Training iter 50, batch loss 1.7303, batch acc 0.8622
12:38:22.291   Training iter 100, batch loss 1.7349, batch acc 0.8582
12:38:22.842   Training iter 150, batch loss 1.7314, batch acc 0.8616
12:38:23.397   Training iter 200, batch loss 1.7393, batch acc 0.8560
12:38:23.956   Training iter 250, batch loss 1.7338, batch acc 0.8608
12:38:24.497   Training iter 300, batch loss 1.7347, batch acc 0.8628
12:38:25.026   Training iter 350, batch loss 1.7323, batch acc 0.8640
12:38:25.551   Training iter 400, batch loss 1.7371, batch acc 0.8524
12:38:26.072   Training iter 450, batch loss 1.7380, batch acc 0.8584
12:38:26.593   Training iter 500, batch loss 1.7359, batch acc 0.8560
12:38:27.122   Training iter 550, batch loss 1.7362, batch acc 0.8518
12:38:27.660   Training iter 600, batch loss 1.7320, batch acc 0.8676
12:38:27.661 Training @ 307 epoch...
12:38:28.198   Training iter 50, batch loss 1.7351, batch acc 0.8590
12:38:28.715   Training iter 100, batch loss 1.7362, batch acc 0.8530
12:38:29.240   Training iter 150, batch loss 1.7325, batch acc 0.8616
12:38:29.768   Training iter 200, batch loss 1.7354, batch acc 0.8600
12:38:30.296   Training iter 250, batch loss 1.7360, batch acc 0.8570
12:38:30.791   Training iter 300, batch loss 1.7364, batch acc 0.8550
12:38:31.269   Training iter 350, batch loss 1.7349, batch acc 0.8596
12:38:31.755   Training iter 400, batch loss 1.7345, batch acc 0.8596
12:38:32.237   Training iter 450, batch loss 1.7359, batch acc 0.8596
12:38:32.718   Training iter 500, batch loss 1.7357, batch acc 0.8600
12:38:33.187   Training iter 550, batch loss 1.7326, batch acc 0.8646
12:38:33.627   Training iter 600, batch loss 1.7303, batch acc 0.8666
12:38:33.629 Training @ 308 epoch...
12:38:34.076   Training iter 50, batch loss 1.7333, batch acc 0.8640
12:38:34.512   Training iter 100, batch loss 1.7344, batch acc 0.8588
12:38:34.976   Training iter 150, batch loss 1.7299, batch acc 0.8654
12:38:35.461   Training iter 200, batch loss 1.7358, batch acc 0.8610
12:38:35.930   Training iter 250, batch loss 1.7337, batch acc 0.8648
12:38:36.394   Training iter 300, batch loss 1.7378, batch acc 0.8528
12:38:36.859   Training iter 350, batch loss 1.7356, batch acc 0.8564
12:38:37.334   Training iter 400, batch loss 1.7360, batch acc 0.8574
12:38:37.808   Training iter 450, batch loss 1.7339, batch acc 0.8616
12:38:38.290   Training iter 500, batch loss 1.7368, batch acc 0.8572
12:38:38.771   Training iter 550, batch loss 1.7359, batch acc 0.8534
12:38:39.253   Training iter 600, batch loss 1.7321, batch acc 0.8626
12:38:39.254 Training @ 309 epoch...
12:38:39.744   Training iter 50, batch loss 1.7340, batch acc 0.8616
12:38:40.251   Training iter 100, batch loss 1.7366, batch acc 0.8556
12:38:40.755   Training iter 150, batch loss 1.7334, batch acc 0.8638
12:38:41.278   Training iter 200, batch loss 1.7339, batch acc 0.8622
12:38:41.796   Training iter 250, batch loss 1.7308, batch acc 0.8620
12:38:42.304   Training iter 300, batch loss 1.7362, batch acc 0.8536
12:38:42.836   Training iter 350, batch loss 1.7314, batch acc 0.8642
12:38:43.420   Training iter 400, batch loss 1.7344, batch acc 0.8620
12:38:43.987   Training iter 450, batch loss 1.7371, batch acc 0.8510
12:38:44.583   Training iter 500, batch loss 1.7402, batch acc 0.8534
12:38:45.206   Training iter 550, batch loss 1.7323, batch acc 0.8628
12:38:45.699   Training iter 600, batch loss 1.7347, batch acc 0.8620
12:38:45.700 Training @ 310 epoch...
12:38:46.183   Training iter 50, batch loss 1.7332, batch acc 0.8604
12:38:46.686   Training iter 100, batch loss 1.7351, batch acc 0.8616
12:38:47.202   Training iter 150, batch loss 1.7382, batch acc 0.8526
12:38:47.777   Training iter 200, batch loss 1.7344, batch acc 0.8638
12:38:48.290   Training iter 250, batch loss 1.7340, batch acc 0.8560
12:38:48.795   Training iter 300, batch loss 1.7315, batch acc 0.8650
12:38:49.290   Training iter 350, batch loss 1.7369, batch acc 0.8518
12:38:49.857   Training iter 400, batch loss 1.7324, batch acc 0.8686
12:38:50.446   Training iter 450, batch loss 1.7339, batch acc 0.8612
12:38:50.950   Training iter 500, batch loss 1.7336, batch acc 0.8610
12:38:51.455   Training iter 550, batch loss 1.7331, batch acc 0.8668
12:38:51.978   Training iter 600, batch loss 1.7384, batch acc 0.8480
12:38:51.980 Testing @ 310 epoch...
12:38:52.024     Testing, total mean loss 1.72718, total acc 0.86970
12:38:52.024 Training @ 311 epoch...
12:38:52.573   Training iter 50, batch loss 1.7336, batch acc 0.8666
12:38:53.077   Training iter 100, batch loss 1.7403, batch acc 0.8518
12:38:53.569   Training iter 150, batch loss 1.7330, batch acc 0.8590
12:38:54.058   Training iter 200, batch loss 1.7317, batch acc 0.8602
12:38:54.559   Training iter 250, batch loss 1.7379, batch acc 0.8572
12:38:55.134   Training iter 300, batch loss 1.7318, batch acc 0.8698
12:38:55.661   Training iter 350, batch loss 1.7356, batch acc 0.8550
12:38:56.197   Training iter 400, batch loss 1.7314, batch acc 0.8640
12:38:56.708   Training iter 450, batch loss 1.7323, batch acc 0.8678
12:38:57.229   Training iter 500, batch loss 1.7325, batch acc 0.8566
12:38:57.746   Training iter 550, batch loss 1.7360, batch acc 0.8582
12:38:58.235   Training iter 600, batch loss 1.7383, batch acc 0.8510
12:38:58.237 Training @ 312 epoch...
12:38:58.714   Training iter 50, batch loss 1.7407, batch acc 0.8538
12:38:59.191   Training iter 100, batch loss 1.7322, batch acc 0.8630
12:38:59.680   Training iter 150, batch loss 1.7350, batch acc 0.8568
12:39:00.160   Training iter 200, batch loss 1.7336, batch acc 0.8636
12:39:00.661   Training iter 250, batch loss 1.7320, batch acc 0.8644
12:39:01.170   Training iter 300, batch loss 1.7334, batch acc 0.8642
12:39:01.729   Training iter 350, batch loss 1.7274, batch acc 0.8672
12:39:02.298   Training iter 400, batch loss 1.7368, batch acc 0.8528
12:39:02.861   Training iter 450, batch loss 1.7371, batch acc 0.8542
12:39:03.428   Training iter 500, batch loss 1.7327, batch acc 0.8608
12:39:03.986   Training iter 550, batch loss 1.7381, batch acc 0.8514
12:39:04.531   Training iter 600, batch loss 1.7352, batch acc 0.8628
12:39:04.533 Training @ 313 epoch...
12:39:05.109   Training iter 50, batch loss 1.7351, batch acc 0.8588
12:39:05.668   Training iter 100, batch loss 1.7340, batch acc 0.8576
12:39:06.199   Training iter 150, batch loss 1.7386, batch acc 0.8538
12:39:06.730   Training iter 200, batch loss 1.7322, batch acc 0.8660
12:39:07.312   Training iter 250, batch loss 1.7320, batch acc 0.8638
12:39:07.874   Training iter 300, batch loss 1.7349, batch acc 0.8586
12:39:08.414   Training iter 350, batch loss 1.7383, batch acc 0.8538
12:39:08.948   Training iter 400, batch loss 1.7330, batch acc 0.8612
12:39:09.485   Training iter 450, batch loss 1.7352, batch acc 0.8558
12:39:10.035   Training iter 500, batch loss 1.7298, batch acc 0.8644
12:39:10.567   Training iter 550, batch loss 1.7370, batch acc 0.8580
12:39:11.093   Training iter 600, batch loss 1.7338, batch acc 0.8638
12:39:11.095 Training @ 314 epoch...
12:39:11.625   Training iter 50, batch loss 1.7355, batch acc 0.8534
12:39:12.160   Training iter 100, batch loss 1.7314, batch acc 0.8578
12:39:12.677   Training iter 150, batch loss 1.7378, batch acc 0.8560
12:39:13.174   Training iter 200, batch loss 1.7318, batch acc 0.8616
12:39:13.682   Training iter 250, batch loss 1.7354, batch acc 0.8542
12:39:14.192   Training iter 300, batch loss 1.7364, batch acc 0.8548
12:39:14.723   Training iter 350, batch loss 1.7370, batch acc 0.8580
12:39:15.261   Training iter 400, batch loss 1.7349, batch acc 0.8614
12:39:15.775   Training iter 450, batch loss 1.7350, batch acc 0.8606
12:39:16.276   Training iter 500, batch loss 1.7314, batch acc 0.8670
12:39:16.799   Training iter 550, batch loss 1.7352, batch acc 0.8630
12:39:17.324   Training iter 600, batch loss 1.7318, batch acc 0.8680
12:39:17.325 Training @ 315 epoch...
12:39:17.848   Training iter 50, batch loss 1.7300, batch acc 0.8670
12:39:18.345   Training iter 100, batch loss 1.7339, batch acc 0.8618
12:39:18.847   Training iter 150, batch loss 1.7388, batch acc 0.8542
12:39:19.342   Training iter 200, batch loss 1.7330, batch acc 0.8610
12:39:19.833   Training iter 250, batch loss 1.7318, batch acc 0.8662
12:39:20.338   Training iter 300, batch loss 1.7334, batch acc 0.8570
12:39:20.906   Training iter 350, batch loss 1.7352, batch acc 0.8574
12:39:21.389   Training iter 400, batch loss 1.7371, batch acc 0.8552
12:39:21.865   Training iter 450, batch loss 1.7367, batch acc 0.8604
12:39:22.359   Training iter 500, batch loss 1.7351, batch acc 0.8578
12:39:22.880   Training iter 550, batch loss 1.7314, batch acc 0.8656
12:39:23.395   Training iter 600, batch loss 1.7369, batch acc 0.8564
12:39:23.397 Testing @ 315 epoch...
12:39:23.439     Testing, total mean loss 1.72707, total acc 0.86970
12:39:23.439 Training @ 316 epoch...
12:39:23.941   Training iter 50, batch loss 1.7369, batch acc 0.8596
12:39:24.439   Training iter 100, batch loss 1.7355, batch acc 0.8516
12:39:24.964   Training iter 150, batch loss 1.7345, batch acc 0.8644
12:39:25.491   Training iter 200, batch loss 1.7362, batch acc 0.8578
12:39:26.009   Training iter 250, batch loss 1.7328, batch acc 0.8620
12:39:26.518   Training iter 300, batch loss 1.7334, batch acc 0.8558
12:39:27.027   Training iter 350, batch loss 1.7348, batch acc 0.8646
12:39:27.543   Training iter 400, batch loss 1.7321, batch acc 0.8574
12:39:28.082   Training iter 450, batch loss 1.7332, batch acc 0.8680
12:39:28.651   Training iter 500, batch loss 1.7334, batch acc 0.8602
12:39:29.224   Training iter 550, batch loss 1.7353, batch acc 0.8576
12:39:29.780   Training iter 600, batch loss 1.7349, batch acc 0.8568
12:39:29.782 Training @ 317 epoch...
12:39:30.270   Training iter 50, batch loss 1.7344, batch acc 0.8624
12:39:30.752   Training iter 100, batch loss 1.7397, batch acc 0.8500
12:39:31.234   Training iter 150, batch loss 1.7289, batch acc 0.8670
12:39:31.699   Training iter 200, batch loss 1.7340, batch acc 0.8576
12:39:32.184   Training iter 250, batch loss 1.7352, batch acc 0.8584
12:39:32.637   Training iter 300, batch loss 1.7255, batch acc 0.8722
12:39:33.111   Training iter 350, batch loss 1.7369, batch acc 0.8550
12:39:33.580   Training iter 400, batch loss 1.7341, batch acc 0.8610
12:39:34.037   Training iter 450, batch loss 1.7375, batch acc 0.8558
12:39:34.506   Training iter 500, batch loss 1.7350, batch acc 0.8620
12:39:34.981   Training iter 550, batch loss 1.7339, batch acc 0.8630
12:39:35.447   Training iter 600, batch loss 1.7378, batch acc 0.8528
12:39:35.449 Training @ 318 epoch...
12:39:35.939   Training iter 50, batch loss 1.7409, batch acc 0.8492
12:39:36.415   Training iter 100, batch loss 1.7378, batch acc 0.8580
12:39:36.882   Training iter 150, batch loss 1.7347, batch acc 0.8610
12:39:37.358   Training iter 200, batch loss 1.7339, batch acc 0.8622
12:39:37.857   Training iter 250, batch loss 1.7365, batch acc 0.8564
12:39:38.346   Training iter 300, batch loss 1.7333, batch acc 0.8624
12:39:38.829   Training iter 350, batch loss 1.7327, batch acc 0.8604
12:39:39.336   Training iter 400, batch loss 1.7319, batch acc 0.8652
12:39:39.837   Training iter 450, batch loss 1.7319, batch acc 0.8572
12:39:40.343   Training iter 500, batch loss 1.7323, batch acc 0.8616
12:39:40.827   Training iter 550, batch loss 1.7317, batch acc 0.8642
12:39:41.311   Training iter 600, batch loss 1.7349, batch acc 0.8596
12:39:41.313 Training @ 319 epoch...
12:39:41.805   Training iter 50, batch loss 1.7356, batch acc 0.8554
12:39:42.304   Training iter 100, batch loss 1.7345, batch acc 0.8618
12:39:42.816   Training iter 150, batch loss 1.7341, batch acc 0.8578
12:39:43.332   Training iter 200, batch loss 1.7405, batch acc 0.8510
12:39:43.821   Training iter 250, batch loss 1.7287, batch acc 0.8728
12:39:44.320   Training iter 300, batch loss 1.7293, batch acc 0.8692
12:39:44.813   Training iter 350, batch loss 1.7332, batch acc 0.8600
12:39:45.311   Training iter 400, batch loss 1.7354, batch acc 0.8588
12:39:45.841   Training iter 450, batch loss 1.7336, batch acc 0.8578
12:39:46.383   Training iter 500, batch loss 1.7321, batch acc 0.8626
12:39:46.932   Training iter 550, batch loss 1.7388, batch acc 0.8520
12:39:47.464   Training iter 600, batch loss 1.7366, batch acc 0.8600
12:39:47.466 Training @ 320 epoch...
12:39:47.960   Training iter 50, batch loss 1.7339, batch acc 0.8584
12:39:48.458   Training iter 100, batch loss 1.7337, batch acc 0.8608
12:39:48.955   Training iter 150, batch loss 1.7333, batch acc 0.8646
12:39:49.455   Training iter 200, batch loss 1.7400, batch acc 0.8490
12:39:49.947   Training iter 250, batch loss 1.7375, batch acc 0.8540
12:39:50.445   Training iter 300, batch loss 1.7326, batch acc 0.8674
12:39:50.945   Training iter 350, batch loss 1.7327, batch acc 0.8570
12:39:51.428   Training iter 400, batch loss 1.7311, batch acc 0.8582
12:39:51.911   Training iter 450, batch loss 1.7328, batch acc 0.8592
12:39:52.409   Training iter 500, batch loss 1.7386, batch acc 0.8536
12:39:52.896   Training iter 550, batch loss 1.7357, batch acc 0.8640
12:39:53.390   Training iter 600, batch loss 1.7301, batch acc 0.8722
12:39:53.392 Testing @ 320 epoch...
12:39:53.434     Testing, total mean loss 1.72697, total acc 0.87030
12:39:53.434 Training @ 321 epoch...
12:39:53.911   Training iter 50, batch loss 1.7348, batch acc 0.8678
12:39:54.384   Training iter 100, batch loss 1.7324, batch acc 0.8608
12:39:54.823   Training iter 150, batch loss 1.7345, batch acc 0.8562
12:39:55.284   Training iter 200, batch loss 1.7373, batch acc 0.8580
12:39:55.728   Training iter 250, batch loss 1.7324, batch acc 0.8668
12:39:56.176   Training iter 300, batch loss 1.7375, batch acc 0.8554
12:39:56.627   Training iter 350, batch loss 1.7308, batch acc 0.8602
12:39:57.087   Training iter 400, batch loss 1.7359, batch acc 0.8578
12:39:57.538   Training iter 450, batch loss 1.7348, batch acc 0.8526
12:39:58.007   Training iter 500, batch loss 1.7343, batch acc 0.8600
12:39:58.478   Training iter 550, batch loss 1.7300, batch acc 0.8662
12:39:58.940   Training iter 600, batch loss 1.7371, batch acc 0.8550
12:39:58.941 Training @ 322 epoch...
12:39:59.416   Training iter 50, batch loss 1.7355, batch acc 0.8574
12:39:59.881   Training iter 100, batch loss 1.7372, batch acc 0.8572
12:40:00.375   Training iter 150, batch loss 1.7342, batch acc 0.8626
12:40:00.857   Training iter 200, batch loss 1.7349, batch acc 0.8548
12:40:01.355   Training iter 250, batch loss 1.7296, batch acc 0.8658
12:40:01.889   Training iter 300, batch loss 1.7378, batch acc 0.8630
12:40:02.396   Training iter 350, batch loss 1.7338, batch acc 0.8606
12:40:02.891   Training iter 400, batch loss 1.7318, batch acc 0.8614
12:40:03.381   Training iter 450, batch loss 1.7322, batch acc 0.8630
12:40:03.870   Training iter 500, batch loss 1.7377, batch acc 0.8516
12:40:04.360   Training iter 550, batch loss 1.7333, batch acc 0.8590
12:40:04.866   Training iter 600, batch loss 1.7338, batch acc 0.8626
12:40:04.868 Training @ 323 epoch...
12:40:05.377   Training iter 50, batch loss 1.7346, batch acc 0.8556
12:40:05.882   Training iter 100, batch loss 1.7357, batch acc 0.8564
12:40:06.376   Training iter 150, batch loss 1.7353, batch acc 0.8544
12:40:06.862   Training iter 200, batch loss 1.7365, batch acc 0.8582
12:40:07.338   Training iter 250, batch loss 1.7361, batch acc 0.8552
12:40:07.832   Training iter 300, batch loss 1.7332, batch acc 0.8684
12:40:08.348   Training iter 350, batch loss 1.7339, batch acc 0.8608
12:40:08.857   Training iter 400, batch loss 1.7317, batch acc 0.8648
12:40:09.345   Training iter 450, batch loss 1.7342, batch acc 0.8582
12:40:09.833   Training iter 500, batch loss 1.7353, batch acc 0.8590
12:40:10.360   Training iter 550, batch loss 1.7337, batch acc 0.8552
12:40:10.871   Training iter 600, batch loss 1.7310, batch acc 0.8704
12:40:10.873 Training @ 324 epoch...
12:40:11.378   Training iter 50, batch loss 1.7362, batch acc 0.8590
12:40:11.878   Training iter 100, batch loss 1.7352, batch acc 0.8572
12:40:12.385   Training iter 150, batch loss 1.7310, batch acc 0.8646
12:40:12.889   Training iter 200, batch loss 1.7362, batch acc 0.8518
12:40:13.371   Training iter 250, batch loss 1.7316, batch acc 0.8668
12:40:13.842   Training iter 300, batch loss 1.7339, batch acc 0.8616
12:40:14.394   Training iter 350, batch loss 1.7356, batch acc 0.8620
12:40:14.939   Training iter 400, batch loss 1.7331, batch acc 0.8586
12:40:15.490   Training iter 450, batch loss 1.7360, batch acc 0.8612
12:40:15.945   Training iter 500, batch loss 1.7310, batch acc 0.8648
12:40:16.391   Training iter 550, batch loss 1.7364, batch acc 0.8492
12:40:16.855   Training iter 600, batch loss 1.7348, batch acc 0.8608
12:40:16.857 Training @ 325 epoch...
12:40:17.346   Training iter 50, batch loss 1.7357, batch acc 0.8568
12:40:17.832   Training iter 100, batch loss 1.7340, batch acc 0.8592
12:40:18.313   Training iter 150, batch loss 1.7345, batch acc 0.8596
12:40:18.788   Training iter 200, batch loss 1.7337, batch acc 0.8602
12:40:19.236   Training iter 250, batch loss 1.7294, batch acc 0.8638
12:40:19.691   Training iter 300, batch loss 1.7356, batch acc 0.8622
12:40:20.158   Training iter 350, batch loss 1.7351, batch acc 0.8590
12:40:20.620   Training iter 400, batch loss 1.7328, batch acc 0.8598
12:40:21.071   Training iter 450, batch loss 1.7325, batch acc 0.8618
12:40:21.522   Training iter 500, batch loss 1.7387, batch acc 0.8548
12:40:21.980   Training iter 550, batch loss 1.7325, batch acc 0.8664
12:40:22.445   Training iter 600, batch loss 1.7366, batch acc 0.8550
12:40:22.447 Testing @ 325 epoch...
12:40:22.489     Testing, total mean loss 1.72687, total acc 0.87010
12:40:22.489 Training @ 326 epoch...
12:40:22.966   Training iter 50, batch loss 1.7398, batch acc 0.8572
12:40:23.465   Training iter 100, batch loss 1.7283, batch acc 0.8652
12:40:23.961   Training iter 150, batch loss 1.7372, batch acc 0.8542
12:40:24.459   Training iter 200, batch loss 1.7354, batch acc 0.8568
12:40:24.963   Training iter 250, batch loss 1.7328, batch acc 0.8554
12:40:25.472   Training iter 300, batch loss 1.7353, batch acc 0.8620
12:40:25.960   Training iter 350, batch loss 1.7301, batch acc 0.8702
12:40:26.461   Training iter 400, batch loss 1.7348, batch acc 0.8566
12:40:26.960   Training iter 450, batch loss 1.7327, batch acc 0.8652
12:40:27.472   Training iter 500, batch loss 1.7346, batch acc 0.8588
12:40:27.974   Training iter 550, batch loss 1.7327, batch acc 0.8654
12:40:28.529   Training iter 600, batch loss 1.7371, batch acc 0.8518
12:40:28.531 Training @ 327 epoch...
12:40:29.057   Training iter 50, batch loss 1.7367, batch acc 0.8568
12:40:29.567   Training iter 100, batch loss 1.7339, batch acc 0.8604
12:40:30.081   Training iter 150, batch loss 1.7341, batch acc 0.8596
12:40:30.609   Training iter 200, batch loss 1.7313, batch acc 0.8620
12:40:31.149   Training iter 250, batch loss 1.7397, batch acc 0.8542
12:40:31.675   Training iter 300, batch loss 1.7307, batch acc 0.8612
12:40:32.214   Training iter 350, batch loss 1.7363, batch acc 0.8512
12:40:32.739   Training iter 400, batch loss 1.7370, batch acc 0.8560
12:40:33.250   Training iter 450, batch loss 1.7332, batch acc 0.8652
12:40:33.769   Training iter 500, batch loss 1.7307, batch acc 0.8634
12:40:34.309   Training iter 550, batch loss 1.7341, batch acc 0.8648
12:40:34.854   Training iter 600, batch loss 1.7327, batch acc 0.8622
12:40:34.856 Training @ 328 epoch...
12:40:35.396   Training iter 50, batch loss 1.7376, batch acc 0.8564
12:40:35.904   Training iter 100, batch loss 1.7315, batch acc 0.8602
12:40:36.422   Training iter 150, batch loss 1.7361, batch acc 0.8578
12:40:36.953   Training iter 200, batch loss 1.7365, batch acc 0.8552
12:40:37.461   Training iter 250, batch loss 1.7310, batch acc 0.8662
12:40:37.970   Training iter 300, batch loss 1.7320, batch acc 0.8636
12:40:38.490   Training iter 350, batch loss 1.7363, batch acc 0.8592
12:40:38.994   Training iter 400, batch loss 1.7348, batch acc 0.8628
12:40:39.521   Training iter 450, batch loss 1.7332, batch acc 0.8636
12:40:40.041   Training iter 500, batch loss 1.7310, batch acc 0.8668
12:40:40.568   Training iter 550, batch loss 1.7368, batch acc 0.8542
12:40:41.110   Training iter 600, batch loss 1.7335, batch acc 0.8558
12:40:41.112 Training @ 329 epoch...
12:40:41.645   Training iter 50, batch loss 1.7349, batch acc 0.8584
12:40:42.178   Training iter 100, batch loss 1.7370, batch acc 0.8530
12:40:42.692   Training iter 150, batch loss 1.7321, batch acc 0.8612
12:40:43.228   Training iter 200, batch loss 1.7345, batch acc 0.8616
12:40:43.762   Training iter 250, batch loss 1.7321, batch acc 0.8630
12:40:44.305   Training iter 300, batch loss 1.7374, batch acc 0.8582
12:40:44.852   Training iter 350, batch loss 1.7366, batch acc 0.8564
12:40:45.388   Training iter 400, batch loss 1.7341, batch acc 0.8636
12:40:45.910   Training iter 450, batch loss 1.7342, batch acc 0.8564
12:40:46.480   Training iter 500, batch loss 1.7295, batch acc 0.8630
12:40:47.040   Training iter 550, batch loss 1.7362, batch acc 0.8564
12:40:47.607   Training iter 600, batch loss 1.7313, batch acc 0.8658
12:40:47.609 Training @ 330 epoch...
12:40:48.175   Training iter 50, batch loss 1.7364, batch acc 0.8562
12:40:48.739   Training iter 100, batch loss 1.7294, batch acc 0.8682
12:40:49.263   Training iter 150, batch loss 1.7353, batch acc 0.8634
12:40:49.759   Training iter 200, batch loss 1.7345, batch acc 0.8550
12:40:50.261   Training iter 250, batch loss 1.7316, batch acc 0.8678
12:40:50.763   Training iter 300, batch loss 1.7356, batch acc 0.8560
12:40:51.273   Training iter 350, batch loss 1.7365, batch acc 0.8586
12:40:51.777   Training iter 400, batch loss 1.7336, batch acc 0.8548
12:40:52.309   Training iter 450, batch loss 1.7334, batch acc 0.8570
12:40:52.831   Training iter 500, batch loss 1.7371, batch acc 0.8570
12:40:53.342   Training iter 550, batch loss 1.7344, batch acc 0.8612
12:40:53.837   Training iter 600, batch loss 1.7319, batch acc 0.8650
12:40:53.838 Testing @ 330 epoch...
12:40:53.881     Testing, total mean loss 1.72677, total acc 0.86980
12:40:53.881 Training @ 331 epoch...
12:40:54.380   Training iter 50, batch loss 1.7294, batch acc 0.8670
12:40:54.877   Training iter 100, batch loss 1.7332, batch acc 0.8598
12:40:55.382   Training iter 150, batch loss 1.7354, batch acc 0.8608
12:40:55.884   Training iter 200, batch loss 1.7363, batch acc 0.8572
12:40:56.380   Training iter 250, batch loss 1.7329, batch acc 0.8590
12:40:56.876   Training iter 300, batch loss 1.7346, batch acc 0.8614
12:40:57.388   Training iter 350, batch loss 1.7355, batch acc 0.8626
12:40:57.912   Training iter 400, batch loss 1.7353, batch acc 0.8578
12:40:58.438   Training iter 450, batch loss 1.7356, batch acc 0.8544
12:40:58.971   Training iter 500, batch loss 1.7390, batch acc 0.8552
12:40:59.528   Training iter 550, batch loss 1.7353, batch acc 0.8562
12:41:00.052   Training iter 600, batch loss 1.7269, batch acc 0.8682
12:41:00.054 Training @ 332 epoch...
12:41:00.548   Training iter 50, batch loss 1.7355, batch acc 0.8602
12:41:01.054   Training iter 100, batch loss 1.7382, batch acc 0.8566
12:41:01.607   Training iter 150, batch loss 1.7344, batch acc 0.8594
12:41:02.183   Training iter 200, batch loss 1.7343, batch acc 0.8514
12:41:02.755   Training iter 250, batch loss 1.7304, batch acc 0.8700
12:41:03.310   Training iter 300, batch loss 1.7328, batch acc 0.8558
12:41:03.859   Training iter 350, batch loss 1.7361, batch acc 0.8578
12:41:04.387   Training iter 400, batch loss 1.7340, batch acc 0.8620
12:41:04.907   Training iter 450, batch loss 1.7295, batch acc 0.8670
12:41:05.402   Training iter 500, batch loss 1.7354, batch acc 0.8618
12:41:05.899   Training iter 550, batch loss 1.7390, batch acc 0.8500
12:41:06.419   Training iter 600, batch loss 1.7296, batch acc 0.8692
12:41:06.420 Training @ 333 epoch...
12:41:06.942   Training iter 50, batch loss 1.7354, batch acc 0.8526
12:41:07.469   Training iter 100, batch loss 1.7326, batch acc 0.8640
12:41:07.979   Training iter 150, batch loss 1.7317, batch acc 0.8622
12:41:08.480   Training iter 200, batch loss 1.7401, batch acc 0.8536
12:41:08.945   Training iter 250, batch loss 1.7324, batch acc 0.8632
12:41:09.423   Training iter 300, batch loss 1.7339, batch acc 0.8584
12:41:09.894   Training iter 350, batch loss 1.7368, batch acc 0.8568
12:41:10.387   Training iter 400, batch loss 1.7352, batch acc 0.8574
12:41:10.928   Training iter 450, batch loss 1.7339, batch acc 0.8578
12:41:11.476   Training iter 500, batch loss 1.7347, batch acc 0.8616
12:41:12.027   Training iter 550, batch loss 1.7286, batch acc 0.8676
12:41:12.521   Training iter 600, batch loss 1.7338, batch acc 0.8660
12:41:12.523 Training @ 334 epoch...
12:41:13.033   Training iter 50, batch loss 1.7351, batch acc 0.8564
12:41:13.544   Training iter 100, batch loss 1.7309, batch acc 0.8670
12:41:14.064   Training iter 150, batch loss 1.7364, batch acc 0.8554
12:41:14.580   Training iter 200, batch loss 1.7370, batch acc 0.8524
12:41:15.066   Training iter 250, batch loss 1.7344, batch acc 0.8592
12:41:15.562   Training iter 300, batch loss 1.7352, batch acc 0.8584
12:41:16.054   Training iter 350, batch loss 1.7317, batch acc 0.8654
12:41:16.539   Training iter 400, batch loss 1.7333, batch acc 0.8632
12:41:17.021   Training iter 450, batch loss 1.7312, batch acc 0.8640
12:41:17.505   Training iter 500, batch loss 1.7349, batch acc 0.8568
12:41:18.002   Training iter 550, batch loss 1.7376, batch acc 0.8564
12:41:18.498   Training iter 600, batch loss 1.7313, batch acc 0.8642
12:41:18.500 Training @ 335 epoch...
12:41:19.010   Training iter 50, batch loss 1.7356, batch acc 0.8606
12:41:19.509   Training iter 100, batch loss 1.7326, batch acc 0.8614
12:41:20.014   Training iter 150, batch loss 1.7260, batch acc 0.8676
12:41:20.515   Training iter 200, batch loss 1.7353, batch acc 0.8538
12:41:21.017   Training iter 250, batch loss 1.7353, batch acc 0.8594
12:41:21.518   Training iter 300, batch loss 1.7360, batch acc 0.8606
12:41:22.021   Training iter 350, batch loss 1.7361, batch acc 0.8586
12:41:22.524   Training iter 400, batch loss 1.7358, batch acc 0.8534
12:41:23.027   Training iter 450, batch loss 1.7327, batch acc 0.8654
12:41:23.526   Training iter 500, batch loss 1.7381, batch acc 0.8550
12:41:24.033   Training iter 550, batch loss 1.7313, batch acc 0.8638
12:41:24.538   Training iter 600, batch loss 1.7338, batch acc 0.8618
12:41:24.540 Testing @ 335 epoch...
12:41:24.582     Testing, total mean loss 1.72668, total acc 0.87010
12:41:24.582 Training @ 336 epoch...
12:41:25.099   Training iter 50, batch loss 1.7343, batch acc 0.8626
12:41:25.590   Training iter 100, batch loss 1.7344, batch acc 0.8632
12:41:26.101   Training iter 150, batch loss 1.7350, batch acc 0.8578
12:41:26.608   Training iter 200, batch loss 1.7349, batch acc 0.8602
12:41:27.129   Training iter 250, batch loss 1.7379, batch acc 0.8602
12:41:27.658   Training iter 300, batch loss 1.7286, batch acc 0.8656
12:41:28.198   Training iter 350, batch loss 1.7345, batch acc 0.8602
12:41:28.742   Training iter 400, batch loss 1.7345, batch acc 0.8540
12:41:29.297   Training iter 450, batch loss 1.7356, batch acc 0.8566
12:41:29.826   Training iter 500, batch loss 1.7339, batch acc 0.8638
12:41:30.355   Training iter 550, batch loss 1.7326, batch acc 0.8608
12:41:30.893   Training iter 600, batch loss 1.7324, batch acc 0.8584
12:41:30.894 Training @ 337 epoch...
12:41:31.416   Training iter 50, batch loss 1.7292, batch acc 0.8672
12:41:31.943   Training iter 100, batch loss 1.7335, batch acc 0.8620
12:41:32.468   Training iter 150, batch loss 1.7334, batch acc 0.8590
12:41:32.980   Training iter 200, batch loss 1.7363, batch acc 0.8604
12:41:33.513   Training iter 250, batch loss 1.7395, batch acc 0.8498
12:41:34.064   Training iter 300, batch loss 1.7337, batch acc 0.8644
12:41:34.599   Training iter 350, batch loss 1.7331, batch acc 0.8618
12:41:35.132   Training iter 400, batch loss 1.7370, batch acc 0.8528
12:41:35.645   Training iter 450, batch loss 1.7351, batch acc 0.8594
12:41:36.145   Training iter 500, batch loss 1.7328, batch acc 0.8616
12:41:36.645   Training iter 550, batch loss 1.7336, batch acc 0.8590
12:41:37.144   Training iter 600, batch loss 1.7311, batch acc 0.8634
12:41:37.146 Training @ 338 epoch...
12:41:37.652   Training iter 50, batch loss 1.7388, batch acc 0.8514
12:41:38.163   Training iter 100, batch loss 1.7327, batch acc 0.8612
12:41:38.675   Training iter 150, batch loss 1.7383, batch acc 0.8554
12:41:39.172   Training iter 200, batch loss 1.7300, batch acc 0.8666
12:41:39.667   Training iter 250, batch loss 1.7355, batch acc 0.8546
12:41:40.170   Training iter 300, batch loss 1.7312, batch acc 0.8620
12:41:40.693   Training iter 350, batch loss 1.7303, batch acc 0.8622
12:41:41.207   Training iter 400, batch loss 1.7315, batch acc 0.8670
12:41:41.668   Training iter 450, batch loss 1.7345, batch acc 0.8596
12:41:42.145   Training iter 500, batch loss 1.7333, batch acc 0.8630
12:41:42.645   Training iter 550, batch loss 1.7398, batch acc 0.8528
12:41:43.136   Training iter 600, batch loss 1.7320, batch acc 0.8662
12:41:43.138 Training @ 339 epoch...
12:41:43.644   Training iter 50, batch loss 1.7282, batch acc 0.8698
12:41:44.131   Training iter 100, batch loss 1.7306, batch acc 0.8610
12:41:44.604   Training iter 150, batch loss 1.7333, batch acc 0.8618
12:41:45.081   Training iter 200, batch loss 1.7345, batch acc 0.8600
12:41:45.562   Training iter 250, batch loss 1.7353, batch acc 0.8620
12:41:46.071   Training iter 300, batch loss 1.7387, batch acc 0.8524
12:41:46.546   Training iter 350, batch loss 1.7351, batch acc 0.8564
12:41:47.083   Training iter 400, batch loss 1.7315, batch acc 0.8600
12:41:47.615   Training iter 450, batch loss 1.7361, batch acc 0.8558
12:41:48.145   Training iter 500, batch loss 1.7331, batch acc 0.8602
12:41:48.685   Training iter 550, batch loss 1.7375, batch acc 0.8574
12:41:49.253   Training iter 600, batch loss 1.7337, batch acc 0.8634
12:41:49.256 Training @ 340 epoch...
12:41:49.815   Training iter 50, batch loss 1.7348, batch acc 0.8656
12:41:50.374   Training iter 100, batch loss 1.7347, batch acc 0.8554
12:41:50.916   Training iter 150, batch loss 1.7347, batch acc 0.8626
12:41:51.452   Training iter 200, batch loss 1.7342, batch acc 0.8556
12:41:51.994   Training iter 250, batch loss 1.7372, batch acc 0.8512
12:41:52.532   Training iter 300, batch loss 1.7294, batch acc 0.8686
12:41:53.032   Training iter 350, batch loss 1.7369, batch acc 0.8570
12:41:53.526   Training iter 400, batch loss 1.7315, batch acc 0.8634
12:41:54.016   Training iter 450, batch loss 1.7322, batch acc 0.8682
12:41:54.502   Training iter 500, batch loss 1.7316, batch acc 0.8586
12:41:54.987   Training iter 550, batch loss 1.7317, batch acc 0.8602
12:41:55.468   Training iter 600, batch loss 1.7386, batch acc 0.8542
12:41:55.470 Testing @ 340 epoch...
12:41:55.512     Testing, total mean loss 1.72660, total acc 0.87020
12:41:55.512 Training @ 341 epoch...
12:41:55.987   Training iter 50, batch loss 1.7354, batch acc 0.8578
12:41:56.469   Training iter 100, batch loss 1.7417, batch acc 0.8484
12:41:56.946   Training iter 150, batch loss 1.7315, batch acc 0.8626
12:41:57.447   Training iter 200, batch loss 1.7284, batch acc 0.8638
12:41:57.964   Training iter 250, batch loss 1.7310, batch acc 0.8618
12:41:58.461   Training iter 300, batch loss 1.7369, batch acc 0.8494
12:41:58.963   Training iter 350, batch loss 1.7329, batch acc 0.8660
12:41:59.475   Training iter 400, batch loss 1.7367, batch acc 0.8650
12:41:59.987   Training iter 450, batch loss 1.7336, batch acc 0.8612
12:42:00.510   Training iter 500, batch loss 1.7329, batch acc 0.8630
12:42:01.022   Training iter 550, batch loss 1.7341, batch acc 0.8600
12:42:01.544   Training iter 600, batch loss 1.7323, batch acc 0.8610
12:42:01.546 Training @ 342 epoch...
12:42:02.106   Training iter 50, batch loss 1.7290, batch acc 0.8652
12:42:02.647   Training iter 100, batch loss 1.7367, batch acc 0.8542
12:42:03.206   Training iter 150, batch loss 1.7344, batch acc 0.8590
12:42:03.773   Training iter 200, batch loss 1.7312, batch acc 0.8632
12:42:04.299   Training iter 250, batch loss 1.7391, batch acc 0.8570
12:42:04.851   Training iter 300, batch loss 1.7300, batch acc 0.8682
12:42:05.393   Training iter 350, batch loss 1.7338, batch acc 0.8594
12:42:05.910   Training iter 400, batch loss 1.7359, batch acc 0.8530
12:42:06.417   Training iter 450, batch loss 1.7339, batch acc 0.8570
12:42:06.950   Training iter 500, batch loss 1.7326, batch acc 0.8612
12:42:07.502   Training iter 550, batch loss 1.7358, batch acc 0.8632
12:42:08.054   Training iter 600, batch loss 1.7351, batch acc 0.8614
12:42:08.056 Training @ 343 epoch...
12:42:08.592   Training iter 50, batch loss 1.7358, batch acc 0.8572
12:42:09.137   Training iter 100, batch loss 1.7351, batch acc 0.8634
12:42:09.696   Training iter 150, batch loss 1.7328, batch acc 0.8612
12:42:10.247   Training iter 200, batch loss 1.7364, batch acc 0.8582
12:42:10.745   Training iter 250, batch loss 1.7316, batch acc 0.8656
12:42:11.261   Training iter 300, batch loss 1.7340, batch acc 0.8564
12:42:11.782   Training iter 350, batch loss 1.7303, batch acc 0.8628
12:42:12.284   Training iter 400, batch loss 1.7411, batch acc 0.8498
12:42:12.791   Training iter 450, batch loss 1.7314, batch acc 0.8638
12:42:13.305   Training iter 500, batch loss 1.7311, batch acc 0.8582
12:42:13.809   Training iter 550, batch loss 1.7347, batch acc 0.8594
12:42:14.333   Training iter 600, batch loss 1.7327, batch acc 0.8656
12:42:14.335 Training @ 344 epoch...
12:42:14.853   Training iter 50, batch loss 1.7365, batch acc 0.8564
12:42:15.352   Training iter 100, batch loss 1.7322, batch acc 0.8592
12:42:15.834   Training iter 150, batch loss 1.7365, batch acc 0.8562
12:42:16.324   Training iter 200, batch loss 1.7346, batch acc 0.8558
12:42:16.823   Training iter 250, batch loss 1.7290, batch acc 0.8692
12:42:17.343   Training iter 300, batch loss 1.7304, batch acc 0.8620
12:42:17.856   Training iter 350, batch loss 1.7353, batch acc 0.8606
12:42:18.375   Training iter 400, batch loss 1.7343, batch acc 0.8598
12:42:18.894   Training iter 450, batch loss 1.7328, batch acc 0.8642
12:42:19.401   Training iter 500, batch loss 1.7381, batch acc 0.8532
12:42:19.914   Training iter 550, batch loss 1.7359, batch acc 0.8600
12:42:20.423   Training iter 600, batch loss 1.7312, batch acc 0.8680
12:42:20.424 Training @ 345 epoch...
12:42:20.932   Training iter 50, batch loss 1.7332, batch acc 0.8586
12:42:21.422   Training iter 100, batch loss 1.7338, batch acc 0.8632
12:42:21.920   Training iter 150, batch loss 1.7348, batch acc 0.8582
12:42:22.430   Training iter 200, batch loss 1.7369, batch acc 0.8574
12:42:22.932   Training iter 250, batch loss 1.7339, batch acc 0.8572
12:42:23.431   Training iter 300, batch loss 1.7331, batch acc 0.8606
12:42:23.911   Training iter 350, batch loss 1.7320, batch acc 0.8674
12:42:24.401   Training iter 400, batch loss 1.7343, batch acc 0.8604
12:42:24.893   Training iter 450, batch loss 1.7320, batch acc 0.8618
12:42:25.411   Training iter 500, batch loss 1.7352, batch acc 0.8588
12:42:25.918   Training iter 550, batch loss 1.7362, batch acc 0.8560
12:42:26.403   Training iter 600, batch loss 1.7313, batch acc 0.8614
12:42:26.405 Testing @ 345 epoch...
12:42:26.447     Testing, total mean loss 1.72652, total acc 0.87020
12:42:26.447 Training @ 346 epoch...
12:42:26.916   Training iter 50, batch loss 1.7314, batch acc 0.8620
12:42:27.383   Training iter 100, batch loss 1.7383, batch acc 0.8520
12:42:27.848   Training iter 150, batch loss 1.7341, batch acc 0.8570
12:42:28.320   Training iter 200, batch loss 1.7310, batch acc 0.8642
12:42:28.783   Training iter 250, batch loss 1.7280, batch acc 0.8654
12:42:29.250   Training iter 300, batch loss 1.7261, batch acc 0.8670
12:42:29.707   Training iter 350, batch loss 1.7354, batch acc 0.8630
12:42:30.171   Training iter 400, batch loss 1.7368, batch acc 0.8598
12:42:30.639   Training iter 450, batch loss 1.7379, batch acc 0.8518
12:42:31.142   Training iter 500, batch loss 1.7351, batch acc 0.8622
12:42:31.651   Training iter 550, batch loss 1.7380, batch acc 0.8596
12:42:32.162   Training iter 600, batch loss 1.7345, batch acc 0.8584
12:42:32.163 Training @ 347 epoch...
12:42:32.669   Training iter 50, batch loss 1.7313, batch acc 0.8616
12:42:33.167   Training iter 100, batch loss 1.7340, batch acc 0.8578
12:42:33.663   Training iter 150, batch loss 1.7280, batch acc 0.8654
12:42:34.170   Training iter 200, batch loss 1.7342, batch acc 0.8604
12:42:34.679   Training iter 250, batch loss 1.7339, batch acc 0.8580
12:42:35.196   Training iter 300, batch loss 1.7344, batch acc 0.8646
12:42:35.697   Training iter 350, batch loss 1.7322, batch acc 0.8652
12:42:36.219   Training iter 400, batch loss 1.7357, batch acc 0.8546
12:42:36.754   Training iter 450, batch loss 1.7376, batch acc 0.8532
12:42:37.292   Training iter 500, batch loss 1.7341, batch acc 0.8624
12:42:37.829   Training iter 550, batch loss 1.7386, batch acc 0.8574
12:42:38.365   Training iter 600, batch loss 1.7324, batch acc 0.8636
12:42:38.366 Training @ 348 epoch...
12:42:38.899   Training iter 50, batch loss 1.7351, batch acc 0.8532
12:42:39.443   Training iter 100, batch loss 1.7286, batch acc 0.8662
12:42:39.988   Training iter 150, batch loss 1.7328, batch acc 0.8608
12:42:40.565   Training iter 200, batch loss 1.7320, batch acc 0.8646
12:42:41.131   Training iter 250, batch loss 1.7394, batch acc 0.8528
12:42:41.657   Training iter 300, batch loss 1.7381, batch acc 0.8618
12:42:42.164   Training iter 350, batch loss 1.7338, batch acc 0.8622
12:42:42.630   Training iter 400, batch loss 1.7323, batch acc 0.8620
12:42:43.107   Training iter 450, batch loss 1.7324, batch acc 0.8634
12:42:43.568   Training iter 500, batch loss 1.7371, batch acc 0.8534
12:42:44.039   Training iter 550, batch loss 1.7302, batch acc 0.8664
12:42:44.505   Training iter 600, batch loss 1.7342, batch acc 0.8572
12:42:44.507 Training @ 349 epoch...
12:42:44.998   Training iter 50, batch loss 1.7285, batch acc 0.8694
12:42:45.487   Training iter 100, batch loss 1.7311, batch acc 0.8652
12:42:45.964   Training iter 150, batch loss 1.7350, batch acc 0.8610
12:42:46.443   Training iter 200, batch loss 1.7313, batch acc 0.8658
12:42:46.934   Training iter 250, batch loss 1.7365, batch acc 0.8552
12:42:47.502   Training iter 300, batch loss 1.7333, batch acc 0.8588
12:42:48.097   Training iter 350, batch loss 1.7310, batch acc 0.8602
12:42:48.639   Training iter 400, batch loss 1.7401, batch acc 0.8456
12:42:49.143   Training iter 450, batch loss 1.7362, batch acc 0.8572
12:42:49.643   Training iter 500, batch loss 1.7346, batch acc 0.8636
12:42:50.149   Training iter 550, batch loss 1.7327, batch acc 0.8622
12:42:50.650   Training iter 600, batch loss 1.7356, batch acc 0.8576
12:42:50.652 Training @ 350 epoch...
12:42:51.151   Training iter 50, batch loss 1.7350, batch acc 0.8650
12:42:51.648   Training iter 100, batch loss 1.7338, batch acc 0.8580
12:42:52.152   Training iter 150, batch loss 1.7299, batch acc 0.8700
12:42:52.687   Training iter 200, batch loss 1.7299, batch acc 0.8620
12:42:53.253   Training iter 250, batch loss 1.7340, batch acc 0.8530
12:42:53.780   Training iter 300, batch loss 1.7302, batch acc 0.8618
12:42:54.289   Training iter 350, batch loss 1.7351, batch acc 0.8580
12:42:54.785   Training iter 400, batch loss 1.7374, batch acc 0.8506
12:42:55.295   Training iter 450, batch loss 1.7381, batch acc 0.8582
12:42:55.787   Training iter 500, batch loss 1.7331, batch acc 0.8646
12:42:56.297   Training iter 550, batch loss 1.7355, batch acc 0.8624
12:42:56.792   Training iter 600, batch loss 1.7340, batch acc 0.8600
12:42:56.794 Testing @ 350 epoch...
12:42:56.836     Testing, total mean loss 1.72645, total acc 0.87030
12:42:56.836 Training @ 351 epoch...
12:42:57.360   Training iter 50, batch loss 1.7360, batch acc 0.8596
12:42:57.870   Training iter 100, batch loss 1.7295, batch acc 0.8674
12:42:58.378   Training iter 150, batch loss 1.7391, batch acc 0.8492
12:42:58.852   Training iter 200, batch loss 1.7336, batch acc 0.8532
12:42:59.333   Training iter 250, batch loss 1.7390, batch acc 0.8520
12:42:59.835   Training iter 300, batch loss 1.7400, batch acc 0.8538
12:43:00.334   Training iter 350, batch loss 1.7261, batch acc 0.8734
12:43:00.816   Training iter 400, batch loss 1.7300, batch acc 0.8628
12:43:01.307   Training iter 450, batch loss 1.7336, batch acc 0.8610
12:43:01.852   Training iter 500, batch loss 1.7324, batch acc 0.8636
12:43:02.383   Training iter 550, batch loss 1.7315, batch acc 0.8680
12:43:02.889   Training iter 600, batch loss 1.7350, batch acc 0.8594
12:43:02.891 Training @ 352 epoch...
12:43:03.413   Training iter 50, batch loss 1.7322, batch acc 0.8640
12:43:03.945   Training iter 100, batch loss 1.7286, batch acc 0.8644
12:43:04.482   Training iter 150, batch loss 1.7331, batch acc 0.8602
12:43:05.017   Training iter 200, batch loss 1.7320, batch acc 0.8644
12:43:05.543   Training iter 250, batch loss 1.7343, batch acc 0.8604
12:43:06.073   Training iter 300, batch loss 1.7365, batch acc 0.8566
12:43:06.590   Training iter 350, batch loss 1.7294, batch acc 0.8668
12:43:07.088   Training iter 400, batch loss 1.7359, batch acc 0.8586
12:43:07.568   Training iter 450, batch loss 1.7351, batch acc 0.8578
12:43:08.045   Training iter 500, batch loss 1.7415, batch acc 0.8416
12:43:08.529   Training iter 550, batch loss 1.7339, batch acc 0.8656
12:43:09.020   Training iter 600, batch loss 1.7329, batch acc 0.8606
12:43:09.022 Training @ 353 epoch...
12:43:09.499   Training iter 50, batch loss 1.7355, batch acc 0.8622
12:43:09.992   Training iter 100, batch loss 1.7319, batch acc 0.8618
12:43:10.494   Training iter 150, batch loss 1.7312, batch acc 0.8654
12:43:10.984   Training iter 200, batch loss 1.7328, batch acc 0.8602
12:43:11.486   Training iter 250, batch loss 1.7367, batch acc 0.8536
12:43:11.994   Training iter 300, batch loss 1.7360, batch acc 0.8566
12:43:12.492   Training iter 350, batch loss 1.7327, batch acc 0.8584
12:43:12.983   Training iter 400, batch loss 1.7344, batch acc 0.8590
12:43:13.486   Training iter 450, batch loss 1.7358, batch acc 0.8562
12:43:13.976   Training iter 500, batch loss 1.7341, batch acc 0.8654
12:43:14.477   Training iter 550, batch loss 1.7303, batch acc 0.8668
12:43:14.992   Training iter 600, batch loss 1.7338, batch acc 0.8614
12:43:14.994 Training @ 354 epoch...
12:43:15.505   Training iter 50, batch loss 1.7339, batch acc 0.8578
12:43:15.996   Training iter 100, batch loss 1.7356, batch acc 0.8540
12:43:16.492   Training iter 150, batch loss 1.7334, batch acc 0.8638
12:43:16.985   Training iter 200, batch loss 1.7411, batch acc 0.8480
12:43:17.500   Training iter 250, batch loss 1.7368, batch acc 0.8588
12:43:17.999   Training iter 300, batch loss 1.7348, batch acc 0.8552
12:43:18.505   Training iter 350, batch loss 1.7318, batch acc 0.8648
12:43:18.987   Training iter 400, batch loss 1.7341, batch acc 0.8614
12:43:19.458   Training iter 450, batch loss 1.7304, batch acc 0.8664
12:43:19.913   Training iter 500, batch loss 1.7285, batch acc 0.8680
12:43:20.376   Training iter 550, batch loss 1.7339, batch acc 0.8600
12:43:20.827   Training iter 600, batch loss 1.7309, batch acc 0.8646
12:43:20.829 Training @ 355 epoch...
12:43:21.303   Training iter 50, batch loss 1.7340, batch acc 0.8584
12:43:21.769   Training iter 100, batch loss 1.7389, batch acc 0.8508
12:43:22.282   Training iter 150, batch loss 1.7338, batch acc 0.8608
12:43:22.798   Training iter 200, batch loss 1.7338, batch acc 0.8596
12:43:23.294   Training iter 250, batch loss 1.7369, batch acc 0.8568
12:43:23.788   Training iter 300, batch loss 1.7328, batch acc 0.8704
12:43:24.314   Training iter 350, batch loss 1.7324, batch acc 0.8596
12:43:24.858   Training iter 400, batch loss 1.7303, batch acc 0.8670
12:43:25.413   Training iter 450, batch loss 1.7294, batch acc 0.8636
12:43:25.962   Training iter 500, batch loss 1.7333, batch acc 0.8624
12:43:26.515   Training iter 550, batch loss 1.7351, batch acc 0.8586
12:43:27.052   Training iter 600, batch loss 1.7342, batch acc 0.8584
12:43:27.054 Testing @ 355 epoch...
12:43:27.096     Testing, total mean loss 1.72638, total acc 0.87020
12:43:27.096 Training @ 356 epoch...
12:43:27.630   Training iter 50, batch loss 1.7383, batch acc 0.8526
12:43:28.155   Training iter 100, batch loss 1.7304, batch acc 0.8672
12:43:28.683   Training iter 150, batch loss 1.7324, batch acc 0.8648
12:43:29.199   Training iter 200, batch loss 1.7381, batch acc 0.8536
12:43:29.678   Training iter 250, batch loss 1.7352, batch acc 0.8548
12:43:30.185   Training iter 300, batch loss 1.7348, batch acc 0.8592
12:43:30.653   Training iter 350, batch loss 1.7321, batch acc 0.8634
12:43:31.179   Training iter 400, batch loss 1.7286, batch acc 0.8676
12:43:31.701   Training iter 450, batch loss 1.7309, batch acc 0.8654
12:43:32.191   Training iter 500, batch loss 1.7331, batch acc 0.8586
12:43:32.680   Training iter 550, batch loss 1.7364, batch acc 0.8566
12:43:33.162   Training iter 600, batch loss 1.7344, batch acc 0.8582
12:43:33.163 Training @ 357 epoch...
12:43:33.646   Training iter 50, batch loss 1.7300, batch acc 0.8690
12:43:34.127   Training iter 100, batch loss 1.7338, batch acc 0.8594
12:43:34.636   Training iter 150, batch loss 1.7323, batch acc 0.8594
12:43:35.153   Training iter 200, batch loss 1.7351, batch acc 0.8610
12:43:35.645   Training iter 250, batch loss 1.7365, batch acc 0.8560
12:43:36.194   Training iter 300, batch loss 1.7344, batch acc 0.8628
12:43:36.742   Training iter 350, batch loss 1.7344, batch acc 0.8578
12:43:37.251   Training iter 400, batch loss 1.7346, batch acc 0.8606
12:43:37.745   Training iter 450, batch loss 1.7327, batch acc 0.8592
12:43:38.256   Training iter 500, batch loss 1.7362, batch acc 0.8510
12:43:38.764   Training iter 550, batch loss 1.7334, batch acc 0.8604
12:43:39.278   Training iter 600, batch loss 1.7313, batch acc 0.8662
12:43:39.280 Training @ 358 epoch...
12:43:39.792   Training iter 50, batch loss 1.7377, batch acc 0.8542
12:43:40.298   Training iter 100, batch loss 1.7328, batch acc 0.8708
12:43:40.806   Training iter 150, batch loss 1.7330, batch acc 0.8608
12:43:41.314   Training iter 200, batch loss 1.7305, batch acc 0.8686
12:43:41.815   Training iter 250, batch loss 1.7334, batch acc 0.8604
12:43:42.320   Training iter 300, batch loss 1.7364, batch acc 0.8594
12:43:42.828   Training iter 350, batch loss 1.7323, batch acc 0.8636
12:43:43.324   Training iter 400, batch loss 1.7279, batch acc 0.8670
12:43:43.820   Training iter 450, batch loss 1.7353, batch acc 0.8530
12:43:44.333   Training iter 500, batch loss 1.7305, batch acc 0.8592
12:43:44.848   Training iter 550, batch loss 1.7381, batch acc 0.8526
12:43:45.371   Training iter 600, batch loss 1.7365, batch acc 0.8576
12:43:45.372 Training @ 359 epoch...
12:43:45.889   Training iter 50, batch loss 1.7339, batch acc 0.8588
12:43:46.397   Training iter 100, batch loss 1.7324, batch acc 0.8610
12:43:46.910   Training iter 150, batch loss 1.7324, batch acc 0.8600
12:43:47.413   Training iter 200, batch loss 1.7370, batch acc 0.8514
12:43:47.914   Training iter 250, batch loss 1.7327, batch acc 0.8640
12:43:48.429   Training iter 300, batch loss 1.7321, batch acc 0.8654
12:43:48.941   Training iter 350, batch loss 1.7368, batch acc 0.8604
12:43:49.449   Training iter 400, batch loss 1.7304, batch acc 0.8666
12:43:49.960   Training iter 450, batch loss 1.7356, batch acc 0.8532
12:43:50.463   Training iter 500, batch loss 1.7342, batch acc 0.8574
12:43:50.974   Training iter 550, batch loss 1.7314, batch acc 0.8646
12:43:51.489   Training iter 600, batch loss 1.7355, batch acc 0.8626
12:43:51.490 Training @ 360 epoch...
12:43:51.995   Training iter 50, batch loss 1.7375, batch acc 0.8588
12:43:52.518   Training iter 100, batch loss 1.7318, batch acc 0.8614
12:43:53.015   Training iter 150, batch loss 1.7374, batch acc 0.8574
12:43:53.502   Training iter 200, batch loss 1.7328, batch acc 0.8598
12:43:53.962   Training iter 250, batch loss 1.7307, batch acc 0.8660
12:43:54.441   Training iter 300, batch loss 1.7351, batch acc 0.8566
12:43:54.924   Training iter 350, batch loss 1.7338, batch acc 0.8568
12:43:55.416   Training iter 400, batch loss 1.7349, batch acc 0.8606
12:43:55.875   Training iter 450, batch loss 1.7309, batch acc 0.8640
12:43:56.346   Training iter 500, batch loss 1.7333, batch acc 0.8632
12:43:56.816   Training iter 550, batch loss 1.7325, batch acc 0.8588
12:43:57.301   Training iter 600, batch loss 1.7334, batch acc 0.8616
12:43:57.303 Testing @ 360 epoch...
12:43:57.347     Testing, total mean loss 1.72631, total acc 0.87050
12:43:57.347 Training @ 361 epoch...
12:43:57.843   Training iter 50, batch loss 1.7334, batch acc 0.8606
12:43:58.344   Training iter 100, batch loss 1.7337, batch acc 0.8606
12:43:58.862   Training iter 150, batch loss 1.7299, batch acc 0.8660
12:43:59.390   Training iter 200, batch loss 1.7324, batch acc 0.8692
12:43:59.902   Training iter 250, batch loss 1.7336, batch acc 0.8586
12:44:00.435   Training iter 300, batch loss 1.7383, batch acc 0.8516
12:44:00.980   Training iter 350, batch loss 1.7344, batch acc 0.8644
12:44:01.548   Training iter 400, batch loss 1.7343, batch acc 0.8586
12:44:02.147   Training iter 450, batch loss 1.7346, batch acc 0.8588
12:44:02.750   Training iter 500, batch loss 1.7344, batch acc 0.8606
12:44:03.312   Training iter 550, batch loss 1.7330, batch acc 0.8598
12:44:03.814   Training iter 600, batch loss 1.7318, batch acc 0.8584
12:44:03.815 Training @ 362 epoch...
12:44:04.320   Training iter 50, batch loss 1.7334, batch acc 0.8682
12:44:04.819   Training iter 100, batch loss 1.7323, batch acc 0.8622
12:44:05.356   Training iter 150, batch loss 1.7343, batch acc 0.8614
12:44:05.875   Training iter 200, batch loss 1.7342, batch acc 0.8600
12:44:06.364   Training iter 250, batch loss 1.7304, batch acc 0.8686
12:44:06.841   Training iter 300, batch loss 1.7374, batch acc 0.8548
12:44:07.350   Training iter 350, batch loss 1.7302, batch acc 0.8650
12:44:07.870   Training iter 400, batch loss 1.7308, batch acc 0.8666
12:44:08.342   Training iter 450, batch loss 1.7347, batch acc 0.8564
12:44:08.797   Training iter 500, batch loss 1.7372, batch acc 0.8508
12:44:09.248   Training iter 550, batch loss 1.7353, batch acc 0.8574
12:44:09.700   Training iter 600, batch loss 1.7337, batch acc 0.8554
12:44:09.702 Training @ 363 epoch...
12:44:10.202   Training iter 50, batch loss 1.7362, batch acc 0.8578
12:44:10.700   Training iter 100, batch loss 1.7341, batch acc 0.8572
12:44:11.203   Training iter 150, batch loss 1.7315, batch acc 0.8584
12:44:11.689   Training iter 200, batch loss 1.7354, batch acc 0.8570
12:44:12.190   Training iter 250, batch loss 1.7350, batch acc 0.8590
12:44:12.680   Training iter 300, batch loss 1.7339, batch acc 0.8598
12:44:13.194   Training iter 350, batch loss 1.7306, batch acc 0.8672
12:44:13.701   Training iter 400, batch loss 1.7381, batch acc 0.8520
12:44:14.205   Training iter 450, batch loss 1.7322, batch acc 0.8684
12:44:14.714   Training iter 500, batch loss 1.7318, batch acc 0.8590
12:44:15.230   Training iter 550, batch loss 1.7312, batch acc 0.8686
12:44:15.742   Training iter 600, batch loss 1.7337, batch acc 0.8590
12:44:15.744 Training @ 364 epoch...
12:44:16.262   Training iter 50, batch loss 1.7309, batch acc 0.8646
12:44:16.757   Training iter 100, batch loss 1.7315, batch acc 0.8612
12:44:17.258   Training iter 150, batch loss 1.7307, batch acc 0.8620
12:44:17.758   Training iter 200, batch loss 1.7368, batch acc 0.8580
12:44:18.259   Training iter 250, batch loss 1.7355, batch acc 0.8622
12:44:18.752   Training iter 300, batch loss 1.7298, batch acc 0.8650
12:44:19.241   Training iter 350, batch loss 1.7330, batch acc 0.8552
12:44:19.741   Training iter 400, batch loss 1.7319, batch acc 0.8658
12:44:20.243   Training iter 450, batch loss 1.7355, batch acc 0.8612
12:44:20.806   Training iter 500, batch loss 1.7325, batch acc 0.8608
12:44:21.386   Training iter 550, batch loss 1.7383, batch acc 0.8578
12:44:21.953   Training iter 600, batch loss 1.7372, batch acc 0.8516
12:44:21.955 Training @ 365 epoch...
12:44:22.525   Training iter 50, batch loss 1.7375, batch acc 0.8532
12:44:23.080   Training iter 100, batch loss 1.7391, batch acc 0.8542
12:44:23.631   Training iter 150, batch loss 1.7361, batch acc 0.8586
12:44:24.185   Training iter 200, batch loss 1.7322, batch acc 0.8634
12:44:24.739   Training iter 250, batch loss 1.7282, batch acc 0.8718
12:44:25.321   Training iter 300, batch loss 1.7314, batch acc 0.8586
12:44:25.835   Training iter 350, batch loss 1.7332, batch acc 0.8624
12:44:26.363   Training iter 400, batch loss 1.7356, batch acc 0.8622
12:44:26.897   Training iter 450, batch loss 1.7324, batch acc 0.8602
12:44:27.449   Training iter 500, batch loss 1.7320, batch acc 0.8572
12:44:27.971   Training iter 550, batch loss 1.7339, batch acc 0.8606
12:44:28.493   Training iter 600, batch loss 1.7317, batch acc 0.8636
12:44:28.495 Testing @ 365 epoch...
12:44:28.540     Testing, total mean loss 1.72625, total acc 0.87030
12:44:28.541 Training @ 366 epoch...
12:44:29.054   Training iter 50, batch loss 1.7302, batch acc 0.8638
12:44:29.571   Training iter 100, batch loss 1.7336, batch acc 0.8636
12:44:30.087   Training iter 150, batch loss 1.7343, batch acc 0.8566
12:44:30.582   Training iter 200, batch loss 1.7342, batch acc 0.8592
12:44:31.115   Training iter 250, batch loss 1.7311, batch acc 0.8636
12:44:31.640   Training iter 300, batch loss 1.7290, batch acc 0.8688
12:44:32.151   Training iter 350, batch loss 1.7378, batch acc 0.8534
12:44:32.650   Training iter 400, batch loss 1.7320, batch acc 0.8630
12:44:33.152   Training iter 450, batch loss 1.7395, batch acc 0.8554
12:44:33.645   Training iter 500, batch loss 1.7350, batch acc 0.8586
12:44:34.142   Training iter 550, batch loss 1.7294, batch acc 0.8660
12:44:34.661   Training iter 600, batch loss 1.7369, batch acc 0.8530
12:44:34.662 Training @ 367 epoch...
12:44:35.193   Training iter 50, batch loss 1.7373, batch acc 0.8554
12:44:35.711   Training iter 100, batch loss 1.7313, batch acc 0.8634
12:44:36.217   Training iter 150, batch loss 1.7376, batch acc 0.8548
12:44:36.713   Training iter 200, batch loss 1.7349, batch acc 0.8570
12:44:37.253   Training iter 250, batch loss 1.7345, batch acc 0.8556
12:44:37.760   Training iter 300, batch loss 1.7292, batch acc 0.8644
12:44:38.259   Training iter 350, batch loss 1.7301, batch acc 0.8710
12:44:38.750   Training iter 400, batch loss 1.7315, batch acc 0.8664
12:44:39.264   Training iter 450, batch loss 1.7375, batch acc 0.8544
12:44:39.789   Training iter 500, batch loss 1.7360, batch acc 0.8570
12:44:40.303   Training iter 550, batch loss 1.7304, batch acc 0.8580
12:44:40.796   Training iter 600, batch loss 1.7326, batch acc 0.8674
12:44:40.797 Training @ 368 epoch...
12:44:41.323   Training iter 50, batch loss 1.7342, batch acc 0.8574
12:44:41.860   Training iter 100, batch loss 1.7309, batch acc 0.8658
12:44:42.391   Training iter 150, batch loss 1.7322, batch acc 0.8628
12:44:42.926   Training iter 200, batch loss 1.7360, batch acc 0.8566
12:44:43.444   Training iter 250, batch loss 1.7394, batch acc 0.8526
12:44:43.964   Training iter 300, batch loss 1.7331, batch acc 0.8582
12:44:44.482   Training iter 350, batch loss 1.7323, batch acc 0.8630
12:44:44.996   Training iter 400, batch loss 1.7327, batch acc 0.8646
12:44:45.508   Training iter 450, batch loss 1.7338, batch acc 0.8578
12:44:46.025   Training iter 500, batch loss 1.7298, batch acc 0.8664
12:44:46.547   Training iter 550, batch loss 1.7354, batch acc 0.8546
12:44:47.057   Training iter 600, batch loss 1.7331, batch acc 0.8632
12:44:47.059 Training @ 369 epoch...
12:44:47.579   Training iter 50, batch loss 1.7287, batch acc 0.8636
12:44:48.100   Training iter 100, batch loss 1.7382, batch acc 0.8584
12:44:48.654   Training iter 150, batch loss 1.7329, batch acc 0.8592
12:44:49.214   Training iter 200, batch loss 1.7352, batch acc 0.8586
12:44:49.771   Training iter 250, batch loss 1.7331, batch acc 0.8586
12:44:50.306   Training iter 300, batch loss 1.7323, batch acc 0.8634
12:44:50.804   Training iter 350, batch loss 1.7319, batch acc 0.8634
12:44:51.299   Training iter 400, batch loss 1.7318, batch acc 0.8608
12:44:51.800   Training iter 450, batch loss 1.7375, batch acc 0.8540
12:44:52.336   Training iter 500, batch loss 1.7337, batch acc 0.8634
12:44:52.865   Training iter 550, batch loss 1.7335, batch acc 0.8582
12:44:53.357   Training iter 600, batch loss 1.7340, batch acc 0.8644
12:44:53.359 Training @ 370 epoch...
12:44:53.868   Training iter 50, batch loss 1.7322, batch acc 0.8626
12:44:54.371   Training iter 100, batch loss 1.7346, batch acc 0.8602
12:44:54.872   Training iter 150, batch loss 1.7370, batch acc 0.8566
12:44:55.394   Training iter 200, batch loss 1.7312, batch acc 0.8644
12:44:55.888   Training iter 250, batch loss 1.7329, batch acc 0.8580
12:44:56.386   Training iter 300, batch loss 1.7377, batch acc 0.8562
12:44:56.886   Training iter 350, batch loss 1.7312, batch acc 0.8608
12:44:57.394   Training iter 400, batch loss 1.7330, batch acc 0.8568
12:44:57.912   Training iter 450, batch loss 1.7350, batch acc 0.8614
12:44:58.405   Training iter 500, batch loss 1.7328, batch acc 0.8642
12:44:58.880   Training iter 550, batch loss 1.7354, batch acc 0.8600
12:44:59.366   Training iter 600, batch loss 1.7297, batch acc 0.8674
12:44:59.367 Testing @ 370 epoch...
12:44:59.410     Testing, total mean loss 1.72619, total acc 0.87050
12:44:59.410 Training @ 371 epoch...
12:44:59.937   Training iter 50, batch loss 1.7342, batch acc 0.8606
12:45:00.469   Training iter 100, batch loss 1.7361, batch acc 0.8562
12:45:00.981   Training iter 150, batch loss 1.7344, batch acc 0.8612
12:45:01.551   Training iter 200, batch loss 1.7304, batch acc 0.8624
12:45:02.087   Training iter 250, batch loss 1.7324, batch acc 0.8612
12:45:02.580   Training iter 300, batch loss 1.7321, batch acc 0.8574
12:45:03.127   Training iter 350, batch loss 1.7365, batch acc 0.8570
12:45:03.638   Training iter 400, batch loss 1.7368, batch acc 0.8562
12:45:04.129   Training iter 450, batch loss 1.7354, batch acc 0.8590
12:45:04.620   Training iter 500, batch loss 1.7306, batch acc 0.8610
12:45:05.160   Training iter 550, batch loss 1.7313, batch acc 0.8644
12:45:05.654   Training iter 600, batch loss 1.7322, batch acc 0.8682
12:45:05.655 Training @ 372 epoch...
12:45:06.141   Training iter 50, batch loss 1.7320, batch acc 0.8658
12:45:06.593   Training iter 100, batch loss 1.7335, batch acc 0.8618
12:45:07.082   Training iter 150, batch loss 1.7325, batch acc 0.8622
12:45:07.585   Training iter 200, batch loss 1.7305, batch acc 0.8596
12:45:08.035   Training iter 250, batch loss 1.7322, batch acc 0.8650
12:45:08.472   Training iter 300, batch loss 1.7360, batch acc 0.8598
12:45:08.921   Training iter 350, batch loss 1.7329, batch acc 0.8598
12:45:09.390   Training iter 400, batch loss 1.7336, batch acc 0.8608
12:45:09.863   Training iter 450, batch loss 1.7402, batch acc 0.8498
12:45:10.321   Training iter 500, batch loss 1.7340, batch acc 0.8576
12:45:10.784   Training iter 550, batch loss 1.7340, batch acc 0.8598
12:45:11.279   Training iter 600, batch loss 1.7309, batch acc 0.8652
12:45:11.281 Training @ 373 epoch...
12:45:11.781   Training iter 50, batch loss 1.7334, batch acc 0.8616
12:45:12.305   Training iter 100, batch loss 1.7355, batch acc 0.8600
12:45:12.821   Training iter 150, batch loss 1.7371, batch acc 0.8554
12:45:13.335   Training iter 200, batch loss 1.7309, batch acc 0.8614
12:45:13.883   Training iter 250, batch loss 1.7340, batch acc 0.8622
12:45:14.398   Training iter 300, batch loss 1.7306, batch acc 0.8626
12:45:14.896   Training iter 350, batch loss 1.7359, batch acc 0.8566
12:45:15.418   Training iter 400, batch loss 1.7341, batch acc 0.8642
12:45:15.938   Training iter 450, batch loss 1.7307, batch acc 0.8618
12:45:16.479   Training iter 500, batch loss 1.7361, batch acc 0.8544
12:45:17.009   Training iter 550, batch loss 1.7321, batch acc 0.8604
12:45:17.554   Training iter 600, batch loss 1.7317, batch acc 0.8672
12:45:17.555 Training @ 374 epoch...
12:45:18.129   Training iter 50, batch loss 1.7381, batch acc 0.8532
12:45:18.749   Training iter 100, batch loss 1.7284, batch acc 0.8702
12:45:19.278   Training iter 150, batch loss 1.7337, batch acc 0.8652
12:45:19.813   Training iter 200, batch loss 1.7335, batch acc 0.8624
12:45:20.356   Training iter 250, batch loss 1.7348, batch acc 0.8600
12:45:20.892   Training iter 300, batch loss 1.7368, batch acc 0.8544
12:45:21.408   Training iter 350, batch loss 1.7337, batch acc 0.8558
12:45:21.916   Training iter 400, batch loss 1.7333, batch acc 0.8598
12:45:22.434   Training iter 450, batch loss 1.7307, batch acc 0.8652
12:45:22.945   Training iter 500, batch loss 1.7321, batch acc 0.8650
12:45:23.455   Training iter 550, batch loss 1.7331, batch acc 0.8596
12:45:23.977   Training iter 600, batch loss 1.7339, batch acc 0.8544
12:45:23.979 Training @ 375 epoch...
12:45:24.485   Training iter 50, batch loss 1.7345, batch acc 0.8610
12:45:25.001   Training iter 100, batch loss 1.7340, batch acc 0.8624
12:45:25.529   Training iter 150, batch loss 1.7350, batch acc 0.8558
12:45:26.049   Training iter 200, batch loss 1.7269, batch acc 0.8622
12:45:26.551   Training iter 250, batch loss 1.7299, batch acc 0.8652
12:45:27.058   Training iter 300, batch loss 1.7309, batch acc 0.8670
12:45:27.573   Training iter 350, batch loss 1.7362, batch acc 0.8590
12:45:28.086   Training iter 400, batch loss 1.7365, batch acc 0.8530
12:45:28.593   Training iter 450, batch loss 1.7330, batch acc 0.8606
12:45:29.092   Training iter 500, batch loss 1.7334, batch acc 0.8644
12:45:29.589   Training iter 550, batch loss 1.7384, batch acc 0.8526
12:45:30.084   Training iter 600, batch loss 1.7333, batch acc 0.8672
12:45:30.085 Testing @ 375 epoch...
12:45:30.128     Testing, total mean loss 1.72613, total acc 0.87070
12:45:30.128 Training @ 376 epoch...
12:45:30.629   Training iter 50, batch loss 1.7323, batch acc 0.8606
12:45:31.130   Training iter 100, batch loss 1.7333, batch acc 0.8686
12:45:31.668   Training iter 150, batch loss 1.7336, batch acc 0.8564
12:45:32.228   Training iter 200, batch loss 1.7321, batch acc 0.8630
12:45:32.782   Training iter 250, batch loss 1.7344, batch acc 0.8584
12:45:33.319   Training iter 300, batch loss 1.7343, batch acc 0.8562
12:45:33.810   Training iter 350, batch loss 1.7375, batch acc 0.8482
12:45:34.302   Training iter 400, batch loss 1.7366, batch acc 0.8624
12:45:34.794   Training iter 450, batch loss 1.7335, batch acc 0.8582
12:45:35.313   Training iter 500, batch loss 1.7321, batch acc 0.8666
12:45:35.816   Training iter 550, batch loss 1.7320, batch acc 0.8606
12:45:36.332   Training iter 600, batch loss 1.7300, batch acc 0.8682
12:45:36.334 Training @ 377 epoch...
12:45:36.859   Training iter 50, batch loss 1.7340, batch acc 0.8548
12:45:37.372   Training iter 100, batch loss 1.7349, batch acc 0.8578
12:45:37.883   Training iter 150, batch loss 1.7343, batch acc 0.8564
12:45:38.370   Training iter 200, batch loss 1.7402, batch acc 0.8494
12:45:38.861   Training iter 250, batch loss 1.7391, batch acc 0.8510
12:45:39.363   Training iter 300, batch loss 1.7323, batch acc 0.8598
12:45:39.824   Training iter 350, batch loss 1.7306, batch acc 0.8666
12:45:40.299   Training iter 400, batch loss 1.7365, batch acc 0.8554
12:45:40.754   Training iter 450, batch loss 1.7299, batch acc 0.8694
12:45:41.229   Training iter 500, batch loss 1.7280, batch acc 0.8720
12:45:41.690   Training iter 550, batch loss 1.7299, batch acc 0.8700
12:45:42.157   Training iter 600, batch loss 1.7320, batch acc 0.8642
12:45:42.159 Training @ 378 epoch...
12:45:42.627   Training iter 50, batch loss 1.7299, batch acc 0.8676
12:45:43.097   Training iter 100, batch loss 1.7383, batch acc 0.8544
12:45:43.562   Training iter 150, batch loss 1.7352, batch acc 0.8598
12:45:44.040   Training iter 200, batch loss 1.7312, batch acc 0.8604
12:45:44.510   Training iter 250, batch loss 1.7321, batch acc 0.8690
12:45:44.985   Training iter 300, batch loss 1.7340, batch acc 0.8590
12:45:45.460   Training iter 350, batch loss 1.7336, batch acc 0.8574
12:45:45.937   Training iter 400, batch loss 1.7340, batch acc 0.8554
12:45:46.411   Training iter 450, batch loss 1.7318, batch acc 0.8636
12:45:46.880   Training iter 500, batch loss 1.7342, batch acc 0.8620
12:45:47.385   Training iter 550, batch loss 1.7299, batch acc 0.8634
12:45:47.907   Training iter 600, batch loss 1.7371, batch acc 0.8568
12:45:47.909 Training @ 379 epoch...
12:45:48.452   Training iter 50, batch loss 1.7351, batch acc 0.8578
12:45:48.983   Training iter 100, batch loss 1.7313, batch acc 0.8644
12:45:49.536   Training iter 150, batch loss 1.7323, batch acc 0.8572
12:45:50.057   Training iter 200, batch loss 1.7374, batch acc 0.8570
12:45:50.582   Training iter 250, batch loss 1.7309, batch acc 0.8648
12:45:51.094   Training iter 300, batch loss 1.7375, batch acc 0.8614
12:45:51.602   Training iter 350, batch loss 1.7379, batch acc 0.8524
12:45:52.126   Training iter 400, batch loss 1.7298, batch acc 0.8680
12:45:52.647   Training iter 450, batch loss 1.7314, batch acc 0.8644
12:45:53.177   Training iter 500, batch loss 1.7326, batch acc 0.8600
12:45:53.686   Training iter 550, batch loss 1.7317, batch acc 0.8608
12:45:54.167   Training iter 600, batch loss 1.7334, batch acc 0.8568
12:45:54.168 Training @ 380 epoch...
12:45:54.661   Training iter 50, batch loss 1.7329, batch acc 0.8538
12:45:55.145   Training iter 100, batch loss 1.7361, batch acc 0.8602
12:45:55.621   Training iter 150, batch loss 1.7323, batch acc 0.8634
12:45:56.107   Training iter 200, batch loss 1.7377, batch acc 0.8540
12:45:56.595   Training iter 250, batch loss 1.7341, batch acc 0.8606
12:45:57.099   Training iter 300, batch loss 1.7342, batch acc 0.8636
12:45:57.596   Training iter 350, batch loss 1.7309, batch acc 0.8630
12:45:58.088   Training iter 400, batch loss 1.7344, batch acc 0.8570
12:45:58.578   Training iter 450, batch loss 1.7356, batch acc 0.8612
12:45:59.100   Training iter 500, batch loss 1.7291, batch acc 0.8676
12:45:59.612   Training iter 550, batch loss 1.7297, batch acc 0.8664
12:46:00.128   Training iter 600, batch loss 1.7343, batch acc 0.8588
12:46:00.130 Testing @ 380 epoch...
12:46:00.174     Testing, total mean loss 1.72608, total acc 0.87060
12:46:00.174 Training @ 381 epoch...
12:46:00.699   Training iter 50, batch loss 1.7318, batch acc 0.8634
12:46:01.212   Training iter 100, batch loss 1.7331, batch acc 0.8608
12:46:01.705   Training iter 150, batch loss 1.7389, batch acc 0.8564
12:46:02.228   Training iter 200, batch loss 1.7341, batch acc 0.8550
12:46:02.704   Training iter 250, batch loss 1.7378, batch acc 0.8516
12:46:03.221   Training iter 300, batch loss 1.7311, batch acc 0.8660
12:46:03.718   Training iter 350, batch loss 1.7304, batch acc 0.8652
12:46:04.205   Training iter 400, batch loss 1.7373, batch acc 0.8548
12:46:04.705   Training iter 450, batch loss 1.7339, batch acc 0.8568
12:46:05.197   Training iter 500, batch loss 1.7320, batch acc 0.8630
12:46:05.676   Training iter 550, batch loss 1.7313, batch acc 0.8640
12:46:06.155   Training iter 600, batch loss 1.7294, batch acc 0.8714
12:46:06.156 Training @ 382 epoch...
12:46:06.669   Training iter 50, batch loss 1.7351, batch acc 0.8596
12:46:07.172   Training iter 100, batch loss 1.7356, batch acc 0.8586
12:46:07.676   Training iter 150, batch loss 1.7305, batch acc 0.8606
12:46:08.185   Training iter 200, batch loss 1.7317, batch acc 0.8626
12:46:08.683   Training iter 250, batch loss 1.7351, batch acc 0.8570
12:46:09.181   Training iter 300, batch loss 1.7340, batch acc 0.8626
12:46:09.690   Training iter 350, batch loss 1.7307, batch acc 0.8644
12:46:10.210   Training iter 400, batch loss 1.7338, batch acc 0.8594
12:46:10.713   Training iter 450, batch loss 1.7335, batch acc 0.8600
12:46:11.213   Training iter 500, batch loss 1.7321, batch acc 0.8654
12:46:11.703   Training iter 550, batch loss 1.7327, batch acc 0.8608
12:46:12.214   Training iter 600, batch loss 1.7362, batch acc 0.8554
12:46:12.216 Training @ 383 epoch...
12:46:12.742   Training iter 50, batch loss 1.7341, batch acc 0.8610
12:46:13.270   Training iter 100, batch loss 1.7333, batch acc 0.8554
12:46:13.778   Training iter 150, batch loss 1.7331, batch acc 0.8596
12:46:14.266   Training iter 200, batch loss 1.7311, batch acc 0.8610
12:46:14.745   Training iter 250, batch loss 1.7332, batch acc 0.8576
12:46:15.226   Training iter 300, batch loss 1.7388, batch acc 0.8534
12:46:15.705   Training iter 350, batch loss 1.7315, batch acc 0.8642
12:46:16.175   Training iter 400, batch loss 1.7349, batch acc 0.8642
12:46:16.672   Training iter 450, batch loss 1.7333, batch acc 0.8618
12:46:17.173   Training iter 500, batch loss 1.7353, batch acc 0.8598
12:46:17.689   Training iter 550, batch loss 1.7291, batch acc 0.8720
12:46:18.189   Training iter 600, batch loss 1.7331, batch acc 0.8598
12:46:18.191 Training @ 384 epoch...
12:46:18.701   Training iter 50, batch loss 1.7329, batch acc 0.8594
12:46:19.214   Training iter 100, batch loss 1.7318, batch acc 0.8596
12:46:19.764   Training iter 150, batch loss 1.7375, batch acc 0.8576
12:46:20.315   Training iter 200, batch loss 1.7334, batch acc 0.8650
12:46:20.858   Training iter 250, batch loss 1.7331, batch acc 0.8590
12:46:21.380   Training iter 300, batch loss 1.7312, batch acc 0.8638
12:46:21.899   Training iter 350, batch loss 1.7344, batch acc 0.8582
12:46:22.434   Training iter 400, batch loss 1.7313, batch acc 0.8706
12:46:22.952   Training iter 450, batch loss 1.7360, batch acc 0.8508
12:46:23.468   Training iter 500, batch loss 1.7308, batch acc 0.8666
12:46:23.985   Training iter 550, batch loss 1.7341, batch acc 0.8616
12:46:24.519   Training iter 600, batch loss 1.7343, batch acc 0.8580
12:46:24.521 Training @ 385 epoch...
12:46:25.059   Training iter 50, batch loss 1.7349, batch acc 0.8590
12:46:25.588   Training iter 100, batch loss 1.7366, batch acc 0.8542
12:46:26.118   Training iter 150, batch loss 1.7307, batch acc 0.8706
12:46:26.642   Training iter 200, batch loss 1.7346, batch acc 0.8620
12:46:27.169   Training iter 250, batch loss 1.7314, batch acc 0.8612
12:46:27.692   Training iter 300, batch loss 1.7313, batch acc 0.8622
12:46:28.225   Training iter 350, batch loss 1.7315, batch acc 0.8632
12:46:28.743   Training iter 400, batch loss 1.7319, batch acc 0.8602
12:46:29.277   Training iter 450, batch loss 1.7351, batch acc 0.8582
12:46:29.791   Training iter 500, batch loss 1.7339, batch acc 0.8578
12:46:30.325   Training iter 550, batch loss 1.7354, batch acc 0.8556
12:46:30.892   Training iter 600, batch loss 1.7335, batch acc 0.8638
12:46:30.894 Testing @ 385 epoch...
12:46:30.937     Testing, total mean loss 1.72602, total acc 0.87040
12:46:30.937 Training @ 386 epoch...
12:46:31.508   Training iter 50, batch loss 1.7362, batch acc 0.8578
12:46:32.095   Training iter 100, batch loss 1.7334, batch acc 0.8574
12:46:32.589   Training iter 150, batch loss 1.7285, batch acc 0.8694
12:46:33.101   Training iter 200, batch loss 1.7335, batch acc 0.8566
12:46:33.609   Training iter 250, batch loss 1.7379, batch acc 0.8532
12:46:34.112   Training iter 300, batch loss 1.7340, batch acc 0.8596
12:46:34.636   Training iter 350, batch loss 1.7344, batch acc 0.8562
12:46:35.155   Training iter 400, batch loss 1.7304, batch acc 0.8704
12:46:35.677   Training iter 450, batch loss 1.7333, batch acc 0.8648
12:46:36.232   Training iter 500, batch loss 1.7314, batch acc 0.8622
12:46:36.778   Training iter 550, batch loss 1.7364, batch acc 0.8522
12:46:37.318   Training iter 600, batch loss 1.7313, batch acc 0.8654
12:46:37.320 Training @ 387 epoch...
12:46:37.862   Training iter 50, batch loss 1.7298, batch acc 0.8622
12:46:38.410   Training iter 100, batch loss 1.7350, batch acc 0.8560
12:46:38.949   Training iter 150, batch loss 1.7369, batch acc 0.8550
12:46:39.492   Training iter 200, batch loss 1.7341, batch acc 0.8578
12:46:40.040   Training iter 250, batch loss 1.7323, batch acc 0.8636
12:46:40.616   Training iter 300, batch loss 1.7311, batch acc 0.8620
12:46:41.166   Training iter 350, batch loss 1.7348, batch acc 0.8578
12:46:41.667   Training iter 400, batch loss 1.7358, batch acc 0.8658
12:46:42.157   Training iter 450, batch loss 1.7367, batch acc 0.8580
12:46:42.648   Training iter 500, batch loss 1.7331, batch acc 0.8604
12:46:43.170   Training iter 550, batch loss 1.7321, batch acc 0.8618
12:46:43.664   Training iter 600, batch loss 1.7287, batch acc 0.8690
12:46:43.666 Training @ 388 epoch...
12:46:44.169   Training iter 50, batch loss 1.7315, batch acc 0.8692
12:46:44.647   Training iter 100, batch loss 1.7322, batch acc 0.8624
12:46:45.132   Training iter 150, batch loss 1.7343, batch acc 0.8582
12:46:45.625   Training iter 200, batch loss 1.7296, batch acc 0.8608
12:46:46.119   Training iter 250, batch loss 1.7320, batch acc 0.8606
12:46:46.619   Training iter 300, batch loss 1.7348, batch acc 0.8588
12:46:47.109   Training iter 350, batch loss 1.7357, batch acc 0.8560
12:46:47.603   Training iter 400, batch loss 1.7292, batch acc 0.8658
12:46:48.111   Training iter 450, batch loss 1.7395, batch acc 0.8554
12:46:48.598   Training iter 500, batch loss 1.7362, batch acc 0.8590
12:46:49.089   Training iter 550, batch loss 1.7324, batch acc 0.8628
12:46:49.606   Training iter 600, batch loss 1.7329, batch acc 0.8594
12:46:49.608 Training @ 389 epoch...
12:46:50.143   Training iter 50, batch loss 1.7324, batch acc 0.8624
12:46:50.663   Training iter 100, batch loss 1.7350, batch acc 0.8542
12:46:51.184   Training iter 150, batch loss 1.7354, batch acc 0.8566
12:46:51.694   Training iter 200, batch loss 1.7299, batch acc 0.8656
12:46:52.218   Training iter 250, batch loss 1.7341, batch acc 0.8592
12:46:52.741   Training iter 300, batch loss 1.7317, batch acc 0.8644
12:46:53.275   Training iter 350, batch loss 1.7345, batch acc 0.8566
12:46:53.802   Training iter 400, batch loss 1.7337, batch acc 0.8616
12:46:54.305   Training iter 450, batch loss 1.7298, batch acc 0.8626
12:46:54.817   Training iter 500, batch loss 1.7332, batch acc 0.8606
12:46:55.375   Training iter 550, batch loss 1.7351, batch acc 0.8588
12:46:55.909   Training iter 600, batch loss 1.7354, batch acc 0.8674
12:46:55.911 Training @ 390 epoch...
12:46:56.471   Training iter 50, batch loss 1.7276, batch acc 0.8672
12:46:57.003   Training iter 100, batch loss 1.7325, batch acc 0.8622
12:46:57.528   Training iter 150, batch loss 1.7330, batch acc 0.8632
12:46:58.023   Training iter 200, batch loss 1.7342, batch acc 0.8542
12:46:58.527   Training iter 250, batch loss 1.7318, batch acc 0.8694
12:46:59.038   Training iter 300, batch loss 1.7376, batch acc 0.8508
12:46:59.542   Training iter 350, batch loss 1.7367, batch acc 0.8576
12:47:00.073   Training iter 400, batch loss 1.7327, batch acc 0.8578
12:47:00.619   Training iter 450, batch loss 1.7361, batch acc 0.8582
12:47:01.125   Training iter 500, batch loss 1.7340, batch acc 0.8600
12:47:01.683   Training iter 550, batch loss 1.7272, batch acc 0.8690
12:47:02.269   Training iter 600, batch loss 1.7365, batch acc 0.8586
12:47:02.271 Testing @ 390 epoch...
12:47:02.316     Testing, total mean loss 1.72597, total acc 0.87060
12:47:02.316 Training @ 391 epoch...
12:47:02.856   Training iter 50, batch loss 1.7335, batch acc 0.8612
12:47:03.413   Training iter 100, batch loss 1.7342, batch acc 0.8540
12:47:03.962   Training iter 150, batch loss 1.7388, batch acc 0.8568
12:47:04.499   Training iter 200, batch loss 1.7287, batch acc 0.8670
12:47:05.008   Training iter 250, batch loss 1.7334, batch acc 0.8626
12:47:05.524   Training iter 300, batch loss 1.7317, batch acc 0.8646
12:47:06.046   Training iter 350, batch loss 1.7330, batch acc 0.8620
12:47:06.542   Training iter 400, batch loss 1.7359, batch acc 0.8536
12:47:07.035   Training iter 450, batch loss 1.7319, batch acc 0.8580
12:47:07.544   Training iter 500, batch loss 1.7327, batch acc 0.8598
12:47:08.063   Training iter 550, batch loss 1.7341, batch acc 0.8650
12:47:08.570   Training iter 600, batch loss 1.7320, batch acc 0.8630
12:47:08.572 Training @ 392 epoch...
12:47:09.086   Training iter 50, batch loss 1.7411, batch acc 0.8522
12:47:09.625   Training iter 100, batch loss 1.7299, batch acc 0.8656
12:47:10.181   Training iter 150, batch loss 1.7301, batch acc 0.8676
12:47:10.717   Training iter 200, batch loss 1.7276, batch acc 0.8640
12:47:11.262   Training iter 250, batch loss 1.7368, batch acc 0.8532
12:47:11.804   Training iter 300, batch loss 1.7329, batch acc 0.8618
12:47:12.359   Training iter 350, batch loss 1.7328, batch acc 0.8636
12:47:12.950   Training iter 400, batch loss 1.7378, batch acc 0.8538
12:47:13.541   Training iter 450, batch loss 1.7309, batch acc 0.8674
12:47:14.123   Training iter 500, batch loss 1.7371, batch acc 0.8558
12:47:14.693   Training iter 550, batch loss 1.7340, batch acc 0.8556
12:47:15.255   Training iter 600, batch loss 1.7289, batch acc 0.8678
12:47:15.257 Training @ 393 epoch...
12:47:15.823   Training iter 50, batch loss 1.7368, batch acc 0.8616
12:47:16.378   Training iter 100, batch loss 1.7280, batch acc 0.8664
12:47:16.924   Training iter 150, batch loss 1.7338, batch acc 0.8572
12:47:17.462   Training iter 200, batch loss 1.7331, batch acc 0.8570
12:47:17.983   Training iter 250, batch loss 1.7381, batch acc 0.8542
12:47:18.498   Training iter 300, batch loss 1.7331, batch acc 0.8644
12:47:18.999   Training iter 350, batch loss 1.7307, batch acc 0.8632
12:47:19.498   Training iter 400, batch loss 1.7341, batch acc 0.8604
12:47:19.990   Training iter 450, batch loss 1.7325, batch acc 0.8636
12:47:20.476   Training iter 500, batch loss 1.7333, batch acc 0.8616
12:47:20.975   Training iter 550, batch loss 1.7328, batch acc 0.8584
12:47:21.472   Training iter 600, batch loss 1.7335, batch acc 0.8644
12:47:21.474 Training @ 394 epoch...
12:47:21.987   Training iter 50, batch loss 1.7372, batch acc 0.8580
12:47:22.487   Training iter 100, batch loss 1.7376, batch acc 0.8574
12:47:22.990   Training iter 150, batch loss 1.7345, batch acc 0.8556
12:47:23.511   Training iter 200, batch loss 1.7341, batch acc 0.8620
12:47:24.012   Training iter 250, batch loss 1.7286, batch acc 0.8664
12:47:24.526   Training iter 300, batch loss 1.7331, batch acc 0.8628
12:47:25.016   Training iter 350, batch loss 1.7312, batch acc 0.8620
12:47:25.527   Training iter 400, batch loss 1.7315, batch acc 0.8672
12:47:26.021   Training iter 450, batch loss 1.7312, batch acc 0.8618
12:47:26.528   Training iter 500, batch loss 1.7345, batch acc 0.8564
12:47:27.085   Training iter 550, batch loss 1.7312, batch acc 0.8600
12:47:27.657   Training iter 600, batch loss 1.7348, batch acc 0.8604
12:47:27.659 Training @ 395 epoch...
12:47:28.262   Training iter 50, batch loss 1.7300, batch acc 0.8642
12:47:28.839   Training iter 100, batch loss 1.7344, batch acc 0.8620
12:47:29.406   Training iter 150, batch loss 1.7292, batch acc 0.8658
12:47:29.880   Training iter 200, batch loss 1.7368, batch acc 0.8534
12:47:30.366   Training iter 250, batch loss 1.7309, batch acc 0.8682
12:47:30.866   Training iter 300, batch loss 1.7337, batch acc 0.8644
12:47:31.386   Training iter 350, batch loss 1.7349, batch acc 0.8560
12:47:31.907   Training iter 400, batch loss 1.7314, batch acc 0.8656
12:47:32.440   Training iter 450, batch loss 1.7351, batch acc 0.8566
12:47:32.953   Training iter 500, batch loss 1.7370, batch acc 0.8550
12:47:33.471   Training iter 550, batch loss 1.7313, batch acc 0.8584
12:47:33.980   Training iter 600, batch loss 1.7348, batch acc 0.8612
12:47:33.981 Testing @ 395 epoch...
12:47:34.028     Testing, total mean loss 1.72593, total acc 0.87070
12:47:34.028 Training @ 396 epoch...
12:47:34.565   Training iter 50, batch loss 1.7362, batch acc 0.8578
12:47:35.082   Training iter 100, batch loss 1.7344, batch acc 0.8608
12:47:35.601   Training iter 150, batch loss 1.7321, batch acc 0.8664
12:47:36.113   Training iter 200, batch loss 1.7358, batch acc 0.8580
12:47:36.615   Training iter 250, batch loss 1.7284, batch acc 0.8712
12:47:37.139   Training iter 300, batch loss 1.7357, batch acc 0.8550
12:47:37.675   Training iter 350, batch loss 1.7326, batch acc 0.8580
12:47:38.215   Training iter 400, batch loss 1.7373, batch acc 0.8574
12:47:38.765   Training iter 450, batch loss 1.7318, batch acc 0.8602
12:47:39.295   Training iter 500, batch loss 1.7296, batch acc 0.8646
12:47:39.812   Training iter 550, batch loss 1.7351, batch acc 0.8568
12:47:40.341   Training iter 600, batch loss 1.7303, batch acc 0.8642
12:47:40.343 Training @ 397 epoch...
12:47:40.869   Training iter 50, batch loss 1.7337, batch acc 0.8656
12:47:41.398   Training iter 100, batch loss 1.7354, batch acc 0.8600
12:47:41.920   Training iter 150, batch loss 1.7387, batch acc 0.8542
12:47:42.444   Training iter 200, batch loss 1.7357, batch acc 0.8542
12:47:42.956   Training iter 250, batch loss 1.7271, batch acc 0.8690
12:47:43.478   Training iter 300, batch loss 1.7339, batch acc 0.8626
12:47:43.976   Training iter 350, batch loss 1.7387, batch acc 0.8518
12:47:44.474   Training iter 400, batch loss 1.7276, batch acc 0.8688
12:47:44.992   Training iter 450, batch loss 1.7318, batch acc 0.8616
12:47:45.506   Training iter 500, batch loss 1.7274, batch acc 0.8688
12:47:46.020   Training iter 550, batch loss 1.7325, batch acc 0.8594
12:47:46.522   Training iter 600, batch loss 1.7368, batch acc 0.8546
12:47:46.524 Training @ 398 epoch...
12:47:47.061   Training iter 50, batch loss 1.7345, batch acc 0.8604
12:47:47.588   Training iter 100, batch loss 1.7340, batch acc 0.8624
12:47:48.120   Training iter 150, batch loss 1.7353, batch acc 0.8614
12:47:48.672   Training iter 200, batch loss 1.7316, batch acc 0.8678
12:47:49.207   Training iter 250, batch loss 1.7286, batch acc 0.8670
12:47:49.727   Training iter 300, batch loss 1.7322, batch acc 0.8588
12:47:50.253   Training iter 350, batch loss 1.7295, batch acc 0.8604
12:47:50.780   Training iter 400, batch loss 1.7357, batch acc 0.8600
12:47:51.300   Training iter 450, batch loss 1.7400, batch acc 0.8534
12:47:51.812   Training iter 500, batch loss 1.7297, batch acc 0.8652
12:47:52.338   Training iter 550, batch loss 1.7325, batch acc 0.8600
12:47:52.873   Training iter 600, batch loss 1.7357, batch acc 0.8552
12:47:52.875 Training @ 399 epoch...
12:47:53.411   Training iter 50, batch loss 1.7358, batch acc 0.8542
12:47:53.940   Training iter 100, batch loss 1.7374, batch acc 0.8566
12:47:54.474   Training iter 150, batch loss 1.7345, batch acc 0.8642
12:47:55.024   Training iter 200, batch loss 1.7299, batch acc 0.8658
12:47:55.587   Training iter 250, batch loss 1.7330, batch acc 0.8604
12:47:56.137   Training iter 300, batch loss 1.7313, batch acc 0.8638
12:47:56.686   Training iter 350, batch loss 1.7366, batch acc 0.8578
12:47:57.238   Training iter 400, batch loss 1.7319, batch acc 0.8584
12:47:57.793   Training iter 450, batch loss 1.7357, batch acc 0.8562
12:47:58.350   Training iter 500, batch loss 1.7299, batch acc 0.8634
12:47:58.887   Training iter 550, batch loss 1.7310, batch acc 0.8688
12:47:59.448   Training iter 600, batch loss 1.7320, batch acc 0.8586
12:47:59.450 Training @ 400 epoch...
12:48:00.020   Training iter 50, batch loss 1.7332, batch acc 0.8586
12:48:00.589   Training iter 100, batch loss 1.7345, batch acc 0.8612
12:48:01.150   Training iter 150, batch loss 1.7285, batch acc 0.8636
12:48:01.672   Training iter 200, batch loss 1.7312, batch acc 0.8664
12:48:02.238   Training iter 250, batch loss 1.7339, batch acc 0.8616
12:48:02.792   Training iter 300, batch loss 1.7373, batch acc 0.8562
12:48:03.343   Training iter 350, batch loss 1.7338, batch acc 0.8642
12:48:03.879   Training iter 400, batch loss 1.7320, batch acc 0.8592
12:48:04.434   Training iter 450, batch loss 1.7367, batch acc 0.8596
12:48:04.981   Training iter 500, batch loss 1.7323, batch acc 0.8578
12:48:05.509   Training iter 550, batch loss 1.7348, batch acc 0.8576
12:48:06.053   Training iter 600, batch loss 1.7307, batch acc 0.8654
12:48:06.055 Testing @ 400 epoch...
12:48:06.099     Testing, total mean loss 1.72588, total acc 0.87080
12:48:06.099 Plot @ 400 epoch...
12:48:06.099 Training @ 401 epoch...
12:48:06.632   Training iter 50, batch loss 1.7301, batch acc 0.8708
12:48:07.173   Training iter 100, batch loss 1.7315, batch acc 0.8610
12:48:07.675   Training iter 150, batch loss 1.7292, batch acc 0.8652
12:48:08.179   Training iter 200, batch loss 1.7335, batch acc 0.8646
12:48:08.700   Training iter 250, batch loss 1.7338, batch acc 0.8626
12:48:09.219   Training iter 300, batch loss 1.7356, batch acc 0.8568
12:48:09.720   Training iter 350, batch loss 1.7350, batch acc 0.8526
12:48:10.235   Training iter 400, batch loss 1.7373, batch acc 0.8536
12:48:10.735   Training iter 450, batch loss 1.7311, batch acc 0.8582
12:48:11.233   Training iter 500, batch loss 1.7368, batch acc 0.8610
12:48:11.693   Training iter 550, batch loss 1.7306, batch acc 0.8630
12:48:12.164   Training iter 600, batch loss 1.7342, batch acc 0.8622
12:48:12.166 Training @ 402 epoch...
12:48:12.657   Training iter 50, batch loss 1.7351, batch acc 0.8556
12:48:13.146   Training iter 100, batch loss 1.7303, batch acc 0.8670
12:48:13.625   Training iter 150, batch loss 1.7316, batch acc 0.8672
12:48:14.089   Training iter 200, batch loss 1.7302, batch acc 0.8642
12:48:14.554   Training iter 250, batch loss 1.7333, batch acc 0.8608
12:48:15.056   Training iter 300, batch loss 1.7377, batch acc 0.8518
12:48:15.573   Training iter 350, batch loss 1.7283, batch acc 0.8634
12:48:16.066   Training iter 400, batch loss 1.7336, batch acc 0.8646
12:48:16.530   Training iter 450, batch loss 1.7368, batch acc 0.8556
12:48:17.014   Training iter 500, batch loss 1.7343, batch acc 0.8582
12:48:17.500   Training iter 550, batch loss 1.7361, batch acc 0.8602
12:48:17.997   Training iter 600, batch loss 1.7316, batch acc 0.8618
12:48:17.999 Training @ 403 epoch...
12:48:18.513   Training iter 50, batch loss 1.7321, batch acc 0.8654
12:48:19.007   Training iter 100, batch loss 1.7305, batch acc 0.8628
12:48:19.496   Training iter 150, batch loss 1.7301, batch acc 0.8672
12:48:19.985   Training iter 200, batch loss 1.7301, batch acc 0.8698
12:48:20.484   Training iter 250, batch loss 1.7362, batch acc 0.8562
12:48:20.978   Training iter 300, batch loss 1.7351, batch acc 0.8542
12:48:21.477   Training iter 350, batch loss 1.7372, batch acc 0.8548
12:48:21.961   Training iter 400, batch loss 1.7374, batch acc 0.8530
12:48:22.446   Training iter 450, batch loss 1.7324, batch acc 0.8606
12:48:22.939   Training iter 500, batch loss 1.7347, batch acc 0.8602
12:48:23.479   Training iter 550, batch loss 1.7326, batch acc 0.8618
12:48:24.054   Training iter 600, batch loss 1.7303, batch acc 0.8652
12:48:24.056 Training @ 404 epoch...
12:48:24.625   Training iter 50, batch loss 1.7313, batch acc 0.8646
12:48:25.186   Training iter 100, batch loss 1.7341, batch acc 0.8598
12:48:25.688   Training iter 150, batch loss 1.7333, batch acc 0.8626
12:48:26.179   Training iter 200, batch loss 1.7288, batch acc 0.8634
12:48:26.674   Training iter 250, batch loss 1.7343, batch acc 0.8600
12:48:27.170   Training iter 300, batch loss 1.7384, batch acc 0.8494
12:48:27.704   Training iter 350, batch loss 1.7349, batch acc 0.8612
12:48:28.255   Training iter 400, batch loss 1.7283, batch acc 0.8708
12:48:28.800   Training iter 450, batch loss 1.7340, batch acc 0.8606
12:48:29.337   Training iter 500, batch loss 1.7311, batch acc 0.8678
12:48:29.875   Training iter 550, batch loss 1.7337, batch acc 0.8564
12:48:30.422   Training iter 600, batch loss 1.7363, batch acc 0.8532
12:48:30.423 Training @ 405 epoch...
12:48:30.933   Training iter 50, batch loss 1.7256, batch acc 0.8736
12:48:31.446   Training iter 100, batch loss 1.7316, batch acc 0.8652
12:48:31.959   Training iter 150, batch loss 1.7318, batch acc 0.8620
12:48:32.501   Training iter 200, batch loss 1.7353, batch acc 0.8590
12:48:33.024   Training iter 250, batch loss 1.7340, batch acc 0.8612
12:48:33.570   Training iter 300, batch loss 1.7332, batch acc 0.8634
12:48:34.066   Training iter 350, batch loss 1.7390, batch acc 0.8534
12:48:34.543   Training iter 400, batch loss 1.7388, batch acc 0.8534
12:48:35.042   Training iter 450, batch loss 1.7307, batch acc 0.8616
12:48:35.566   Training iter 500, batch loss 1.7324, batch acc 0.8606
12:48:36.072   Training iter 550, batch loss 1.7312, batch acc 0.8570
12:48:36.571   Training iter 600, batch loss 1.7348, batch acc 0.8600
12:48:36.573 Testing @ 405 epoch...
12:48:36.617     Testing, total mean loss 1.72584, total acc 0.87060
12:48:36.618 Training @ 406 epoch...
12:48:37.132   Training iter 50, batch loss 1.7337, batch acc 0.8642
12:48:37.595   Training iter 100, batch loss 1.7341, batch acc 0.8578
12:48:38.074   Training iter 150, batch loss 1.7351, batch acc 0.8642
12:48:38.588   Training iter 200, batch loss 1.7295, batch acc 0.8650
12:48:39.072   Training iter 250, batch loss 1.7309, batch acc 0.8664
12:48:39.552   Training iter 300, batch loss 1.7382, batch acc 0.8528
12:48:40.028   Training iter 350, batch loss 1.7371, batch acc 0.8492
12:48:40.522   Training iter 400, batch loss 1.7318, batch acc 0.8586
12:48:41.011   Training iter 450, batch loss 1.7299, batch acc 0.8604
12:48:41.498   Training iter 500, batch loss 1.7292, batch acc 0.8678
12:48:41.983   Training iter 550, batch loss 1.7364, batch acc 0.8590
12:48:42.468   Training iter 600, batch loss 1.7325, batch acc 0.8682
12:48:42.470 Training @ 407 epoch...
12:48:42.989   Training iter 50, batch loss 1.7285, batch acc 0.8694
12:48:43.535   Training iter 100, batch loss 1.7347, batch acc 0.8528
12:48:44.063   Training iter 150, batch loss 1.7323, batch acc 0.8602
12:48:44.570   Training iter 200, batch loss 1.7374, batch acc 0.8584
12:48:45.088   Training iter 250, batch loss 1.7370, batch acc 0.8552
12:48:45.628   Training iter 300, batch loss 1.7325, batch acc 0.8600
12:48:46.162   Training iter 350, batch loss 1.7314, batch acc 0.8622
12:48:46.677   Training iter 400, batch loss 1.7310, batch acc 0.8606
12:48:47.203   Training iter 450, batch loss 1.7326, batch acc 0.8624
12:48:47.727   Training iter 500, batch loss 1.7315, batch acc 0.8606
12:48:48.271   Training iter 550, batch loss 1.7338, batch acc 0.8696
12:48:48.810   Training iter 600, batch loss 1.7354, batch acc 0.8598
12:48:48.812 Training @ 408 epoch...
12:48:49.339   Training iter 50, batch loss 1.7331, batch acc 0.8552
12:48:49.862   Training iter 100, batch loss 1.7321, batch acc 0.8638
12:48:50.379   Training iter 150, batch loss 1.7297, batch acc 0.8628
12:48:50.905   Training iter 200, batch loss 1.7323, batch acc 0.8614
12:48:51.431   Training iter 250, batch loss 1.7355, batch acc 0.8634
12:48:51.945   Training iter 300, batch loss 1.7334, batch acc 0.8586
12:48:52.471   Training iter 350, batch loss 1.7387, batch acc 0.8592
12:48:52.979   Training iter 400, batch loss 1.7341, batch acc 0.8560
12:48:53.491   Training iter 450, batch loss 1.7334, batch acc 0.8694
12:48:53.994   Training iter 500, batch loss 1.7306, batch acc 0.8612
12:48:54.482   Training iter 550, batch loss 1.7327, batch acc 0.8604
12:48:54.975   Training iter 600, batch loss 1.7325, batch acc 0.8610
12:48:54.977 Training @ 409 epoch...
12:48:55.489   Training iter 50, batch loss 1.7321, batch acc 0.8610
12:48:55.979   Training iter 100, batch loss 1.7363, batch acc 0.8508
12:48:56.466   Training iter 150, batch loss 1.7317, batch acc 0.8624
12:48:56.946   Training iter 200, batch loss 1.7315, batch acc 0.8618
12:48:57.426   Training iter 250, batch loss 1.7323, batch acc 0.8628
12:48:57.914   Training iter 300, batch loss 1.7332, batch acc 0.8614
12:48:58.402   Training iter 350, batch loss 1.7347, batch acc 0.8648
12:48:58.872   Training iter 400, batch loss 1.7315, batch acc 0.8614
12:48:59.381   Training iter 450, batch loss 1.7381, batch acc 0.8570
12:48:59.896   Training iter 500, batch loss 1.7326, batch acc 0.8632
12:49:00.466   Training iter 550, batch loss 1.7324, batch acc 0.8596
12:49:01.027   Training iter 600, batch loss 1.7316, batch acc 0.8644
12:49:01.029 Training @ 410 epoch...
12:49:01.612   Training iter 50, batch loss 1.7306, batch acc 0.8606
12:49:02.213   Training iter 100, batch loss 1.7372, batch acc 0.8568
12:49:02.791   Training iter 150, batch loss 1.7360, batch acc 0.8588
12:49:03.361   Training iter 200, batch loss 1.7279, batch acc 0.8644
12:49:03.942   Training iter 250, batch loss 1.7341, batch acc 0.8650
12:49:04.521   Training iter 300, batch loss 1.7335, batch acc 0.8636
12:49:05.081   Training iter 350, batch loss 1.7297, batch acc 0.8622
12:49:05.648   Training iter 400, batch loss 1.7330, batch acc 0.8592
12:49:06.194   Training iter 450, batch loss 1.7291, batch acc 0.8644
12:49:06.734   Training iter 500, batch loss 1.7344, batch acc 0.8614
12:49:07.271   Training iter 550, batch loss 1.7354, batch acc 0.8638
12:49:07.780   Training iter 600, batch loss 1.7370, batch acc 0.8532
12:49:07.782 Testing @ 410 epoch...
12:49:07.824     Testing, total mean loss 1.72580, total acc 0.87060
12:49:07.824 Training @ 411 epoch...
12:49:08.321   Training iter 50, batch loss 1.7309, batch acc 0.8714
12:49:08.829   Training iter 100, batch loss 1.7305, batch acc 0.8640
12:49:09.326   Training iter 150, batch loss 1.7359, batch acc 0.8554
12:49:09.822   Training iter 200, batch loss 1.7285, batch acc 0.8662
12:49:10.333   Training iter 250, batch loss 1.7370, batch acc 0.8514
12:49:10.829   Training iter 300, batch loss 1.7333, batch acc 0.8674
12:49:11.330   Training iter 350, batch loss 1.7290, batch acc 0.8632
12:49:11.819   Training iter 400, batch loss 1.7381, batch acc 0.8508
12:49:12.305   Training iter 450, batch loss 1.7314, batch acc 0.8650
12:49:12.800   Training iter 500, batch loss 1.7328, batch acc 0.8562
12:49:13.289   Training iter 550, batch loss 1.7365, batch acc 0.8610
12:49:13.779   Training iter 600, batch loss 1.7340, batch acc 0.8606
12:49:13.781 Training @ 412 epoch...
12:49:14.269   Training iter 50, batch loss 1.7346, batch acc 0.8582
12:49:14.760   Training iter 100, batch loss 1.7338, batch acc 0.8632
12:49:15.252   Training iter 150, batch loss 1.7307, batch acc 0.8664
12:49:15.756   Training iter 200, batch loss 1.7359, batch acc 0.8540
12:49:16.290   Training iter 250, batch loss 1.7367, batch acc 0.8542
12:49:16.813   Training iter 300, batch loss 1.7289, batch acc 0.8702
12:49:17.353   Training iter 350, batch loss 1.7339, batch acc 0.8604
12:49:17.896   Training iter 400, batch loss 1.7304, batch acc 0.8676
12:49:18.438   Training iter 450, batch loss 1.7323, batch acc 0.8558
12:49:18.969   Training iter 500, batch loss 1.7325, batch acc 0.8564
12:49:19.494   Training iter 550, batch loss 1.7339, batch acc 0.8622
12:49:19.992   Training iter 600, batch loss 1.7343, batch acc 0.8616
12:49:19.994 Training @ 413 epoch...
12:49:20.527   Training iter 50, batch loss 1.7339, batch acc 0.8660
12:49:21.038   Training iter 100, batch loss 1.7361, batch acc 0.8554
12:49:21.522   Training iter 150, batch loss 1.7281, batch acc 0.8692
12:49:21.988   Training iter 200, batch loss 1.7337, batch acc 0.8634
12:49:22.437   Training iter 250, batch loss 1.7321, batch acc 0.8610
12:49:22.877   Training iter 300, batch loss 1.7352, batch acc 0.8582
12:49:23.326   Training iter 350, batch loss 1.7308, batch acc 0.8644
12:49:23.810   Training iter 400, batch loss 1.7330, batch acc 0.8584
12:49:24.259   Training iter 450, batch loss 1.7339, batch acc 0.8612
12:49:24.720   Training iter 500, batch loss 1.7365, batch acc 0.8512
12:49:25.196   Training iter 550, batch loss 1.7327, batch acc 0.8612
12:49:25.671   Training iter 600, batch loss 1.7318, batch acc 0.8620
12:49:25.673 Training @ 414 epoch...
12:49:26.159   Training iter 50, batch loss 1.7339, batch acc 0.8626
12:49:26.634   Training iter 100, batch loss 1.7365, batch acc 0.8560
12:49:27.106   Training iter 150, batch loss 1.7313, batch acc 0.8662
12:49:27.581   Training iter 200, batch loss 1.7315, batch acc 0.8650
12:49:28.029   Training iter 250, batch loss 1.7334, batch acc 0.8560
12:49:28.486   Training iter 300, batch loss 1.7352, batch acc 0.8580
12:49:28.939   Training iter 350, batch loss 1.7332, batch acc 0.8606
12:49:29.405   Training iter 400, batch loss 1.7282, batch acc 0.8684
12:49:29.867   Training iter 450, batch loss 1.7374, batch acc 0.8566
12:49:30.368   Training iter 500, batch loss 1.7295, batch acc 0.8622
12:49:30.863   Training iter 550, batch loss 1.7322, batch acc 0.8612
12:49:31.328   Training iter 600, batch loss 1.7354, batch acc 0.8592
12:49:31.330 Training @ 415 epoch...
12:49:31.784   Training iter 50, batch loss 1.7290, batch acc 0.8620
12:49:32.270   Training iter 100, batch loss 1.7337, batch acc 0.8610
12:49:32.765   Training iter 150, batch loss 1.7326, batch acc 0.8616
12:49:33.259   Training iter 200, batch loss 1.7313, batch acc 0.8658
12:49:33.759   Training iter 250, batch loss 1.7318, batch acc 0.8622
12:49:34.260   Training iter 300, batch loss 1.7358, batch acc 0.8592
12:49:34.777   Training iter 350, batch loss 1.7316, batch acc 0.8608
12:49:35.310   Training iter 400, batch loss 1.7328, batch acc 0.8674
12:49:35.835   Training iter 450, batch loss 1.7343, batch acc 0.8598
12:49:36.346   Training iter 500, batch loss 1.7336, batch acc 0.8652
12:49:36.855   Training iter 550, batch loss 1.7365, batch acc 0.8540
12:49:37.383   Training iter 600, batch loss 1.7343, batch acc 0.8528
12:49:37.385 Testing @ 415 epoch...
12:49:37.427     Testing, total mean loss 1.72576, total acc 0.87060
12:49:37.427 Training @ 416 epoch...
12:49:37.966   Training iter 50, batch loss 1.7288, batch acc 0.8644
12:49:38.478   Training iter 100, batch loss 1.7279, batch acc 0.8632
12:49:39.022   Training iter 150, batch loss 1.7353, batch acc 0.8550
12:49:39.533   Training iter 200, batch loss 1.7379, batch acc 0.8548
12:49:40.038   Training iter 250, batch loss 1.7355, batch acc 0.8582
12:49:40.547   Training iter 300, batch loss 1.7360, batch acc 0.8636
12:49:41.039   Training iter 350, batch loss 1.7322, batch acc 0.8608
12:49:41.547   Training iter 400, batch loss 1.7328, batch acc 0.8568
12:49:42.045   Training iter 450, batch loss 1.7320, batch acc 0.8646
12:49:42.537   Training iter 500, batch loss 1.7296, batch acc 0.8646
12:49:43.042   Training iter 550, batch loss 1.7357, batch acc 0.8636
12:49:43.530   Training iter 600, batch loss 1.7337, batch acc 0.8622
12:49:43.532 Training @ 417 epoch...
12:49:44.025   Training iter 50, batch loss 1.7284, batch acc 0.8714
12:49:44.520   Training iter 100, batch loss 1.7346, batch acc 0.8556
12:49:45.013   Training iter 150, batch loss 1.7337, batch acc 0.8584
12:49:45.503   Training iter 200, batch loss 1.7327, batch acc 0.8596
12:49:45.975   Training iter 250, batch loss 1.7329, batch acc 0.8588
12:49:46.448   Training iter 300, batch loss 1.7346, batch acc 0.8588
12:49:46.923   Training iter 350, batch loss 1.7326, batch acc 0.8666
12:49:47.405   Training iter 400, batch loss 1.7334, batch acc 0.8668
12:49:47.889   Training iter 450, batch loss 1.7379, batch acc 0.8518
12:49:48.415   Training iter 500, batch loss 1.7351, batch acc 0.8602
12:49:48.939   Training iter 550, batch loss 1.7338, batch acc 0.8588
12:49:49.438   Training iter 600, batch loss 1.7276, batch acc 0.8652
12:49:49.439 Training @ 418 epoch...
12:49:49.931   Training iter 50, batch loss 1.7345, batch acc 0.8604
12:49:50.407   Training iter 100, batch loss 1.7358, batch acc 0.8588
12:49:50.880   Training iter 150, batch loss 1.7337, batch acc 0.8600
12:49:51.371   Training iter 200, batch loss 1.7306, batch acc 0.8688
12:49:51.861   Training iter 250, batch loss 1.7318, batch acc 0.8618
12:49:52.371   Training iter 300, batch loss 1.7361, batch acc 0.8546
12:49:52.900   Training iter 350, batch loss 1.7328, batch acc 0.8676
12:49:53.439   Training iter 400, batch loss 1.7326, batch acc 0.8616
12:49:53.987   Training iter 450, batch loss 1.7338, batch acc 0.8578
12:49:54.474   Training iter 500, batch loss 1.7350, batch acc 0.8586
12:49:54.970   Training iter 550, batch loss 1.7316, batch acc 0.8610
12:49:55.465   Training iter 600, batch loss 1.7289, batch acc 0.8622
12:49:55.466 Training @ 419 epoch...
12:49:55.946   Training iter 50, batch loss 1.7334, batch acc 0.8592
12:49:56.430   Training iter 100, batch loss 1.7339, batch acc 0.8538
12:49:56.920   Training iter 150, batch loss 1.7284, batch acc 0.8702
12:49:57.443   Training iter 200, batch loss 1.7351, batch acc 0.8546
12:49:57.975   Training iter 250, batch loss 1.7348, batch acc 0.8638
12:49:58.497   Training iter 300, batch loss 1.7348, batch acc 0.8592
12:49:59.009   Training iter 350, batch loss 1.7344, batch acc 0.8598
12:49:59.516   Training iter 400, batch loss 1.7316, batch acc 0.8652
12:50:00.022   Training iter 450, batch loss 1.7362, batch acc 0.8580
12:50:00.561   Training iter 500, batch loss 1.7313, batch acc 0.8588
12:50:01.080   Training iter 550, batch loss 1.7338, batch acc 0.8614
12:50:01.619   Training iter 600, batch loss 1.7294, batch acc 0.8684
12:50:01.621 Training @ 420 epoch...
12:50:02.199   Training iter 50, batch loss 1.7317, batch acc 0.8678
12:50:02.741   Training iter 100, batch loss 1.7390, batch acc 0.8506
12:50:03.290   Training iter 150, batch loss 1.7346, batch acc 0.8560
12:50:03.830   Training iter 200, batch loss 1.7271, batch acc 0.8702
12:50:04.385   Training iter 250, batch loss 1.7313, batch acc 0.8652
12:50:04.946   Training iter 300, batch loss 1.7356, batch acc 0.8552
12:50:05.516   Training iter 350, batch loss 1.7357, batch acc 0.8568
12:50:06.073   Training iter 400, batch loss 1.7338, batch acc 0.8658
12:50:06.626   Training iter 450, batch loss 1.7322, batch acc 0.8644
12:50:07.161   Training iter 500, batch loss 1.7315, batch acc 0.8632
12:50:07.712   Training iter 550, batch loss 1.7275, batch acc 0.8662
12:50:08.285   Training iter 600, batch loss 1.7372, batch acc 0.8522
12:50:08.286 Testing @ 420 epoch...
12:50:08.329     Testing, total mean loss 1.72573, total acc 0.87060
12:50:08.329 Training @ 421 epoch...
12:50:08.866   Training iter 50, batch loss 1.7317, batch acc 0.8588
12:50:09.394   Training iter 100, batch loss 1.7322, batch acc 0.8574
12:50:09.904   Training iter 150, batch loss 1.7314, batch acc 0.8642
12:50:10.422   Training iter 200, batch loss 1.7267, batch acc 0.8734
12:50:10.956   Training iter 250, batch loss 1.7305, batch acc 0.8626
12:50:11.479   Training iter 300, batch loss 1.7313, batch acc 0.8620
12:50:11.994   Training iter 350, batch loss 1.7350, batch acc 0.8576
12:50:12.517   Training iter 400, batch loss 1.7381, batch acc 0.8536
12:50:13.039   Training iter 450, batch loss 1.7333, batch acc 0.8626
12:50:13.564   Training iter 500, batch loss 1.7362, batch acc 0.8608
12:50:14.077   Training iter 550, batch loss 1.7372, batch acc 0.8586
12:50:14.610   Training iter 600, batch loss 1.7333, batch acc 0.8606
12:50:14.612 Training @ 422 epoch...
12:50:15.099   Training iter 50, batch loss 1.7287, batch acc 0.8726
12:50:15.570   Training iter 100, batch loss 1.7322, batch acc 0.8636
12:50:16.057   Training iter 150, batch loss 1.7311, batch acc 0.8624
12:50:16.522   Training iter 200, batch loss 1.7315, batch acc 0.8666
12:50:16.963   Training iter 250, batch loss 1.7307, batch acc 0.8650
12:50:17.418   Training iter 300, batch loss 1.7349, batch acc 0.8576
12:50:17.881   Training iter 350, batch loss 1.7311, batch acc 0.8600
12:50:18.358   Training iter 400, batch loss 1.7364, batch acc 0.8562
12:50:18.854   Training iter 450, batch loss 1.7327, batch acc 0.8638
12:50:19.353   Training iter 500, batch loss 1.7340, batch acc 0.8574
12:50:19.873   Training iter 550, batch loss 1.7374, batch acc 0.8564
12:50:20.407   Training iter 600, batch loss 1.7361, batch acc 0.8510
12:50:20.409 Training @ 423 epoch...
12:50:20.960   Training iter 50, batch loss 1.7372, batch acc 0.8532
12:50:21.514   Training iter 100, batch loss 1.7324, batch acc 0.8628
12:50:22.066   Training iter 150, batch loss 1.7337, batch acc 0.8598
12:50:22.597   Training iter 200, batch loss 1.7282, batch acc 0.8756
12:50:23.127   Training iter 250, batch loss 1.7307, batch acc 0.8648
12:50:23.657   Training iter 300, batch loss 1.7343, batch acc 0.8566
12:50:24.196   Training iter 350, batch loss 1.7352, batch acc 0.8600
12:50:24.733   Training iter 400, batch loss 1.7313, batch acc 0.8658
12:50:25.291   Training iter 450, batch loss 1.7295, batch acc 0.8644
12:50:25.832   Training iter 500, batch loss 1.7335, batch acc 0.8580
12:50:26.354   Training iter 550, batch loss 1.7344, batch acc 0.8594
12:50:26.835   Training iter 600, batch loss 1.7365, batch acc 0.8546
12:50:26.837 Training @ 424 epoch...
12:50:27.321   Training iter 50, batch loss 1.7320, batch acc 0.8558
12:50:27.815   Training iter 100, batch loss 1.7322, batch acc 0.8652
12:50:28.330   Training iter 150, batch loss 1.7353, batch acc 0.8552
12:50:28.851   Training iter 200, batch loss 1.7279, batch acc 0.8722
12:50:29.359   Training iter 250, batch loss 1.7300, batch acc 0.8682
12:50:29.866   Training iter 300, batch loss 1.7343, batch acc 0.8634
12:50:30.386   Training iter 350, batch loss 1.7315, batch acc 0.8606
12:50:30.897   Training iter 400, batch loss 1.7316, batch acc 0.8602
12:50:31.406   Training iter 450, batch loss 1.7313, batch acc 0.8702
12:50:31.904   Training iter 500, batch loss 1.7365, batch acc 0.8550
12:50:32.411   Training iter 550, batch loss 1.7376, batch acc 0.8552
12:50:32.922   Training iter 600, batch loss 1.7365, batch acc 0.8520
12:50:32.924 Training @ 425 epoch...
12:50:33.438   Training iter 50, batch loss 1.7349, batch acc 0.8608
12:50:33.932   Training iter 100, batch loss 1.7350, batch acc 0.8522
12:50:34.443   Training iter 150, batch loss 1.7310, batch acc 0.8628
12:50:34.952   Training iter 200, batch loss 1.7343, batch acc 0.8578
12:50:35.469   Training iter 250, batch loss 1.7345, batch acc 0.8620
12:50:36.004   Training iter 300, batch loss 1.7322, batch acc 0.8626
12:50:36.539   Training iter 350, batch loss 1.7299, batch acc 0.8630
12:50:37.069   Training iter 400, batch loss 1.7374, batch acc 0.8626
12:50:37.605   Training iter 450, batch loss 1.7289, batch acc 0.8680
12:50:38.150   Training iter 500, batch loss 1.7309, batch acc 0.8622
12:50:38.670   Training iter 550, batch loss 1.7352, batch acc 0.8568
12:50:39.197   Training iter 600, batch loss 1.7324, batch acc 0.8640
12:50:39.199 Testing @ 425 epoch...
12:50:39.241     Testing, total mean loss 1.72570, total acc 0.87060
12:50:39.241 Training @ 426 epoch...
12:50:39.779   Training iter 50, batch loss 1.7333, batch acc 0.8608
12:50:40.326   Training iter 100, batch loss 1.7300, batch acc 0.8652
12:50:40.860   Training iter 150, batch loss 1.7347, batch acc 0.8568
12:50:41.378   Training iter 200, batch loss 1.7305, batch acc 0.8626
12:50:41.888   Training iter 250, batch loss 1.7296, batch acc 0.8662
12:50:42.356   Training iter 300, batch loss 1.7293, batch acc 0.8638
12:50:42.838   Training iter 350, batch loss 1.7397, batch acc 0.8562
12:50:43.332   Training iter 400, batch loss 1.7342, batch acc 0.8536
12:50:43.822   Training iter 450, batch loss 1.7359, batch acc 0.8540
12:50:44.302   Training iter 500, batch loss 1.7330, batch acc 0.8688
12:50:44.789   Training iter 550, batch loss 1.7328, batch acc 0.8622
12:50:45.287   Training iter 600, batch loss 1.7335, batch acc 0.8654
12:50:45.288 Training @ 427 epoch...
12:50:45.779   Training iter 50, batch loss 1.7336, batch acc 0.8642
12:50:46.274   Training iter 100, batch loss 1.7379, batch acc 0.8614
12:50:46.752   Training iter 150, batch loss 1.7295, batch acc 0.8656
12:50:47.237   Training iter 200, batch loss 1.7310, batch acc 0.8608
12:50:47.724   Training iter 250, batch loss 1.7303, batch acc 0.8610
12:50:48.216   Training iter 300, batch loss 1.7351, batch acc 0.8560
12:50:48.735   Training iter 350, batch loss 1.7303, batch acc 0.8670
12:50:49.273   Training iter 400, batch loss 1.7325, batch acc 0.8560
12:50:49.810   Training iter 450, batch loss 1.7320, batch acc 0.8670
12:50:50.319   Training iter 500, batch loss 1.7379, batch acc 0.8522
12:50:50.831   Training iter 550, batch loss 1.7351, batch acc 0.8612
12:50:51.363   Training iter 600, batch loss 1.7314, batch acc 0.8614
12:50:51.365 Training @ 428 epoch...
12:50:51.893   Training iter 50, batch loss 1.7291, batch acc 0.8654
12:50:52.417   Training iter 100, batch loss 1.7367, batch acc 0.8580
12:50:52.932   Training iter 150, batch loss 1.7363, batch acc 0.8568
12:50:53.442   Training iter 200, batch loss 1.7342, batch acc 0.8632
12:50:53.940   Training iter 250, batch loss 1.7353, batch acc 0.8562
12:50:54.451   Training iter 300, batch loss 1.7319, batch acc 0.8594
12:50:54.957   Training iter 350, batch loss 1.7307, batch acc 0.8640
12:50:55.481   Training iter 400, batch loss 1.7322, batch acc 0.8646
12:50:55.945   Training iter 450, batch loss 1.7301, batch acc 0.8628
12:50:56.406   Training iter 500, batch loss 1.7357, batch acc 0.8546
12:50:56.869   Training iter 550, batch loss 1.7328, batch acc 0.8630
12:50:57.333   Training iter 600, batch loss 1.7315, batch acc 0.8634
12:50:57.335 Training @ 429 epoch...
12:50:57.812   Training iter 50, batch loss 1.7305, batch acc 0.8654
12:50:58.294   Training iter 100, batch loss 1.7320, batch acc 0.8642
12:50:58.801   Training iter 150, batch loss 1.7328, batch acc 0.8610
12:50:59.312   Training iter 200, batch loss 1.7344, batch acc 0.8564
12:50:59.812   Training iter 250, batch loss 1.7312, batch acc 0.8686
12:51:00.326   Training iter 300, batch loss 1.7359, batch acc 0.8556
12:51:00.838   Training iter 350, batch loss 1.7294, batch acc 0.8634
12:51:01.358   Training iter 400, batch loss 1.7308, batch acc 0.8632
12:51:01.914   Training iter 450, batch loss 1.7354, batch acc 0.8576
12:51:02.468   Training iter 500, batch loss 1.7367, batch acc 0.8574
12:51:02.984   Training iter 550, batch loss 1.7336, batch acc 0.8630
12:51:03.514   Training iter 600, batch loss 1.7336, batch acc 0.8570
12:51:03.516 Training @ 430 epoch...
12:51:04.066   Training iter 50, batch loss 1.7277, batch acc 0.8714
12:51:04.612   Training iter 100, batch loss 1.7325, batch acc 0.8616
12:51:05.154   Training iter 150, batch loss 1.7345, batch acc 0.8628
12:51:05.686   Training iter 200, batch loss 1.7330, batch acc 0.8606
12:51:06.226   Training iter 250, batch loss 1.7376, batch acc 0.8528
12:51:06.741   Training iter 300, batch loss 1.7295, batch acc 0.8650
12:51:07.290   Training iter 350, batch loss 1.7325, batch acc 0.8608
12:51:07.842   Training iter 400, batch loss 1.7351, batch acc 0.8592
12:51:08.364   Training iter 450, batch loss 1.7329, batch acc 0.8628
12:51:08.894   Training iter 500, batch loss 1.7336, batch acc 0.8540
12:51:09.404   Training iter 550, batch loss 1.7306, batch acc 0.8662
12:51:09.921   Training iter 600, batch loss 1.7368, batch acc 0.8546
12:51:09.922 Testing @ 430 epoch...
12:51:09.965     Testing, total mean loss 1.72566, total acc 0.87080
12:51:09.965 Training @ 431 epoch...
12:51:10.508   Training iter 50, batch loss 1.7331, batch acc 0.8630
12:51:11.043   Training iter 100, batch loss 1.7356, batch acc 0.8624
12:51:11.576   Training iter 150, batch loss 1.7327, batch acc 0.8592
12:51:12.109   Training iter 200, batch loss 1.7320, batch acc 0.8644
12:51:12.650   Training iter 250, batch loss 1.7309, batch acc 0.8668
12:51:13.201   Training iter 300, batch loss 1.7330, batch acc 0.8556
12:51:13.745   Training iter 350, batch loss 1.7332, batch acc 0.8598
12:51:14.276   Training iter 400, batch loss 1.7350, batch acc 0.8544
12:51:14.778   Training iter 450, batch loss 1.7319, batch acc 0.8622
12:51:15.268   Training iter 500, batch loss 1.7372, batch acc 0.8558
12:51:15.776   Training iter 550, batch loss 1.7298, batch acc 0.8706
12:51:16.294   Training iter 600, batch loss 1.7317, batch acc 0.8600
12:51:16.296 Training @ 432 epoch...
12:51:16.825   Training iter 50, batch loss 1.7310, batch acc 0.8682
12:51:17.331   Training iter 100, batch loss 1.7363, batch acc 0.8578
12:51:17.841   Training iter 150, batch loss 1.7338, batch acc 0.8578
12:51:18.356   Training iter 200, batch loss 1.7331, batch acc 0.8674
12:51:18.854   Training iter 250, batch loss 1.7338, batch acc 0.8630
12:51:19.352   Training iter 300, batch loss 1.7317, batch acc 0.8678
12:51:19.854   Training iter 350, batch loss 1.7334, batch acc 0.8572
12:51:20.357   Training iter 400, batch loss 1.7293, batch acc 0.8630
12:51:20.845   Training iter 450, batch loss 1.7354, batch acc 0.8510
12:51:21.352   Training iter 500, batch loss 1.7329, batch acc 0.8590
12:51:21.869   Training iter 550, batch loss 1.7318, batch acc 0.8624
12:51:22.371   Training iter 600, batch loss 1.7337, batch acc 0.8596
12:51:22.372 Training @ 433 epoch...
12:51:22.887   Training iter 50, batch loss 1.7308, batch acc 0.8634
12:51:23.400   Training iter 100, batch loss 1.7368, batch acc 0.8526
12:51:23.920   Training iter 150, batch loss 1.7385, batch acc 0.8576
12:51:24.436   Training iter 200, batch loss 1.7313, batch acc 0.8630
12:51:24.949   Training iter 250, batch loss 1.7352, batch acc 0.8580
12:51:25.471   Training iter 300, batch loss 1.7328, batch acc 0.8604
12:51:25.976   Training iter 350, batch loss 1.7328, batch acc 0.8612
12:51:26.498   Training iter 400, batch loss 1.7349, batch acc 0.8552
12:51:27.038   Training iter 450, batch loss 1.7297, batch acc 0.8692
12:51:27.551   Training iter 500, batch loss 1.7322, batch acc 0.8630
12:51:28.067   Training iter 550, batch loss 1.7310, batch acc 0.8660
12:51:28.586   Training iter 600, batch loss 1.7302, batch acc 0.8660
12:51:28.588 Training @ 434 epoch...
12:51:29.116   Training iter 50, batch loss 1.7343, batch acc 0.8602
12:51:29.647   Training iter 100, batch loss 1.7339, batch acc 0.8650
12:51:30.149   Training iter 150, batch loss 1.7320, batch acc 0.8632
12:51:30.634   Training iter 200, batch loss 1.7331, batch acc 0.8580
12:51:31.138   Training iter 250, batch loss 1.7292, batch acc 0.8710
12:51:31.626   Training iter 300, batch loss 1.7329, batch acc 0.8576
12:51:32.123   Training iter 350, batch loss 1.7372, batch acc 0.8554
12:51:32.605   Training iter 400, batch loss 1.7358, batch acc 0.8572
12:51:33.101   Training iter 450, batch loss 1.7280, batch acc 0.8642
12:51:33.584   Training iter 500, batch loss 1.7346, batch acc 0.8610
12:51:34.069   Training iter 550, batch loss 1.7321, batch acc 0.8596
12:51:34.551   Training iter 600, batch loss 1.7329, batch acc 0.8624
12:51:34.553 Training @ 435 epoch...
12:51:35.007   Training iter 50, batch loss 1.7288, batch acc 0.8680
12:51:35.458   Training iter 100, batch loss 1.7340, batch acc 0.8650
12:51:35.904   Training iter 150, batch loss 1.7339, batch acc 0.8646
12:51:36.367   Training iter 200, batch loss 1.7329, batch acc 0.8618
12:51:36.824   Training iter 250, batch loss 1.7319, batch acc 0.8598
12:51:37.282   Training iter 300, batch loss 1.7333, batch acc 0.8578
12:51:37.742   Training iter 350, batch loss 1.7359, batch acc 0.8562
12:51:38.206   Training iter 400, batch loss 1.7384, batch acc 0.8574
12:51:38.685   Training iter 450, batch loss 1.7346, batch acc 0.8546
12:51:39.154   Training iter 500, batch loss 1.7325, batch acc 0.8630
12:51:39.640   Training iter 550, batch loss 1.7269, batch acc 0.8658
12:51:40.140   Training iter 600, batch loss 1.7327, batch acc 0.8600
12:51:40.142 Testing @ 435 epoch...
12:51:40.184     Testing, total mean loss 1.72563, total acc 0.87100
12:51:40.185 Training @ 436 epoch...
12:51:40.664   Training iter 50, batch loss 1.7333, batch acc 0.8624
12:51:41.140   Training iter 100, batch loss 1.7365, batch acc 0.8554
12:51:41.629   Training iter 150, batch loss 1.7323, batch acc 0.8622
12:51:42.093   Training iter 200, batch loss 1.7327, batch acc 0.8652
12:51:42.562   Training iter 250, batch loss 1.7292, batch acc 0.8660
12:51:43.036   Training iter 300, batch loss 1.7320, batch acc 0.8600
12:51:43.507   Training iter 350, batch loss 1.7310, batch acc 0.8712
12:51:43.984   Training iter 400, batch loss 1.7326, batch acc 0.8604
12:51:44.452   Training iter 450, batch loss 1.7298, batch acc 0.8612
12:51:44.910   Training iter 500, batch loss 1.7333, batch acc 0.8606
12:51:45.380   Training iter 550, batch loss 1.7350, batch acc 0.8550
12:51:45.878   Training iter 600, batch loss 1.7380, batch acc 0.8520
12:51:45.880 Training @ 437 epoch...
12:51:46.440   Training iter 50, batch loss 1.7251, batch acc 0.8704
12:51:46.989   Training iter 100, batch loss 1.7360, batch acc 0.8592
12:51:47.522   Training iter 150, batch loss 1.7345, batch acc 0.8610
12:51:47.975   Training iter 200, batch loss 1.7338, batch acc 0.8606
12:51:48.471   Training iter 250, batch loss 1.7313, batch acc 0.8638
12:51:48.931   Training iter 300, batch loss 1.7302, batch acc 0.8642
12:51:49.382   Training iter 350, batch loss 1.7304, batch acc 0.8606
12:51:49.841   Training iter 400, batch loss 1.7340, batch acc 0.8582
12:51:50.296   Training iter 450, batch loss 1.7357, batch acc 0.8558
12:51:50.757   Training iter 500, batch loss 1.7341, batch acc 0.8608
12:51:51.240   Training iter 550, batch loss 1.7322, batch acc 0.8630
12:51:51.726   Training iter 600, batch loss 1.7384, batch acc 0.8552
12:51:51.728 Training @ 438 epoch...
12:51:52.214   Training iter 50, batch loss 1.7299, batch acc 0.8666
12:51:52.693   Training iter 100, batch loss 1.7342, batch acc 0.8582
12:51:53.175   Training iter 150, batch loss 1.7308, batch acc 0.8648
12:51:53.655   Training iter 200, batch loss 1.7327, batch acc 0.8630
12:51:54.135   Training iter 250, batch loss 1.7349, batch acc 0.8594
12:51:54.621   Training iter 300, batch loss 1.7334, batch acc 0.8606
12:51:55.101   Training iter 350, batch loss 1.7372, batch acc 0.8584
12:51:55.581   Training iter 400, batch loss 1.7367, batch acc 0.8578
12:51:56.065   Training iter 450, batch loss 1.7280, batch acc 0.8662
12:51:56.565   Training iter 500, batch loss 1.7343, batch acc 0.8580
12:51:57.060   Training iter 550, batch loss 1.7319, batch acc 0.8582
12:51:57.568   Training iter 600, batch loss 1.7317, batch acc 0.8638
12:51:57.570 Training @ 439 epoch...
12:51:58.092   Training iter 50, batch loss 1.7385, batch acc 0.8520
12:51:58.577   Training iter 100, batch loss 1.7339, batch acc 0.8650
12:51:59.086   Training iter 150, batch loss 1.7315, batch acc 0.8600
12:51:59.598   Training iter 200, batch loss 1.7326, batch acc 0.8608
12:52:00.123   Training iter 250, batch loss 1.7326, batch acc 0.8628
12:52:00.633   Training iter 300, batch loss 1.7307, batch acc 0.8616
12:52:01.163   Training iter 350, batch loss 1.7345, batch acc 0.8572
12:52:01.725   Training iter 400, batch loss 1.7293, batch acc 0.8674
12:52:02.290   Training iter 450, batch loss 1.7309, batch acc 0.8656
12:52:02.822   Training iter 500, batch loss 1.7306, batch acc 0.8650
12:52:03.344   Training iter 550, batch loss 1.7349, batch acc 0.8538
12:52:03.849   Training iter 600, batch loss 1.7355, batch acc 0.8652
12:52:03.851 Training @ 440 epoch...
12:52:04.388   Training iter 50, batch loss 1.7331, batch acc 0.8612
12:52:04.929   Training iter 100, batch loss 1.7286, batch acc 0.8622
12:52:05.459   Training iter 150, batch loss 1.7341, batch acc 0.8580
12:52:05.978   Training iter 200, batch loss 1.7361, batch acc 0.8642
12:52:06.520   Training iter 250, batch loss 1.7341, batch acc 0.8652
12:52:07.080   Training iter 300, batch loss 1.7318, batch acc 0.8618
12:52:07.590   Training iter 350, batch loss 1.7361, batch acc 0.8560
12:52:08.063   Training iter 400, batch loss 1.7318, batch acc 0.8592
12:52:08.544   Training iter 450, batch loss 1.7302, batch acc 0.8658
12:52:09.058   Training iter 500, batch loss 1.7304, batch acc 0.8620
12:52:09.546   Training iter 550, batch loss 1.7329, batch acc 0.8632
12:52:10.019   Training iter 600, batch loss 1.7363, batch acc 0.8552
12:52:10.021 Testing @ 440 epoch...
12:52:10.064     Testing, total mean loss 1.72560, total acc 0.87090
12:52:10.064 Training @ 441 epoch...
12:52:10.584   Training iter 50, batch loss 1.7256, batch acc 0.8718
12:52:11.102   Training iter 100, batch loss 1.7332, batch acc 0.8558
12:52:11.638   Training iter 150, batch loss 1.7332, batch acc 0.8594
12:52:12.166   Training iter 200, batch loss 1.7333, batch acc 0.8598
12:52:12.715   Training iter 250, batch loss 1.7356, batch acc 0.8590
12:52:13.265   Training iter 300, batch loss 1.7346, batch acc 0.8570
12:52:13.823   Training iter 350, batch loss 1.7313, batch acc 0.8628
12:52:14.387   Training iter 400, batch loss 1.7341, batch acc 0.8602
12:52:14.929   Training iter 450, batch loss 1.7327, batch acc 0.8608
12:52:15.481   Training iter 500, batch loss 1.7320, batch acc 0.8626
12:52:16.036   Training iter 550, batch loss 1.7367, batch acc 0.8604
12:52:16.587   Training iter 600, batch loss 1.7332, batch acc 0.8656
12:52:16.589 Training @ 442 epoch...
12:52:17.139   Training iter 50, batch loss 1.7340, batch acc 0.8576
12:52:17.678   Training iter 100, batch loss 1.7353, batch acc 0.8584
12:52:18.230   Training iter 150, batch loss 1.7345, batch acc 0.8634
12:52:18.758   Training iter 200, batch loss 1.7327, batch acc 0.8610
12:52:19.271   Training iter 250, batch loss 1.7377, batch acc 0.8548
12:52:19.782   Training iter 300, batch loss 1.7327, batch acc 0.8572
12:52:20.298   Training iter 350, batch loss 1.7335, batch acc 0.8646
12:52:20.824   Training iter 400, batch loss 1.7349, batch acc 0.8612
12:52:21.363   Training iter 450, batch loss 1.7302, batch acc 0.8612
12:52:21.881   Training iter 500, batch loss 1.7267, batch acc 0.8730
12:52:22.410   Training iter 550, batch loss 1.7330, batch acc 0.8558
12:52:22.929   Training iter 600, batch loss 1.7304, batch acc 0.8646
12:52:22.931 Training @ 443 epoch...
12:52:23.452   Training iter 50, batch loss 1.7336, batch acc 0.8568
12:52:23.967   Training iter 100, batch loss 1.7350, batch acc 0.8558
12:52:24.500   Training iter 150, batch loss 1.7339, batch acc 0.8656
12:52:25.025   Training iter 200, batch loss 1.7279, batch acc 0.8682
12:52:25.553   Training iter 250, batch loss 1.7358, batch acc 0.8632
12:52:26.075   Training iter 300, batch loss 1.7326, batch acc 0.8590
12:52:26.583   Training iter 350, batch loss 1.7308, batch acc 0.8632
12:52:27.095   Training iter 400, batch loss 1.7324, batch acc 0.8626
12:52:27.615   Training iter 450, batch loss 1.7359, batch acc 0.8566
12:52:28.132   Training iter 500, batch loss 1.7324, batch acc 0.8622
12:52:28.642   Training iter 550, batch loss 1.7325, batch acc 0.8554
12:52:29.164   Training iter 600, batch loss 1.7325, batch acc 0.8640
12:52:29.166 Training @ 444 epoch...
12:52:29.682   Training iter 50, batch loss 1.7349, batch acc 0.8608
12:52:30.182   Training iter 100, batch loss 1.7321, batch acc 0.8646
12:52:30.683   Training iter 150, batch loss 1.7324, batch acc 0.8620
12:52:31.184   Training iter 200, batch loss 1.7317, batch acc 0.8632
12:52:31.696   Training iter 250, batch loss 1.7373, batch acc 0.8576
12:52:32.221   Training iter 300, batch loss 1.7369, batch acc 0.8566
12:52:32.720   Training iter 350, batch loss 1.7321, batch acc 0.8612
12:52:33.224   Training iter 400, batch loss 1.7303, batch acc 0.8600
12:52:33.722   Training iter 450, batch loss 1.7296, batch acc 0.8666
12:52:34.228   Training iter 500, batch loss 1.7345, batch acc 0.8582
12:52:34.744   Training iter 550, batch loss 1.7325, batch acc 0.8624
12:52:35.255   Training iter 600, batch loss 1.7308, batch acc 0.8622
12:52:35.257 Training @ 445 epoch...
12:52:35.765   Training iter 50, batch loss 1.7321, batch acc 0.8634
12:52:36.281   Training iter 100, batch loss 1.7298, batch acc 0.8630
12:52:36.785   Training iter 150, batch loss 1.7318, batch acc 0.8660
12:52:37.257   Training iter 200, batch loss 1.7317, batch acc 0.8632
12:52:37.732   Training iter 250, batch loss 1.7353, batch acc 0.8558
12:52:38.202   Training iter 300, batch loss 1.7324, batch acc 0.8646
12:52:38.671   Training iter 350, batch loss 1.7322, batch acc 0.8644
12:52:39.152   Training iter 400, batch loss 1.7340, batch acc 0.8520
12:52:39.643   Training iter 450, batch loss 1.7359, batch acc 0.8566
12:52:40.129   Training iter 500, batch loss 1.7316, batch acc 0.8646
12:52:40.621   Training iter 550, batch loss 1.7342, batch acc 0.8614
12:52:41.109   Training iter 600, batch loss 1.7341, batch acc 0.8600
12:52:41.110 Testing @ 445 epoch...
12:52:41.153     Testing, total mean loss 1.72557, total acc 0.87070
12:52:41.153 Training @ 446 epoch...
12:52:41.645   Training iter 50, batch loss 1.7360, batch acc 0.8628
12:52:42.143   Training iter 100, batch loss 1.7260, batch acc 0.8718
12:52:42.645   Training iter 150, batch loss 1.7312, batch acc 0.8648
12:52:43.134   Training iter 200, batch loss 1.7318, batch acc 0.8598
12:52:43.652   Training iter 250, batch loss 1.7360, batch acc 0.8552
12:52:44.168   Training iter 300, batch loss 1.7355, batch acc 0.8516
12:52:44.700   Training iter 350, batch loss 1.7336, batch acc 0.8602
12:52:45.248   Training iter 400, batch loss 1.7322, batch acc 0.8600
12:52:45.798   Training iter 450, batch loss 1.7309, batch acc 0.8634
12:52:46.352   Training iter 500, batch loss 1.7336, batch acc 0.8636
12:52:46.894   Training iter 550, batch loss 1.7355, batch acc 0.8614
12:52:47.438   Training iter 600, batch loss 1.7327, batch acc 0.8608
12:52:47.440 Training @ 447 epoch...
12:52:47.972   Training iter 50, batch loss 1.7316, batch acc 0.8626
12:52:48.510   Training iter 100, batch loss 1.7333, batch acc 0.8620
12:52:49.036   Training iter 150, batch loss 1.7371, batch acc 0.8542
12:52:49.564   Training iter 200, batch loss 1.7312, batch acc 0.8626
12:52:50.086   Training iter 250, batch loss 1.7323, batch acc 0.8666
12:52:50.612   Training iter 300, batch loss 1.7334, batch acc 0.8654
12:52:51.116   Training iter 350, batch loss 1.7321, batch acc 0.8568
12:52:51.609   Training iter 400, batch loss 1.7360, batch acc 0.8548
12:52:52.107   Training iter 450, batch loss 1.7297, batch acc 0.8644
12:52:52.609   Training iter 500, batch loss 1.7339, batch acc 0.8584
12:52:53.134   Training iter 550, batch loss 1.7327, batch acc 0.8624
12:52:53.652   Training iter 600, batch loss 1.7319, batch acc 0.8644
12:52:53.654 Training @ 448 epoch...
12:52:54.187   Training iter 50, batch loss 1.7385, batch acc 0.8554
12:52:54.707   Training iter 100, batch loss 1.7338, batch acc 0.8592
12:52:55.209   Training iter 150, batch loss 1.7335, batch acc 0.8636
12:52:55.707   Training iter 200, batch loss 1.7352, batch acc 0.8566
12:52:56.206   Training iter 250, batch loss 1.7316, batch acc 0.8642
12:52:56.703   Training iter 300, batch loss 1.7318, batch acc 0.8586
12:52:57.217   Training iter 350, batch loss 1.7333, batch acc 0.8616
12:52:57.717   Training iter 400, batch loss 1.7343, batch acc 0.8588
12:52:58.215   Training iter 450, batch loss 1.7310, batch acc 0.8644
12:52:58.717   Training iter 500, batch loss 1.7317, batch acc 0.8606
12:52:59.211   Training iter 550, batch loss 1.7293, batch acc 0.8682
12:52:59.709   Training iter 600, batch loss 1.7309, batch acc 0.8642
12:52:59.710 Training @ 449 epoch...
12:53:00.229   Training iter 50, batch loss 1.7392, batch acc 0.8534
12:53:00.757   Training iter 100, batch loss 1.7352, batch acc 0.8550
12:53:01.291   Training iter 150, batch loss 1.7353, batch acc 0.8590
12:53:01.862   Training iter 200, batch loss 1.7336, batch acc 0.8642
12:53:02.419   Training iter 250, batch loss 1.7314, batch acc 0.8634
12:53:02.958   Training iter 300, batch loss 1.7324, batch acc 0.8532
12:53:03.502   Training iter 350, batch loss 1.7316, batch acc 0.8632
12:53:04.043   Training iter 400, batch loss 1.7330, batch acc 0.8634
12:53:04.587   Training iter 450, batch loss 1.7314, batch acc 0.8646
12:53:05.132   Training iter 500, batch loss 1.7300, batch acc 0.8648
12:53:05.683   Training iter 550, batch loss 1.7320, batch acc 0.8636
12:53:06.214   Training iter 600, batch loss 1.7299, batch acc 0.8658
12:53:06.216 Training @ 450 epoch...
12:53:06.755   Training iter 50, batch loss 1.7388, batch acc 0.8582
12:53:07.272   Training iter 100, batch loss 1.7329, batch acc 0.8658
12:53:07.775   Training iter 150, batch loss 1.7273, batch acc 0.8640
12:53:08.265   Training iter 200, batch loss 1.7291, batch acc 0.8630
12:53:08.778   Training iter 250, batch loss 1.7321, batch acc 0.8626
12:53:09.297   Training iter 300, batch loss 1.7361, batch acc 0.8548
12:53:09.798   Training iter 350, batch loss 1.7338, batch acc 0.8592
12:53:10.308   Training iter 400, batch loss 1.7320, batch acc 0.8620
12:53:10.818   Training iter 450, batch loss 1.7336, batch acc 0.8596
12:53:11.317   Training iter 500, batch loss 1.7344, batch acc 0.8590
12:53:11.822   Training iter 550, batch loss 1.7302, batch acc 0.8684
12:53:12.330   Training iter 600, batch loss 1.7345, batch acc 0.8586
12:53:12.332 Testing @ 450 epoch...
12:53:12.374     Testing, total mean loss 1.72554, total acc 0.87090
12:53:12.374 Training @ 451 epoch...
12:53:12.907   Training iter 50, batch loss 1.7326, batch acc 0.8596
12:53:13.427   Training iter 100, batch loss 1.7238, batch acc 0.8732
12:53:13.955   Training iter 150, batch loss 1.7365, batch acc 0.8604
12:53:14.470   Training iter 200, batch loss 1.7369, batch acc 0.8562
12:53:14.979   Training iter 250, batch loss 1.7311, batch acc 0.8610
12:53:15.494   Training iter 300, batch loss 1.7312, batch acc 0.8644
12:53:16.017   Training iter 350, batch loss 1.7331, batch acc 0.8580
12:53:16.551   Training iter 400, batch loss 1.7298, batch acc 0.8678
12:53:17.088   Training iter 450, batch loss 1.7349, batch acc 0.8550
12:53:17.625   Training iter 500, batch loss 1.7393, batch acc 0.8542
12:53:18.159   Training iter 550, batch loss 1.7326, batch acc 0.8658
12:53:18.712   Training iter 600, batch loss 1.7332, batch acc 0.8612
12:53:18.714 Training @ 452 epoch...
12:53:19.264   Training iter 50, batch loss 1.7295, batch acc 0.8682
12:53:19.814   Training iter 100, batch loss 1.7340, batch acc 0.8598
12:53:20.361   Training iter 150, batch loss 1.7375, batch acc 0.8534
12:53:20.902   Training iter 200, batch loss 1.7353, batch acc 0.8548
12:53:21.444   Training iter 250, batch loss 1.7337, batch acc 0.8568
12:53:21.997   Training iter 300, batch loss 1.7339, batch acc 0.8660
12:53:22.559   Training iter 350, batch loss 1.7307, batch acc 0.8600
12:53:23.105   Training iter 400, batch loss 1.7335, batch acc 0.8654
12:53:23.625   Training iter 450, batch loss 1.7310, batch acc 0.8648
12:53:24.136   Training iter 500, batch loss 1.7330, batch acc 0.8596
12:53:24.646   Training iter 550, batch loss 1.7330, batch acc 0.8600
12:53:25.169   Training iter 600, batch loss 1.7298, batch acc 0.8668
12:53:25.171 Training @ 453 epoch...
12:53:25.706   Training iter 50, batch loss 1.7296, batch acc 0.8680
12:53:26.240   Training iter 100, batch loss 1.7305, batch acc 0.8660
12:53:26.772   Training iter 150, batch loss 1.7326, batch acc 0.8642
12:53:27.322   Training iter 200, batch loss 1.7373, batch acc 0.8564
12:53:27.861   Training iter 250, batch loss 1.7297, batch acc 0.8600
12:53:28.403   Training iter 300, batch loss 1.7361, batch acc 0.8556
12:53:28.956   Training iter 350, batch loss 1.7318, batch acc 0.8608
12:53:29.495   Training iter 400, batch loss 1.7298, batch acc 0.8654
12:53:30.015   Training iter 450, batch loss 1.7342, batch acc 0.8576
12:53:30.552   Training iter 500, batch loss 1.7373, batch acc 0.8542
12:53:31.092   Training iter 550, batch loss 1.7326, batch acc 0.8646
12:53:31.634   Training iter 600, batch loss 1.7331, batch acc 0.8612
12:53:31.636 Training @ 454 epoch...
12:53:32.159   Training iter 50, batch loss 1.7378, batch acc 0.8590
12:53:32.679   Training iter 100, batch loss 1.7348, batch acc 0.8592
12:53:33.221   Training iter 150, batch loss 1.7331, batch acc 0.8588
12:53:33.788   Training iter 200, batch loss 1.7327, batch acc 0.8588
12:53:34.348   Training iter 250, batch loss 1.7323, batch acc 0.8616
12:53:34.900   Training iter 300, batch loss 1.7320, batch acc 0.8684
12:53:35.480   Training iter 350, batch loss 1.7304, batch acc 0.8648
12:53:36.045   Training iter 400, batch loss 1.7313, batch acc 0.8572
12:53:36.597   Training iter 450, batch loss 1.7310, batch acc 0.8686
12:53:37.159   Training iter 500, batch loss 1.7298, batch acc 0.8608
12:53:37.739   Training iter 550, batch loss 1.7341, batch acc 0.8636
12:53:38.340   Training iter 600, batch loss 1.7353, batch acc 0.8560
12:53:38.342 Training @ 455 epoch...
12:53:38.913   Training iter 50, batch loss 1.7311, batch acc 0.8650
12:53:39.457   Training iter 100, batch loss 1.7308, batch acc 0.8628
12:53:40.004   Training iter 150, batch loss 1.7327, batch acc 0.8552
12:53:40.594   Training iter 200, batch loss 1.7326, batch acc 0.8576
12:53:41.193   Training iter 250, batch loss 1.7308, batch acc 0.8630
12:53:41.791   Training iter 300, batch loss 1.7329, batch acc 0.8638
12:53:42.354   Training iter 350, batch loss 1.7315, batch acc 0.8638
12:53:42.906   Training iter 400, batch loss 1.7381, batch acc 0.8536
12:53:43.451   Training iter 450, batch loss 1.7347, batch acc 0.8642
12:53:43.990   Training iter 500, batch loss 1.7335, batch acc 0.8648
12:53:44.527   Training iter 550, batch loss 1.7353, batch acc 0.8560
12:53:45.043   Training iter 600, batch loss 1.7306, batch acc 0.8658
12:53:45.045 Testing @ 455 epoch...
12:53:45.087     Testing, total mean loss 1.72552, total acc 0.87090
12:53:45.087 Training @ 456 epoch...
12:53:45.625   Training iter 50, batch loss 1.7293, batch acc 0.8660
12:53:46.170   Training iter 100, batch loss 1.7338, batch acc 0.8650
12:53:46.703   Training iter 150, batch loss 1.7319, batch acc 0.8638
12:53:47.254   Training iter 200, batch loss 1.7410, batch acc 0.8478
12:53:47.807   Training iter 250, batch loss 1.7346, batch acc 0.8622
12:53:48.363   Training iter 300, batch loss 1.7340, batch acc 0.8610
12:53:48.916   Training iter 350, batch loss 1.7351, batch acc 0.8570
12:53:49.447   Training iter 400, batch loss 1.7374, batch acc 0.8540
12:53:49.970   Training iter 450, batch loss 1.7347, batch acc 0.8538
12:53:50.502   Training iter 500, batch loss 1.7284, batch acc 0.8654
12:53:51.032   Training iter 550, batch loss 1.7287, batch acc 0.8664
12:53:51.545   Training iter 600, batch loss 1.7255, batch acc 0.8718
12:53:51.546 Training @ 457 epoch...
12:53:52.040   Training iter 50, batch loss 1.7348, batch acc 0.8632
12:53:52.523   Training iter 100, batch loss 1.7277, batch acc 0.8670
12:53:53.022   Training iter 150, batch loss 1.7325, batch acc 0.8584
12:53:53.554   Training iter 200, batch loss 1.7350, batch acc 0.8590
12:53:54.078   Training iter 250, batch loss 1.7341, batch acc 0.8594
12:53:54.590   Training iter 300, batch loss 1.7326, batch acc 0.8602
12:53:55.092   Training iter 350, batch loss 1.7341, batch acc 0.8622
12:53:55.597   Training iter 400, batch loss 1.7319, batch acc 0.8606
12:53:56.106   Training iter 450, batch loss 1.7303, batch acc 0.8614
12:53:56.607   Training iter 500, batch loss 1.7333, batch acc 0.8674
12:53:57.102   Training iter 550, batch loss 1.7353, batch acc 0.8572
12:53:57.600   Training iter 600, batch loss 1.7328, batch acc 0.8584
12:53:57.602 Training @ 458 epoch...
12:53:58.105   Training iter 50, batch loss 1.7290, batch acc 0.8702
12:53:58.638   Training iter 100, batch loss 1.7330, batch acc 0.8620
12:53:59.127   Training iter 150, batch loss 1.7343, batch acc 0.8496
12:53:59.636   Training iter 200, batch loss 1.7355, batch acc 0.8522
12:54:00.169   Training iter 250, batch loss 1.7359, batch acc 0.8520
12:54:00.687   Training iter 300, batch loss 1.7304, batch acc 0.8650
12:54:01.212   Training iter 350, batch loss 1.7361, batch acc 0.8660
12:54:01.752   Training iter 400, batch loss 1.7350, batch acc 0.8620
12:54:02.292   Training iter 450, batch loss 1.7290, batch acc 0.8676
12:54:02.795   Training iter 500, batch loss 1.7302, batch acc 0.8652
12:54:03.307   Training iter 550, batch loss 1.7309, batch acc 0.8632
12:54:03.837   Training iter 600, batch loss 1.7349, batch acc 0.8606
12:54:03.838 Training @ 459 epoch...
12:54:04.359   Training iter 50, batch loss 1.7362, batch acc 0.8610
12:54:04.860   Training iter 100, batch loss 1.7352, batch acc 0.8592
12:54:05.394   Training iter 150, batch loss 1.7317, batch acc 0.8584
12:54:05.896   Training iter 200, batch loss 1.7368, batch acc 0.8524
12:54:06.379   Training iter 250, batch loss 1.7336, batch acc 0.8656
12:54:06.919   Training iter 300, batch loss 1.7347, batch acc 0.8540
12:54:07.439   Training iter 350, batch loss 1.7281, batch acc 0.8708
12:54:07.959   Training iter 400, batch loss 1.7290, batch acc 0.8648
12:54:08.474   Training iter 450, batch loss 1.7386, batch acc 0.8530
12:54:08.966   Training iter 500, batch loss 1.7279, batch acc 0.8696
12:54:09.467   Training iter 550, batch loss 1.7302, batch acc 0.8638
12:54:09.968   Training iter 600, batch loss 1.7323, batch acc 0.8644
12:54:09.969 Training @ 460 epoch...
12:54:10.480   Training iter 50, batch loss 1.7314, batch acc 0.8586
12:54:10.971   Training iter 100, batch loss 1.7297, batch acc 0.8664
12:54:11.454   Training iter 150, batch loss 1.7339, batch acc 0.8652
12:54:11.924   Training iter 200, batch loss 1.7364, batch acc 0.8580
12:54:12.399   Training iter 250, batch loss 1.7358, batch acc 0.8584
12:54:12.887   Training iter 300, batch loss 1.7310, batch acc 0.8632
12:54:13.366   Training iter 350, batch loss 1.7310, batch acc 0.8606
12:54:13.851   Training iter 400, batch loss 1.7323, batch acc 0.8582
12:54:14.334   Training iter 450, batch loss 1.7367, batch acc 0.8606
12:54:14.823   Training iter 500, batch loss 1.7282, batch acc 0.8728
12:54:15.316   Training iter 550, batch loss 1.7335, batch acc 0.8570
12:54:15.802   Training iter 600, batch loss 1.7343, batch acc 0.8570
12:54:15.804 Testing @ 460 epoch...
12:54:15.846     Testing, total mean loss 1.72550, total acc 0.87090
12:54:15.846 Training @ 461 epoch...
12:54:16.348   Training iter 50, batch loss 1.7333, batch acc 0.8628
12:54:16.839   Training iter 100, batch loss 1.7341, batch acc 0.8568
12:54:17.330   Training iter 150, batch loss 1.7310, batch acc 0.8606
12:54:17.810   Training iter 200, batch loss 1.7387, batch acc 0.8516
12:54:18.303   Training iter 250, batch loss 1.7284, batch acc 0.8706
12:54:18.788   Training iter 300, batch loss 1.7308, batch acc 0.8630
12:54:19.267   Training iter 350, batch loss 1.7335, batch acc 0.8614
12:54:19.749   Training iter 400, batch loss 1.7307, batch acc 0.8672
12:54:20.240   Training iter 450, batch loss 1.7346, batch acc 0.8602
12:54:20.744   Training iter 500, batch loss 1.7323, batch acc 0.8596
12:54:21.241   Training iter 550, batch loss 1.7330, batch acc 0.8636
12:54:21.721   Training iter 600, batch loss 1.7337, batch acc 0.8586
12:54:21.723 Training @ 462 epoch...
12:54:22.223   Training iter 50, batch loss 1.7338, batch acc 0.8532
12:54:22.720   Training iter 100, batch loss 1.7383, batch acc 0.8546
12:54:23.239   Training iter 150, batch loss 1.7325, batch acc 0.8612
12:54:23.740   Training iter 200, batch loss 1.7379, batch acc 0.8528
12:54:24.247   Training iter 250, batch loss 1.7268, batch acc 0.8714
12:54:24.744   Training iter 300, batch loss 1.7341, batch acc 0.8642
12:54:25.260   Training iter 350, batch loss 1.7363, batch acc 0.8512
12:54:25.780   Training iter 400, batch loss 1.7286, batch acc 0.8690
12:54:26.291   Training iter 450, batch loss 1.7325, batch acc 0.8652
12:54:26.799   Training iter 500, batch loss 1.7286, batch acc 0.8676
12:54:27.312   Training iter 550, batch loss 1.7343, batch acc 0.8568
12:54:27.839   Training iter 600, batch loss 1.7307, batch acc 0.8674
12:54:27.841 Training @ 463 epoch...
12:54:28.386   Training iter 50, batch loss 1.7319, batch acc 0.8666
12:54:28.921   Training iter 100, batch loss 1.7340, batch acc 0.8578
12:54:29.456   Training iter 150, batch loss 1.7333, batch acc 0.8606
12:54:29.990   Training iter 200, batch loss 1.7307, batch acc 0.8624
12:54:30.556   Training iter 250, batch loss 1.7356, batch acc 0.8610
12:54:31.081   Training iter 300, batch loss 1.7313, batch acc 0.8624
12:54:31.602   Training iter 350, batch loss 1.7361, batch acc 0.8576
12:54:32.107   Training iter 400, batch loss 1.7325, batch acc 0.8606
12:54:32.616   Training iter 450, batch loss 1.7293, batch acc 0.8680
12:54:33.139   Training iter 500, batch loss 1.7347, batch acc 0.8596
12:54:33.708   Training iter 550, batch loss 1.7336, batch acc 0.8600
12:54:34.280   Training iter 600, batch loss 1.7311, batch acc 0.8602
12:54:34.282 Training @ 464 epoch...
12:54:34.855   Training iter 50, batch loss 1.7343, batch acc 0.8586
12:54:35.409   Training iter 100, batch loss 1.7361, batch acc 0.8564
12:54:35.922   Training iter 150, batch loss 1.7297, batch acc 0.8650
12:54:36.432   Training iter 200, batch loss 1.7359, batch acc 0.8574
12:54:36.960   Training iter 250, batch loss 1.7321, batch acc 0.8598
12:54:37.484   Training iter 300, batch loss 1.7302, batch acc 0.8604
12:54:38.025   Training iter 350, batch loss 1.7323, batch acc 0.8670
12:54:38.558   Training iter 400, batch loss 1.7311, batch acc 0.8628
12:54:39.088   Training iter 450, batch loss 1.7353, batch acc 0.8612
12:54:39.600   Training iter 500, batch loss 1.7295, batch acc 0.8630
12:54:40.110   Training iter 550, batch loss 1.7347, batch acc 0.8572
12:54:40.639   Training iter 600, batch loss 1.7327, batch acc 0.8680
12:54:40.641 Training @ 465 epoch...
12:54:41.158   Training iter 50, batch loss 1.7320, batch acc 0.8578
12:54:41.679   Training iter 100, batch loss 1.7350, batch acc 0.8558
12:54:42.207   Training iter 150, batch loss 1.7331, batch acc 0.8654
12:54:42.713   Training iter 200, batch loss 1.7335, batch acc 0.8610
12:54:43.195   Training iter 250, batch loss 1.7392, batch acc 0.8520
12:54:43.679   Training iter 300, batch loss 1.7338, batch acc 0.8584
12:54:44.139   Training iter 350, batch loss 1.7299, batch acc 0.8618
12:54:44.599   Training iter 400, batch loss 1.7313, batch acc 0.8656
12:54:45.048   Training iter 450, batch loss 1.7312, batch acc 0.8588
12:54:45.474   Training iter 500, batch loss 1.7298, batch acc 0.8656
12:54:45.898   Training iter 550, batch loss 1.7345, batch acc 0.8656
12:54:46.326   Training iter 600, batch loss 1.7306, batch acc 0.8694
12:54:46.328 Testing @ 465 epoch...
12:54:46.370     Testing, total mean loss 1.72547, total acc 0.87080
12:54:46.370 Training @ 466 epoch...
12:54:46.833   Training iter 50, batch loss 1.7388, batch acc 0.8504
12:54:47.262   Training iter 100, batch loss 1.7336, batch acc 0.8624
12:54:47.717   Training iter 150, batch loss 1.7362, batch acc 0.8488
12:54:48.164   Training iter 200, batch loss 1.7320, batch acc 0.8636
12:54:48.619   Training iter 250, batch loss 1.7320, batch acc 0.8642
12:54:49.035   Training iter 300, batch loss 1.7335, batch acc 0.8598
12:54:49.443   Training iter 350, batch loss 1.7330, batch acc 0.8610
12:54:49.848   Training iter 400, batch loss 1.7325, batch acc 0.8640
12:54:50.272   Training iter 450, batch loss 1.7278, batch acc 0.8736
12:54:50.691   Training iter 500, batch loss 1.7313, batch acc 0.8626
12:54:51.094   Training iter 550, batch loss 1.7281, batch acc 0.8706
12:54:51.519   Training iter 600, batch loss 1.7350, batch acc 0.8572
12:54:51.521 Training @ 467 epoch...
12:54:51.939   Training iter 50, batch loss 1.7328, batch acc 0.8618
12:54:52.341   Training iter 100, batch loss 1.7341, batch acc 0.8636
12:54:52.754   Training iter 150, batch loss 1.7314, batch acc 0.8644
12:54:53.159   Training iter 200, batch loss 1.7358, batch acc 0.8592
12:54:53.580   Training iter 250, batch loss 1.7328, batch acc 0.8602
12:54:54.012   Training iter 300, batch loss 1.7341, batch acc 0.8616
12:54:54.430   Training iter 350, batch loss 1.7297, batch acc 0.8618
12:54:54.840   Training iter 400, batch loss 1.7329, batch acc 0.8616
12:54:55.234   Training iter 450, batch loss 1.7297, batch acc 0.8650
12:54:55.633   Training iter 500, batch loss 1.7315, batch acc 0.8620
12:54:56.049   Training iter 550, batch loss 1.7332, batch acc 0.8586
12:54:56.505   Training iter 600, batch loss 1.7360, batch acc 0.8564
12:54:56.507 Training @ 468 epoch...
12:54:56.965   Training iter 50, batch loss 1.7313, batch acc 0.8706
12:54:57.404   Training iter 100, batch loss 1.7324, batch acc 0.8590
12:54:57.860   Training iter 150, batch loss 1.7345, batch acc 0.8582
12:54:58.289   Training iter 200, batch loss 1.7305, batch acc 0.8636
12:54:58.717   Training iter 250, batch loss 1.7334, batch acc 0.8634
12:54:59.118   Training iter 300, batch loss 1.7329, batch acc 0.8590
12:54:59.547   Training iter 350, batch loss 1.7356, batch acc 0.8548
12:54:59.977   Training iter 400, batch loss 1.7338, batch acc 0.8576
12:55:00.419   Training iter 450, batch loss 1.7327, batch acc 0.8620
12:55:00.841   Training iter 500, batch loss 1.7317, batch acc 0.8674
12:55:01.278   Training iter 550, batch loss 1.7332, batch acc 0.8604
12:55:01.755   Training iter 600, batch loss 1.7318, batch acc 0.8612
12:55:01.757 Training @ 469 epoch...
12:55:02.265   Training iter 50, batch loss 1.7332, batch acc 0.8604
12:55:02.788   Training iter 100, batch loss 1.7315, batch acc 0.8644
12:55:03.302   Training iter 150, batch loss 1.7317, batch acc 0.8556
12:55:03.788   Training iter 200, batch loss 1.7320, batch acc 0.8596
12:55:04.297   Training iter 250, batch loss 1.7321, batch acc 0.8626
12:55:05.011   Training iter 300, batch loss 1.7347, batch acc 0.8550
12:55:05.734   Training iter 350, batch loss 1.7276, batch acc 0.8684
12:55:06.337   Training iter 400, batch loss 1.7344, batch acc 0.8692
12:55:06.989   Training iter 450, batch loss 1.7310, batch acc 0.8646
12:55:07.700   Training iter 500, batch loss 1.7371, batch acc 0.8542
12:55:08.339   Training iter 550, batch loss 1.7335, batch acc 0.8606
12:55:08.855   Training iter 600, batch loss 1.7348, batch acc 0.8616
12:55:08.857 Training @ 470 epoch...
12:55:09.342   Training iter 50, batch loss 1.7317, batch acc 0.8598
12:55:09.883   Training iter 100, batch loss 1.7318, batch acc 0.8612
12:55:10.469   Training iter 150, batch loss 1.7324, batch acc 0.8612
12:55:10.981   Training iter 200, batch loss 1.7291, batch acc 0.8682
12:55:11.496   Training iter 250, batch loss 1.7347, batch acc 0.8566
12:55:12.019   Training iter 300, batch loss 1.7349, batch acc 0.8644
12:55:12.542   Training iter 350, batch loss 1.7363, batch acc 0.8578
12:55:13.121   Training iter 400, batch loss 1.7292, batch acc 0.8684
12:55:13.744   Training iter 450, batch loss 1.7286, batch acc 0.8686
12:55:14.271   Training iter 500, batch loss 1.7353, batch acc 0.8562
12:55:14.763   Training iter 550, batch loss 1.7312, batch acc 0.8620
12:55:15.248   Training iter 600, batch loss 1.7385, batch acc 0.8540
12:55:15.249 Testing @ 470 epoch...
12:55:15.296     Testing, total mean loss 1.72545, total acc 0.87090
12:55:15.296 Training @ 471 epoch...
12:55:15.766   Training iter 50, batch loss 1.7304, batch acc 0.8592
12:55:16.257   Training iter 100, batch loss 1.7318, batch acc 0.8578
12:55:16.825   Training iter 150, batch loss 1.7321, batch acc 0.8658
12:55:17.369   Training iter 200, batch loss 1.7352, batch acc 0.8588
12:55:17.849   Training iter 250, batch loss 1.7284, batch acc 0.8694
12:55:18.367   Training iter 300, batch loss 1.7326, batch acc 0.8670
12:55:18.879   Training iter 350, batch loss 1.7377, batch acc 0.8530
12:55:19.399   Training iter 400, batch loss 1.7341, batch acc 0.8622
12:55:19.914   Training iter 450, batch loss 1.7323, batch acc 0.8594
12:55:20.438   Training iter 500, batch loss 1.7327, batch acc 0.8642
12:55:20.946   Training iter 550, batch loss 1.7362, batch acc 0.8548
12:55:21.459   Training iter 600, batch loss 1.7301, batch acc 0.8664
12:55:21.461 Training @ 472 epoch...
12:55:21.975   Training iter 50, batch loss 1.7319, batch acc 0.8586
12:55:22.451   Training iter 100, batch loss 1.7339, batch acc 0.8582
12:55:22.932   Training iter 150, batch loss 1.7317, batch acc 0.8624
12:55:23.430   Training iter 200, batch loss 1.7279, batch acc 0.8698
12:55:23.901   Training iter 250, batch loss 1.7353, batch acc 0.8640
12:55:24.392   Training iter 300, batch loss 1.7359, batch acc 0.8592
12:55:24.895   Training iter 350, batch loss 1.7344, batch acc 0.8618
12:55:25.419   Training iter 400, batch loss 1.7353, batch acc 0.8558
12:55:25.903   Training iter 450, batch loss 1.7299, batch acc 0.8636
12:55:26.401   Training iter 500, batch loss 1.7302, batch acc 0.8648
12:55:26.897   Training iter 550, batch loss 1.7344, batch acc 0.8552
12:55:27.396   Training iter 600, batch loss 1.7328, batch acc 0.8630
12:55:27.397 Training @ 473 epoch...
12:55:27.910   Training iter 50, batch loss 1.7344, batch acc 0.8568
12:55:28.421   Training iter 100, batch loss 1.7293, batch acc 0.8700
12:55:28.918   Training iter 150, batch loss 1.7314, batch acc 0.8604
12:55:29.438   Training iter 200, batch loss 1.7309, batch acc 0.8630
12:55:29.961   Training iter 250, batch loss 1.7349, batch acc 0.8536
12:55:30.517   Training iter 300, batch loss 1.7334, batch acc 0.8582
12:55:31.059   Training iter 350, batch loss 1.7316, batch acc 0.8638
12:55:31.605   Training iter 400, batch loss 1.7337, batch acc 0.8608
12:55:32.148   Training iter 450, batch loss 1.7311, batch acc 0.8608
12:55:32.688   Training iter 500, batch loss 1.7326, batch acc 0.8648
12:55:33.229   Training iter 550, batch loss 1.7327, batch acc 0.8670
12:55:33.763   Training iter 600, batch loss 1.7373, batch acc 0.8574
12:55:33.764 Training @ 474 epoch...
12:55:34.310   Training iter 50, batch loss 1.7330, batch acc 0.8664
12:55:34.849   Training iter 100, batch loss 1.7295, batch acc 0.8634
12:55:35.341   Training iter 150, batch loss 1.7288, batch acc 0.8740
12:55:35.823   Training iter 200, batch loss 1.7354, batch acc 0.8616
12:55:36.320   Training iter 250, batch loss 1.7312, batch acc 0.8620
12:55:36.810   Training iter 300, batch loss 1.7315, batch acc 0.8594
12:55:37.315   Training iter 350, batch loss 1.7313, batch acc 0.8656
12:55:37.830   Training iter 400, batch loss 1.7369, batch acc 0.8564
12:55:38.355   Training iter 450, batch loss 1.7349, batch acc 0.8580
12:55:38.880   Training iter 500, batch loss 1.7356, batch acc 0.8526
12:55:39.397   Training iter 550, batch loss 1.7324, batch acc 0.8616
12:55:39.895   Training iter 600, batch loss 1.7328, batch acc 0.8566
12:55:39.897 Training @ 475 epoch...
12:55:40.412   Training iter 50, batch loss 1.7382, batch acc 0.8542
12:55:40.947   Training iter 100, batch loss 1.7303, batch acc 0.8676
12:55:41.481   Training iter 150, batch loss 1.7318, batch acc 0.8680
12:55:42.027   Training iter 200, batch loss 1.7337, batch acc 0.8638
12:55:42.549   Training iter 250, batch loss 1.7324, batch acc 0.8618
12:55:43.092   Training iter 300, batch loss 1.7342, batch acc 0.8596
12:55:43.615   Training iter 350, batch loss 1.7304, batch acc 0.8616
12:55:44.201   Training iter 400, batch loss 1.7336, batch acc 0.8578
12:55:44.763   Training iter 450, batch loss 1.7313, batch acc 0.8608
12:55:45.288   Training iter 500, batch loss 1.7381, batch acc 0.8528
12:55:45.817   Training iter 550, batch loss 1.7282, batch acc 0.8692
12:55:46.344   Training iter 600, batch loss 1.7314, batch acc 0.8586
12:55:46.345 Testing @ 475 epoch...
12:55:46.391     Testing, total mean loss 1.72543, total acc 0.87100
12:55:46.391 Training @ 476 epoch...
12:55:46.905   Training iter 50, batch loss 1.7339, batch acc 0.8624
12:55:47.378   Training iter 100, batch loss 1.7386, batch acc 0.8558
12:55:47.855   Training iter 150, batch loss 1.7321, batch acc 0.8586
12:55:48.342   Training iter 200, batch loss 1.7334, batch acc 0.8570
12:55:48.832   Training iter 250, batch loss 1.7319, batch acc 0.8634
12:55:49.343   Training iter 300, batch loss 1.7312, batch acc 0.8654
12:55:49.855   Training iter 350, batch loss 1.7343, batch acc 0.8572
12:55:50.365   Training iter 400, batch loss 1.7259, batch acc 0.8700
12:55:50.862   Training iter 450, batch loss 1.7356, batch acc 0.8590
12:55:51.363   Training iter 500, batch loss 1.7338, batch acc 0.8606
12:55:51.867   Training iter 550, batch loss 1.7316, batch acc 0.8670
12:55:52.388   Training iter 600, batch loss 1.7312, batch acc 0.8592
12:55:52.390 Training @ 477 epoch...
12:55:52.907   Training iter 50, batch loss 1.7335, batch acc 0.8582
12:55:53.405   Training iter 100, batch loss 1.7301, batch acc 0.8654
12:55:53.856   Training iter 150, batch loss 1.7293, batch acc 0.8650
12:55:54.297   Training iter 200, batch loss 1.7335, batch acc 0.8654
12:55:54.760   Training iter 250, batch loss 1.7353, batch acc 0.8556
12:55:55.232   Training iter 300, batch loss 1.7364, batch acc 0.8568
12:55:55.709   Training iter 350, batch loss 1.7290, batch acc 0.8648
12:55:56.201   Training iter 400, batch loss 1.7306, batch acc 0.8606
12:55:56.654   Training iter 450, batch loss 1.7316, batch acc 0.8596
12:55:57.133   Training iter 500, batch loss 1.7292, batch acc 0.8646
12:55:57.628   Training iter 550, batch loss 1.7385, batch acc 0.8580
12:55:58.143   Training iter 600, batch loss 1.7365, batch acc 0.8620
12:55:58.145 Training @ 478 epoch...
12:55:58.657   Training iter 50, batch loss 1.7296, batch acc 0.8638
12:55:59.166   Training iter 100, batch loss 1.7314, batch acc 0.8624
12:55:59.690   Training iter 150, batch loss 1.7314, batch acc 0.8618
12:56:00.233   Training iter 200, batch loss 1.7338, batch acc 0.8654
12:56:00.777   Training iter 250, batch loss 1.7339, batch acc 0.8600
12:56:01.339   Training iter 300, batch loss 1.7335, batch acc 0.8620
12:56:01.935   Training iter 350, batch loss 1.7347, batch acc 0.8662
12:56:02.507   Training iter 400, batch loss 1.7322, batch acc 0.8606
12:56:03.085   Training iter 450, batch loss 1.7280, batch acc 0.8644
12:56:03.637   Training iter 500, batch loss 1.7372, batch acc 0.8534
12:56:04.196   Training iter 550, batch loss 1.7312, batch acc 0.8614
12:56:04.743   Training iter 600, batch loss 1.7363, batch acc 0.8554
12:56:04.744 Training @ 479 epoch...
12:56:05.311   Training iter 50, batch loss 1.7309, batch acc 0.8622
12:56:05.891   Training iter 100, batch loss 1.7338, batch acc 0.8596
12:56:06.477   Training iter 150, batch loss 1.7289, batch acc 0.8706
12:56:07.053   Training iter 200, batch loss 1.7340, batch acc 0.8576
12:56:07.595   Training iter 250, batch loss 1.7342, batch acc 0.8624
12:56:08.134   Training iter 300, batch loss 1.7356, batch acc 0.8550
12:56:08.666   Training iter 350, batch loss 1.7272, batch acc 0.8684
12:56:09.182   Training iter 400, batch loss 1.7333, batch acc 0.8620
12:56:09.707   Training iter 450, batch loss 1.7335, batch acc 0.8606
12:56:10.234   Training iter 500, batch loss 1.7335, batch acc 0.8584
12:56:10.762   Training iter 550, batch loss 1.7371, batch acc 0.8534
12:56:11.276   Training iter 600, batch loss 1.7312, batch acc 0.8650
12:56:11.278 Training @ 480 epoch...
12:56:11.778   Training iter 50, batch loss 1.7351, batch acc 0.8580
12:56:12.259   Training iter 100, batch loss 1.7314, batch acc 0.8594
12:56:12.730   Training iter 150, batch loss 1.7328, batch acc 0.8550
12:56:13.217   Training iter 200, batch loss 1.7316, batch acc 0.8668
12:56:13.756   Training iter 250, batch loss 1.7339, batch acc 0.8600
12:56:14.274   Training iter 300, batch loss 1.7299, batch acc 0.8688
12:56:14.795   Training iter 350, batch loss 1.7340, batch acc 0.8596
12:56:15.312   Training iter 400, batch loss 1.7321, batch acc 0.8664
12:56:15.822   Training iter 450, batch loss 1.7349, batch acc 0.8594
12:56:16.328   Training iter 500, batch loss 1.7345, batch acc 0.8618
12:56:16.837   Training iter 550, batch loss 1.7294, batch acc 0.8620
12:56:17.354   Training iter 600, batch loss 1.7336, batch acc 0.8614
12:56:17.356 Testing @ 480 epoch...
12:56:17.399     Testing, total mean loss 1.72541, total acc 0.87090
12:56:17.399 Training @ 481 epoch...
12:56:17.908   Training iter 50, batch loss 1.7344, batch acc 0.8622
12:56:18.431   Training iter 100, batch loss 1.7383, batch acc 0.8504
12:56:18.947   Training iter 150, batch loss 1.7339, batch acc 0.8570
12:56:19.430   Training iter 200, batch loss 1.7303, batch acc 0.8674
12:56:19.888   Training iter 250, batch loss 1.7247, batch acc 0.8768
12:56:20.369   Training iter 300, batch loss 1.7339, batch acc 0.8612
12:56:20.851   Training iter 350, batch loss 1.7310, batch acc 0.8670
12:56:21.334   Training iter 400, batch loss 1.7323, batch acc 0.8600
12:56:21.821   Training iter 450, batch loss 1.7340, batch acc 0.8614
12:56:22.317   Training iter 500, batch loss 1.7264, batch acc 0.8666
12:56:22.845   Training iter 550, batch loss 1.7374, batch acc 0.8534
12:56:23.366   Training iter 600, batch loss 1.7365, batch acc 0.8518
12:56:23.368 Training @ 482 epoch...
12:56:23.896   Training iter 50, batch loss 1.7288, batch acc 0.8652
12:56:24.424   Training iter 100, batch loss 1.7340, batch acc 0.8630
12:56:24.951   Training iter 150, batch loss 1.7322, batch acc 0.8616
12:56:25.482   Training iter 200, batch loss 1.7262, batch acc 0.8678
12:56:26.005   Training iter 250, batch loss 1.7358, batch acc 0.8584
12:56:26.533   Training iter 300, batch loss 1.7380, batch acc 0.8584
12:56:27.060   Training iter 350, batch loss 1.7387, batch acc 0.8520
12:56:27.588   Training iter 400, batch loss 1.7301, batch acc 0.8626
12:56:28.123   Training iter 450, batch loss 1.7306, batch acc 0.8656
12:56:28.650   Training iter 500, batch loss 1.7302, batch acc 0.8664
12:56:29.188   Training iter 550, batch loss 1.7330, batch acc 0.8642
12:56:29.742   Training iter 600, batch loss 1.7356, batch acc 0.8536
12:56:29.744 Training @ 483 epoch...
12:56:30.298   Training iter 50, batch loss 1.7311, batch acc 0.8644
12:56:30.851   Training iter 100, batch loss 1.7290, batch acc 0.8640
12:56:31.412   Training iter 150, batch loss 1.7338, batch acc 0.8604
12:56:31.982   Training iter 200, batch loss 1.7344, batch acc 0.8560
12:56:32.541   Training iter 250, batch loss 1.7307, batch acc 0.8634
12:56:33.094   Training iter 300, batch loss 1.7315, batch acc 0.8642
12:56:33.651   Training iter 350, batch loss 1.7302, batch acc 0.8632
12:56:34.203   Training iter 400, batch loss 1.7340, batch acc 0.8586
12:56:34.767   Training iter 450, batch loss 1.7355, batch acc 0.8602
12:56:35.293   Training iter 500, batch loss 1.7347, batch acc 0.8628
12:56:35.811   Training iter 550, batch loss 1.7323, batch acc 0.8638
12:56:36.319   Training iter 600, batch loss 1.7359, batch acc 0.8558
12:56:36.321 Training @ 484 epoch...
12:56:36.835   Training iter 50, batch loss 1.7391, batch acc 0.8554
12:56:37.340   Training iter 100, batch loss 1.7371, batch acc 0.8548
12:56:37.843   Training iter 150, batch loss 1.7332, batch acc 0.8614
12:56:38.336   Training iter 200, batch loss 1.7309, batch acc 0.8640
12:56:38.802   Training iter 250, batch loss 1.7250, batch acc 0.8736
12:56:39.283   Training iter 300, batch loss 1.7346, batch acc 0.8586
12:56:39.752   Training iter 350, batch loss 1.7335, batch acc 0.8550
12:56:40.233   Training iter 400, batch loss 1.7294, batch acc 0.8614
12:56:40.711   Training iter 450, batch loss 1.7354, batch acc 0.8630
12:56:41.214   Training iter 500, batch loss 1.7302, batch acc 0.8618
12:56:41.712   Training iter 550, batch loss 1.7344, batch acc 0.8646
12:56:42.222   Training iter 600, batch loss 1.7301, batch acc 0.8646
12:56:42.224 Training @ 485 epoch...
12:56:42.735   Training iter 50, batch loss 1.7351, batch acc 0.8570
12:56:43.257   Training iter 100, batch loss 1.7303, batch acc 0.8690
12:56:43.763   Training iter 150, batch loss 1.7351, batch acc 0.8542
12:56:44.262   Training iter 200, batch loss 1.7312, batch acc 0.8656
12:56:44.761   Training iter 250, batch loss 1.7331, batch acc 0.8586
12:56:45.278   Training iter 300, batch loss 1.7301, batch acc 0.8676
12:56:45.778   Training iter 350, batch loss 1.7260, batch acc 0.8694
12:56:46.287   Training iter 400, batch loss 1.7364, batch acc 0.8538
12:56:46.810   Training iter 450, batch loss 1.7336, batch acc 0.8590
12:56:47.310   Training iter 500, batch loss 1.7332, batch acc 0.8632
12:56:47.808   Training iter 550, batch loss 1.7381, batch acc 0.8540
12:56:48.321   Training iter 600, batch loss 1.7308, batch acc 0.8648
12:56:48.323 Testing @ 485 epoch...
12:56:48.365     Testing, total mean loss 1.72540, total acc 0.87100
12:56:48.365 Training @ 486 epoch...
12:56:48.883   Training iter 50, batch loss 1.7362, batch acc 0.8592
12:56:49.390   Training iter 100, batch loss 1.7326, batch acc 0.8614
12:56:49.896   Training iter 150, batch loss 1.7271, batch acc 0.8702
12:56:50.400   Training iter 200, batch loss 1.7315, batch acc 0.8704
12:56:50.908   Training iter 250, batch loss 1.7301, batch acc 0.8632
12:56:51.408   Training iter 300, batch loss 1.7324, batch acc 0.8584
12:56:51.913   Training iter 350, batch loss 1.7357, batch acc 0.8586
12:56:52.425   Training iter 400, batch loss 1.7340, batch acc 0.8642
12:56:52.935   Training iter 450, batch loss 1.7321, batch acc 0.8634
12:56:53.445   Training iter 500, batch loss 1.7362, batch acc 0.8480
12:56:53.962   Training iter 550, batch loss 1.7310, batch acc 0.8620
12:56:54.504   Training iter 600, batch loss 1.7339, batch acc 0.8586
12:56:54.505 Training @ 487 epoch...
12:56:55.027   Training iter 50, batch loss 1.7334, batch acc 0.8618
12:56:55.551   Training iter 100, batch loss 1.7279, batch acc 0.8648
12:56:56.063   Training iter 150, batch loss 1.7329, batch acc 0.8632
12:56:56.550   Training iter 200, batch loss 1.7346, batch acc 0.8662
12:56:57.030   Training iter 250, batch loss 1.7315, batch acc 0.8622
12:56:57.519   Training iter 300, batch loss 1.7334, batch acc 0.8626
12:56:58.009   Training iter 350, batch loss 1.7316, batch acc 0.8658
12:56:58.489   Training iter 400, batch loss 1.7271, batch acc 0.8698
12:56:59.002   Training iter 450, batch loss 1.7361, batch acc 0.8526
12:56:59.551   Training iter 500, batch loss 1.7341, batch acc 0.8620
12:57:00.094   Training iter 550, batch loss 1.7391, batch acc 0.8454
12:57:00.639   Training iter 600, batch loss 1.7312, batch acc 0.8610
12:57:00.640 Training @ 488 epoch...
12:57:01.192   Training iter 50, batch loss 1.7333, batch acc 0.8606
12:57:01.765   Training iter 100, batch loss 1.7356, batch acc 0.8546
12:57:02.354   Training iter 150, batch loss 1.7324, batch acc 0.8604
12:57:02.934   Training iter 200, batch loss 1.7338, batch acc 0.8574
12:57:03.546   Training iter 250, batch loss 1.7286, batch acc 0.8712
12:57:04.125   Training iter 300, batch loss 1.7348, batch acc 0.8560
12:57:04.679   Training iter 350, batch loss 1.7342, batch acc 0.8586
12:57:05.243   Training iter 400, batch loss 1.7299, batch acc 0.8672
12:57:05.802   Training iter 450, batch loss 1.7297, batch acc 0.8648
12:57:06.360   Training iter 500, batch loss 1.7323, batch acc 0.8612
12:57:06.919   Training iter 550, batch loss 1.7354, batch acc 0.8628
12:57:07.458   Training iter 600, batch loss 1.7328, batch acc 0.8616
12:57:07.460 Training @ 489 epoch...
12:57:07.995   Training iter 50, batch loss 1.7308, batch acc 0.8658
12:57:08.502   Training iter 100, batch loss 1.7344, batch acc 0.8580
12:57:09.014   Training iter 150, batch loss 1.7320, batch acc 0.8606
12:57:09.525   Training iter 200, batch loss 1.7328, batch acc 0.8612
12:57:10.035   Training iter 250, batch loss 1.7364, batch acc 0.8600
12:57:10.546   Training iter 300, batch loss 1.7303, batch acc 0.8636
12:57:11.054   Training iter 350, batch loss 1.7358, batch acc 0.8528
12:57:11.557   Training iter 400, batch loss 1.7341, batch acc 0.8582
12:57:12.064   Training iter 450, batch loss 1.7319, batch acc 0.8646
12:57:12.569   Training iter 500, batch loss 1.7331, batch acc 0.8664
12:57:13.085   Training iter 550, batch loss 1.7332, batch acc 0.8586
12:57:13.588   Training iter 600, batch loss 1.7279, batch acc 0.8690
12:57:13.590 Training @ 490 epoch...
12:57:14.100   Training iter 50, batch loss 1.7347, batch acc 0.8540
12:57:14.601   Training iter 100, batch loss 1.7314, batch acc 0.8620
12:57:15.108   Training iter 150, batch loss 1.7309, batch acc 0.8644
12:57:15.626   Training iter 200, batch loss 1.7331, batch acc 0.8594
12:57:16.170   Training iter 250, batch loss 1.7327, batch acc 0.8624
12:57:16.704   Training iter 300, batch loss 1.7336, batch acc 0.8598
12:57:17.219   Training iter 350, batch loss 1.7314, batch acc 0.8646
12:57:17.734   Training iter 400, batch loss 1.7331, batch acc 0.8610
12:57:18.243   Training iter 450, batch loss 1.7374, batch acc 0.8552
12:57:18.758   Training iter 500, batch loss 1.7343, batch acc 0.8596
12:57:19.261   Training iter 550, batch loss 1.7261, batch acc 0.8726
12:57:19.774   Training iter 600, batch loss 1.7341, batch acc 0.8600
12:57:19.776 Testing @ 490 epoch...
12:57:19.818     Testing, total mean loss 1.72537, total acc 0.87090
12:57:19.818 Training @ 491 epoch...
12:57:20.351   Training iter 50, batch loss 1.7360, batch acc 0.8598
12:57:20.876   Training iter 100, batch loss 1.7312, batch acc 0.8640
12:57:21.414   Training iter 150, batch loss 1.7329, batch acc 0.8650
12:57:21.939   Training iter 200, batch loss 1.7315, batch acc 0.8586
12:57:22.458   Training iter 250, batch loss 1.7286, batch acc 0.8648
12:57:22.949   Training iter 300, batch loss 1.7363, batch acc 0.8572
12:57:23.446   Training iter 350, batch loss 1.7347, batch acc 0.8578
12:57:23.944   Training iter 400, batch loss 1.7309, batch acc 0.8668
12:57:24.450   Training iter 450, batch loss 1.7312, batch acc 0.8564
12:57:24.958   Training iter 500, batch loss 1.7347, batch acc 0.8602
12:57:25.478   Training iter 550, batch loss 1.7333, batch acc 0.8630
12:57:25.995   Training iter 600, batch loss 1.7314, batch acc 0.8648
12:57:25.997 Training @ 492 epoch...
12:57:26.522   Training iter 50, batch loss 1.7343, batch acc 0.8562
12:57:27.060   Training iter 100, batch loss 1.7309, batch acc 0.8676
12:57:27.595   Training iter 150, batch loss 1.7343, batch acc 0.8554
12:57:28.123   Training iter 200, batch loss 1.7331, batch acc 0.8592
12:57:28.639   Training iter 250, batch loss 1.7323, batch acc 0.8658
12:57:29.159   Training iter 300, batch loss 1.7297, batch acc 0.8646
12:57:29.680   Training iter 350, batch loss 1.7360, batch acc 0.8586
12:57:30.191   Training iter 400, batch loss 1.7323, batch acc 0.8662
12:57:30.669   Training iter 450, batch loss 1.7341, batch acc 0.8566
12:57:31.144   Training iter 500, batch loss 1.7320, batch acc 0.8658
12:57:31.621   Training iter 550, batch loss 1.7317, batch acc 0.8598
12:57:32.103   Training iter 600, batch loss 1.7319, batch acc 0.8618
12:57:32.105 Training @ 493 epoch...
12:57:32.594   Training iter 50, batch loss 1.7329, batch acc 0.8634
12:57:33.078   Training iter 100, batch loss 1.7284, batch acc 0.8702
12:57:33.586   Training iter 150, batch loss 1.7336, batch acc 0.8600
12:57:34.090   Training iter 200, batch loss 1.7291, batch acc 0.8680
12:57:34.585   Training iter 250, batch loss 1.7357, batch acc 0.8560
12:57:35.112   Training iter 300, batch loss 1.7358, batch acc 0.8566
12:57:35.627   Training iter 350, batch loss 1.7329, batch acc 0.8594
12:57:36.138   Training iter 400, batch loss 1.7280, batch acc 0.8672
12:57:36.634   Training iter 450, batch loss 1.7347, batch acc 0.8572
12:57:37.159   Training iter 500, batch loss 1.7370, batch acc 0.8550
12:57:37.715   Training iter 550, batch loss 1.7304, batch acc 0.8624
12:57:38.287   Training iter 600, batch loss 1.7342, batch acc 0.8598
12:57:38.289 Training @ 494 epoch...
12:57:38.839   Training iter 50, batch loss 1.7289, batch acc 0.8614
12:57:39.396   Training iter 100, batch loss 1.7314, batch acc 0.8556
12:57:39.950   Training iter 150, batch loss 1.7317, batch acc 0.8664
12:57:40.523   Training iter 200, batch loss 1.7372, batch acc 0.8556
12:57:41.088   Training iter 250, batch loss 1.7310, batch acc 0.8658
12:57:41.640   Training iter 300, batch loss 1.7320, batch acc 0.8638
12:57:42.200   Training iter 350, batch loss 1.7339, batch acc 0.8612
12:57:42.759   Training iter 400, batch loss 1.7342, batch acc 0.8606
12:57:43.322   Training iter 450, batch loss 1.7299, batch acc 0.8628
12:57:43.880   Training iter 500, batch loss 1.7370, batch acc 0.8582
12:57:44.437   Training iter 550, batch loss 1.7314, batch acc 0.8644
12:57:44.990   Training iter 600, batch loss 1.7340, batch acc 0.8604
12:57:44.992 Training @ 495 epoch...
12:57:45.555   Training iter 50, batch loss 1.7335, batch acc 0.8604
12:57:46.108   Training iter 100, batch loss 1.7312, batch acc 0.8674
12:57:46.672   Training iter 150, batch loss 1.7314, batch acc 0.8646
12:57:47.195   Training iter 200, batch loss 1.7328, batch acc 0.8618
12:57:47.736   Training iter 250, batch loss 1.7322, batch acc 0.8622
12:57:48.297   Training iter 300, batch loss 1.7321, batch acc 0.8658
12:57:48.838   Training iter 350, batch loss 1.7370, batch acc 0.8526
12:57:49.389   Training iter 400, batch loss 1.7306, batch acc 0.8588
12:57:49.944   Training iter 450, batch loss 1.7322, batch acc 0.8584
12:57:50.508   Training iter 500, batch loss 1.7332, batch acc 0.8620
12:57:51.033   Training iter 550, batch loss 1.7320, batch acc 0.8622
12:57:51.588   Training iter 600, batch loss 1.7344, batch acc 0.8614
12:57:51.590 Testing @ 495 epoch...
12:57:51.636     Testing, total mean loss 1.72536, total acc 0.87110
12:57:51.636 Training @ 496 epoch...
12:57:52.184   Training iter 50, batch loss 1.7278, batch acc 0.8730
12:57:52.726   Training iter 100, batch loss 1.7365, batch acc 0.8546
12:57:53.246   Training iter 150, batch loss 1.7348, batch acc 0.8564
12:57:53.764   Training iter 200, batch loss 1.7327, batch acc 0.8592
12:57:54.270   Training iter 250, batch loss 1.7305, batch acc 0.8708
12:57:54.785   Training iter 300, batch loss 1.7291, batch acc 0.8658
12:57:55.384   Training iter 350, batch loss 1.7310, batch acc 0.8622
12:57:55.887   Training iter 400, batch loss 1.7329, batch acc 0.8622
12:57:56.393   Training iter 450, batch loss 1.7309, batch acc 0.8620
12:57:56.889   Training iter 500, batch loss 1.7346, batch acc 0.8560
12:57:57.396   Training iter 550, batch loss 1.7362, batch acc 0.8560
12:57:57.910   Training iter 600, batch loss 1.7355, batch acc 0.8586
12:57:57.911 Training @ 497 epoch...
12:57:58.418   Training iter 50, batch loss 1.7355, batch acc 0.8572
12:57:58.948   Training iter 100, batch loss 1.7322, batch acc 0.8620
12:57:59.671   Training iter 150, batch loss 1.7330, batch acc 0.8618
12:58:00.422   Training iter 200, batch loss 1.7310, batch acc 0.8632
12:58:01.093   Training iter 250, batch loss 1.7310, batch acc 0.8656
12:58:01.787   Training iter 300, batch loss 1.7352, batch acc 0.8554
12:58:02.388   Training iter 350, batch loss 1.7351, batch acc 0.8610
12:58:02.919   Training iter 400, batch loss 1.7316, batch acc 0.8588
12:58:03.440   Training iter 450, batch loss 1.7289, batch acc 0.8730
12:58:03.956   Training iter 500, batch loss 1.7330, batch acc 0.8610
12:58:04.460   Training iter 550, batch loss 1.7343, batch acc 0.8584
12:58:04.968   Training iter 600, batch loss 1.7316, batch acc 0.8618
12:58:04.970 Training @ 498 epoch...
12:58:05.475   Training iter 50, batch loss 1.7289, batch acc 0.8686
12:58:05.975   Training iter 100, batch loss 1.7330, batch acc 0.8694
12:58:06.467   Training iter 150, batch loss 1.7317, batch acc 0.8622
12:58:06.927   Training iter 200, batch loss 1.7314, batch acc 0.8618
12:58:07.404   Training iter 250, batch loss 1.7354, batch acc 0.8564
12:58:07.882   Training iter 300, batch loss 1.7319, batch acc 0.8654
12:58:08.379   Training iter 350, batch loss 1.7339, batch acc 0.8578
12:58:08.865   Training iter 400, batch loss 1.7319, batch acc 0.8552
12:58:09.375   Training iter 450, batch loss 1.7328, batch acc 0.8612
12:58:09.878   Training iter 500, batch loss 1.7371, batch acc 0.8572
12:58:10.389   Training iter 550, batch loss 1.7348, batch acc 0.8572
12:58:10.889   Training iter 600, batch loss 1.7296, batch acc 0.8646
12:58:10.890 Training @ 499 epoch...
12:58:11.414   Training iter 50, batch loss 1.7319, batch acc 0.8652
12:58:11.913   Training iter 100, batch loss 1.7285, batch acc 0.8622
12:58:12.438   Training iter 150, batch loss 1.7392, batch acc 0.8564
12:58:12.964   Training iter 200, batch loss 1.7360, batch acc 0.8548
12:58:13.488   Training iter 250, batch loss 1.7278, batch acc 0.8672
12:58:14.021   Training iter 300, batch loss 1.7276, batch acc 0.8736
12:58:14.546   Training iter 350, batch loss 1.7338, batch acc 0.8648
12:58:15.069   Training iter 400, batch loss 1.7295, batch acc 0.8614
12:58:15.590   Training iter 450, batch loss 1.7298, batch acc 0.8650
12:58:16.127   Training iter 500, batch loss 1.7412, batch acc 0.8472
12:58:16.664   Training iter 550, batch loss 1.7347, batch acc 0.8622
12:58:17.180   Training iter 600, batch loss 1.7323, batch acc 0.8574
======================================================
12:58:17.182 Testing @ final epoch...
12:58:17.225     Testing, total mean loss 1.72534, total acc 0.87100
training time: 3119 seconds
